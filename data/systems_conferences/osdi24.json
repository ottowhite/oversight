[
  {
    "title": "Sabre: Hardware-Accelerated Snapshot Compression for Serverless MicroVMs",
    "authors": "Nikita Lazarev and Varun Gohil,MIT, CSAIL;James Tsai, Andy Anderson, and Bhushan Chitlur,Intel Labs;Zhiru Zhang,Cornell University;Christina Delimitrou,MIT, CSAIL",
    "abstract": "MicroVM snapshotting significantly reduces the cold start overheads in serverless applications. Snapshotting enables storing part of the physical memory of a microVM guest into a file, and later restoring from it to avoid long cold start-up times. Prefetching memory pages from snapshots can further improve the effectiveness of snapshotting. However, the efficacy of prefetching depends on the size of the memory that needs to be restored. Lossless page compression is therefore a great way to improve the coverage of the memory footprint that snapshotting with prefetching achieves. Unfortunately, the high overhead and high CPU cost of software-based (de)compression makes this impractical. We introduce Sabre, a novel approach to snapshot page prefetching based on hardware-accelerated (de)compression. Sabre leverages an increasingly pervasive near-memory analytics accelerator available in modern datacenter processors. We show that by appropriately leveraging such accelerators, microVM snapshots of serverless applications can be compressed up to a factor of 4.5×, with nearly negligible decompression costs. We use this insight to build an efficient page prefetching library capable of speeding up memory restoration from snapshots by up to 55%. We integrate the library with the production-grade Firecracker microVMs and evaluate its end-to-end performance on a wide set of serverless applications.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/lazarev",
    "session_title": "Memory Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "9feaf3f0-31b7-4aa0-9a31-5abcedafe7b8"
  },
  {
    "title": "Nomad: Non-Exclusive Memory Tiering via Transactional Page Migration",
    "authors": "Lingfeng Xiang, Zhen Lin, Weishu Deng, Hui Lu, and Jia Rao,The University of Texas at Arlington;Yifan Yuan and Ren Wang,Intel Labs",
    "abstract": "With the advent of byte-addressable memory devices, such as CXL memory, persistent memory, and storage-class memory, tiered memory systems have become a reality. Page migration is the de facto method within operating systems for managing tiered memory. It aims to bring hot data whenever possible into fast memory to optimize the performance of data accesses while using slow memory to accommodate data spilled from fast memory. While the existing research has demonstrated the effectiveness of various optimizations on page migration, it falls short of addressing a fundamental question: Is exclusive memory tiering, in which a page is either present in fast memory or slow memory, but not both simultaneously, the optimal strategy for tiered memory management?\nWe demonstrate that page migration-based exclusive memory tiering suffers significant performance degradation when fast memory is under pressure. In this paper, we propose non-exclusive memory tiering, a page management strategy that retains a copy of pages recently promoted from slow memory to fast memory to mitigate memory thrashing. To enable non-exclusive memory tiering, we develop NOMAD, a new page management mechanism for Linux that features transactional page migration and page shadowing. NOMAD helps remove page migration off the critical path of program execution and makes migration completely asynchronous. Evaluations with carefully crafted micro-benchmarks and real-world applications show that NOMAD is able to achieve up to 6x performance improvement over the state-of-the-art transparent page placement (TPP) approach in Linux when under memory pressure. We also compare NOMAD with a recently proposed hardware-assisted, access sampling-based page migration approach and demonstrate NOMAD's strengths and potential weaknesses in various scenarios.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/xiang",
    "session_title": "Memory Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "a42c2b95-2736-4feb-aa7f-90a2ed141f12"
  },
  {
    "title": "Managing Memory Tiers with CXL in Virtualized Environments",
    "authors": "Yuhong Zhong,Columbia University, Microsoft Azure;Daniel S. Berger,Microsoft Azure, University of Washington;Carl Waldspurger,Carl Waldspurger Consulting;Ryan Wee,Columbia University;Ishwar Agarwal, Rajat Agarwal, Frank Hady, and Karthik Kumar,Intel;Mark D. Hill,University of Wisconsin–Madison;Mosharaf Chowdhury,University of Michigan;Asaf Cidon,Columbia University",
    "abstract": "Cloud providers seek to deploy CXL-based memory to increase aggregate memory capacity, reduce costs, and lower carbon emissions. However, CXL accesses incur higher latency than local DRAM. Existing systems use software to manage data placement across memory tiers at page granularity. Cloud providers are reluctant to deploy software-based tiering due to high overheads in virtualized environments. Hardware-based memory tiering could place data at cacheline granularity, mitigating these drawbacks. However, hardware is oblivious to application-level performance.\nWe propose combining hardware-managed tiering with software-managed performance isolation to overcome the pitfalls of either approach. We introduce Intel® Flat Memory Mode, the first hardware-managed tiering system for CXL. Our evaluation on a full-system prototype demonstrates that it provides performance close to regular DRAM, with no more than 5% degradation for more than 82% of workloads. Despite such small slowdowns, we identify two challenges that can still degrade performance by up to 34% for \"outlier\" workloads: (1) memory contention across tenants, and (2) intra-tenant contention due to conflicting access patterns.\nTo address these challenges, we introduce Memstrata, a lightweight multi-tenant memory allocator. Memstrata employs page coloring to eliminate inter-VM contention. It improves performance for VMs with access patterns that are sensitive to hardware tiering by allocating them more local DRAM using an online slowdown estimator. In multi-VM experiments on prototype hardware, Memstrata is able to identify performance outliers and reduce their degradation from above 30% to below 6%, providing consistent performance across a wide range of workloads.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhong-yuhong",
    "session_title": "Memory Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "c63919b9-391a-4b50-b881-b28e0f8e9bdb"
  },
  {
    "title": "Harvesting Memory-bound CPU Stall Cycles in Software with MSH",
    "authors": "Zhihong Luo, Sam Son, and Sylvia Ratnasamy,UC Berkeley;Scott Shenker,UC Berkeley & ICSI",
    "abstract": "Memory-bound stalls account for a significant portion of CPU cycles in datacenter workloads, which makes harvesting them to execute other useful work highly valuable. However, mainstream implementations of the hardware harvesting mechanism, simultaneous multithreading (SMT), are unsatisfactory. They incur high latency overhead and do not offer fine-grained configurability of the trade-off between latency and harvesting throughput, which hinders wide adoption for latency-critical services; and they support only limited degrees of concurrency, which prevents full harvesting of memory stall cycles.\nWe present MSH, the first system that transparently and efficiently harvests memory-bound stall cycles in software. MSH makes full use of stall cycles with concurrency scaling, while incurring minimal and configurable latency overhead. MSH achieves these with a novel co-design of profiling, program analysis, binary instrumentation and runtime scheduling. Our evaluation shows that MSH achieves up to 72% harvesting throughput of SMT for latency SLOs under which SMT has to be disabled, and that strategically combining MSH with SMT leads to higher throughput than SMT due to MSH's capability to fully harvest memory-bound stall cycles.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/luo",
    "session_title": "Memory Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "58bd2049-254b-4702-8de4-ffb1d4a27c46"
  },
  {
    "title": "A Tale of Two Paths: Toward a Hybrid Data Plane for Efficient Far-Memory Applications",
    "authors": "Lei Chen,University of Chinese Academy of Sciences;Shi Liu,UCLA;Chenxi Wang,University of Chinese Academy of Sciences;Haoran Ma and Yifan Qiao,UCLA;Zhe Wang and Chenggang Wu,University of Chinese Academy of Sciences;Youyou Lu,Tsinghua University;Xiaobing Feng and Huimin Cui,University of Chinese Academy of Sciences;Shan Lu,Microsoft Research;Harry Xu,UCLA",
    "abstract": "With rapid advances in network hardware, far memory has gained a great deal of traction due to its ability to break the memory capacity wall. Existing far memory systems fall into one of two data paths: one that uses the kernel's paging system to transparently access far memory at the page granularity, and a second that bypasses the kernel, fetching data at the object granularity. While it is generally believed that object fetching outperforms paging due to its fine-grained access, it requires significantly more compute resources to run object-level LRU and eviction.\nWe built Atlas, a hybrid data plane enabled by a runtime-kernel co-design that simultaneously enables accesses via these two data paths to provide high efficiency for real-world applications. Atlas usesalways-onprofiling to continuously measure page locality. For workloads already with good locality, paging is used to fetch data, whereas for those without, object fetching is employed. Object fetching moves objects that are accessed close in time to contiguous local space, dynamically improving locality and making the execution increasingly amenable to paging, which is much more resource-efficient. Our evaluation shows that Atlas improves the throughput (e.g., by 1.5x and 3.2x) and reduces the tail latency (e.g., by one and two orders of magnitude) when using remote memory, compared with AIFM and Fastswap, the state-of-the-art techniques respectively in the two categories.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/chen-lei",
    "session_title": "Memory Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "7adb3f37-6897-46cc-8eee-fcf46fc975fc"
  },
  {
    "title": "DRust: Language-Guided Distributed Shared Memory with Fine Granularity, Full Transparency, and Ultra Efficiency",
    "authors": "Haoran Ma, Yifan Qiao, Shi Liu, and Shan Yu,UCLA;Yuanjiang Ni, Qingda Lu, and Jiesheng Wu,Alibaba Group;Yiying Zhang,UCSD;Miryung Kim and Harry Xu,UCLA",
    "abstract": "Despite being a powerful concept, distributed shared memory (DSM) has not been made practical due to the extensive synchronization needed between servers to implement memory coherence. This paper shows a practical DSM implementation based on the insight that the ownership model embedded in programming languages such as Rust automatically constrains the order of read and write, providing opportunities for significantly simplifying the coherence implementation if the ownership semantics can be exposed to and leveraged by the runtime. This paper discusses the design and implementation of DRust, a Rust-based DSM system that outperforms the two state-of-the-art DSM systems GAM and Grappa by up to 2.64× and 29.16× in throughput, and scales much better with the number of servers.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/ma-haoran",
    "session_title": "Memory Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "967635b4-20c9-4c8e-aac7-0bf46fc1fe55"
  },
  {
    "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve",
    "authors": "Amey Agrawal,Georgia Institute of Technology;Nitin Kedia, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, and Bhargav Gulavani,Microsoft Research India;Alexey Tumanov,Georgia Institute of Technology;Ramachandran Ramjee,Microsoft Research India",
    "abstract": "Each LLM serving request goes through two phases. The first is prefill which processes the entire input prompt and produces the first output token and the second is decode which generates the rest of output tokens, one-at-a-time. Prefill iterations have high latency but saturate GPU compute due to parallel processing of the input prompt. In contrast, decode iterations have low latency but also low compute utilization because a decode iteration processes only a single token per request. This makes batching highly effective for decodes and consequently for overall throughput. However, batching multiple requests leads to an interleaving of prefill and decode iterations which makes it challenging to achieve both high throughput and low latency.\nWe introduce an efficient LLM inference scheduler, Sarathi-Serve, to address this throughput-latency tradeoff. Sarathi-Serve introduces chunked-prefills which splits a prefill request into near equal sized chunks and creates stall-free schedules that adds new requests in a batch without pausing ongoing decodes. Stall-free scheduling unlocks the opportunity to improve throughput with large batch sizes while minimizing the effect of batching on latency. Furthermore, uniform batches in Sarathi-Serve ameliorate the imbalance between iterations resulting in minimal pipeline bubbles.\nOur techniques yield significant improvements in inference performance across models and hardware under tail latency constraints. For Mistral-7B on single A100 GPUs, we achieve 2.6x higher serving capacity and up to 3.7x higher serving capacity for the Yi-34B model on two A100 GPUs as compared to vLLM. When used with pipeline parallelism on Falcon-180B, Sarathi-Serve provides up to 5.6× gain in the end-to-end serving capacity. The source code for Sarathi-Serve is available athttps://github.com/microsoft/sarathi-serve.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/agrawal",
    "session_title": "Low-Latency LLM Serving",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "a078ced4-9f88-4ec6-b586-8378443f7d17"
  },
  {
    "title": "ServerlessLLM: Low-Latency Serverless Inference for Large Language Models",
    "authors": "Yao Fu, Leyang Xue, Yeqi Huang, and Andrei-Octavian Brabete,University of Edinburgh;Dmitrii Ustiugov,NTU Singapore;Yuvraj Patel and Luo Mai,University of Edinburgh",
    "abstract": "This paper presents ServerlessLLM, a distributed system designed to support low-latency serverless inference for Large Language Models (LLMs). By harnessing the substantial near-GPU storage and memory capacities of inference servers, ServerlessLLM achieves effective local checkpoint storage, minimizing the need for remote checkpoint downloads and ensuring efficient checkpoint loading. The design of ServerlessLLM features three core contributions: (i) fast multi-tier checkpoint loading, featuring a new loading-optimized checkpoint format and a multi-tier loading system, fully utilizing the bandwidth of complex storage hierarchies on GPU servers; (ii) efficient live migration of LLM inference, which enables newly initiated inferences to capitalize on local checkpoint storage while ensuring minimal user interruption; and (iii) startup-time-optimized model scheduling, which assesses the locality statuses of checkpoints on each server and schedules the model onto servers that minimize the time to start the inference. Comprehensive evaluations, including microbenchmarks and real-world scenarios, demonstrate that ServerlessLLM dramatically outperforms state-of-the-art serverless systems, reducing latency by 10 - 200X across various LLM inference workloads.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/fu",
    "session_title": "Low-Latency LLM Serving",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "56e83d21-b2bb-459e-891a-1c8613aef42e"
  },
  {
    "title": "InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management",
    "authors": "Wonbeom Lee, Jungi Lee, Junghwan Seo, and Jaewoong Sim,Seoul National University",
    "abstract": "Transformer-based large language models (LLMs) demonstrate impressive performance across various natural language processing tasks. Serving LLM inference for generating long contents, however, poses a challenge due to the enormous memory footprint of the transient state, known as the key-value (KV) cache, which scales with the sequence length and batch size. In this paper, we present InfiniGen, a novel KV cache management framework tailored for long-text generation, which synergistically works with modern offloading-based inference systems. InfiniGen leverages the key insight that a few important tokens that are essential for computing the subsequent attention layer in the Transformer can be speculated by performing a minimal rehearsal with the inputs of the current layer and part of the query weight and key cache of the subsequent layer. This allows us to prefetch only the essential KV cache entries (without fetching them all), thereby mitigating the fetch overhead from the host memory in offloading-based LLM serving systems. Our evaluation on several representative LLMs shows that InfiniGen improves the overall performance of a modern offloading-based system by up to 3.00× compared to prior KV cache management methods while offering substantially better model accuracy.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/lee",
    "session_title": "Low-Latency LLM Serving",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "1ac1a2e4-2804-44a5-bde5-9d77d05f704d"
  },
  {
    "title": "Llumnix: Dynamic Scheduling for Large Language Model Serving",
    "authors": "Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, and Wei Lin,Alibaba Group",
    "abstract": "Inference serving for large language models (LLMs) is the key to unleashing their potential in people's daily lives. However, efficient LLM serving remains challenging today because the requests are inherently heterogeneous and unpredictable in terms of resource and latency requirements, as a result of the diverse applications and the dynamic execution nature of LLMs. Existing systems are fundamentally limited in handling these characteristics and cause problems such as severe queuing delays, poor tail latencies, and SLO violations.\nWe introduce Llumnix, an LLM serving system that reacts to such heterogeneous and unpredictable requests by runtime rescheduling across multiple model instances.\nSimilar to context switching across CPU cores in modern operating systems, Llumnix reschedules requests to improve load balancing and isolation, mitigate resource fragmentation, and differentiate request priorities and SLOs. Llumnix implements the rescheduling with an efficient and scalable live migration mechanism for requests and their in-memory states, and exploits it in a dynamic scheduling policy that unifies the multiple rescheduling scenarios elegantly.\nOur evaluations show that Llumnix improves tail latencies by an order of magnitude, accelerates high-priority requests by up to 1.5×, and delivers up to 36% cost savings while achieving similar tail latencies, compared against state-of-the-art LLM serving systems. Llumnix is publicly available athttps://github.com/AlibabaPAI/llumnix.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/sun-biao",
    "session_title": "Low-Latency LLM Serving",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "09e3433d-94d6-4421-8f5f-e8750ca51fc7"
  },
  {
    "title": "DistServe: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving",
    "authors": "Yinmin Zhong and Shengyu Liu,Peking University;Junda Chen,UC San Diego;Jianbo Hu,Peking University;Yibo Zhu,StepFun;Xuanzhe Liu and Xin Jin,Peking University;Hao Zhang,UC San Diego",
    "abstract": "DistServe improves the performance of large language models (LLMs) serving by disaggregating the prefill and decoding computation. Existing LLM serving systems colocate the two phases and batch the computation of prefill and decoding across all users and requests. We find that this strategy not only leads to strong prefill-decoding interferences but also couples the resource allocation and parallelism plans for both phases. LLM applications often emphasize individual latency for each phase: time to first token (TTFT) for the prefill phase and time per output token (TPOT) of each request for the decoding phase. In the presence of stringent latency requirements, existing systems have to prioritize one latency over the other, or over-provision compute resources to meet both.\nDistServe assigns prefill and decoding computation to different GPUs, hence eliminating prefill-decoding interferences. Given the application's TTFT and TPOT requirements, DistServe co-optimizes the resource allocation and parallelism strategy tailored for each phase. DistServe also places the two phases according to the serving cluster's bandwidth to minimize the communication caused by disaggregation. As a result, DistServe significantly improves LLM serving performance in terms of the maximum rate that can be served within both TTFT and TPOT constraints on each GPU. Our evaluations show that on various popular LLMs, applications, and latency requirements, DistServe can serve 7.4× more requests or 12.6× tighter SLO, compared to state-of-the-art systems, while staying within latency constraints for > 90% of requests.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin",
    "session_title": "Low-Latency LLM Serving",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "7693369e-f8bb-45c5-9e0d-4906fc905118"
  },
  {
    "title": "ACCL+: an FPGA-Based Collective Engine for Distributed Applications",
    "authors": "Zhenhao He, Dario Korolija, Yu Zhu, and Benjamin Ramhorst,Systems Group, ETH Zurich;Tristan Laan,University of Amsterdam;Lucian Petrica and Michaela Blott,AMD Research;Gustavo Alonso,Systems Group, ETH Zurich",
    "abstract": "FPGAs are increasingly prevalent in cloud deployments, serving as Smart-NICs or network-attached accelerators. To facilitate the development of distributed applications with FPGAs, in this paper we propose ACCL+, an open-source, FPGA-based collective communication library. Portable across different platforms and supporting UDP, TCP, as well as RDMA, ACCL+ empowers FPGA applications to initiate direct FPGA-to-FPGA collective communication. Additionally, it can serve as a collective offload engine for CPU applications, freeing the CPU from networking tasks. It is user-extensible, allowing new collectives to be implemented and deployed without having to re-synthesize the entire design. \nWe evaluated ACCL+ on an FPGA cluster with 100 Gb/s networking, comparing its performance against software MPI over RDMA. The results demonstrate ACCL+'s significant advantages for FPGA-based distributed applications and its competitive performance for CPU applications. We showcase ACCL+'s dual role with two use cases: as a collective offload engine to distribute CPU-based vector-matrix multiplication, and as a component in designing fully FPGA-based distributed deep-learning recommendation inference.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/he",
    "session_title": "Distributed Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "1468c2d8-cbe3-4c21-879a-87564a3614c1"
  },
  {
    "title": "Beaver: Practical Partial Snapshots for Distributed Cloud Services",
    "authors": "Liangcheng Yu,University of Pennsylvania;Xiao Zhang,Shanghai Jiao Tong University;Haoran Zhang,University of Pennsylvania;John Sonchack,Princeton University;Dan Ports,Microsoft / University of Washington;Vincent Liu,University of Pennsylvania",
    "abstract": "Distributed snapshots are a classic class of protocols used for capturing a causally consistent view of states across machines. Although effective, existing protocols presume an isolated universe of processes to snapshot and require instrumentation and coordination of all. This assumption does not match today's cloud services—it is not always practical to instrument all involved processes nor realistic to assume zero interaction of the machines of interest with the external world.\nTo bridge this gap, this paper presents Beaver, the first practical partial snapshot protocol that ensures causal consistency under external traffic interference. Beaver presents a unique design point that tightly couples its protocol with the regularities of the underlying data center environment. By exploiting the placement of software load balancers in public clouds and their associated communication pattern, Beaver not only requires minimal changes to today's data center operations but also eliminates any form of blocking to existing communication, thus incurring near-zero overhead to user traffic. We demonstrate the Beaver's effectiveness through extensive testbed experiments and novel use cases.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/yu",
    "session_title": "Distributed Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "0715d17f-132f-4d81-88c6-249466d4fd9f"
  },
  {
    "title": "Fast and Scalable In-network Lock Management Using Lock Fission",
    "authors": "Hanze Zhang,Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Shanghai AI Laboratory; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University;Ke Cheng,Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China;Rong Chen,Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Shanghai AI Laboratory; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China;Haibo Chen,Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China; Key Laboratory of System Software (Chinese Academy of Sciences)",
    "abstract": "Distributed lock services are extensively utilized in distributed systems to serialize concurrent accesses to shared resources. The need for fast and scalable lock services has become more pronounced with decreasing task execution times and expanding dataset scales. However, traditional lock managers, reliant on server CPUs to handle lock requests, experience significant queuing delays in lock grant latency. Advanced network hardware (e.g. programmable switches) presents an avenue to manage locks without queuing delays due to their high packet processing power. Nevertheless, their constrained memory capacity restricts the manageable lock scale, thereby limiting their effect in large-scale workloads.\nThis paper presents FISSLOCK, a fast and scalable distributed lock service that exploits the programmable switch to improve (tail) latency and peak throughput for millions of locks. The key idea behind FISSLOCK is the concept of lock fission, which decouples lock management into grant decision and participant maintenance. FISSLOCK leverages the programmable switch to decide lock grants synchronously and relies on servers to maintain participants (i.e., holders and waiters) asynchronously. By using the programmable switch for routing, FISSLOCK enables on-demand fine-grained lock migration, thereby reducing the lock grant and release delays. FISSLOCK carefully designs and implements grant decision procedure on the programmable switch, supporting over one million locks. Evaluation using various benchmarks and a real-world application shows the efficiency of FISSLOCK. Compared to the state-of-the-art switch-based approach (NetLock), FISSLOCK cuts up to 79.1% (from 43.0%) of median lock grant time in the microbenchmark and improves transaction throughput for TATP and TPC-C by 1.76× and 2.28×, respectively.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhang-hanze",
    "session_title": "Distributed Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "059cf18d-a6f9-4031-ae17-42f06acda932"
  },
  {
    "title": "Chop Chop: Byzantine Atomic Broadcast to the Network Limit",
    "authors": "Martina Camaioni, Rachid Guerraoui, Matteo Monti, Pierre-Louis Roman, Manuel Vidigueira, and Gauthier Voron,EPFL",
    "abstract": "At the heart of state machine replication, the celebrated technique enabling decentralized and secure universal computation, lies Atomic Broadcast, a fundamental communication primitive that orders, authenticates, and deduplicates messages. This paper presents Chop Chop, a Byzantine Atomic Broadcast system that uses a novel authenticated memory pool to amortize the cost of ordering, authenticating and deduplicating messages, achieving \"line rate\" (i.e., closely matching the complexity of a protocol that does not ensure any ordering, authentication or Byzantine resilience) even when processing messages as small as 8 bytes. Chop Chop attains this performance by means of a new form of batching we call distillation. A distilled batch is a set of messages that are fast to authenticate, deduplicate, and order. Batches are distilled using a novel interactive protocol involving brokers, an untrusted layer of facilitating processes between clients and servers. In a geo-distributed deployment of 64 medium-sized servers, Chop Chop processes 43,600,000 messages per second with an average latency of 3.6 seconds. Under the same conditions, state-of-the-art alternatives offer two orders of magnitude less throughput for the same latency. We showcase three simple Chop Chop applications: a Payment system, an Auction house and a \"Pixel war\" game, respectively achieving 32, 2.3 and 35 million operations per second.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/camaioni",
    "session_title": "Distributed Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "dfa287d5-6cc4-46bd-8584-7e635a456545"
  },
  {
    "title": "Enabling Tensor Language Model to Assist in Generating High-Performance Tensor Programs for Deep Learning",
    "authors": "Yi Zhai,University of Science and Technology of China;Sijia Yang,Huawei Technologies Co., Ltd.;Keyu Pan,ByteDance Ltd.;Renwei Zhang,Huawei Technologies Co., Ltd.;Shuo Liu,University of Science and Technology of China;Chao Liu and Zichun Ye,Huawei Technologies Co., Ltd.;Jianmin Ji,University of Science and Technology of China;Jie Zhao,Hunan University;Yu Zhang and Yanyong Zhang,University of Science and Technology of China",
    "abstract": "Obtaining high-performance tensor programs with high efficiency continues to be a substantial challenge. Approaches that favor efficiency typically limit their exploration space through heuristic constraints, which often lack generalizability. Conversely, approaches targeting high performance tend to create an expansive exploration space but employ ineffective exploration strategies.\nWe propose a tensor program generation framework for deep learning applications. Its core idea involves maintaining an expansive space to ensure high performance while performing powerful exploration with the help of language models to generate tensor programs efficiently. We thus transform the tensor program exploration task into a language model generation task. To facilitate this, we explicitly design the language model-friendly tensor language that records decision information to represent tensor programs. During the compilation of target workloads, the tensor language model (TLM) combines knowledge from offline learning and previously made decisions to probabilistically sample the best decision in the current decision space. This approach allows more informed space exploration than random sampling commonly used in previously proposed approaches.\nExperimental results indicate that TLM excels in delivering both efficiency and performance. Compared to fully tuned Ansor/MetaSchedule, TLM matches their performance with a compilation speedup of 61×. Furthermore, when evaluated against Roller, with the same compilation time, TLM improves the performance by 2.25×. Code available athttps://github.com/zhaiyi000/tlm.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhai",
    "session_title": "Deep Learning",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "fa919f27-8dc8-4a86-814c-257c291bacc4"
  },
  {
    "title": "Ladder: Enabling Efficient Low-Precision Deep Learning Computing through Hardware-aware Tensor Transformation",
    "authors": "Lei Wang,University of Chinese Academy of Sciences & Microsoft Research;Lingxiao Ma, Shijie Cao, Quanlu Zhang, and Jilong Xue,Microsoft Research;Yining Shi,Peking University & Microsoft Research;Ningxin Zheng, Ziming Miao, Fan Yang, Ting Cao, Yuqing Yang, and Mao Yang,Microsoft Research",
    "abstract": "The increasing demand for improving deep learning model performance has led to a paradigm shift in supporting low-precision computation to harness the robustness of deep learning to errors. Despite the emergence of new low-precision data types and optimization approaches, existing hardware and software have insufficient and inefficient support for those evolving data types, making it challenging to achieve real performance gains through low-precision computing.\nThis paper introduces Ladder, a novel compiler designed to bridge the gap between evolving custom data types and the fixed precision formats supported by current hardware. Leveraging a general type system, tType, and an extended tensor expression, Ladder transforms deep neural network (DNN) computations into optimized computing pipelines with custom data types as the first-class citizen, exposing an optimization space for efficiently handling data storage, accesses, and type conversions. Ladder employs a new set of tensor scheduling primitives and a hardware-aware optimization policy to navigate the complex transformation space, ensuring optimal performance across different memory layers and DNN operators. Our evaluation demonstrates Ladder's capability to systematically support a wide array of low-bit precision custom data types, significantly enhancing the performance of DNN computations on modern accelerators without necessitating hardware modifications. This innovation empowers model designers with the ability to explore data type optimizations and offers hardware vendors a flexible solution to expand their support for diverse precision formats.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/wang-lei",
    "session_title": "Deep Learning",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "9dc89fcc-54ea-4472-8680-5eb9301bdf49"
  },
  {
    "title": "Caravan: Practical Online Learning of In-Network ML Models with Labeling Agents",
    "authors": "Qizheng Zhang,Stanford University;Ali Imran,Purdue University;Enkeleda Bardhi,Sapienza University of Rome;Tushar Swamy and Nathan Zhang,Stanford University;Muhammad Shahbaz,Purdue University and University of Michigan;Kunle Olukotun,Stanford University",
    "abstract": "Recent work on in-network machine learning (ML) anticipates offline models to operate well in modern networking environments. However, upon deployment, these models struggle to cope with fluctuating traffic patterns and network conditions and, therefore, must be validated and updated frequently in an online fashion.\nThis paper presents CARAVAN, a practical online learning system for in-network ML models. We tackle two primary challenges in facilitating online learning for networking: (a) the automatic labeling of evolving traffic and (b) the efficient monitoring and detection of model performance degradation to trigger retraining. CARAVAN repurposes existing systems (e.g., heuristics, access control lists, and foundation models)— not directly suitable for such dynamic environments—into high-quality labeling sources for generating labeled data for online learning. CARAVAN also introduces a new metric, accuracy proxy, to track model degradation and potential drift to efficiently trigger retraining. Our evaluations show that CARAVAN's labeling strategy enables in-network ML models to closely follow the changes in the traffic dynamics with a 30.3% improvement in F1 score (on average), compared to offline models. Moreover, CARAVAN sustains comparable inference accuracy to that of a continuous-learning system while consuming 61.3% less GPU compute time (on average) via accuracy proxy and retraining triggers.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhang-qizheng",
    "session_title": "Deep Learning",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "91d74184-dca4-47df-b4fc-d118d8b3f8e0"
  },
  {
    "title": "nnScaler: Constraint-Guided Parallelization Plan Generation for Deep Learning Training",
    "authors": "Zhiqi Lin,University of Science and Technology of China;Youshan Miao, Quanlu Zhang, Fan Yang, and Yi Zhu,Microsoft Research;Cheng Li,University of Science and Technology of China;Saeed Maleki,xAI;Xu Cao, Ning Shang, Yilei Yang, Weijiang Xu, and Mao Yang,Microsoft Research;Lintao Zhang,BaseBit Technologies;Lidong Zhou,Microsoft Research",
    "abstract": "With the growing model size of deep neural networks (DNN), deep learning training is increasingly relying on handcrafted search spaces to find efficient parallelization execution plans. However, our study shows that existing search spaces exclude plans that significantly impact the training performance of well-known DNN models (e.g., AlphaFold2) under important settings, such as when handling large embedding tables in large language models.\nTo address this problem, we propose nnScaler, a framework that generates efficient parallelization plans for deep learning training. Instead of relying on the existing search space, nnScaler advocates a more general approach that empowers domain experts to construct their own search space through three primitives, op-trans, op-assign, and op-order, which capture model transformation and the temporal-spatial scheduling of the transformed model of any parallelization plans. To avoid space explosion, nnScaler allows the application of constraints to those primitives during space construction. With the proposed primitives and constraints, nnScaler can compose existing search spaces as well as new ones. Experiments show that nnScaler can find new parallelization plans in new search spaces that achieve up to 3.5× speedup compared to solutions such as DeepSpeed, Megatron-LM, and Alpa for popular DNN models like SwinTransformer and AlphaFold2.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/lin-zhiqi",
    "session_title": "Deep Learning",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "e56037ff-9f07-4d9f-a9a0-5fc425b53497"
  },
  {
    "title": "ChameleonAPI: Automatic and Efficient Customization of Neural Networks for ML Applications",
    "authors": "Yuhan Liu,University of Chicago;Chengcheng Wan,East China Normal University;Kuntai Du, Henry Hoffmann, and Junchen Jiang,University of Chicago;Shan Lu,University of Chicago and Microsoft Research;Michael Maire,University of Chicago",
    "abstract": "ML APIs have greatly relieved application developers of the burden to design and train their own neural network models—classifying objects in an image can now be as simple as one line of Python code to call an API. However, these APIs offer the same pre-trained models regardless of how their output is used by different applications. This can be suboptimal as not all ML inference errors can cause application failures, and the distinction between inference errors that can or cannot cause failures varies greatly across applications.\nTo tackle this problem, we first study 77 real-world applications, which collectively use six ML APIs from two providers, to reveal common patterns of how ML API output affects applications' decision processes. Inspired by the findings, we propose ChameleonAPI, an optimization framework for ML APIs, which takes effect without changing the application source code. ChameleonAPI provides application developers with a parser that automatically analyzes the application to produce an abstract of its decision process, which is then used to devise an application-specific loss function that only penalizes API output errors critical to the application. ChameleonAPI uses the loss function to efficiently train a neural network model customized for each application and deploys it to serve API invocations from the respective application via existing interface. Compared to a baseline that selects the best-of-all commercial ML API, we show that ChameleonAPI reduces incorrect application decisions by 43%.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/liu",
    "session_title": "Deep Learning",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "ee7a3f4f-3397-4d21-9423-8d61356c3538"
  },
  {
    "title": "SquirrelFS: using the Rust compiler to check file-system crash consistency",
    "authors": "Hayley LeBlanc, Nathan Taylor, James Bornholt, and Vijay Chidambaram,University of Texas at Austin",
    "abstract": "This work introduces a new approach to building crash-safe file systems for persistent memory. We exploit the fact that Rust's typestate pattern allows compile-time enforcement of a specific order of operations. We introduce a novel crash-consistency mechanism, Synchronous Soft Updates, that boils down crash safety to enforcing ordering among updates to file-system metadata. We employ this approach to build SquirrelFS, a new file system with crash-consistency guarantees that are checked at compile time. SquirrelFS avoids the need for separate proofs, instead incorporating correctness guarantees into the typestate itself. Compiling SquirrelFS only takes tens of seconds; successful compilation indicates crash consistency, while an error provides a starting point for fixing the bug. We evaluate SquirrelFS against state of the art file systems such as NOVA and WineFS, and find that SquirrelFS achieves similar or better performance on a wide range of benchmarks and applications.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/leblanc",
    "session_title": "Operating Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "2c41cbd6-c0ea-4597-9418-9733bfc9d213"
  },
  {
    "title": "High-throughput and Flexible Host Networking for Accelerated Computing",
    "authors": "Athinagoras Skiadopoulos, Zhiqiang Xie, and Mark Zhao,Stanford University;Qizhe Cai and Saksham Agarwal,Cornell University;Jacob Adelmann, David Ahern, Carlo Contavalli, Michael Goldflam, Vitaly Mayatskikh, Raghu Raja, and Daniel Walton,Enfabrica;Rachit Agarwal,Cornell University;Shrijeet Mukherjee,Enfabrica;Christos Kozyrakis,Stanford University",
    "abstract": "Modern network hardware is able to meet the stringent bandwidth demands of applications like GPU-accelerated AI. However, existing host network stacks offer a hard tradeoff between performance (in terms of sustained throughput when compared to network hardware capacity) and flexibility (in terms of the ability to select, customize, and extend different network protocols).\nThis paper explores a clean-slate approach to simultaneously offer high performance and flexibility. We present a co-design of the NIC hardware and the software stack to achieve this. The key idea in our design is the physical separation of the data path (payload transfer between network and application buffers) and the control path (header processing and transport-layer decisions). The NIC enables a high-performance zero-copy data path, independent of the placement of the application (CPU, GPU, FPGA, or other accelerators). The software stack provides a flexible control path by enabling the integration of any network protocol, executing in any environment (in the kernel, in user space, or in an accelerator).\nWe implement and evaluate ZeroNIC, a prototype that combines an FPGA-based NIC with a software stack that integrates the Linux TCP protocol. We demonstrate that ZeroNIC achieves RDMA-like throughput while maintaining the benefits of robust protocols like TCP under various network perturbations. For instance, ZeroNIC enables a single TCP flow to saturate a 100Gbps link while utilizing only 17% of a single CPU core. ZeroNIC improves NCCL and Redis throughput by 2.66X and 3.71X, respectively, over Linux TCP on a Mellanox ConnectX-6 NIC, without requiring application modifications.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/skiadopoulos",
    "session_title": "Operating Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "2c688818-c1e6-4f72-8b0e-bf85f49cb1dc"
  },
  {
    "title": "IntOS: Persistent Embedded Operating System and Language Support for Multi-threaded Intermittent Computing",
    "authors": "Yilun Wu,Stony Brook University;Byounguk Min,Purdue University;Mohannad Ismail and Wenjie Xiong,Virginia Tech;Changhee Jung,Purdue University;Dongyoon Lee,Stony Brook University",
    "abstract": "This paper introduces INTOS, an embedded operating system and language support for multi-threaded intermittent computing on a battery-less energy-harvesting platform. INTOS simplifies programming with a traditional \"thread\" and a \"transaction\" with automatic undo-logging of persistent objects in non-volatile memory. While INTOS allows the use of volatile memory for performance and energy efficiency, conventional transactions do not ensure crash consistency of volatile register and memory states. To address this challenge, INTOS proposes a novel replay-and-bypass approach, eliminating the need for users to checkpoint volatile states. Upon power restoration, INTOS recovers non-volatile states by undoing the updates of power-interrupted transactions. To reconstruct volatile states, INTOS restarts each thread bypassing committed transactions and system calls by returning recorded results without re-execution. INTOS seeks to build a persistent, full-fledged embedded OS, supporting priority-based preemptive multithreading while ensuring crash consistency even if power failure occurs during a system call or while some threads are blocked. Experiments on a commodity platform MSP430FR5994 show that when subjected to an extreme power failure frequency of 1 ms, INTOS demonstrated 1.24x lower latency and 1.29x less energy consumption than prior work leveraging idempotent processing. This trend turns out to be more pronounced on Apollo 4 Blue Plus.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/wu-yilun",
    "session_title": "Operating Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "3bfd1844-0cff-4ec6-8414-702e88aea1b0"
  },
  {
    "title": "Data-flow Availability: Achieving Timing Assurance in Autonomous Systems",
    "authors": "Ao Li and Ning Zhang,Washington University in St. Louis",
    "abstract": "Due to the continuous interaction with the physical world, autonomous cyber-physical systems (CPS) require both functional and temporal correctness. Despite recent advances in the theoretical foundation of real-time computing, leveraging these results efficiently in modern CPS platforms often requires domain expertise, and presents non-trivial challenges to many developers.\nTo understand the practical challenges in building real-time software, we conducted a survey of 189 software issues from 7 representative CPS open-source projects. Through this exercise, we found that most bugs are due to misalignment in time between cyber and physical states. This inspires us to abstract three key temporal properties: freshness, consistency, and stability. Using a newly developed concept, Data-flow Availability (DFA), which aims to capture temporal/availability expectation of data flow, we show how these essential properties can be represented as timing constraints on data flows. To realize the timing assurance from DFA, we designed and implemented Kairos, which automatically detects and mitigates timing constraint violations. To detect violations, Kairos translates the policy definition from the API-based annotations into run-time program instrumentation. To mitigate the violations, it provides an infrastructure to bridge semantic gaps between schedulers at different abstraction layers to allow for coordinated efforts. End-to-end evaluation on three real-world CPS platforms shows that Kairos improves timing predictability and safety while introducing a minimal 2.8% run-time overhead.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/li",
    "session_title": "Operating Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "6e2c2447-7fd1-436a-bcaf-ee3a311d4a2c"
  },
  {
    "title": "Microkernel Goes General: Performance and Compatibility in the HongMeng Production Microkernel",
    "authors": "Haibo Chen,Huawei Central Software Institute and Shanghai Jiao Tong University;Xie Miao, Ning Jia, Nan Wang, Yu Li, Nian Liu, Yutao Liu, Fei Wang, Qiang Huang, Kun Li, Hongyang Yang, Hui Wang, Jie Yin, Yu Peng, and Fengwei Xu,Huawei Central Software Institute",
    "abstract": "The virtues of security, reliability, and extensibility have made state-of-the-art microkernels prevalent in embedded and safety-critical scenarios. However, they face performance and compatibility issues when targeting more general scenarios, such as smartphones and smart vehicles.\nThis paper presents the design and implementation of HongMeng kernel (HM), a commercialized general-purpose microkernel that preserves most of the virtues of microkernels while addressing the above challenges. For the sake of commercial practicality, we design HM to be compatible with the Linux API and ABI to reuse its rich applications and driver ecosystems. To make it performant despite the constraints of compatibility and being general-purpose, we re-examine the traditional microkernel wisdom, including IPC, capability-based access control, and userspace paging, and retrofit them accordingly. Specifically, we argue that per-invocation IPC is not the only concern for performance, but IPC frequency, state double bookkeeping among OS services, and capabilities that hide kernel objects contribute to significant performance degradation. We mitigate them accordingly with a set of techniques, including differentiated isolation classes, flexible composition, policy-free kernel paging, and address-token-based access control.\nHM consists of a minimal core kernel and a set of least-privileged OS services, and it can run complex frameworks like AOSP and OpenHarmony. HM has been deployed in production on tens of millions of devices in emerging scenarios, including smart routers, smart vehicles and smartphones, typically with improved performance and security over their Linux counterparts.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/chen-haibo",
    "session_title": "Operating Systems",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "acc137f9-8f65-41ef-a653-510c85cd9e1a"
  },
  {
    "title": "When will my ML Job finish? Toward providing Completion Time Estimates through Predictability-Centric Scheduling",
    "authors": "Abdullah Bin Faisal, Noah Martin, Hafiz Mohsin Bashir, Swaminathan Lamelas, and Fahad R. Dogar,Tufts University",
    "abstract": "In this paper, we make a case for providing job completion time estimates to GPU cluster users, similar to providing the delivery date of a package or arrival time of a booked ride. Our analysis reveals that providing predictability can come at the expense of performance and fairness. Existing GPU schedulers optimize for extreme points in the trade-off space, making them either extremely unpredictable or impractical.\nTo address this challenge, we present PCS, a new scheduling framework that aims to provide predictability while balancing other traditional objectives. The key idea behind PCS is to use Weighted-Fair-Queueing (WFQ) and find a suitable configuration of different WFQ parameters (e.g., queue weights) that meets specific goals for predictability. It uses a simulation-aided search strategy to efficiently discover WFQ configurations that lie around the Pareto front of the trade-off space between these objectives. We implement and evaluate PCS in the context of scheduling ML training workloads on GPUs. Our evaluation, on a small-scale GPU testbed and larger-scale simulations, shows that PCS can provide accurate completion time estimates while marginally compromising on performance and fairness.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/bin-faisal",
    "session_title": "Cloud Computing",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "a2d800b1-9118-4bfc-a625-227cb1ed40e1"
  },
  {
    "title": "Optimizing Resource Allocation in Hyperscale Datacenters: Scalability, Usability, and Experiences",
    "authors": "Neeraj Kumar, Pol Mauri Ruiz, Vijay Menon, Igor Kabiljo, Mayank Pundir, Andrew Newell, Daniel Lee, Liyuan Wang, and Chunqiang Tang,Meta Platforms",
    "abstract": "Meta's private cloud uses millions of servers to host tens of thousands of services that power multiple products for billions of users. This complex environment has various optimization problems involving resource allocation, including hardware placement, server allocation, ML training & inference placement, traffic routing, database & container migration for load balancing, grouping serverless functions for locality, etc.\nThe main challenges for a reusable resource-allocation framework are its usability and scalability. Usability is impeded by practitioners struggling to translate real-life policies into precise mathematical formulas required by formal optimization methods, while scalability is hampered by NP-hard problems that cannot be solved efficiently by commercial solvers.\nThese challenges are addressed by Rebalancer, Meta's resource-allocation framework.\nIt has been applied to dozens of large-scale use cases over the past seven years, demonstrating its usability, scalability, and generality. At the core of Rebalancer is an expression graph that enables its optimization algorithm to run more efficiently than past algorithms. Moreover, Rebalancer offers a high-level specification language to lower the barrier for adoption by systems practitioners.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/kumar",
    "session_title": "Cloud Computing",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "e488ea09-3201-40d5-83c9-1809ab12fb11"
  },
  {
    "title": "μSlope: High Compression and Fast Search on Semi-Structured Logs",
    "authors": "Rui Wang,YScope;Devin Gibson,YScope and University of Toronto;Kirk Rodrigues,YScope;Yu Luo,YScope, Uber, and University of Toronto;Yun Zhang, Kaibo Wang, Yupeng Fu, and Ting Chen,Uber;Ding Yuan,YScope and University of Toronto",
    "abstract": "Internet-scale services can produce a large amount of logs. Such logs are increasingly appearing in semi-structured formats such as JSON. At Uber, the amount of semi-structured log data can exceed 10PB/day. It is prohibitively expensive to store and analyze them. As a result, logs are only kept searchable for a few days.\nThis paper proposes μSlope, a system that losslessly compresses semi-structured log data, and allows search without full decompression. It concisely represents the schema structures, and only keeps this representation stored once per dataset instead of interspersing it with each record. It further \"structurizes\" the semi-structured data by grouping the records with the same schema structure into the same table, so that each table is well structured. Our evaluation shows that μSlope achieves 21.9:1 to 186.8:1 compression ratio, which is at least a few times higher than any existing semi-structured data management systems (SSDMS); The compression ratio is even 2.34x as much as Zstandard and the search speed is on 5.77x of other SSDMSes.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/wang-rui",
    "session_title": "Cloud Computing",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "85fc6809-d121-4dd0-9641-1418ead98cc1"
  },
  {
    "title": "ServiceLab: Preventing Tiny Performance Regressions at Hyperscale through Pre-Production Testing",
    "authors": "Mike Chow,Meta Platforms;Yang Wang,Meta Platforms and The Ohio State University;William Wang, Ayichew Hailu, Rohan Bopardikar, Bin Zhang, Jialiang Qu, David Meisner, Santosh Sonawane, Yunqi Zhang, Rodrigo Paim, Mack Ward, Ivor Huang, Matt McNally, Daniel Hodges, Zoltan Farkas, Caner Gocmen, Elvis Huang, and Chunqiang Tang,Meta Platforms",
    "abstract": "This paper presents ServiceLab, a large-scale performance testing platform developed at Meta. Currently, the diverse set of applications and ML models it tests consumes millions of machines in production, and each year it detects performance regressions that could otherwise lead to the wastage of millions of machines. A major challenge for ServiceLab is to detect small performance regressions, sometimes as tiny as 0.01%. These minor regressions matter due to our large fleet size and their potential to accumulate over time. For instance, the median regression detected by ServiceLab for our large serverless platform, running on more than half a million machines, is only 0.14%. Another challenge is running performance tests in our private cloud, which, like the public cloud, is a noisy environment that exhibits inherent performance variances even for machines of the same instance type. To address these challenges, we conduct a large-scale study with millions of performance experiments to identify machine factors, such as the kernel, CPU, and datacenter location, that introduce variance to test results. Moreover, we present statistical analysis methods to robustly identify small regressions. Finally, we share our seven years of operational experience in dealing with a diverse set of applications.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/chow",
    "session_title": "Cloud Computing",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "5c4a8f13-f4a3-4d3e-85ee-4a34b4c25fdf"
  },
  {
    "title": "MAST: Global Scheduling of ML Training across Geo-Distributed Datacenters at Hyperscale",
    "authors": "Arnab Choudhury,Meta Platforms;Yang Wang,Meta Platforms and The Ohio State University;Tuomas Pelkonen,Meta Platforms;Kutta Srinivasan,LinkedIn;Abha Jain, Shenghao Lin, Delia David, Siavash Soleimanifard, Michael Chen, Abhishek Yadav, Ritesh Tijoriwala, Denis Samoylov, and Chunqiang Tang,Meta Platforms",
    "abstract": "In public clouds, users must manually select a datacenter region to upload their ML training data and launch ML training workloads in the same region to ensure data and computation colocation. Unfortunately, isolated decisions by individual users can lead to a mismatch between workload demand and hardware supply across regions, hurting the cloud provider's hardware utilization and profitability.  To address this problem in Meta's hyperscale private cloud, we provide a global-scheduling abstraction to all ML training workloads. Users simply submit their training workloads to MAST, our global scheduler, and rely on it to intelligently place both data and training workloads to different regions.  We describe three design principles that enable MAST to schedule complex ML training workloads at a global scale: temporal decoupling, scope decoupling, and exhaustive search. MAST successfully balances the load across global regions. Before MAST, the most overloaded region had a GPU demand-to-supply ratio of 2.63 for high-priority workloads. With MAST, this ratio has been reduced to 0.98, effectively eliminating the overload.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/choudhury",
    "session_title": "Cloud Computing",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "61f4583e-de0c-4454-a269-04eae5ea9dc3"
  },
  {
    "title": "Automatically Reasoning About How Systems Code Uses the CPU Cache",
    "authors": "Rishabh Iyer, Katerina Argyraki, and George Candea,EPFL",
    "abstract": "We present a technique, called CFAR, that developers can use to reason precisely about how their code, as well as third-party code, uses the CPU cache. Given a piece of systems code P, CFAR employs program analysis and binary instrumentation to automatically \"distill\" how P accesses memory, and uses \"projectors\" on top of the extracted distillates to answer specific questions about P's cache usage. CFAR comes with three example projectors that report (1) how P's cache footprint scales across unseen inputs; (2) the cache hits and misses incurred by P for each class of inputs; and (3) potential vulnerabilities in cryptographic code caused by secretdependent cache-access patterns.\nWe implemented CFAR in an eponymous tool with which we analyze a performance-critical subset of four TCP stacks— two versions of the Linux stack, a stack used by the IX kernel-bypass OS, and the lwIP TCP stack for embedded systems— as well as 7 algorithm implementations from the OpenSSL cryptographic library, all 51 system calls of the Hyperkernel, and 2 hash-table implementations. We show how CFAR enables developers to not only identify performance bugs and security vulnerabilities in their own code but also understand the performance impact of incorporating third-party code into their systems without doing elaborate benchmarking.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/iyer",
    "session_title": "Formal Verification",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "d685ca06-7858-4704-b53c-60a335724668"
  },
  {
    "title": "VeriSMo: A Verified Security Module for Confidential VMs",
    "authors": "Ziqiao Zhou,Microsoft Research;Anjali,University of Wisconsin-Madison;Weiteng Chen,Microsoft Research;Sishuai Gong,Purdue University;Chris Hawblitzel and Weidong Cui,Microsoft Research",
    "abstract": "Hardware vendors have introduced confidential VM architectures (e.g., AMD SEV-SNP, Intel TDX and Arm CCA) in recent years. They eliminate the trust in the hypervisor and lead to the need for security modules such as AMD Secure VMService Module (SVSM). These security modules aim to provide a guest with security features that previously were offered by the hypervisor. Since the security of such modules is critical, Rust is used to implement them for its known memory safety features. However, using Rust for implementation does not guarantee correctness, and the use of unsafe Rust compromises the memory safety guarantee.\nIn this paper, we introduce VERISMO, the first verified security module for confidential VMs on AMD SEV-SNP. VERISMO is fully functional and provides security features such as code integrity, runtime measurement, and secret management. More importantly, as a Rust-based implementation, VERISMO is fully verified for functional correctness, secure information flow, and VM confidentiality and integrity. The key challenge in verifying VERISMO is that the untrusted hypervisor can interrupt VERISMO's execution and modify the hardware state at any time. We address this challenge by dividing verification into two layers. The upper layer handles the concurrent hypervisor execution, while the lower layer handles VERISMO's own concurrent execution. When compared with a C-based implementation, VERISMO achieves similar performance. When verifying VERISMO, we identified a subtle requirement for VM confidentiality and found that it was overlooked by AMD SVSM. This demonstrates the necessity for formal verification.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhou",
    "session_title": "Formal Verification",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "5842fe25-c8c7-477b-b541-f03b56eea1d8"
  },
  {
    "title": "Validating the eBPF Verifier via State Embedding",
    "authors": "Hao Sun and Zhendong Su,ETH Zurich",
    "abstract": "This paper introducesstate embedding, a novel and highly effective technique for validating the correctness of the eBPF verifier, a critical component for Linux kernel security. To check whether a program is safe to execute, the verifier must track over-approximated program states along each potential control-flow path; any concrete state not contained in the tracked approximation may invalidate the verifier's conclusion. Our key insight is that one can effectively detect logic bugs in the verifier by embedding a program with certain approximation-correctness checks expected to be validated by the verifier. Indeed, for a program deemed safe by the verifier, our approach embeds concrete states via eBPF program constructs as correctness checks. By construction, the resulting state-embedded program allows the verifier to validate whether the embedded concrete states are correctly approximated by itself; any validation failure therefore reveals a logic bug in the verifier. We realizestate embeddingas a practical tool and apply it to test the eBPF verifier. Our evaluation results highlight its effectiveness. Despite the extensive scrutiny and testing undertaken on the eBPF verifier, our approach, within one month, uncovered 15 previously unknown logic bugs, 10 of which have already been fixed. Many of the detected bugs are severe,e.g., two are exploitable and can lead to local privilege escalation.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/sun-hao",
    "session_title": "Formal Verification",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "93629bde-52ea-4efd-9eab-4d75aae22ff1"
  },
  {
    "title": "Using Dynamically Layered Definite Releases for Verifying the RefFS File System",
    "authors": "Mo Zou, Dong Du, and Mingkai Dong,Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China;Haibo Chen,Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems, Ministry of Education, China; Huawei Technologies Co. Ltd",
    "abstract": "RefFS is the first concurrent file system that guarantees both liveness and safety, backed by a machine-checkable proof. Unlike earlier concurrent file systems, RefFS provably avoids termination bugs such as livelocks and deadlocks, through the dynamically layered definite releases specification. This specification enables handling of general blocking scenarios (including ad-hoc synchronization), facilitates modular reasoning for nested blocking, and eliminates the possibility of circular blocking.\nThe methodology underlying the aforementioned specification is integrated into a framework called MoLi (Modular Liveness Verification). This framework helps developers verify concurrent file systems. We further validate the correctness of the locking scheme for the Linux Virtual File System (VFS). Remarkably, even without conducting code proofs, we uncovered a critical flaw in a recent version of the locking scheme, which may lead to deadlocks of the entire OS (confirmed by Linux maintainers). RefFS achieves better overall performance than AtomFS, a state-of-the-art, verified concurrent file system without the liveness guarantee.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zou",
    "session_title": "Formal Verification",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "7dae14a8-5487-46fc-a5a8-1065115e143e"
  },
  {
    "title": "Anvil: Verifying Liveness of Cluster Management Controllers",
    "authors": "Xudong Sun, Wenjie Ma, Jiawei Tyler Gu, and Zicheng Ma,University of Illinois Urbana-Champaign;Tej Chajed,University of Wisconsin-Madison;Jon Howell, Andrea Lattuada, and Oded Padon,VMware Research;Lalith Suresh,Feldera;Adriana Szekeres,VMware Research;Tianyin Xu,University of Illinois Urbana-Champaign",
    "abstract": "Modern clouds depend crucially on an extensible ecosystem of thousands of controllers, each managing critical systems (e.g., a ZooKeeper cluster). A controller continuously reconciles the current state of the system to a desired state according to a declarative description. However, controllers have bugs that make them never achieve the desired state, due to concurrency, asynchrony, and failures; there are cases where after an inopportune failure, a controller can make no further progress. Formal verification is promising for avoiding bugs in distributed systems, but most work so far focused on safety, whereas reconciliation is fundamentally not a safety property.\nThis paper develops the first tool to apply formal verification to the problem of controller correctness, with a general specification we call eventually stable reconciliation, written as a concise temporal logic liveness property. We present Anvil, a framework for developing controller implementations in Rust and verifying that the controllers correctly implement eventually stable reconciliation. We use Anvil to verify three Kubernetes controllers for managing ZooKeeper, RabbitMQ, and FluentBit, which can readily be deployed in Kubernetes platforms and are comparable in terms of features and performance to widely used unverified controllers.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/sun-xudong",
    "session_title": "Formal Verification",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "47db3349-6bfd-4309-8040-2b63da240eb8"
  },
  {
    "title": "DSig: Breaking the Barrier of Signatures in Data Centers",
    "authors": "Marcos K. Aguilera,VMware Research Group;Clément Burgelin, Rachid Guerraoui, and Antoine Murat,École Polytechnique Fédérale de Lausanne (EPFL);Athanasios Xygkis,Oracle Labs;Igor Zablotchi,Mysten Labs",
    "abstract": "Data centers increasingly host mutually distrustful users on shared infrastructure. A powerful tool to safeguard such users are digital signatures. Digital signatures have revolutionized Internet-scale applications, but current signatures are too slow for the growing genre of microsecond-scale systems in modern data centers. We propose DSig, the first digital signature system to achieve single-digit microsecond latency to sign, transmit, and verify signatures in data center systems. DSig is based on the observation that, in many data center applications, the signer of a message knows most of the time who will verify its signature. We introduce a new hybrid signature scheme that combines cheap single-use hash-based signatures verified in the foreground with traditional signatures pre-verified in the background. Compared to prior state-of-the-art signatures, DSig reduces signing time from 18.9 to 0.7 μs and verification time from 35.6 to 5.1 μs, while keeping signature transmission time below 2.5 μs. Moreover, DSig achieves 2.5× higher signing throughput and 6.9× higher verification throughput than the state of the art. We use DSig to (a) bring auditability to two key-value stores (HERD and Redis) and a financial trading system (based on Liquibook) for 86% lower added latency than the state of the art, and (b) replace signatures in BFT broadcast and BFT replication, reducing their latency by 73% and 69%, respectively.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/aguilera",
    "session_title": "Cloud Security",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "7d91f3c5-c1c1-4246-a035-c3b315d7dfec"
  },
  {
    "title": "Ransom Access Memories: Achieving Practical Ransomware Protection in Cloud with DeftPunk",
    "authors": "Zhongyu Wang, Yaheng Song, Erci Xu, Haonan Wu, Guangxun Tong, Shizhuo Sun, Haoran Li, Jincheng Liu, Lijun Ding, Rong Liu, Jiaji Zhu, and Jiesheng Wu,Alibaba Group",
    "abstract": "In this paper, we focus on building a ransomware detection and recovery system for cloud block stores. We start by discussing the possibility of directly using existing methods or porting one to our scenario with modifications. These attempts, though failed, led us to identify the unique IO characteristics of ransomware, and further drove us to build DeftPunk, a block-level ransomware detection and recovery system. DeftPunk uses a two-layer classifier for fast and accurate detection, creates pre-/post-attack snapshots to avoid data loss, and leverages log-structured support for low overhead recovery. Our large-scale benchmark shows that DeftPunk can achieve nearly 100% recall across 13 types of ransomware and low runtime overhead.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/wang-zhongyu",
    "session_title": "Cloud Security",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "54f3ac30-f3b3-479d-af0f-d0ee6d450a5d"
  },
  {
    "title": "Secret Key Recovery in a Global-Scale End-to-End Encryption System",
    "authors": "Graeme Connell,Signal Messenger;Vivian Fang,UC Berkeley;Rolfe Schmidt,Signal Messenger;Emma Dauterman and Raluca Ada Popa,UC Berkeley",
    "abstract": "End-to-end encrypted messaging applications ensure that an attacker cannot read a user's message history without their decryption keys. While this provides strong privacy, it creates a usability problem: if a user loses their devices and cannot access their decryption keys, they can no longer access their message history. To solve this usability problem, users should be able to back up their decryption keys with the messaging provider. For privacy, the provider should not have access to users' decryption keys. To solve this problem, we present Secure Value Recovery 3 (SVR3), a secret key recovery system that distributes trust across different types of hardware enclaves run by different cloud providers in order to protect users' decryption keys. SVR3 is the first deployed secret key recovery system to split trust across heterogeneous enclaves managed by different cloud providers: this design ensures that a single type of enclave does not become a central point of attack. SVR3 protects decryption keys via rollback protection and fault tolerance techniques tailored to the enclaves' security guarantees. SVR3 costs $0.0025/user/year and takes 365ms for a user to recover their key, which is a rare operation. A part of SVR3 has been rolled out to millions of real users in a deployment with capacity for over 500 million users, demonstrating the ability to operate at scale.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/connell",
    "session_title": "Cloud Security",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "df4a2787-bde0-449f-88ed-a1f0fe864c5a"
  },
  {
    "title": "Flock: A Framework for Deploying On-Demand Distributed Trust",
    "authors": "Darya Kaviani and Sijun Tan,UC Berkeley;Pravein Govindan Kannan,IBM Research;Raluca Ada Popa,UC Berkeley",
    "abstract": "Recent years have exhibited an increase in applications that distribute trust across n servers to protect user data from a central point of attack. However, these deployments remain limited due to a core obstacle: establishing n distinct trust domains. An application provider, a single trust domain, cannot directly deploy multiple trust domains. As a result, application providers forge business relationships to enlist third-parties as trust domains, which is a manual, lengthy, and expensive process, inaccessible to many application developers.\nWe introduce the on-demand distributed-trust architecture that enables an application provider to deploy distributed trust automatically and immediately without controlling the other trust domains. The insight lies in reversing the deployment method such that each user's client drives deployment instead of the application provider. While at a first glance, this approach appears infeasible due to cost, performance, and resource abuse concerns, our system Flock resolves these challenges. We implement and evaluate Flock on 3 major cloud providers and 8 distributed-trust applications. On average, Flock achieves 1.05x the latency and 0.68-2.27x the cloud cost of a traditional distributed-trust deployment, without reliance on third-party relationships.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/kaviani",
    "session_title": "Cloud Security",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "8f804c06-1118-4f50-b484-fa9e50ae2bb0"
  },
  {
    "title": "FairyWREN: A Sustainable Cache for Emerging Write-Read-Erase Flash Interfaces",
    "authors": "Sara McAllister and Yucong \"Sherry\" Wang,Carnegie Mellon University;Benjamin Berg,UNC Chapel Hill;Daniel S. Berger,Microsoft Azure and University of Washington;George Amvrosiadis, Nathan Beckmann, and Gregory R. Ganger,Carnegie Mellon University",
    "abstract": "Datacenters need to reduce embodied carbon emissions, particularly for flash, which accounts for 40% of embodied carbon in servers. However, decreasing flash's embodied emissions is challenging due to flash's limited write endurance, which more than halves with each generation of denser flash. Reducing embodied emissions requires extending flash lifetime, stressing its limited write endurance even further. The legacy Logical Block-Addressable Device (LBAD) interface exacerbates the problem by forcing devices to perform garbage collection, leading to even more writes.\nFlash-based caches in particular write frequently, limiting the lifetimes and densities of the devices they use. These flash caches illustrate the need to break away from LBAD and switch to the new Write-Read-Erase iNterfaces (WREN) now coming to market. WREN affords applications control over data placement and garbage collection. We present FairyWREN, a flash cache designed for WREN. FairyWREN reduces writes by co-designing caching policies and flash garbage collection. FairyWREN provides a 12.5× write reduction over state-of-the-art LBAD caches. This decrease in writes allows flash devices to last longer, decreasing flash cost by 35% and flash carbon emissions by 33%.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/mcallister",
    "session_title": "Data Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "25ed1710-3968-43a7-af07-d986461c3b89"
  },
  {
    "title": "Massively Parallel Multi-Versioned Transaction Processing",
    "authors": "Shujian Qian and Ashvin Goel,University of Toronto",
    "abstract": "Multi-version concurrency control can avoid most read-write conflicts in OLTP workloads. However, multi-versioned systems often have higher complexity and overheads compared to single-versioned systems due to the need for allocating, searching and garbage collecting versions. Consequently, single-versioned systems can often dramatically outperform multi-versioned systems.\nWe introduce Epic, the first multi-versioned GPU-based deterministic OLTP database. Epic utilizes a batched execution scheme, performing concurrency control initialization for a batch of transactions before executing the transactions deterministically. By leveraging the predetermined ordering of transactions, Epic eliminates version search entirely and significantly reduces version allocation and garbage collection overheads. Our approach utilizes the computational power of the GPU architecture to accelerate Epic's concurrency control initialization and efficiently parallelize batched transaction execution, while ensuring low latency. Our evaluation demonstrates that Epic achieves comparable performance under low contention and consistently higher performance under medium to high contention versus state-of-the-art single and multi-versioned systems.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/qian",
    "session_title": "Data Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "1c87af7f-9237-49b2-8746-af48290d913e"
  },
  {
    "title": "Burstable Cloud Block Storage with Data Processing Units",
    "authors": "Junyi Shu,School of Computer Science, Peking University and Alibaba Cloud;Kun Qian and Ennan Zhai,Alibaba Cloud;Xuanzhe Liu and Xin Jin,School of Computer Science, Peking University",
    "abstract": "Cloud block storage (CBS) is a key pillar of public clouds. Today's CBS distinguishes itself from physical counterparts (e.g., SSDs) by offering unique burst capability as well as enhanced throughput, capacity, and availability. We conduct an initial characterization of our CBS product, a globally deployed cloud block storage service at public cloud provider Alibaba Cloud. A key observation is that the storage agent (SA) running on a data processing unit (DPU) which connects user VMs to the backend storage is the major source of performance fluctuation with burst capability provided. In this paper, we propose a hardware-software co-designed I/O scheduling system BurstCBS to address load imbalance and tenant interference at SA. BurstCBS exploits high-performance queue scaling to achieve near-perfect load balancing at line rate. To mitigate tenant interference, we design a novel burstable I/O scheduler that prioritizes resource allocation for base-level usage while supporting bursts. We employ a vectorized I/O cost estimator for comprehensive measurements of the consumed resources of different types of I/Os. Our evaluation shows that BurstCBS reduces average latency by up to 85% and provides up to 5× throughput for base-level tenants under congestion with minimal overhead. We verify the benefits brought by BurstCBS with a database service that internally relies on CBS, and show that up to 83% latency reduction is observed on customer workloads.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/shu",
    "session_title": "Data Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "9114d4c2-9043-4e28-bc6b-8fe59b61f67e"
  },
  {
    "title": "Motor: Enabling Multi-Versioning for Distributed Transactions on Disaggregated Memory",
    "authors": "Ming Zhang, Yu Hua, and Zhijun Yang,Wuhan National Laboratory for Optoelectronics, School of Computer, Huazhong University of Science and Technology",
    "abstract": "In modern datacenters, memory disaggregation unpacks monolithic servers to build network-connected distributed compute and memory pools to improve resource utilization and deliver high performance. The compute pool leverages distributed transactions to access remote data in the memory pool to provide atomicity and strong consistency. Existing single-versioning designs have been constrained due to limited system concurrency and high logging overheads. Although the multi-versioning design in the conventional monolithic servers is promising to offer high concurrency and reduce logging overheads, which however fails to work in the disaggregated memory. In order to bridge the gap between the multi-versioning design and the disaggregated memory, we propose Motor that holistically redesigns the version structure and transaction protocol to enable multi-versioning for fast distributed transaction processing on the disaggregated memory. To efficiently organize different versions of data in the memory pool, Motor leverages a new consecutive version tuple (CVT) structure to store the versions together in a continuous manner, which allows the compute pool to obtain the target version in a single network round trip. On top of CVT, Motor leverages a fully one-sided RDMA-based MVCC protocol to support fast distributed transactions with flexible isolation levels. Experimental results demonstrate that Motor improves the throughput by up to 98.1% and reduces the latency by up to 55.8% compared with state-of-the-art systems.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhang-ming",
    "session_title": "Data Management",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "1809294a-1145-420b-8c24-53deb2401485"
  },
  {
    "title": "Detecting Logic Bugs in Database Engines via Equivalent Expression Transformation",
    "authors": "Zu-Ming Jiang and Zhendong Su,ETH Zurich",
    "abstract": "Database management systems (DBMSs) are crucial for storing and fetching data. To improve the reliability of such systems, approaches have been proposed to detect logic bugs that cause DBMSs to process data incorrectly. These approaches manipulate queries and check whether the query results produced by DBMSs follow the expectations. However, such query-level manipulation cannot handle complex query semantics and thus needs to limit the patterns of generated queries, degrading testing effectiveness.\nIn this paper, we tackle the problem using a fine-grained methodology—expression-level manipulation—which empowers the proposed approach to be applicable to arbitrary queries. To find logic bugs in DBMSs, we design a novel and general approach, equivalent expression transformation (EET). Our core idea is that manipulating expressions of a query in a semantic-preserving manner also preserves the semantics of the entire query and is independent of query patterns. EET validates DBMSs by checking whether the transformed queries still produce the same results as the corresponding original queries. We realize our approach and evaluate it on 5 widely used and extensively tested DBMSs: MySQL, PostgreSQL, SQLite, ClickHouse, and TiDB. In total, EET found 66 unique bugs, 35 of which are logic bugs. We expect the generality and effectiveness of EET to inspire follow-up research and benefit the reliability of many DBMSs.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/jiang",
    "session_title": "Analysis of Correctness",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "717ee862-49fc-448c-8a0f-b61cb0fd9637"
  },
  {
    "title": "Inductive Invariants That Spark Joy: Using Invariant Taxonomies to Streamline Distributed Protocol Proofs",
    "authors": "Tony Nuda Zhang,University of Michigan;Travis Hance,Carnegie Mellon University;Manos Kapritsos,University of Michigan;Tej Chajed,University of Wisconsin–Madison;Bryan Parno,Carnegie Mellon University",
    "abstract": "Proving the correctness of a distributed protocol is a challenging endeavor. Central to this task is finding an inductive invariant for the protocol. Currently, automated invariant inference algorithms require developers to describe protocols using a restricted logic. If the developer wants to prove a protocol expressed without these restrictions, they must devise an inductive invariant manually.\nWe propose an approach that simplifies and partially automates finding the inductive invariant of a distributed protocol, as well as proving that it really is an invariant. The key insight is to identify an invariant taxonomy that divides invariants into Regular Invariants, which have one of a few simple low-level structures, and Protocol Invariants, which capture the higher-level host relationships that make the protocol work.\nBuilding on the insight of this taxonomy, we describe the Kondo methodology for proving the correctness of a distributed protocol modeled as a state machine. The developer first manually devises the Protocol Invariants by proving a synchronous version of the protocol correct. In this simpler version, sends and receives are replaced with atomic variable assignments. The Kondo tool then automatically generates the asynchronous protocol description, Regular Invariants, and proofs that the Regular Invariants are inductive on their own. Finally, Kondo combines these with the synchronous proof into a draft proof of the asynchronous protocol, which may then require a small amount of user effort to complete. Our evaluation shows that Kondo reduces developer effort for a wide variety of distributed protocols.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhang-nuda",
    "session_title": "Analysis of Correctness",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "dcd43e6b-8ba1-4872-849c-0d5738e1212b"
  },
  {
    "title": "Performance Interfaces for Hardware Accelerators",
    "authors": "Jiacheng Ma, Rishabh Iyer, Sahand Kashani, Mahyar Emami, Thomas Bourgeat, and George Candea,EPFL",
    "abstract": "Designing and building a system that reaps the performance benefits of hardware accelerators is challenging, because they provide little concrete visibility into their expected performance. Developers must invest many person-months into benchmarking, to determine if their system would indeed benefit from using a particular accelerator. This must be done carefully, because accelerators can actually hurt performance for some classes of inputs, even if they help for others.\nWe demonstrate that it is possible for hardware accelerators to ship with performance interfaces that provide actionable visibility into their performance, just like semantic interfaces do for functionality. We propose an intermediate representation (IR) for accelerator performance that precisely captures all performance-relevant details of the accelerator while abstracting away all other information, including functionality. We develop a toolchain (ltc) that, based on the proposed IR, automatically produces human-readable performance interfaces that help developers make informed design decisions. ltc can also automatically produce formal proofs of performance properties of the accelerator, and can act as a fast performance simulator for concrete workloads.\nWe evaluate our approach on accelerators used for deep learning, serialization of RPC messages, JPEG image decoding, genome sequence alignment, and on an RMT pipeline used in programmable network switches. We demonstrate that the performance IR provides an accurate and complete representation of performance behavior, and we describe a variety of use cases for ltc and the resulting performance interfaces. ltc is open-source and freely available at https://\ndslab.epfl.ch/research/perf.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/ma-jiacheng",
    "session_title": "Analysis of Correctness",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "a7fb5991-7342-4a77-8b8d-45dcbcd956e5"
  },
  {
    "title": "IronSpec: Increasing the Reliability of Formal Specifications",
    "authors": "Eli Goldweber, Weixin Yu, Seyed Armin Vakil Ghahani, and Manos Kapritsos,University of Michigan",
    "abstract": "The guarantees of formally verified systems are only as strong as their trusted specifications (specs). As observed by previous studies, bugs in formal specs invalidate the assurances that proofs provide. Unfortunately, specs—by their very nature—cannot be proven correct. Currently, the only way to identify spec bugs is by careful, manual inspection.\nIn this paper we introduce IronSpec, a framework of automatic and manual techniques to increase the reliability of formal specifications. IronSpec draws inspiration from classical software testing practices, which we adapt to the realm of formal specs. IronSpec facilitates spec testing with automated sanity checking, a methodology for writing SpecTesting Proofs (STPs), and automated spec mutation testing.\nWe evaluate IronSpec on 14 specs, including six specs of real-world verified codebases. Our results show that IronSpec is effective at flagging discrepancies between the spec and the developer's intent, and has led to the discovery of ten specification bugs across all six real-world verified systems.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/goldweber",
    "session_title": "Analysis of Correctness",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "61511f2d-a55d-4f8c-9852-14dabc49a795"
  },
  {
    "title": "Identifying On-/Off-CPU Bottlenecks Together with Blocked Samples",
    "authors": "Minwoo Ahn and Jeongmin Han,Sungkyunkwan University;Youngjin Kwon,Korea Advanced Institute of Science and Technology (KAIST);Jinkyu Jeong,Yonsei University",
    "abstract": "The rapid advancement of computer system components has necessitated a comprehensive profiling approach for both on-CPU and off-CPU events simultaneously. However, the conventional approach lacks profiling both on- and off-CPU events, so they fall short of accurately assessing the overhead of each bottleneck in modern applications.\nIn this paper, we propose a sampling-based profiling technique called blocked samples that is designed to capture all types of off-CPU events, such as I/O waiting, blocking synchronization, and waiting in CPU runqueue. Using the blocked samples technique, this paper proposes two profilers, bperf and BCOZ. Leveraging blocked samples, bperf profiles applications by providing symbol-level profile information when a thread is either on the CPU or off the CPU, awaiting scheduling or I/O requests. Using the information, BCOZ performs causality analysis of collected on- and off-CPU events to precisely identify performance bottlenecks and the potential impact of optimizations. The profiling capability of BCOZ is verified using real applications. From our profiling results followed by actual optimization, BCOZ identifies bottlenecks with off-CPU events precisely, and their optimization results are aligned with the predicted performance improvement by BCOZ's causality analysis.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/ahn",
    "session_title": "Analysis of Correctness",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "6ae55d3e-5e33-474b-a1f4-8e6c68dba997"
  },
  {
    "title": "dLoRA: Dynamically Orchestrating Requests and Adapters for LoRA LLM Serving",
    "authors": "Bingyang Wu, Ruidong Zhu, and Zili Zhang,School of Computer Science, Peking University;Peng Sun,Shanghai AI Lab;Xuanzhe Liu and Xin Jin,School of Computer Science, Peking University",
    "abstract": "Low-rank adaptation (LoRA) is a popular approach to finetune pre-trained large language models (LLMs) to specific domains. This paper introduces dLoRA, an inference serving system for LoRA models. dLoRA achieves high serving efficiency by dynamically orchestrating requests and LoRA adapters in terms of two aspects: (i) dynamically merge and unmerge adapters with the base model; and (ii) dynamically migrate requests and adapters between different worker replicas. These capabilities are designed based on two insights. First, despite the allure of batching without merging a LoRA adapter into the base model, it is not always beneficial to unmerge, especially when the types of requests are skewed. Second, the autoregressive nature of LLM requests introduces load imbalance between worker replicas due to varying input and output lengths, even if the input requests are distributed uniformly to the replicas. We design a credit-based batching algorithm to decide when to merge and unmerge, and a request-adapter co-migration algorithm to decide when to migrate. The experimental results show that dLoRA improves the throughput by up to 57.9× and 26.0×, compared to vLLM and HugginFace PEFT, respectively. Compared to the concurrent work S-LoRA, dLoRA achieves up to 1.8× lower average latency.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/wu-bingyang",
    "session_title": "ML Scheduling",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "8ac9dad2-39ca-4102-9b24-e646c0a18b99"
  },
  {
    "title": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
    "authors": "Chaofan Lin,Shanghai Jiao Tong University;Zhenhua Han, Chengruidong Zhang, Yuqing Yang, and Fan Yang,Microsoft Research;Chen Chen,Shanghai Jiao Tong University;Lili Qiu,Microsoft Research",
    "abstract": "The rise of large language models (LLMs) has enabled LLM-based applications (a.k.a. AI agents or co-pilots), a new software paradigm that combines the strength of LLM and conventional software. Diverse LLM applications from different tenants could design complex workflows using multiple LLM requests to accomplish one task. However, they have to use the over-simplified request-level API provided by today's public LLM services, losing essential application-level information. Public LLM services have to blindly optimize individual LLM requests, leading to sub-optimal end-to-end performance of LLM applications.\nThis paper introduces Parrot, an LLM service system that focuses on the end-to-end experience of LLM-based applications. Parrot proposes Semantic Variable, a unified abstraction to expose application-level knowledge to public LLM services. A Semantic Variable annotates an input/output variable in the prompt of a request, and creates the data pipeline when connecting multiple LLM requests, providing a natural way to program LLM applications. Exposing Semantic Variables to the public LLM service allows it to perform conventional data flow analysis to uncover the correlation across multiple LLM requests. This correlation opens a brand-new optimization space for the end-to-end performance of LLM-based applications. Extensive evaluations demonstrate that Parrot can achieve up to an order-of-magnitude improvement for popular and practical use cases of LLM applications.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/lin-chaofan",
    "session_title": "ML Scheduling",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "d375ca00-56a2-4ab0-950d-9e0bf7dc86d4"
  },
  {
    "title": "USHER: Holistic Interference Avoidance for Resource Optimized ML Inference",
    "authors": "Sudipta Saha Shubha and Haiying Shen,University of Virginia;Anand Iyer,Georgia Institute of Technology",
    "abstract": "Minimizing monetary cost and maximizing the goodput of inference serving systems are increasingly important with the ever-increasing popularity of deep learning models. While it is desirable to spatially multiplex GPU resources to improve utilization, existing techniques suffer from inter-model interference, which prevents them from achieving both high computation and memory utilizations. We present USHER, a system that maximizes resource utilization in a holistic fashion while being interference-aware. USHER consists of three key components: 1) a cost-efficient and fast GPU kernel-based model resource requirement estimator, 2) a lightweight heuristic-based interference-aware resource utilization-maximizing scheduler that decides the batch size, model replication degree, and model placement to minimize monetary cost while satisfying latency SLOs or maximize the goodput, and 3) a novel operator graph merger to merge multiple models to minimize interference in GPU cache. Large-scale experiments using production workloads show that USHER achieves up to 2.6× higher goodput and 3.5× better cost-efficiency compared to existing methods, while scaling to thousands of GPUs.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/shubha",
    "session_title": "ML Scheduling",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "1f3f61e3-15b8-443a-9691-99f0186e6c03"
  },
  {
    "title": "Fairness in Serving Large Language Models",
    "authors": "Ying Sheng,UC Berkeley and Stanford University;Shiyi Cao, Dacheng Li, Banghua Zhu, and Zhuohan Li,UC Berkeley;Danyang Zhuo,Duke University;Joseph E. Gonzalez and Ion Stoica,UC Berkeley",
    "abstract": "High-demand LLM inference services (e.g., ChatGPT and BARD) support a wide range of requests from short chat conversations to long document reading. To ensure that all client requests are processed fairly, most major LLM inference services have request rate limits, to ensure that no client can dominate the request queue. However, this rudimentary notion of fairness also results in under-utilization of the resources and poor client experience when there is spare capacity. While there is a rich literature on fair scheduling, serving LLMs presents new challenges due to their unpredictable request lengths and their unique batching characteristics on parallel accelerators. This paper introduces the definition of LLM serving fairness based on a cost function that accounts for the number of input and output tokens processed. To achieve fairness in serving, we propose a novel scheduling algorithm, the Virtual Token Counter (VTC), a fair scheduler based on the continuous batching mechanism. We prove a 2× tight upper bound on the service difference between two backlogged clients, adhering to the requirement of work-conserving. Through extensive experiments, we demonstrate the superior performance of VTC in ensuring fairness, especially in contrast to other baseline methods, which exhibit shortcomings under various conditions. The reproducible code is available athttps://github.com/Ying1123/VTC-artifact.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/sheng",
    "session_title": "ML Scheduling",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "e6b59687-4e4b-404b-af64-0535c0d81068"
  },
  {
    "title": "MonoNN: Enabling a New Monolithic Optimization Space for Neural Network Inference Tasks on Modern GPU-Centric Architectures",
    "authors": "Donglin Zhuang,The University of Sydney;Zhen Zheng,Alibaba Group;Haojun Xia,The University of Sydney;Xiafei Qiu, Junjie Bai, and Wei Lin,Alibaba Group;Shuaiwen Leon Song,The University of Sydney",
    "abstract": "In this work, we reveal that the kernel-by-kernel execution scheme in the existing machine learning optimizing compilers is no longer effective in fully utilizing hardware resources provided by the advances of modern GPU architectures. Specifically, such scheme suffers from severe non-computation overhead and off-chip memory traffic, making the optimization efforts from the state-of-the-art compiler techniques greatly attenuated on the newer generations of GPUs. To address this emerging challenge, we propose MonoNN, the first machine learning optimizing compiler that enables a new monolithic design and optimization space for common static neural network (NN) inference tasks on a single GPU. MonoNN can accommodate an entire neural network into a single GPU kernel, drastically reducing non-computation overhead and providing further fine-grained optimization opportunities from the newly formed monolithic optimization space. Most importantly, MonoNN identifies the resource incompatibility issue between various NN operators as the key design bottleneck for creating such a monolithic optimization space. Then MonoNN effectively tackles it by systematically exploring and exploiting the parallelism compensation strategy and resource trade-offs across different types of NN computations, and by proposing a novel schedule-independent group tuning technique to significantly shrink the extremely large tuning space. Finally, MonoNN provides a compiler implementation that incorporates our proposed optimizations and automatically generates highly efficient kernel code. Extensive evaluation on a set of popular production inference tasks demonstrates that MonoNN achieves an average speedup of 2.01× over the state-of-the-art frameworks and compilers. Specifically, MonoNN outperforms TVM, TensorRT, XLA, and AStitch by up to 7.3×, 5.9×, 1.7× and 2.9× in terms of end-to-end inference performance, respectively. MonoNN source code is publicly available athttps://github.com/AlibabaResearch/mononn.",
    "link": "https://www.usenix.org/conference/osdi24/presentation/zhuang",
    "session_title": "ML Scheduling",
    "conference_name": "OSDI",
    "date": "2024-07-11",
    "paper_id": "6aff7f14-8c2b-4cb7-8da5-1d7a33f8db42"
  }
]