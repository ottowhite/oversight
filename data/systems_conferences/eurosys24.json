[
  {
    "title": "WiseGraph: Optimizing GNN with Joint Workload Partition of Graph and Operations",
    "authors": "Kezhao  Huang (Tsinghua University), Jidong  Zhai (Tsinghua University), Liyan  Zheng (Tsinghua University), Haojie  Wang (Tsinghua University), Yuyang  Jin (Tsinghua University), Qihao  Zhang (Tsinghua University), Runqing  Zhang (Tsinghua University), Zhen  Zheng (Microsoft), Youngmin  Yi (University of Seoul), Xipeng  Shen (North Carolina State University)",
    "abstract": "Graph Neural Network (GNN) has emerged as an important workload for learning on graphs. With the size of graph data and the complexity of GNN model architectures increasing, developing an efficient GNN system grows more important. As GNN has heavy neural computation workloads on a large graph, it is crucial to partition the entire workload into smaller parts for parallel execution and optimization. However, existing approaches separately partition graph data and GNN operations, resulting in inefficiency and large data movement overhead. To address this problem, we present WiseGraph, a GNN training framework exploring the joint optimization space of graph data partition and GNN operation partition. To bridge the gap between the two classes of partitions, we propose a workload abstraction tailored to GNN, gTask, which can not only describe existing GNN partition strategies as special cases but also exploit new optimization opportunities. Based on gTasks, WiseGraph effectively generates partition plans adaptive to input graph data and GNN models. Evaluation on five typical GNN models shows that WiseGraph outperforms existing GNN frameworks by 2.04× and 2.22× for single and multiple GPU training. WiseGraph is publicly available at https://github.com/xxcclong/CxGNN-Compute/.",
    "link": "https://www.semanticscholar.org/paper/16b136b0d588f01ffefa51d230281306ab047706",
    "session_title": "Session A: Graphs",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "d3231a6c-74cb-4d9f-ad4a-bcecd3fffe08"
  },
  {
    "title": "Core Graph: Exploiting Edge Centrality to Speedup the Evaluation of Iterative Graph Queries",
    "authors": "Xiaolin  Jiang (UC Riverside), Mahbod  Afarin (UC Riverside), Zhijia  Zhao (UC Riverside), Nael  Abu-Ghazaleh (UC Riverside), Rajiv  Gupta (UC Riverside)",
    "abstract": "When evaluating an iterative graph query over a large graph, systems incur significant overheads due to repeated graph transfer across the memory hierarchy coupled with repeated (redundant) propagation of values over the edges in the graph. An approach for reducing these overheads combines the use of a small proxy graph and the large original graph in a two phase query evaluation. The first phase evaluates the query on the proxy graph incurring low overheads and producing mostly precise results. The second phase uses these mostly precise results to bootstrap query evaluation on the larger original graph producing fully precise results. The effectiveness of this approach depends upon the quality of the proxy graph. Prior methods find proxy graphs that are either large or produce highly imprecise results. We present a new form of proxy graph named the Core Graph (CG) that is not only small, it also produces highly precise results. A CG is a subgraph of the larger input graph that contains all vertices but on average contains only 10.7% of edges and yet produces precise results for 94.5-99.9% vertices in the graph for different queries. The finding of such an effective CG is based on our key new insight, namely, a small subset of non-zero centrality edges are responsible for determining the converged results of nearly all the vertices across different queries. We develop techniques to identify a CG that produces precise results for most vertices and optimizations to efficiently compute precise results of remaining vertices. Across six kinds of graph queries and four input graphs, CGs improved the performance of GPU-based Subway system by up to 4.48×, of out-of-core disk-based GridGraph system by up to 13.62×, and of Ligra in-memory graph processing system by up to 9.31×.",
    "link": "https://www.semanticscholar.org/paper/096819782ce34058faa7ad3d81c9b3d49a5cf527",
    "session_title": "Session A: Graphs",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "ac815609-8924-4780-8b19-72d0b2d1aa00"
  },
  {
    "title": "LSGraph: A Locality-centric High-performance Streaming Graph Engine",
    "authors": "Hao  Qi (Huazhong University of Science and Technology), Yiyang  Wu (Huazhong University of Science and Technology), Ligang  He (University of Warwick), Yu  Zhang (Huazhong University of Science and Technology), Kang  Luo (Huazhong University of Science and Technology), Minzhi  Cai (Huazhong University of Science and Technology), Hai  Jin (Huazhong University of Science and Technology), Zhan  Zhang (Zhejiang Lab, China), Jin  Zhao (Huazhong University of Science and Technology)",
    "abstract": "Streaming graph has been broadly employed across various application domains. It involves updating edges to the graph and then performing analytics on the updated graph. However, existing solutions either suffer from poor data locality and high computation complexity for streaming graph analytics, or need high overhead to search and move graph data to ensure ordered neighbors during streaming graph update. This paper presents a novel locality-centric streaming graph engine, called LSGraph, to enable efficient both graph analytics and graph update. The main novelty of this engine is a differentiated hierarchical indexed streaming graph representation approach to achieve efficient data search and movement for graph update and also maintain data locality and ordered neighbors for efficient graph analytics simultaneously. Besides, a locality-aware streaming graph data update mechanism is also proposed to efficiently regulate the distance of data movement, minimizing the overhead of memory access during graph update. We have implemented LSGraph and conducted a systematic evaluation on both real-world and synthetic datasets. Compared with three cutting-edge streaming graph engines, i.e., Terrace, Aspen, and PaC-tree, LSGraph achieves 2.98×-81.08×, 1.46×-12.56×, and 1.26×-10.31× speedups during graph update, while obtaining 1.02×-4.28×, 1.58×-3.55×, and 1.20×-2.72× speedups during graph analytics, respectively.",
    "link": "https://www.semanticscholar.org/paper/555ae71cc38f9e8f8c61ae03c621842deade5804",
    "session_title": "Session A: Graphs",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "b0faeb8a-5ff9-42d3-adee-07d8226c8e33"
  },
  {
    "title": "Contigra: Graph Mining with Containment Constraints",
    "authors": "Joanna  Che (Simon Fraser University), Kasra  Jamshidi (Simon Fraser University), Keval  Vora (Simon Fraser University)",
    "abstract": "While graph mining systems employ efficient task-parallel strategies to quickly explore subgraphs of interest (or matches), they remain oblivious to containment constraints like maximality and minimality, resulting in expensive constraint checking on every explored match as well as redundant explorations that limit their scalability. In this paper, we develop Contigra for efficient graph mining with containment constraints. We first model the impact of constraints in terms of dependencies across exploration tasks, and then exploit the dependencies to develop: (a) task fusion that merges correlated tasks to increase cache reuse; (b) task promotion that allows explorations to continue from available subgraphs and skip re-exploring subgraphs from scratch; (c) task cancelations that avoid unnecessary constraint checking and prioritizes faster constraint validations; and (d) task skipping that safely skips certain exploration and validation tasks. Experimental results show that Contigra scale to graph mining workloads with containment constraints, which could not be handled by existing state-of-the-art systems.",
    "link": "https://www.semanticscholar.org/paper/21b9536a97c60da4f81b59dc1525d28aade8f0f2",
    "session_title": "Session A: Graphs",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "10534b8f-a5c7-44aa-9c74-d985056e75bd"
  },
  {
    "title": "Halflife: An Adaptive Flowlet-based Load Balancer with Fading Timeout in DCNs",
    "authors": "Sen  Liu (Fudan University), Yongbo  Gao (Fudan University), Zixuan  Chen (Fudan University), Jiarui  Ye (Fudan University), Haiyang  Xu (Fudan University), Furong  Liang (Fudan University), Wei  Yan (Fudan University), Zerui  Tian (Fudan University), Quanwei  Sun (Fudan University), Zehua  Guo (Beijing Institute of Technology, Beijing Institute of Technology Zhengzhou Academy of Intelligent Technology, Zhejiang Lab), Yang  Xu (Fudan University, Peng Cheng Laboratory)",
    "abstract": "Modern data centers (DCs) employ various traffic load balancers to achieve high bisection bandwidth. Among them, flowlet switching has shown remarkable performance in both load balancing and upper-layer protocol (e.g., TCP) friendliness. However, flowlet-based load balancers suffer from the inflexibility of flowlet timeout value (FTV) and result in sub-optimal performance under various application workloads. To this end, we propose Halflife, a novel flowlet-based load balancer that leverages fading FTVs to reroute traffic promptly under different workloads without any prior knowledge. Halflife not only balances traffic better, but also avoids the performance degradation caused by frequent oscillation or shifting of lows between paths. Furthermore, Halflife's fading mechanism is not only compatible with most flowlet-based load balancers, such as CONGA and LetFlow, but also improves their performance when leveraging flowlet switching in RDMA network. Through testbed experiments and simulations, we prove that Halflife improves the performance of CONGA and LetFlow by 10% ~ 150%, and it outperforms other load balancers by 30% ~ 200% across most application workloads.",
    "link": "https://www.semanticscholar.org/paper/6fb2968b2108921d475c41eaafa9b9a1863d4e9e",
    "session_title": "Session B: Networks",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "3be51619-e855-48e8-b3ee-07165c22a99a"
  },
  {
    "title": "Hoda: a High-performance Open vSwitch Dataplane with Multiple Specialized Data Paths",
    "authors": "Heng  Pan (Computer Network Information Center, Chinese Academy of Sciences), Peng  He (ByteDance Inc.), Zhenyu  Li (Institute of Computing Technology, Chinese Academy of Sciences), Pan  Zhang (Intel Corporation), Junjie  Wan (ByteDance Inc.), Yuhao  Zhou (ByteDance Inc.), XiongChun  Duan (ByteDance Inc.), Yu  Zhang (ByteDance Inc.), Gaogang  Xie (Computer Network Information Center, Chinese Academy of Sciences)",
    "abstract": "Open vSwitch (OvS) has been widely used in cloud networks in view of its programmability and flexibility. However, we observe a huge performance drop when it loads practical cloud networking services (e.g., tunneling and firewalling). Our further analysis reveals that the root cause lies in the gap between the needs of supporting various selections of packet header fields and the one-size-fits-all data path in the vanilla OvS. Motivated by this, we design Hoda, a high-performance OvS dataplane with multiple specialized data paths. Specifically, Hoda constructs the specialized parser and microflow cache for each OpenFlow program so as to achieve lightweight parsing and caching. We also propose a configurable version of Hoda that introduces configuration knobs in the data path to ease specialization. The experiments with real-life OpenFlow rules show that Hoda achieves up to 1.7× speed up over the state-of-the-art OvS and 1.5× speed up over mSwitch. Hoda has also been deployed in a large cloud to serve various online services; the A/B test in the cloud reveals a 20% request process time reduction for Ngnix services.",
    "link": "https://www.semanticscholar.org/paper/630bd4318f39dda169ebff839bbc84f5aa416e98",
    "session_title": "Session B: Networks",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "4ed345bc-06ef-46cb-bec3-23428fae2e23"
  },
  {
    "title": "Astraea: Towards Fair and Efficient Learning-based Congestion Control",
    "authors": "Xudong  Liao (Hong Kong University of Science and Technology), Han  Tian (Hong Kong University of Science and Technology), Chaoliang  Zeng (Hong Kong University of Science and Technology), Xinchen  Wan (Hong Kong University of Science and Technology), Kai  Chen (Hong Kong University of Science and Technology)",
    "abstract": "Recent years have witnessed a plethora of learning-based solutions for congestion control (CC) that demonstrate better performance over traditional TCP schemes. However, they fail to provide consistently good convergence properties, including fairness, fast convergence and stability, due to the mismatch between their objective functions and these properties. Despite being intuitive, integrating these properties into existing learning-based CC is challenging, because: 1) their training environments are designed for the performance optimization of single flow but incapable of cooperative multi-flow optimization, and 2) there is no directly measurable metric to represent these properties into the training objective function. We present Astraea, a new learning-based congestion control that ensures fast convergence to fairness with stability. At the heart of Astraea is a multi-agent deep reinforcement learning framework that explicitly optimizes these convergence properties during the training process by enabling the learning of interactive policy between multiple competing flows, while maintaining high performance. We further build a faithful multi-flow environment that emulates the competing behaviors of concurrent flows, explicitly expressing convergence properties to enable their optimization during training. We have fully implemented Astraea and our comprehensive experiments show that Astraea can quickly converge to fairness point and exhibit better stability than its counterparts. For example, Astraea achieves near-optimal bandwidth sharing (i.e., fairness) when multiple flows compete for the same bottleneck, delivers up to 8.4× faster convergence speed and 2.8× smaller throughput deviation, while achieving comparable or even better performance over prior solutions.",
    "link": "https://www.semanticscholar.org/paper/67b9ff9b9d220fc0a9c6193be4c72f7edcdb03d2",
    "session_title": "Session B: Networks",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "67d4fea9-390a-4792-9ecc-f47c233fab56"
  },
  {
    "title": "Unison: A Parallel-Efficient and User-Transparent Network Simulation Kernel",
    "authors": "Songyuan  Bai (Nanjing University), Hao  Zheng (Nanjing University), Chen  Tian (Nanjing University), Xiaoliang  Wang (Nanjing University), Chang  Liu (Nanjing University), Xin  Jin (Peking University), Fu  Xiao (Nanjing University of Posts and Telecommunications), Qiao  Xiang (Xiamen University), Wanchun  Dou (Nanjing University), Guihai  Chen (Nanjing University)",
    "abstract": "Discrete-event simulation (DES) is a prevalent tool for evaluating network designs. Although DES offers full fidelity and generality, its slow performance limits its application. To speed up DES, many network simulators employ parallel discrete-event simulation (PDES). However, adapting existing network simulation models to PDES requires complex reconfigurations and often yields limited performance improvement. In this paper, we address this gap by proposing a parallel-efficient and user-transparent network simulation kernel, Unison, that adopts fine-grained partition and load-adaptive scheduling optimized for network scenarios. We prototype Unison based on ns-3. Existing network simulation models of ns-3 can be seamlessly transitioned to Unison. Testbed experiments on commodity servers demonstrate that Unison can achieve a 40× speedup over DES using 24 CPU cores, and a 10× speedup compared with existing PDES algorithms under the same CPU cores.",
    "link": "https://www.semanticscholar.org/paper/5acfcf2bbaac76d7f5aa0aa6034727628a23ff75",
    "session_title": "Session B: Networks",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "99d75729-f8a2-4bd3-aed6-df7efcd93c80"
  },
  {
    "title": "Serialization/Deserialization-free State Transfer in Serverless Workflows",
    "authors": "Fangming  Lu (Shanghai Jiao Tong University), Xingda  Wei (Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory), Zhuobin  Huang (University of Electronic Science and Technology of China), Rong  Chen (Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory), Mingyu  Wu (Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory), Haibo  Chen (Shanghai Jiao Tong University)",
    "abstract": "Serialization and deserialization play a dominant role in the state transfer time of serverless workflows, leading to substantial performance penalties during workflow execution. We identify the key reason as a lack of ability to efficiently access the (remote) memory of another function. We propose RMMap, an OS primitive for remote memory map. It allows a serverless function to directly access the memory of another function, even if it is located remotely. RMMap is the first to completely eliminates serialization and deserialization when transferring states between any pairs of functions in (unmodified) serverless workflows. To make remote memory map efficient and feasible, we co-design it with fast networking (RDMA), OS, language runtime, and serverless platform. Evaluations using real-world serverless workloads show that integrating RMMap with Knative reduces the serverless workflow execution time on Knative by up to 2.6 × and improves resource utilizations by 86.3%.",
    "link": "https://www.semanticscholar.org/paper/0999ed6bb10bd2ce3b4076e2a1a70bdae3f3285d",
    "session_title": "Session B: Networks",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "3ff9909d-a0fd-4fc5-b15c-1b673d20ef8f"
  },
  {
    "title": "Occam: A Programming System for Reliable Network Management",
    "authors": "Jiarong  Xing (Rice University), Kuo-Feng  Hsu (Meta), Yiting  Xia (Max Planck Institute for Informatics), Yan  Cai (Meta), Yanping  Li (Meta), Ying  Zhang (Meta), Ang  Chen (University of Michigan)",
    "abstract": "The complexity of large networks makes their management a daunting task. State-of-the-art network management tools use workflow systems for automation, but they do not adequately address the substantial challenges in operation reliability. This paper presents Occam, a programming system that simplifies the development of reliable network management tasks. We leverage the fact that most modern network management systems are backed with a source-of-truth database, and thus customize database techniques to the context of network management. Occam exposes an easy-to-use programming model for network operators to express the key management logic, while shielding them from reliability concerns, such as operational conflicts and task atomicity. Instead, the Occam runtime provides these reliability guardrails automatically. Our evaluation demonstrates Occam's effectiveness in simplifying management tasks, minimizing network vulnerable time and assisting with failure recovery.",
    "link": "https://www.semanticscholar.org/paper/3bc84ed45d15cb91720557f8d6f21dbe38565cb3",
    "session_title": "Session B: Networks",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "b51fc53f-57a8-4bd4-81c4-fa3534d4fb07"
  },
  {
    "title": "Aceso: Efficient Parallel DNN Training through Iterative Bottleneck Alleviation",
    "authors": "Guodong  Liu (Institute of Computing Technology, CAS), Youshan  Miao (Microsoft Research), Zhiqi  Lin (University of Science and Technology of China), Xiaoxiang  Shi (Shanghai Jiao Tong University), Saeed  Maleki (Microsoft Research), Fan  Yang (Microsoft Research), Yungang  Bao (Institute of Computing Technology, CAS), Sa  Wang (Institute of Computing Technology, CAS)",
    "abstract": "Many parallel mechanisms, including data parallelism, tensor parallelism, and pipeline parallelism, have been proposed and combined together to support training increasingly large deep neural networks (DNN) on massive GPU devices. Given a DNN model and GPU cluster, finding the optimal configuration by combining these parallelism mechanisms is an NP-hard problem. Widely adopted mathematical programming approaches have been proposed to search in a configuration subspace, but they are still too costly when scaling to large models over numerous devices. Aceso is a scalable parallel-mechanism auto-configuring system that operates iteratively. For a given parallel configuration, Aceso identifies a performance bottleneck and then, by summarizing all possible configuration adjustments with their resource consumption changes, infers their performance impacts to the bottleneck and selects one that mitigates the bottleneck. This process repeats for many iterations until a desired final configuration is found. Unlike mathematical programming approaches that examine the configurations subspace to find the optimal solution, Aceso searches in the configuration space in a stochastic approach by repeatedly identifying and alleviating bottlenecks. Aceso significantly reduces configuration searching cost by taking the approach of resolving one bottleneck at a time. This allows Aceso to find configurations that would be usually missed in subspace search approaches. We implemented and tested Aceso on representative DNN models. Evaluations show that it can scale to 1K-layer models. Compared to state-of-the-art systems, Aceso achieves up to 1.33× throughput improvement with less than 5% of the searching cost.",
    "link": "https://www.semanticscholar.org/paper/bbe2e5cbcb7abaed960708f10fe1c160663b7293",
    "session_title": "Session C: Distributed ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "7ef37441-b39e-4437-b26b-101d4170e41a"
  },
  {
    "title": "Totoro: A Scalable Federated Learning Engine for the Edge",
    "authors": "Cheng-Wei  Ching (University of California Santa Cruz), Xin  Chen (Georgia Institute of Technology), Taehwan  Kim (Virginia Tech), Bo  Ji (Virginia Tech), Qingyang  Wang (Louisiana State University), Dilma  Da Silva (Texas A&M University), Liting  Hu (University of California Santa Cruz and Virginia Tech)",
    "abstract": "Federated Learning (FL) is an emerging distributed machine learning (ML) technique that enables in-situ model training and inference on decentralized edge devices. We propose Totoro, a novel scalable FL engine, that enables massive FL applications to run simultaneously on edge networks. The key insight is to explore a distributed hash table (DHT)-based peer-to-peer (P2P) model to re-architect the centralized FL system design into a fully decentralized one. In contrast to previous studies where many FL applications shared one centralized parameter server, Totoro assigns a dedicated parameter server to each individual application. Any edge node can act as any application's coordinator, aggregator, client selector, worker (participant device), or any combination of the above, thereby radically improving scalability and adaptivity. Totoro introduces three innovations to realize its design: a locality-aware P2P multi-ring structure, a publish/subscribe-based forest abstraction, and a bandit-based exploitation-exploration path planning model. Real-world experiments on 500 Amazon EC2 servers show that Totoro scales gracefully with the number of FL applications and N edge nodes, speeds up the total training time by 1.2 × -14.0×, achieves O (logN) hops for model dissemination and gradient aggregation with millions of nodes, and efficiently adapts to the practical edge networks and churns.",
    "link": "https://vtechworks.lib.vt.edu/bitstreams/d489b7db-7e56-4ef3-879b-6ec1d22af353/download",
    "session_title": "Session C: Distributed ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "5b4945b9-a340-4b87-aee8-0e74bef6e971"
  },
  {
    "title": "FLOAT: Federated Learning Optimizations with Automated Tuning",
    "authors": "Ahmad Faraz  Khan (Virginia Tech), Azal Ahmad  Khan (Indian Institute of Technology, Guwahati), Ahmed M.  Abdelmoniem (Queen Mary University of London), Samuel  Fountain (University of Minnesota), Ali R.  Butt (Virginia Tech), Ali  Anwar (University of Minnesota)",
    "abstract": "Federated Learning (FL) has emerged as a powerful approach that enables collaborative distributed model training without the need for data sharing. However, FL grapples with inherent heterogeneity challenges leading to issues such as stragglers, dropouts, and performance variations. Selection of clients to run an FL instance is crucial, but existing strategies introduce biases and participation issues and do not consider resource efficiency. Communication and training acceleration solutions proposed to increase client participation also fall short due to the dynamic nature of system resources. We address these challenges in this paper by designing FLOAT, a novel framework designed to boost FL client resource awareness. FLOAT optimizes resource utilization dynamically for meeting training deadlines, and mitigates stragglers and dropouts through various optimization techniques; leading to enhanced model convergence and improved performance. FLOAT leverages multi-objective Reinforcement Learning with Human Feedback (RLHF) to automate the selection of the optimization techniques and their configurations, tailoring them to individual client resource conditions. Moreover, FLOAT seamlessly integrates into existing FL systems, maintaining non-intrusiveness and versatility for both asynchronous and synchronous FL settings. As per our evaluations, FLOAT increases accuracy by up to 53%, reduces client dropouts by up to 78×, and improves communication, computation, and memory utilization by up to 81×, 44×, and 20× respectively.",
    "link": "https://vtechworks.lib.vt.edu/bitstreams/a12ef872-83eb-4203-ba33-df906607107e/download",
    "session_title": "Session C: Distributed ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "ed48a8d2-1821-48d3-b309-3b3f8956b4e7"
  },
  {
    "title": "DeTA: Minimizing Data Leaks in Federated Learning via Decentralized and Trustworthy Aggregation",
    "authors": "Pau-Chen  Cheng (IBM Research), Kevin  Eykholt (IBM Research), Zhongshu  Gu (IBM Research), Hani  Jamjoom (IBM Research), K. R.  Jayaram (IBM Research), Enriquillo  Valdez (IBM Research), Ashish  Verma (Amazon Inc.)",
    "abstract": "Federated learning (FL) relies on a central authority to oversee and aggregate model updates contributed by multiple participating parties in the training process. This centralization of sensitive model updates naturally raises concerns about the trustworthiness of the central aggregation server, as well as the potential risks associated with server failures or breaches, which could result in loss and leaks of model updates. Moreover, recent attacks have demonstrated that, by obtaining the leaked model updates, malicious actors can even reconstruct substantial amounts of private data belonging to training participants. This underscores the critical necessity to rethink the existing FL system architecture to mitigate emerging attacks in the evolving threat landscape. One straightforward approach is to fortify the central aggregator with confidential computing (CC), which offers hardware-assisted protection for runtime computation and can be remotely verified for execution integrity. However, a growing number of security vulnerabilities have surfaced in tandem with the adoption of CC, indicating that depending solely on this singular defense may not provide the requisite resilience to thwart data leaks. To address the security challenges inherent in the centralized aggregation paradigm and enhance system resilience, we introduce DeTA, an FL system architecture that employs a decentralized and trustworthy aggregation strategy with a defense-in-depth design. In DeTA, FL parties locally divide and shuffle their model updates at the parameter level, creating random partitions designated for multiple aggregators, all of which are shielded within CC execution environments. Moreover, to accommodate the multi-aggregator FL ecosystem, we have implemented a two-phase authentication protocol that enables new parties to verify all CC-protected aggregators and establish secure channels to upstream their model updates. With DeTA, model aggregation algorithms can function without any alterations. However, each aggregator is now oblivious to model architectures, possessing only a fragmented and shuffled view of each model update. This approach effectively mitigates attacks aimed at tampering with the aggregation process or exploiting leaked model updates, while also preserving training accuracy and minimizing performance overheads.",
    "link": "https://www.semanticscholar.org/paper/d269e2a677165679e3b85a012518f029510a697b",
    "session_title": "Session C: Distributed ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "593d1f08-9773-49ec-9730-d1c7b77ba0c3"
  },
  {
    "title": "ScheMoE: An Extensible Mixture-of-Experts Distributed Training System with Tasks Scheduling",
    "authors": "Shaohuai  Shi (Harbin Institute of Technology, Shenzhen), Xinglin  Pan (The Hong Kong University of Science and Technology (Guangzhou)), Qiang  Wang (Harbin Institute of Technology, Shenzhen), Chengjian  Liu (Shenzhen Technology University), Xiaozhe  Ren (Huawei Central Research Institute, Huawei Technologies), Zhongzhe  Hu (Huawei Central Research Institute, Huawei Technologies), Yu  Yang (Huawei Central Research Institute, Huawei Technologies), Bo  Li (The Hong Kong University of Science and Technology), Xiaowen  Chu (The Hong Kong University of Science and Technology (Guangzhou))",
    "abstract": "In recent years, large-scale models can be easily scaled to trillions of parameters with sparsely activated mixture-of-experts (MoE), which significantly improves the model quality while only requiring a sub-linear increase in computational costs. However, MoE layers require the input data to be dynamically routed to a particular GPU for computing during distributed training. The highly dynamic property of data routing and high communication costs in MoE make the training system low scaling efficiency on GPU clusters. In this work, we propose an extensible and efficient MoE training system, ScheMoE, which is equipped with several features. 1) ScheMoE provides a generic scheduling framework that allows the communication and computation tasks in training MoE models to be scheduled in an optimal way. 2) ScheMoE integrates our proposed novel all-to-all collective which better utilizes intra- and inter-connect bandwidths. 3) ScheMoE supports easy extensions of customized all-to-all collectives and data compression approaches while enjoying our scheduling algorithm. Extensive experiments are conducted on a 32-GPU cluster and the results show that ScheMoE outperforms existing state-of-the-art MoE systems, Tutel and Faster-MoE, by 9%-30%.",
    "link": "https://www.semanticscholar.org/paper/83858f08aef77ced91428207b8891a82e7087cd2",
    "session_title": "Session C: Distributed ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "ed71d402-956e-4a27-87df-92a6eb3ac937"
  },
  {
    "title": "Dashing and Star: Byzantine Fault Tolerance with Weak Certificates",
    "authors": "Sisi  Duan (Tsinghua University), Haibin  Zhang (Yangtze Delta Region Institute of Tsinghua University, Zhejiang), Xiao  Sui (Shandong University), Baohan  Huang (Beijing Institute of Technology), Changchun  Mu (Digital Currency Institute, the People's Bank of China), Gang  Di (Digital Currency Institute, the People's Bank of China and Tsinghua University), Xiaoyun  Wang (Tsinghua University)",
    "abstract": "State-of-the-art Byzantine fault-tolerant (BFT) protocols assuming partial synchrony such as SBFT and HotStuff use regular certificates obtained from 2f + 1 (partial) signatures. We show that one can use weak certificates obtained from only f + 1 signatures to assist in designing more robust and more efficient BFT protocols. We design and implement two BFT systems: Dashing (a family of two HotStuff-style BFT protocols) and Star (a parallel BFT framework). We first present Dashingl that targets both efficiency and robustness using weak certificates. Dashingl is also network-adaptive in the sense that it can leverage network connection discrepancy to improve performance. We show that Dashing1 outperforms HotStuff in various failure-free and failure scenarios. We then present Dashing2 enabling a one-phase fast path by using strong certificates from 3f + 1 signatures. We then leverage weak certificates to build Star, a highly scalable BFT framework that delivers transactions from n - f replicas. Star compares favorably with existing protocols in terms of liveness, communication, state transfer, scalability, and/or robustness under failures. We demonstrate that Dashing achieves 47%-107% higher peak throughput than HotStuff for experiments on Amazon EC2. Meanwhile, unlike all known BFT protocols whose performance degrades as f grows large, the peak throughput of Star increases as f grows. When deployed in a WAN with 91 replicas across five continents, Star achieves an impressive throughput of 256 ktx/sec, 2.38x that of Narwhal.",
    "link": "https://www.semanticscholar.org/paper/f5dc7940e6630e83a876ff2a11e9346703ead403",
    "session_title": "Session D: BFT, Serverless, Cloud and Edge",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "45739a61-3fc8-4c59-a98b-d5f304b82900"
  },
  {
    "title": "Bandle: Asynchronous State Machine Replication Made Efficient",
    "authors": "Bo  Wang (Central South University and Shanghai Jiao Tong University), Shengyun  Liu (Shanghai Jiao Tong University), He  Dong (Shanghai Jiao Tong University), Xiangzhe  Wang (Shanghai Jiao Tong University), Wenbo  Xu (AntChain Platform Division, Ant Group), Jingjing  Zhang (Fudan University), Ping  Zhong (Central South University), Yiming  Zhang (XMU)",
    "abstract": "State machine replication (SMR) uses consensus as its core component for reaching agreement among a group of processes, in order to provide fault-tolerant services. Most SMR protocols, such as Paxos and Raft, are designed in the partial synchrony model. Partially synchronous protocols rely on timing assumptions to elect a special role (such as the leader), which may become the performance bottleneck under a heavy workload. From an engineering perspective, partially synchronous protocols have to wait for a pre-defined period of time and implement a (complicated) failover mechanism in order to replace the faulty leader. In contrast, asynchronous protocols are immune to such problems. This paper presents Bandle, a simple and highly efficient asynchronous SMR protocol. Instead of electing a special role, Bandle evenly assigns sequence numbers to each process and proceeds in a leaderless manner. We further propose a binary agreement protocol, referred to as FlashBA, which decides whether a given proposal can be committed. FlashBA is inspired by Ben-Or's randomized algorithm but leverages a promise mechanism to achieve optimal latency (i.e., one message delay in the best case). An empirical study on the Amazon EC2 platform shows that Bandle delivers exceptional performance when deployed within a data center and across the globe.",
    "link": "https://www.semanticscholar.org/paper/ebfdbb669c06e1ad3220565930b44871b89ca2bf",
    "session_title": "Session D: BFT, Serverless, Cloud and Edge",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "a5cee108-82d6-4689-8137-d711063c55cc"
  },
  {
    "title": "Characterization and Reclamation of Frozen Garbage in Managed FaaS Workloads",
    "authors": "Ziming  Zhao (Shanghai Jiao Tong University), Mingyu  Wu (Shanghai Jiao Tong University), Haibo  Chen (Shanghai Jiao Tong University), Binyu  Zang (Shanghai Jiao Tong University)",
    "abstract": "FaaS (function-as-a-service) is becoming a popular workload in cloud environments due to its virtues such as auto-scaling and pay-as-you-go. High-level languages like JavaScript and Java are commonly used in FaaS for programmability, but their managed runtimes complicate memory management in the cloud. This paper first observes the issue of frozen garbage, which is caused by freezing cached function instances where their threads have been paused but the unused memory (e.g., garbage) is not reclaimed due to the semantic gap between FaaS and the managed runtime. This paper presents the first characterization of the negative effects induced by frozen garbage with various functions, which uncovers that it can occupy more than half of FaaS instances' memory resources on average. To this end, this paper proposes Desiccant, a freeze-aware memory manager for managed workloads in FaaS, which reclaims idle memory resources consumed by frozen garbage from managed runtime instances and thus notably improves memory efficiency. The evaluation on various FaaS workloads shows that Desiccant can reduce FaaS functions' peak memory consumption by up to 6.72×. Such saved memory consumption allows caching more FaaS instances to reduce the frequency of cold boots (creating instances before function execution) and p99 latency by up to 4.49× and 37.5%, respectively.",
    "link": "https://www.semanticscholar.org/paper/04a9ddc10b1113d965519172f7afc07947839c56",
    "session_title": "Session D: BFT, Serverless, Cloud and Edge",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "a80fe400-10f9-4b1d-8904-bd8125d53054"
  },
  {
    "title": "Pronghorn: Effective Checkpoint Orchestration for Serverless Hot-Starts",
    "authors": "Sumer  Kohli (Stanford University), Shreyas  Kharbanda (Cornell University), Rodrigo  Bruno (INESC-ID, IST, ULisboa), Joao  Carreira (University of California, Berkeley), Pedro  Fonseca (Purdue University)",
    "abstract": "Serverless computing allows developers to deploy and scale stateless functions in ephemeral workers easily. As a result, serverless computing has been widely used for many applications, such as computer vision, video processing, and HTML generation. However, we find that the stateless nature of serverless computing wastes many of the important benefits modern language runtimes have to offer. A notable example is the extensive profiling and Just-in-Time (JIT) compilation effort that runtimes implement to achieve acceptable performance of popular high-level languages, such as Java, JavaScript, and Python. Unfortunately, when modern language runtimes are naively adopted in serverless computing, all of these efforts are lost upon worker eviction. Checkpoint-restore methods alleviate the problem by resuming workers from snapshots taken after initialization. However, production-grade language runtimes can take up to thousands of invocations to fully optimize a single function, thus rendering naive checkpoint-restore policies ineffective. This paper proposes Pronghorn, a snapshot serverless orchestrator that automatically monitors the function performance and decides (1) when it is the right moment to take a snapshot and (2) which snapshot to use for new workers. Pronghorn is agnostic to the underlying platform and JIT runtime, thus easing its integration into existing runtimes and worker deployment environments (container, virtual machine, etc.). On a set of representative serverless benchmarks, Pronghorn provides end-to-end median latency improvements of 37.2% across 9 out of 13 benchmarks (20-58% latency reduction) when compared to state-of-art checkpointing policies.",
    "link": "https://www.semanticscholar.org/paper/7717a43059cecc8d40651f83187a77d8cf8089a5",
    "session_title": "Session D: BFT, Serverless, Cloud and Edge",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "9a69c7de-3d7f-474c-b210-8171aa2a2994"
  },
  {
    "title": "Improving Resource and Energy Efficiency for Cloud 3D through Excessive Rendering Reduction",
    "authors": "Tianyi  Liu (University of Texas at San Antonio), Jerry  Lucas (University of Texas at San Antonio), Sen  He (University of Arizona), Tongping  Liu (University of Massachusetts Amherst), Xiaoyin  Wang (University of Texas at San Antonio), Wei  Wang (University of Texas at San Antonio)",
    "abstract": "The rise of cloud gaming makes interactive 3D applications an emerging type of data center workload. However, the excessive rendering in current cloud 3D systems leads to large gaps between the cloud and client frame rates (FPS, frames per second), thus wasting resources and power. Although FPS regulation can remove excessive rendering, due to the highly-varying frame processing time and the use of rendering delays, existing cloud FPS regulation solutions have low FPS and slow motion-to-photon (MtP) latency, causing violations of Quality-of-Service (QoS) requirements. In this paper, we present a novel cloud FPS regulation solution, called OnDemand Rendering (ODR). ODR employs multi-buffering, dynamic rendering delay/acceleration, and input processing prioritization to reduce excessive rendering and ensure QoS satisfaction. ODR was evaluated in our private cloud and Google cloud. Evaluation results showed that ODR effectively removed excessive rendering, thus improving DRAM performance by 19% and reducing power usage by 16% over no FPS regulation. Better memory efficiency also allowed ODR to increase client FPS by 5.5%. Moreover, ODR reduced average MtP latency by more than 92% and outperformed existing FPS regulations. More importantly, ODR's high FPS and low latency make it feasible to deploy 3D applications to conventional public clouds.",
    "link": "https://www.semanticscholar.org/paper/505c66dfa2076c28b8d82352dc3721234d899f18",
    "session_title": "Session D: BFT, Serverless, Cloud and Edge",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "f17d2184-fa52-4c1a-ab17-b2d22d691ebc"
  },
  {
    "title": "Draconis: Network-Accelerated Scheduling for Microsecond-Scale Workloads",
    "authors": "Sreeharsha  Udayashankar (University of Waterloo), Ashraf  Abdel-Hadi (University of Waterloo), Ali  Mashtizadeh (University of Waterloo), Samer  Al-Kiswany (University of Waterloo)",
    "abstract": "We present Draconis, a novel scheduler for workloads in the range of tens to hundreds of microseconds. Draconis challenges the popular belief that programmable switches cannot house the complex data structures, such as queues, needed to support an in-network scheduler. Using programmable switches, Draconis achieves the low scheduling tail latency and high throughput needed to support these microsecond-scale workloads on large clusters. Furthermore, Draconis supports a wide range of complex scheduling policies, including locality-aware scheduling, priority-based scheduling, and resource-based scheduling. Draconis reduces the 99th percentile scheduling latencies by 3×-200× when compared to state-of-the-art software-based and network-accelerated schedulers, on a range of synthetic workloads. Our evaluation also demonstrates that Draconis has 52× higher throughput than server-based scheduling systems.",
    "link": "https://www.semanticscholar.org/paper/131fa149215b0a8fc9e374ed9cf2c29ffa0fb20b",
    "session_title": "Session D: BFT, Serverless, Cloud and Edge",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "502c3b70-db60-4785-9de1-f94345d31969"
  },
  {
    "title": "Snatch: Online Streaming Analytics at the Network Edge",
    "authors": "Yunming  Xiao (Northwestern University), Yibo  Zhao (Boston University), Sen  Lin (Northwestern University), Aleksandar  Kuzmanovic (Northwestern University)",
    "abstract": "In recent years, we have witnessed a growing trend of content hyper-giants deploying server infrastructure and services close to end-users, in \"eyeball\" networks. Still, one of the services that remained largely unaffected by this trend is online streaming analytics. This is despite the fact that most of the \"big data\" is received in real time and is most valuable at the time of arrival. The inability to process requests at the network edge is caused by a common setting where user profiles, necessary for analytics, are stored deep in the data center back-ends. This setting also carries privacy concerns as such user profiles are individually identifiable, yet the users are almost blind to what data is associated with their identities and how the data is analyzed. In this paper, we revise this arrangement, and plant encrypted semantic cookies at the user end. Without altering any of the existing protocols, this enables capturing and analytically pre-processing user requests soon after they are generated, at edge ISPs or content providers' off-nets. In addition, it ensures user anonymity perseverance during the analytics. We design and implement Snatch, a QUIC-based streaming analytics prototype, and demonstrate that it speeds up user analytics by up to 200x, and by 10-30x in the common case.",
    "link": "https://www.semanticscholar.org/paper/3f8df8f5d0bcd13110e5159e4802fbac8b5816ce",
    "session_title": "Session D: BFT, Serverless, Cloud and Edge",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "79601f23-4723-42ef-ab88-b2d0fbaa3961"
  },
  {
    "title": "Blaze: Holistic Caching for Iterative Data Processing",
    "authors": "Won Wook  SONG (FriendliAI), Jeongyoon  Eo (Seoul National University), Taegeon  Um (Samsung Research), Myeongjae  Jeon (UNIST), Byung-Gon  Chun (Seoul National University and FriendliAI)",
    "abstract": "Modern data processing workloads, such as machine learning and graph processing, involve iterative computations to converge generated models into higher accuracy. An effective caching mechanism is vital to expedite iterative computations since the intermediate data that needs to be stored in memory grows larger over iterations, often exceeding the memory capacity. However, existing systems handle intermediate data through separate operational layers (e.g., caching, eviction, and recovery), with each layer working independently in a greedy or cost-agnostic manner. These layers typically rely on user annotations and past access patterns, failing to make globally optimal decisions for the workload. To overcome these limitations, Blaze introduces a unified caching mechanism that integrates the separate operational layers. Blaze dynamically captures the workload structure and metrics using profiling and inductive regression, and automatically estimates the potential data caching efficiency associated with different operational decisions based on the profiled information. To achieve this goal, Blaze incorporates potential data recovery costs across stages into a single cost optimization function, which informs the optimal partition state and location. This approach reduces the significant disk I/O overheads caused by oversized partitions and the recomputation overheads for partitions with long lineages, while efficiently utilizing the constrained memory space. Our evaluations demonstrate that Blaze can accelerate end-to-end application completion time by 2.02 - 2.52× compared to recomputation-based MEM_ONLY Spark, and by 1.08 - 2.86× compared to checkpoint-based MEM+DISK Spark, while reducing the cache data stored on disk by 95% on average.",
    "link": "https://www.semanticscholar.org/paper/c05af9286bd306bf148818428f7502edcdaa1aa7",
    "session_title": "Session E: Caching, Databases, Data Processing",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "4968d992-cbe5-441b-bd55-d0b5dcba706e"
  },
  {
    "title": "TTLs Matter: Efficient Cache Sizing with TTL-Aware Miss Ratio Curves and Working Set Sizes",
    "authors": "Sari  Sultan (University of Toronto), Kia  Shakiba (University of Toronto), Albert  Lee (University of Toronto), Paul  Chen (Huawei), Michael  Stumm (University of Toronto)",
    "abstract": "In-memory caches play a pivotal role in optimizing distributed systems by significantly reducing query response times. Correctly sizing these caches is critical, especially considering that prominent organizations use terabytes and even petabytes of DRAM for these caches. The Miss Ratio Curve (MRC) and Working Set Size (WSS) are the most widely used tools for sizing these caches. Modern cache workloads employ Time-to-Live (TTL) limits to define the lifespan of cached objects, a feature essential for ensuring data freshness and adhering to regulations like GDPR. Surprisingly, none of the existing MRC and WSS tools accommodate TTLs. Based on 28 real-world cache workloads that contain 113 billion accesses, we show that taking TTL limits into consideration allows an average of 69% (and up to 99%) lower memory footprint for in-memory caches without a degradation in the hit rate. This paper describes how TTLs can be integrated into today's most important MRC generation and WSS estimation algorithms. We also describe how the widely used HyperLogLog (HLL) cardinality estimator can be extended to accommodate TTLs, and show how it can be used to efficiently estimate the WSS. Our extended algorithms maintain comparable performance levels to the original algorithms. All our extended approximate algorithms are efficient, run in constant space, and enable more resource-efficient and cost-effective cache management.",
    "link": "https://www.semanticscholar.org/paper/d91dd5a2141541e0449c9cd85bd4a2b5a0f75419",
    "session_title": "Session E: Caching, Databases, Data Processing",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "c4ab4fc5-1caf-4dd6-9c33-6ca67c15ae7e"
  },
  {
    "title": "Trinity: A Fast Compressed Multi-attribute Data Store",
    "authors": "Ziming  Mao (UC Berkeley), Kiran  Srinivasan (NetApp), Anurag  Khandelwal (Yale University)",
    "abstract": "With the proliferation of attribute-rich machine-generated data, emerging real-time monitoring, diagnosis, and visualization tools ingest and analyze such data across multiple attributes simultaneously. Due to the sheer volume of the data, applications need storage-efficient and performant data representations to analyze them efficiently. We present TRINITY, a system that simultaneously facilitates query and storage efficiency across large volumes of multi-attribute records. Trinity accomplishes this through a new dynamic, succinct, multi-dimensional data structure, MdTrie. MdTrie employs a combination of novel Morton code generalization, a multi-attribute query algorithm, and a self-indexed trie structure to achieve the above goals. Our evaluation of TRINITY for real-world use-cases shows that compared to state-of-the-art systems, it supports (1) 7.2-59.6× faster multi-attribute searches, (2) storage footprint comparable to OLAP columnar stores and 4.8-15.1× lower than NoSQL stores and OLTP databases, and (3) point query throughput comparable to NoSQL stores and 1.7-52.5× higher than OLTP databases and OLAP columnar stores.",
    "link": "https://www.semanticscholar.org/paper/38535313ef7c82cc5128204bff9ff527353f8ff3",
    "session_title": "Session E: Caching, Databases, Data Processing",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "f82af47b-8043-4b77-b2f4-0d594f7120ce"
  },
  {
    "title": "FLOWS: Balanced MRC Profiling for Heterogeneous Object-Size Cache",
    "authors": "Xiaojun  Guo (Huazhong University of Science and Technology,Tencent Inc.), Hua  Wang (Huazhong University of Science and Technology), Ke  Zhou (Huazhong University of Science and Technology), Hong  Jiang (University of Texas at Arlington), Yaodong  Han (Huazhong University of Science and Technology), Guangjie  Xing (Huazhong University of Science and Technology)",
    "abstract": "While Miss Ratio Curve (MRC) profiling methods based on spatial sampling are effective in modeling cache behaviors, previous MRC studies lack in-depth analysis of profiling errors and primarily target homogeneous object-size scenarios. This has caused imbalanced errors of existing MRC approaches when employed in heterogeneous object-size caches. For instance, in CDN traces, the error of the Byte Miss Ratio Curve (BMRC) could be two orders of magnitude larger than that of the Object Miss Ratio Curve (OMRC). In this paper, we reveal an important insight from our experimental analysis, namely, the source of profiling inaccuracy is twofold, the \"imbalanced requests\" and the heterogeneous object-size distribution. To this end, we propose FLOWS, a Filtered LOw-variance Weighted Sampling approach, to address the root causes of the problem by combining a Cache Filter, designed to balance sampled requests, with a Weighted Sampling technique, designed to reduce bytelevel estimation error. FLOWS constructs a more accurate MRC for traces with heterogeneous content popularity and object sizes. Evaluation on real-world traces demonstrates that FLOWS reduces the error of the BMRC and OMRC profiling by 16× and 3×, respectively, compared with state-of-the-art approaches. Additionally, FLOWS enables cache systems to effectively balance the Byte Hit Ratio (BHR) and Object Hit Ratio (OHR), achieving an improvement of up to 26.5% in overall hit rate compared to other methods.",
    "link": "https://www.semanticscholar.org/paper/c2b9be63bc94fb95361293f2600a5c37d409ed9f",
    "session_title": "Session E: Caching, Databases, Data Processing",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "cf5be4a2-680d-4ca9-afcc-e00c3e2565ab"
  },
  {
    "title": "CCL-BTree: A Crash-Consistent Locality-Aware B+-Tree for Reducing XPBuffer-Induced Write Amplification in Persistent Memory",
    "authors": "Zhenxin  Li (Zhejiang University), Shuibing  He (Zhejiang University), Zheng  Dang (Zhejiang University), Peiyi  Hong (Zhejiang University), Xuechen  Zhang (Washington State University Vancouver), Rui  Wang (Zhejiang University), Fei  Wu (Zhejiang University)",
    "abstract": "In persistent B+ -Tree, random updates of small key-value (KV) pairs will cause severe XPBuffer-induced write amplification (XBI-amplification) because CPU cacheline size is smaller than media access granularity in persistent memory (PM). We observe that XBI-amplification directly determines the application performance when the PM bandwidth is exhausted in multi-thread scenarios. However, none of the existing work can efficiently address the XBI-amplification issue while maintaining superior range query performance. In this paper, we design a novel crash-consistent locality-aware B+-Tree (CCL-BTree). It preserves the key order between adjacent leaf nodes for efficient range query and proposes a leaf-node centric buffering strategy that merges writes and then flushes them together to reduce the number of flushes to the PM media. For crash-consistency, all the buffered KVs are recorded in write-ahead logs. CCL-BTree further devises write-conservative logging to skip unnecessary log operations, and locality-aware garbage collection to avoid random PM writes in reclaiming log data. Our experiments show that CCL-BTree reduces the XBI-amplification by up to 81%, improves the insert throughput by up to 9.35×, and achieves good range query performance compared to state-of-the-art persistent B+-Trees.",
    "link": "https://www.semanticscholar.org/paper/455958a0e1a9e16bcba96466c7f5b996217299a3",
    "session_title": "Session E: Caching, Databases, Data Processing",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "0c41e4c1-2803-4ca5-b9f8-6879d7ecc2e8"
  },
  {
    "title": "Wormhole Filters: Caching Your Hash on Persistent Memory",
    "authors": "Hancheng  Wang (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China), Haipeng  Dai (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China), Rong  Gu (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China), Youyou  Lu (Tsinghua University), Jiaqi  Zheng (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China), Jingsong  Dai (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China), Shusen  Chen (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China), Zhiyuan  Chen (Huawei Technologies Co., Ltd.), Shuaituan  Li (Huawei Technologies Co., Ltd.), Guihai  Chen (State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China)",
    "abstract": "Approximate membership query (AMQ) data structures can approximately determine whether an element is in the set with high efficiency. They are widely used in distributed systems, database systems, bioinformatics, IoT applications, data stream mining, etc. However, the memory consumption of AMQ data structures grows rapidly as the data scale grows, which limits the system's ability to process a massive amount of data. The emerging persistent memory provides a close-to-DRAM access speed and terabyte-level capacity, facilitating AMQ data structures to handle massive data. Nevertheless, existing AMQ data structures perform poorly on persistent memory due to intensive random accesses and/or sequential writes. Therefore, we propose a novel AMQ data structure called wormhole filter, which achieves high performance on persistent memory by reducing random accesses and sequential writes. In addition, we reduce the number of log records for lower recovery overhead. Theoretical analysis and experimental results show that wormhole filters significantly outperform competitive state-of-the-art AMQ data structures. For example, wormhole filters achieve 23.26× insertion throughput, 1.98× positive lookup throughput, and 8.82× deletion throughput of the best competing baseline.",
    "link": "https://www.semanticscholar.org/paper/10b52ab84324f0c723092daefd59dedfbc919681",
    "session_title": "Session E: Caching, Databases, Data Processing",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "d1662ef1-3ba5-459d-8b9d-7bca7fa52644"
  },
  {
    "title": "Dordis: Efficient Federated Learning with Dropout-Resilient Differential Privacy",
    "authors": "Zhifeng  Jiang (Hong Kong University of Science and Technology), Wei  Wang (Hong Kong University of Science and Technology), Ruichuan  Chen (Nokia Bell Labs)",
    "abstract": "Federated learning (FL) is increasingly deployed among multiple clients to train a shared model over decentralized data. To address privacy concerns, FL systems need to safeguard the clients' data from disclosure during training and control data leakage through trained models when exposed to untrusted domains. Distributed differential privacy (DP) offers an appealing solution in this regard as it achieves a balanced tradeoff between privacy and utility without a trusted server. However, existing distributed DP mechanisms are impractical in the presence of client dropout, resulting in poor privacy guarantees or degraded training accuracy. In addition, these mechanisms suffer from severe efficiency issues. We present Dordis, a distributed differentially private FL framework that is highly efficient and resilient to client dropout. Specifically, we develop a novel 'add-then-remove' scheme that enforces a required noise level precisely in each training round, even if some sampled clients drop out. This ensures that the privacy budget is utilized prudently, despite unpredictable client dynamics. To boost performance, Dordis operates as a distributed parallel architecture via encapsulating the communication and computation operations into stages. It automatically divides the global model aggregation into several chunk-aggregation tasks and pipelines them for optimal speedup. Large-scale deployment evaluations demonstrate that Dordis efficiently handles client dropout in various realistic FL scenarios, achieving the optimal privacy-utility tradeoff and accelerating training by up to 2.4× compared to existing solutions.",
    "link": "https://arxiv.org/pdf/2209.12528",
    "session_title": "Session F: Privacy in ML + Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "a5ec23c2-1672-4b20-9417-33d27a93e60a"
  },
  {
    "title": "Accelerating Privacy-Preserving Machine Learning With GeniBatch",
    "authors": "Xinyang  Huang (Hong Kong University of Science and Technology), Junxue  Zhang (Hong Kong University of Science and Technology), Xiaodian  Cheng (Hong Kong University of Science and Technology), Hong  Zhang (University of Waterloo), Yilun  Jin (The Hong Kong University of Science and Technology), Shuihai  Hu (The Hong Kong University of Science and Technology), Han  Tian (Hong Kong University of Science and Technology), Kai  Chen (Hong Kong University of Science and Technology)",
    "abstract": "Cross-silo privacy-preserving machine learning (PPML) adopt; Partial Homomorphic Encryption (PHE) for secure data combination and high-quality model training across multiple organizations (e.g., medical and financial). However, PHE introduces significant computation and communication overheads due to data inflation. Batch optimization is an encouraging direction to mitigate the problem by compressing multiple data into a single ciphertext. While promising, it is impractical for a large number of cross-silo PPML applications due to the limited vector operations support and severe data corruption. In this paper, we present GeniBatch, a batch compiler that translates a PPML program with PHE into an efficient program with batch optimization. GeniBatch adopts a set of conversion rules to allow PHE programs involving all vector operations required in cross-silo PPML and ensures end-to-end result consistency before/after compiling. By proposing bit-reserving algorithms, GeniBatch avoids bit-overflow for the correctness of compiled programs and maximizes the compression ratio. We have integrated GeniBatch into FATE, a representative cross-silo PPML framework, and provided SIMD APIs to harness hardware acceleration. Experiments across six popular applications show that GeniBatch achieves up to 22.6× speedup and reduces network traffic by 5.4×-23.8× for generic cross-silo PPML applications.",
    "link": "https://www.semanticscholar.org/paper/2a46412d139900d52eba0622501032eeaaf88da1",
    "session_title": "Session F: Privacy in ML + Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "c19091a0-78cd-4008-8ec9-f86301c71b35"
  },
  {
    "title": "GMorph: Accelerating Multi-DNN Inference via Model Fusion",
    "authors": "Qizheng  Yang (University of Massachusetts Amherst), Tianyi  Yang (University of Massachusetts Amherst), Mingcan  Xiang (University of Massachusetts Amherst), Lijun  Zhang (University of Massachusetts Amherst), Haoliang  Wang (Adobe Research), Marco  Serafini (University of Massachusetts Amherst), Hui  Guan (University of Massachusetts Amherst)",
    "abstract": "AI-powered applications often involve multiple deep neural network (DNN)-based prediction tasks to support application-level functionalities. However, executing multi-DNNs can be challenging due to the high resource demands and computation costs that increase linearly with the number of DNNs. Multi-task learning (MTL) addresses this problem by designing a multi-task model that shares parameters across tasks based on a single backbone DNN. This paper explores an alternative approach called model fusion: rather than training a single multi-task model from scratch as MTL does, model fusion fuses multiple task-specific DNNs that are pre-trained separately and can have heterogeneous architectures into a single multi-task model. We materialize model fusion in a software framework called GMorph to accelerate multi-DNN inference while maintaining task accuracy. GMorph features three main technical contributions: graph mutations to fuse multi-DNNs into resource-efficient multi-task models, search-space sampling algorithms, and predictive filtering to reduce the high search costs. Our experiments show that GMorph can outperform MTL baselines and reduce the inference latency of multi-DNNs by 1.1-3× while meeting the target task accuracy.",
    "link": "https://www.semanticscholar.org/paper/2ed2c19159a6ccbe429225f7230bc18502833717",
    "session_title": "Session F: Privacy in ML + Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "feebd245-a451-4747-8f74-17b245aa9231"
  },
  {
    "title": "HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis",
    "authors": "Shiwei  Zhang (The University of Hong Kong), Lansong  Diao (Alibaba Group), Chuan  Wu (The University of Hong Kong), Zongyan  Cao (Alibaba Group), Siyu  Wang (Alibaba Group), Wei  Lin (Alibaba Group)",
    "abstract": "Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted to train large deep neural networks (DNNs). Few studies have explored its applicability on heterogeneous clusters, to fully exploit available resources for large model learning. This paper presents HAP, an automated system designed to expedite SPMD DNN training on heterogeneous clusters. HAP jointly optimizes the tensor sharding strategy, sharding ratios across heterogeneous devices and the communication methods for tensor exchanges for optimized distributed training with SPMD parallelism. We novelly formulate model partitioning as a program synthesis problem, in which we generate a distributed program from scratch on a distributed instruction set that semantically resembles the program designed for a single device, and systematically explore the solution space with an A-based search algorithm. We derive the optimal tensor sharding ratios by formulating it as a linear programming problem. Additionally, HAP explores tensor communication optimization in a heterogeneous cluster and integrates it as part of the program synthesis process, for automatically choosing optimal collective communication primitives and applying sufficient factor broadcasting technique. Extensive experiments on representative workloads demonstrate that HAP achieves up to 2.41x speed-up on heterogeneous clusters.",
    "link": "https://arxiv.org/pdf/2401.05965",
    "session_title": "Session F: Privacy in ML + Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "9e3c28d9-a00d-44f1-8dbf-9121e01c1122"
  },
  {
    "title": "DynaPipe: Optimizing Multi-task Training through Dynamic Pipelines",
    "authors": "Chenyu  Jiang (The University of Hong Kong), Zhen  Jia (Amazon Web Services), Shuai  Zheng (Boson AI), Yida  Wang (Amazon Web Services), Chuan  Wu (The University of Hong Kong)",
    "abstract": "Multi-task model training has been adopted to enable a single deep neural network model (often a large language model) to handle multiple tasks (e.g., question answering and text summarization). Multi-task training commonly receives input sequences of highly different lengths due to the diverse contexts of different tasks. Padding (to the same sequence length) or packing (short examples into long sequences of the same length) is usually adopted to prepare input samples for model training, which is nonetheless not space or computation efficient. This paper proposes a dynamic micro-batching approach to tackle sequence length variation and enable efficient multi-task model training. We advocate pipelineparallel training of the large model with variable-length micro-batches, each of which potentially comprises a different number of samples. We optimize micro-batch construction using a dynamic programming-based approach, and handle micro-batch execution time variation through dynamic pipeline and communication scheduling, enabling highly efficient pipeline training. Extensive evaluation on the FLANv2 dataset demonstrates up to 4.39x higher training throughput when training T5, and 3.25x when training GPT, as compared with packing-based baselines. DynaPipe's source code is publicly available at https://github.com/awslabs/optimizing-multitask-training-through-dynamic-pipelines.",
    "link": "https://arxiv.org/pdf/2311.10418",
    "session_title": "Session F: Privacy in ML + Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "41f20ebc-872a-450c-ba58-09b2485a7bd7"
  },
  {
    "title": "ZKML: An Optimizing System for ML Inference in Zero-Knowledge Proofs",
    "authors": "Bing-Jyue  Chen (UIUC), Suppakit  Waiwitlikhit (Stanford), Ion  Stoica (UC Berkeley), Daniel  Kang (UIUC)",
    "abstract": "Machine learning (ML) is increasingly used behind closed systems and APIs to make important decisions. For example, social media uses ML-based recommendation algorithms to decide what to show users, and millions of people pay to use ChatGPT for information every day. Because ML is deployed behind these closed systems, there are increasing calls for transparency, such as releasing model weights. However, these service providers have legitimate reasons not to release this information, including for privacy and trade secrets. To bridge this gap, recent work has proposed using zero-knowledge proofs (specifically a form called ZK-SNARKs) for certifying computation with private models but has only been applied to unrealistically small models. In this work, we present the first framework, ZKML, to produce ZK-SNARKs for realistic ML models, including state-of-the-art vision models, a distilled GPT-2, and the ML model powering Twitter's recommendations. We accomplish this by designing an optimizing compiler from TensorFlow to circuits in the halo2 ZK-SNARK proving system. There are many equivalent ways to implement the same operations within ZK-SNARK circuits, and these design choices can affect performance by 24×. To efficiently compile ML models, ZKML contains two parts: gadgets (efficient constraints for low-level operations) and an optimizer to decide how to lay out the gadgets within a circuit. Combined, these optimizations enable proving on a wider range of models, faster proving, faster verification, and smaller proofs compared to prior work.",
    "link": "https://www.semanticscholar.org/paper/918b7df561fff08b25c15efd2731ba06659018bc",
    "session_title": "Session F: Privacy in ML + Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "e922b756-0ae0-4330-8902-70c2430a76cd"
  },
  {
    "title": "Puddles: Application-Independent Recovery and Location-Independent Data for Persistent Memory",
    "authors": "Suyash  Mahar (UC San Diego), Mingyao  Shen (UC San Diego), TJ  Smith (UC San Diego), Joseph  Izraelevitz (University of Colorado, Boulder), Steven  Swanson (UCSD)",
    "abstract": "In this paper, we argue that current work has failed to provide a comprehensive and maintainable in-memory representation for persistent memory. PM data should be easily mappable into a process address space, shareable across processes, shippable between machines, consistent after a crash, and accessible to legacy code with fast, efficient pointers as first-class abstractions. While existing systems have provided niceties like mmap()-based load/store access, they have not been able to support all these necessary properties due to conflicting requirements. We propose Puddles, a new persistent memory abstraction, to solve these problems. Puddles provide application-independent recovery after a power outage; they make recovery from a system failure a system-level property of the stored data rather than the responsibility of the programs that access it. Puddles use native pointers, so they are compatible with existing code. Finally, Puddles implement support for sharing and shipping of PM data between processes and systems without expensive serialization and deserialization. Compared to existing systems, Puddles are at least as fast as and up to 1.34× faster than PMDK while being competitive with other PM libraries across YCSB workloads. Moreover, to demonstrate Puddles' ability to relocate data, we showcase a sensor network data-aggregation workload that results in a 4.7× speedup over PMDK.",
    "link": "https://arxiv.org/pdf/2310.02183",
    "session_title": "Session G: Files and Storage",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "930d4994-6b9e-434e-8c68-a65fc3a8a77d"
  },
  {
    "title": "SplitFT: Fault Tolerance for Disaggregated Datacenters via Remote Memory Logging",
    "authors": "Xuhao  Luo (University of Illinois at Urbana-Champaign), Ramnatthan  Alagappan (University of Illinois Urbana-Champaign and VMware Research), Aishwarya  Ganesan (University of Illinois Urbana-Champaign and VMware Research)",
    "abstract": "We introduce SplitFt, a new fault-tolerance approach for storage-centric applications in disaggregated data centers. SplitFt uses a novel split architecture, where large writes are directly performed on the underlying disaggregated storage system, while small writes are made fault-tolerant within the compute layer. The split architecture enables applications to achieve strong durability guarantees without compromising performance. SplitFt makes small writes fault-tolerant using a new abstraction called near-compute logs or Ncl, which leverages underutilized memory on remote nodes to log small writes in a fast, cheap, and transparent manner. We port three POSIX applications (RocksDB, Redis, and SQLite) to SplitFt and show that they offer strong guarantees compared to weak versions of the applications that can lose data; SplitFt applications do so while approximating weak versions' performance (only 0.1%-10% overhead under YCSB). Compared to strong versions, SplitFt improves performance significantly (2.5× to 27× under write-heavy workloads).",
    "link": "https://www.semanticscholar.org/paper/fba306fe567168e7e2a49115a3e7af36fc0b4802",
    "session_title": "Session G: Files and Storage",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "c2033ce5-4e13-4a32-8ddf-149140b67609"
  },
  {
    "title": "CSAL: the Next-Gen Local Disks for the Cloud",
    "authors": "Yanbo  Zhou (Alibaba Group), Erci  Xu (Alibaba Group), Li  Zhang (Alibaba Group), Kapil  Karkra (Solidigm), Mariusz  Barczak (Solidigm), Wayne  Gao (Solidigm), Wojciech  Malikowski (Solidigm), Mateusz  Kozlowski (Solidigm), Åukasz  Åasek (Solidigm), Ruiming  Lu (Alibaba Group), Feng  Yang (Alibaba Group), Lilong  Huang (Alibaba Group), Xiaolu  Zhang (Alibaba Group), Keqiang  Niu (Alibaba Group), Jiaji  Zhu (Alibaba Group), Jiesheng  Wu (Alibaba Group)",
    "abstract": "Cloud local disks are attractive for their affordable price and high performance. The recent advancement in CPUs motivates cloud vendors to further multiplex the computing resources to serve more users. Unfortunately, such proposals are constrained by the limited offerings of cloud local disks per server as the underlying storage devices are either large but slow (e.g., HDDs) or fast yet small (e.g., NVMe SSDs). In this paper, we explore the possibility of leveraging high-capacity QLC-based SSDs for cloud local disks. However, the three preliminary unsuccessful attempts indicate that QLC SSDs cannot simply work as drop-in replacement. The root cause is the two levels of write amplification caused by device-level address mapping with Indirection Unit and NAND-level garbage collection. With these lessons learned, we propose CSAL, the next-gen local disks in Alibaba Cloud. CSAL includes a high-performance SSD as write buffers and a large-capacity QLC SSD for persistence. With a two-level Logical to Physical (L2P) address mapping table, CSAL achieves fine-grained (4KB) data accessing and significantly alleviates the two levels of write amplification. Results show that CSAL always prevails with superior performance and can achieve up to 2.22×, 1.82×, and 2.03× speedups against the second-best peers in micro, application, and deployment benchmarking, respectively. As of now, we have deployed CSAL on thousands of servers and made CSAL open-source to the public.",
    "link": "https://www.semanticscholar.org/paper/b907da4046c1eb712ff90bc2c63edad7ea1448a2",
    "session_title": "Session G: Files and Storage",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "2dc44d66-b7ef-49a4-865d-45797bf79dde"
  },
  {
    "title": "Exploring the Asynchrony of Slow Memory Filesystem with EasyIO",
    "authors": "Bohong  Zhu (Xiamen University), Youmin  Chen (Tsinghua University), Jiwu  Shu (Tsinghua University)",
    "abstract": "We introduce EasyIO, a new approach to explore asynchronous I/O on filesystems designed for (disaggregated) nonvolatile memories to improve CPU efficiency. EasyIO offloads expensive memory movement operations to the on-chip DMA engine and harvests the unleashed CPU cycles by transparently interleaving asynchronous I/Os with fine-grained application tasks. We further adopt a completion buffer-centric design to improve EasyIO's efficiency and schedulability; internally, orderless file operation and two-level locking are incorporated to break the serial order between file metadata and data, thus accelerating read and write operations and defusing deadlock risks. EasyIO also introduces a traffic-aware channel manager to fulfill the diverse performance goals of applications. Extensive experimental results show that, compared to conventional synchronous filesystems, EasyIO significantly reduces CPU consumption (using less than 88% of cores at most) while achieving comparable peak bandwidth; EasyIO also achieves 1.03-2.3× speedups across eight real-world applications. When achieving these goals, EasyIO exhibits higher but tolerable latencies for read operations due to the task interleaving.",
    "link": "https://www.semanticscholar.org/paper/2c57aa99977c0692f081b67c43bd064e52091503",
    "session_title": "Session G: Files and Storage",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "01ef2e43-550d-4aff-b1c4-b552990ad771"
  },
  {
    "title": "ScaleCache: A Scalable Page Cache for Multiple Solid-State Drives",
    "authors": "Kiet Tuan  Pham (Chung-Ang University), Seokjoo  Cho (Chung-Ang University), Sangjin  Lee (Chung-Ang University), Lan Anh  Nguyen (Chung-Ang University), Hyeongi  Yeo (Chung-Ang University), Ipoom  Jeong (University of Illinois Urbana-Champaign), Sungjin  Lee (DGIST), Nam Sung  Kim (University of Illinois Urbana-Champaign), Yongseok  Son (Chung-Ang University)",
    "abstract": "This paper presents a scalable page cache called ScaleCache for improving SSD scalability. Specifically, we first propose a concurrent data structure of page cache based on XArray (ccXArray) to enable access and update the page cache concurrently. Second, we introduce a direct page flush (dflush) which directly flushes pages to storage devices in a parallel and opportunistic manner. We implement ScaleCache with two techniques in the Linux kernel and evaluate it on a 64-core machine with eight NVMe SSDs. Our evaluations show that ScaleCache improves the performance of Linux file systems by up to 6.81× and 4.50× compared with the existing scheme and scalable scheme for multiple SSDs, respectively.",
    "link": "https://www.semanticscholar.org/paper/6913c284397b557dc8d3826cc17f85e2ce253ece",
    "session_title": "Session G: Files and Storage",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "a60899d7-1129-4495-95cb-e0a535b3a5d9"
  },
  {
    "title": "Volley: Accelerating Write-Read Orders in Disaggregated Storage",
    "authors": "Shaoxun  Zeng (Tsinghua University), Xiaojian  Liao (Tsinghua University), Hao  Guo (Tsinghua University), Youyou  Lu (Tsinghua University)",
    "abstract": "Modern data centers deploy disaggregated storage systems (e.g., NVMe over Fabrics, NVMe-oF) for fine-grained resource elasticity and high resource utilization. A client-side writeback cache is used to absorb writes and buffer frequently accessed data, thereby eliminating unnecessary remote storage accesses and improving performance. Yet, a cache miss on the full cache triggers an evict-and-fetch operation which evicts the old entries before new data blocks are fetched. Existing systems perform the evict-and-fetch operation by sequentially executing write and read I/O operations, which reduces the concurrency and makes it challenging to fully utilize the fast network and storage devices. To overcome this problem, we propose Volley, a network storage protocol that guarantees the execution order of the write and read I/O operations, enabling the writeback cache to issue eviction and fetch operations simultaneously. We implement the Volley protocol by extending NVMe-oF atop commodity network and storage hardware and further the ACK free-rides and notification acceleration techniques to reduce the network overhead. We adopt Volley into two writeback caching systems (V-Cache and V-TriCache) for virtual machine storage and out-of-core computing, respectively. Evaluations show that V-Cache outperforms Linux page cache and SPDK OCF (a state-of-the-art caching system) by up to 6.84× and 3.01×. Experiments on a production workload of Facebook (Mixgraph) show that V-TriCache reduces the total running time by up to 16.7% compared with the state-of-the-art out-of-core computing system, TriCache.",
    "link": "https://www.semanticscholar.org/paper/2d0e5ba12f2463774fe4f16caa5d92414991172f",
    "session_title": "Session G: Files and Storage",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "e17f4a9b-4668-4de9-a51b-9c1e359f888c"
  },
  {
    "title": "Automatic Root Cause Analysis via Large Language Models for Cloud Incidents",
    "authors": "Yinfang  Chen (University of Illinois at Urbana-Champaign, Champaign, USA), Huaibing  Xie (Peking University, Beijing, China), Minghua  Ma (Microsoft, Beijing, China), Yu  Kang (Microsoft, Shanghai, China), Xin  Gao (Microsoft, Suzhou, China), Liu  Shi (Microsoft, Suzhou, China), Yunjie  Cao (Microsoft, Suzhou, China), Xuedong  Gao (Microsoft, Suzhou, China), Hao  Fan (Microsoft, Suzhou, China), Ming  WEN (Huazhong University of Science and Technology, Wuhan, China), Jun  Zeng (National University of Singapore, Singapore), Supriyo  Ghosh (Microsoft, Bengaluru, India), Xuchao  Zhang (Microsoft, Redmond, USA), Chaoyun  Zhang (Microsoft, Beijing, China), Qingwei  Lin (Microsoft, Beijing, China), Saravan  Rajmohan (Microsoft, Redmond, USA), Dongmei  Zhang (Microsoft, Beijing, China), Tianyin  Xu (University of Illinois at Urbana-Champaign, Champaign, USA)",
    "abstract": "Ensuring the reliability and availability of cloud services necessitates efficient root cause analysis (RCA) for cloud incidents. Traditional RCA methods, which rely on manual investigations of data sources such as logs and traces, are often laborious, error-prone, and challenging for on-call engineers. In this paper, we introduce RCACopilot, an innovative on-call system empowered by the large language model for automating RCA of cloud incidents. RCACopilot matches incoming incidents to corresponding incident handlers based on their alert types, aggregates the critical runtime diagnostic information, predicts the incident's root cause category, and provides an explanatory narrative. We evaluate RCACopilot using a real-world dataset consisting of a year's worth of incidents from Microsoft. Our evaluation demonstrates that RCACopilot achieves RCA accuracy up to 0.766. Furthermore, the diagnostic information collection component of RCACopilot has been successfully in use at Microsoft for over four years.",
    "link": "https://arxiv.org/pdf/2305.15778",
    "session_title": "Session H: Dealing with Bugs, Failures and Inconsistency",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "f949f81e-29fd-4904-9957-36c067d0307e"
  },
  {
    "title": "Finding Correctness Bugs in eBPF Verifier with Structured and Sanitized Program",
    "authors": "Hao  Sun (Tsinghua University), Yiru  Xu (Tsinghua University), Jianzhong  Liu (Tsinghua University), Yuheng  Shen (Tsinghua University), Nan  Guan (City University of Hong Kong), Yu  Jiang (Tsinghua University)",
    "abstract": "eBPF is an inspiring technique in Linux that allows user space processes to extend the kernel by dynamically injecting programs. However, it poses security issues, since the untrusted user code is now executed in the kernel space. eBPF utilizes a verifier to validate the safety of the provided programs, thus its correctness is of paramount importance as attackers may exploit vulnerabilities within it to inject malicious programs. Bug-finding tools like kernel fuzzers currently can detect memory bugs in eBPF system calls, but they experience difficulties in finding correctness bugs in the verifier, e.g., incorrect validations that allow the loading of unsafe programs. Because, unlike detecting memory bugs, where sanitizers can capture such errors once observed, automatically uncovering correctness bugs is very difficult, without an effective test oracle that determines if the verifier behaves correctly for given programs. In this paper, we propose an effective approach to automatically detect the verifier's correctness bugs. Our core observation is that since the verifier aims to ensure that eBPF programs do not affect the security of the kernel, any illegal behaviors in verified programs are indicators of correctness bugs in the verifier. Indeed, we can convert the detection of logical errors in the verifier to traditional bug finding in eBPF programs. Based on such insight, we devise two indicators for correctness bugs and propose corresponding sanitation mechanisms to capture them, both of which naturally form an effective test oracle. We implemented our idea in a tool, namely BVF, which generates structured eBPF programs to pass the verifier, and subsequently, it finds correctness bugs by detecting runtime errors in verified programs with the indicators. Experiments show that although the verifier has received extensive scrutiny and has been intensively tested by tools like Syzkaller and Buzzer, BVF still found 11 previously unknown vulnerabilities in eBPF, of which six are correctness bugs of critical severity in the verifier.",
    "link": "https://www.semanticscholar.org/paper/87548334762f069e787918fd6d4a821a81a8312b",
    "session_title": "Session H: Dealing with Bugs, Failures and Inconsistency",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "dcf90336-eb46-4379-b79e-d30dbfe5ea74"
  },
  {
    "title": "Noctua: Towards Practical and Automated Fine-grained Consistency Analysis",
    "authors": "Kai  Ma (University of Science and Technology of China), Cheng  Li (University of Science and Technology of China), Enzuo  Zhu (University of California, Riverside), Ruichuan  Chen (Nokia Bell Labs), Feng  Yan (University of Houston), Kang  Chen (Tsinghua University)",
    "abstract": "Relaxing strong consistency plays a vital role in achieving scalability and availability for geo-replicated web applications. However, making relaxation correct in modern implementations, typically written in dynamic languages and utilizing high-level object-oriented database abstractions, remains a challenge, despite the existence of numerous proposed analysis tools. Here, we present a fully automated verification framework Noctua for understanding fine-grained consistency semantics in web applications. At its core is a simple intermediate representation SOIR that bridges the semantic gaps between high-level languages, database interactions and SMT solver's verification. Noctua's lightweight program analyzer reuses the ability of the language runtime and framework to translate consistency-related effects and application invariants from codebases into SOIR code. Finally, Noctua's verification backend maps SOIR code into SMT verification conditions with a new SMT encoding that decouples order information and thus allows more database semantics to be covered. We have implemented Noctua1 for a popular dynamic language, Python, and evaluated its correctness and applicability with two synthetic and four existing Python web applications. The evaluation results highlight that Noctua is able to understand consistency semantics considerably fast from the real codebases with no user input. For synthetic applications that existing solutions can be applied, Noctua's delivered consistent analysis results.",
    "link": "https://www.semanticscholar.org/paper/e14dc99bdf2fc1616a0bd9eaa6bac27088b723e7",
    "session_title": "Session H: Dealing with Bugs, Failures and Inconsistency",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "7d7ccff7-5d92-4551-acd3-119add0da8e9"
  },
  {
    "title": "Effective Bug Detection with Unused Definitions",
    "authors": "Li  Zhong (UCSD), Chengcheng  Xiang (Meta), Haochen  Huang (Whova), Bingyu  Shen (Meta), Eric  Mugnier (UC San Diego), Yuanyuan  Zhou (UCSD)",
    "abstract": "Unused definitions are values assigned to variables but not used. Since unused definitions are usually considered redundant code causing no severe consequences except for wasting CPU cycles, system developers usually treat them as mild warnings and simply remove them. In this paper, we reevaluate the effect of unused definitions and discover that some unused definitions could indicate non-trivial bugs like security issues or data corruption, which calls for more attention from developers. Although there are existing techniques to detect unused definitions, it is still challenging to detect critical bugs from unused definitions because only a small proportion of unused definitions are real bugs. In this paper, we present a static analysis framework ValueCheck to address the challenges of detecting bugs from unused definitions. First, we make a unique observation that the unused definitions on the boundary of developers' interactions are prone to be bugs. Second, we summarize syntactic and semantic patterns where unused definitions are intentionally written, which should not be considered bugs. Third, to distill bugs from unused definitions, we adopt the code familiarity metrics from the software engineering field to rank the detected bugs, which enables developers to prioritize their focus. We evaluate ValueCheck with large system software and libraries including Linux, MySQL, OpenSSL, and NFS-ganesha. ValueCheck helps detect 210 unknown bugs from these applications. 154 bugs are confirmed by developers. Compared to state-of-the-art tools, ValueCheck demonstrates to effectively detect bugs with low false positives.",
    "link": "https://www.semanticscholar.org/paper/261e9558eba5a1cf0e74fe10aff74b5a8f71839c",
    "session_title": "Session H: Dealing with Bugs, Failures and Inconsistency",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "c6b30c3f-17f6-4942-84ec-31291eba6641"
  },
  {
    "title": "SandTable: Scalable Distributed System Model Checking with Specification-Level State Exploration",
    "authors": "Ruize  Tang (Nanjing University), Xudong  Sun (University of Illinois at Urbana-Champaign), Yu  Huang (Nanjing University), Yuyang  Wei (Nanjing University), Lingzhi  Ouyang (Nanjing University), Xiaoxing  Ma (Nanjing University)",
    "abstract": "Implementation-level distributed system model checkers (DMCKs) have proven valuable in verifying the correctness of real distributed systems. However, they primarily focus on state space reduction, and often have a bottleneck on another crucial dimension: exploration speed. To scale DMCK, we introduce SandTable, a technique for lifting state-space exploration from the implementation level to the specification level, and confirming bugs at the implementation level. We made SandTable practical through a methodology consisting of four essential parts: (1) writing specifications that adhere to the implementation, (2) checking conformance to enhance specification quality and reduce false positives and false negatives, (3) exploring the state space with heuristics for effectiveness and efficiency, and (4) confirming bugs and verifying their fixes in the implementation. We implemented SandTable with the design of transparently verifying unmodified distributed systems on POSIX systems. SandTable was integrated into eight well-established open-source distributed systems that implement consensus protocols such as Raft and Zab. SandTable identified 23 bugs in total, with 18 new bugs, 17 confirmed, and 13 fixed. SandTable demonstrates exceptional scalability, with one machine-day of specification-level exploration checking up to 109 distinct states. Furthermore, specification-level exploration offers a significant speedup, 114×---2989× faster than implementation-level exploration.",
    "link": "https://www.semanticscholar.org/paper/e8f5f62de79fa68d38baae443f80ceb4fdfbad71",
    "session_title": "Session H: Dealing with Bugs, Failures and Inconsistency",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "0086f323-d19c-4abd-a7a2-f09cecc7dc3b"
  },
  {
    "title": "Validating Database System Isolation Level Implementations with Version Certificate Recovery",
    "authors": "Jack  Clark (Imperial College London), Alastair F.  Donaldson (Imperial College London), John  Wickerson (Imperial College London), Manuel  Rigger (National University of Singapore)",
    "abstract": "Transactions are a key feature of database systems and isolation levels specify the behavior of concurrently executing transactions. Ensuring their correct behavior is crucial. Recently, many isolation anomalies have been found in production database systems. Checkers can be used to validate that a particular execution conforms to a desired isolation level. However, state-of-the-art checkers cannot handle predicate operations, which are both common in real-world workloads and essential for distinguishing between the repeatable read and serializable isolation levels. In this work, we address this issue by proposing an efficient white-box checker, Emme. Our key idea is to use information that is easily provided by database systems to efficiently check the isolation level of a given transaction history. We present version certificate recovery, a method of recovering the version order and each operation's version from the database system under test. For efficiency, we also propose the concept of an expected serialization order, which obviates the need to define and recover a version certificate for many serializable concurrency control protocols. We have implemented version certificate recovery for three widely used database systems---PostgreSQL, CockroachDB, and TiDB. We demonstrate that Emme is 1.2-3.6× faster than Elle, a state-of-the-art checker. Using the expected serialization order, we obtain a further speedup of 34-430× compared to Emme when checking histories containing predicate operations. We show that our approach can identify invalid histories that cannot be detected by Elle and also show that it can find realistic bugs purposely introduced by an engineer.",
    "link": "https://www.semanticscholar.org/paper/04178561c9d8cd42904ed0a836e08e55b84703cf",
    "session_title": "Session H: Dealing with Bugs, Failures and Inconsistency",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "f1af8983-33f1-49c2-adc2-961d4b3c8c0b"
  },
  {
    "title": "Improving GPU Energy Efficiency through an Application-transparent Frequency Scaling Policy with Performance Assurance",
    "authors": "Yijia  Zhang (Peng Cheng Laboratory), Qiang  Wang (Harbin Institute of Technology (Shenzhen)), Zhe  Lin (Sun Yat-sen University), Pengxiang  Xu (Peng Cheng Laboratory), Bingqiang  Wang (Peng Cheng Laboratory)",
    "abstract": "Power consumption is one of the top limiting factors in high-performance computing systems and data centers, and dynamic voltage and frequency scaling (DVFS) is an important mechanism to control power. Existing works using DVFS to improve GPU energy efficiency suffer from the limitation that their policies either impact performance too much or require offline application profiling or code modification, which severely limits their applicability on large clusters. To address this issue, we propose a novel GPU DVFS policy, GEEPAFS, which improves the energy efficiency of GPUs while providing performance assurance. GEEPAFS is application-transparent as it does not require any offline profiling or code modification on user applications. To achieve this, GEEPAFS models application performance online based on our quantitative analysis of a correlation between performance and GPU memory bandwidth utilization. Based on their relationship, GEEPAFS builds a fold-line frequency-performance model for applications being executed, and it applies the model to guide the setting of GPU frequency to maximize energy efficiency while ensuring the performance loss is bounded. Through experiments on NVIDIA V100 and A100 GPUs, we show that GEEPAFS is able to improve the energy efficiency by 26.7% and 20.2% on average. While achieving this improvement, the average performance loss is only 5.8%, and the worst-case performance loss is 12.5% among all 33 tested applications.",
    "link": "https://www.semanticscholar.org/paper/717f268655c65e879772a91cf383df92a6edf039",
    "session_title": "Session I: Hardware-SW co-design",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "0632ca56-bf83-441b-974b-ad8954323b7e"
  },
  {
    "title": "Minuet: Accelerating 3D Sparse Convolutions on GPUs",
    "authors": "Jiacheng  Yang (University of Toronto / Vector Institute), Christina  Giannoula (University of Toronto), Jun  Wu (Amazon), Mostafa  Elhoushi (Meta), James  Gleeson (Samsung AI Centre Toronto), Gennady  Pekhimenko (University of Toronto / Vector Institute)",
    "abstract": "Sparse Convolution (SC) is widely used for processing 3D point clouds that are inherently sparse. Different from dense convolution, SC preserves the sparsity of the input point cloud by only allowing outputs to specific locations. To efficiently compute SC, prior SC engines first use hash tables to build a kernel map that stores the necessary General Matrix Multiplication (GEMM) operations to be executed (Map step), and then use a Gather-GEMM-Scatter process to execute these GEMM operations (GMaS step). In this work, we analyze the shortcomings of prior state-of-the-art SC engines, and propose Minuet, a novel memory-efficient SC engine tailored for modern GPUs. Minuet proposes to (i) replace the hash tables used in the Map step with a novel segmented sorting double-traversed binary search algorithm that highly utilizes the on-chip memory hierarchy of GPUs, (ii) use a lightweight scheme to autotune the tile size in the Gather and Scatter operations of the GMaS step, such that to adapt the execution to the particular characteristics of each SC layer, dataset, and GPU architecture, and (iii) employ a padding-efficient GEMM grouping approach that reduces both memory padding and kernel launching overheads. Our evaluations show that Minuet significantly outperforms prior SC engines by on average 1.74× (up to 2.22×) for end-to-end point cloud network executions. Our novel segmented sorting double-traversed binary search algorithm achieves superior speedups by 15.8× on average (up to 26.8×) over prior SC engines in the Map step. The source code of Minuet is publicly available at https://github.com/UofT-EcoSystem/Minuet.",
    "link": "https://arxiv.org/pdf/2401.06145",
    "session_title": "Session I: Hardware-SW co-design",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "675587a6-bc43-43b8-829b-aa54e7ec74b0"
  },
  {
    "title": "MTM: Rethinking Memory Profiling and Migration for Multi-Tiered Large Memory",
    "authors": "Jie  Ren (William & Mary), Dong  Xu (University of California, Merced), Junhee  Ryu (SK hynix), Kwangsik  Shin (SK hynix), Daewoo  Kim (SK hynix), Dong  Li (University of California, Merced)",
    "abstract": "Multi-terabyte large memory systems are often characterized by more than two memory tiers with different latency and bandwidth. Multi-tiered large memory systems call for rethinking of memory profiling and migration because of the unique problems unseen in the traditional memory systems with smaller capacity and fewer tiers. We develop MTM, an application-transparent Multi-Tiered Memory management framework, based on three principles: (1) connecting the control of profiling overhead with the profiling mechanism for high-quality profiling; (2) building a universal page migration policy on the complex multi-tiered memory for high performance; and (3) introducing huge page awareness. We evaluate MTM using common big-data applications with realistic working sets (hundreds of GB to 1 TB). MTM outperforms seven solutions by up to 42% (17% on average).",
    "link": "https://www.semanticscholar.org/paper/6e5863ca58165d3303153c4183e4b76099a76bd1",
    "session_title": "Session I: Hardware-SW co-design",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "3f54584b-f1b7-4cc9-9e49-b028327133fa"
  },
  {
    "title": "Exploring Performance and Cost Optimization with ASIC-Based CXL Memory",
    "authors": "Yupeng  Tang (Yale University), Ping  Zhou (ByteDance), Wenhui  Zhang (ByteDance), Henry  Hu (ByteDance), Qirui  Yang (ByteDance), Hao  Xiang (ByteDance), Tongping  Liu (ByteDance), Jiaxin  Shan (ByteDance), Ruoyun  Huang (ByteDance), Cheng  Zhao (ByteDance), Cheng  Chen (ByteDance), Hui  Zhang (ByteDance), Fei  Liu (ByteDance), Shuai  Zhang (ByteDance), Xiaoning  Ding (ByteDance), Jianjun  Chen (ByteDance)",
    "abstract": "As memory-intensive applications continue to drive the need for advanced architectural solutions, Compute Express Link (CXL) has risen as a promising interconnect technology that enables seamless high-speed, low-latency communication between host processors and various peripheral devices. In this study, we explore the application performance of ASIC CXL memory in various data-center scenarios. We then further explore multiple potential impacts (e.g., throughput, latency, and cost reduction) of employing CXL memory via carefully designed policies and strategies. Our empirical results show the high potential of CXL memory, reveal multiple intriguing observations of CXL memory and contribute to the wide adoption of CXL memory in real-world deployment environments. Based on our benchmarks, we also develop an Abstract Cost Model that can estimate the cost benefit from using CXL memory.",
    "link": "https://www.semanticscholar.org/paper/85277be97098ad9fd5fc47c28eddbff3569b3e91",
    "session_title": "Session I: Hardware-SW co-design",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "cf09b50a-1607-4d29-b246-1d207b39bb66"
  },
  {
    "title": "HD-IOV: SW-HW Co-designed I/O Virtualization with Scalability and Flexibility for Hyper-Density Cloud",
    "authors": "Zongpu  Zhang (Shanghai Jiao Tong University), Jiangtao  Chen (Shanghai Jiao Tong University), Banghao  Ying (Shanghai Jiao Tong University), Yahui  Cao (Intel Asia-Pacific R&D Ltd.), Lingyu  Liu (Intel Asia-Pacific R&D Ltd.), Jian  Li (Shanghai Jiao Tong University), Xin  Zeng (Intel Asia-Pacific R&D Ltd.), Junyuan  Wang (Intel Asia-Pacific R&D Ltd.), Weigang  Li (Intel Asia-Pacific R&D Ltd.), Haibing  Guan (Shanghai Jiao Tong University)",
    "abstract": "As the resource density of cloud servers increases, cloud providers deploy hundreds of VMs concurrently on a single server, requiring a high-performance, scalable, flexible and high-density I/O virtualization method. Hardware assisted virtualization such as device pass-through with SR-IOV can achieve near-native performance, however, at the expense of flexibility and a limited device count. Traditional software-based I/O virtualization systems tend to dedicate additional computing cores for higher performance, but suffer from critical scalability problems especially in high-density cloud. In this paper, the proposed Hyper-Density I/O virtualization (HD-IOV) system tries to achieve pass-through level performance without scalability and flexibility limitations in previous works. HD-IOV is a software-hardware co-designed I/O virtualization solution. The core insight of HD-IOV is to decouple virtualization and resource management logic from hardware devices to software, reducing device complexity and enabling more flexible hardware resource management. DMA transactions and interrupts are sent directly to guest VMs without VM exits. Isolation is achieved by leveraging an existing PCIe feature, allowing IOMMU to enforce queue pair level isolation. Extensive experiments show that HD-IOV achieves similar performance as SR-IOV for both network and accelerator devices. Furthermore, HD-IOV supports maximally 2.96x higher device count with 2.9x faster median device initialization time, which is critical for emerging container and lightweight VM systems.",
    "link": "https://www.semanticscholar.org/paper/8e898798bf3f33d254385476d3b4828a1dbd9020",
    "session_title": "Session I: Hardware-SW co-design",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "f4353df0-1454-4333-b82d-37e717b8b404"
  },
  {
    "title": "SmartNIC Security Isolation in the Cloud with S-NIC",
    "authors": "Yang  Zhou (Harvard University), Mark  Wilkening (Harvard University), James  Mickens (Harvard University), Minlan  Yu (Harvard University)",
    "abstract": "Modern smart NICs provide little isolation between the network functions belonging to different tenants. These NICs also do not protect network functions from the datacenter-provided management OS which runs on the smart NIC. We describe concrete attacks which allow a network function's state to leak to (or be modified by) another network function or the management OS. We then introduce S-NIC, a new hardware design for smart NICs that provides strong isolation guarantees. S-NIC pervasively virtualizes hardware accelerators, enforces single-owner semantics for each line in on-NIC cache and RAM, and provides dedicated bus bandwidth for each network function. Using this design, we eliminate side channels involving shared hardware state, and give each network function the illusion of having a private smart NIC. We show how these virtual NICs can be integrated with preexisting datacenter technologies for virtual LANs and trusted host-level computations like SGX enclaves. The overall result is that S-NIC enables strongly-isolated, NIC-accelerated datacenter applications; in these applications, network functions and host-level code receive hardware-guaranteed isolation from other applications and the datacenter provider.",
    "link": "https://www.semanticscholar.org/paper/36bb96e67fa46ecb2be1a266f426715d0715c554",
    "session_title": "Session I: Hardware-SW co-design",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "fb8e2bd5-74a2-4e19-a498-298d29ab4564"
  },
  {
    "title": "Atlas: Hybrid Cloud Migration Advisor for Interactive Microservices",
    "authors": "Ka-Ho  Chow (Georgia Institute of Technology), Umesh  Deshpande (IBM Research - Almaden), Veera  Deenadhayalan (IBM Research - Almaden), Sangeetha  Seshadri (IBM Research - Almaden), Ling  Liu (Georgia Institute of Technology)",
    "abstract": "Hybrid cloud provides an attractive solution to microservices for better resource elasticity. A subset of application components can be offloaded from the on-premises cluster to the cloud, where they can readily access additional resources. However, the selection of this subset is challenging because of the large number of possible combinations. A poor choice degrades the application performance, disrupts the critical services, and increases the cost to the extent of making the use of hybrid cloud unviable. This paper presents Atlas, a hybrid cloud migration advisor. Atlas uses a data-driven approach to learn how each user-facing API utilizes different components and their network footprints to drive the migration decision. It learns to accelerate the discovery of high-quality migration plans from millions and offers recommendations with customizable trade-offs among three quality indicators: end-to-end latency of user-facing APIs representing application performance, service availability, and cloud hosting costs. Atlas continuously monitors the application even after the migration for proactive recommendations. Our evaluation shows that Atlas can achieve 21% better API performance (latency) and 11% cheaper cost with less service disruption than widely used solutions.",
    "link": "https://arxiv.org/pdf/2311.06962",
    "session_title": "Session J: Scheduling, Shifting and Scaling Workloads",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "f6c7008f-b4c0-4d88-9e68-de6f217a64e5"
  },
  {
    "title": "Erlang: Application-Aware Autoscaling for Cloud Microservices",
    "authors": "Vighnesh  Sachidananda (Google), Anirudh  Sivaraman (NYU)",
    "abstract": "As cloud applications shift from monoliths to loosely coupled microservices, application developers must decide how many compute resources (e.g., number of replicated containers) to assign to each microservice within an application. This decision affects both (1) the dollar cost to the application developer and (2) the end-to-end latency perceived by the application user. Today, individual microservices are autoscaled independently by adding VMs whenever per-microservice CPU or memory utilization crosses a configurable threshold. However, an application user's end-to-end latency consists of time spent on multiple microservices and each microservice might need a different number of VMs to achieve an overall end-to-end latency. We present Erlang, an autoscaler for microservice-based applications, which collectively allocates VMs to microservices with a global goal of minimizing dollar cost while keeping end-to-end application latency under a given target. Using 5 open-source applications, we compared Erlang to several utilization and machine learning based autoscalers. We evaluate Erlang across different compute settings on Google Kubernetes Engine (GKE) in which users manage compute resources, GKE standard, and a new mode of operation in which the cloud provider manages compute infrastructure, GKE Autopilot. Erlang meets a desired median or tail latency target on 53 of 63 workloads where it provides a cost reduction of 19.3%, on average, over the next cheapest autoscaler. Erlang is the most cost effective autoscaling policy for 48 of these 53 workloads. The cost savings from managing a cluster with Erlang result in Erlang paying for its training cost in a few days. On smaller applications, for which we can exhaustively search microservice configurations, we find that Erlang is optimal for 90% of cases and near optimal otherwise. Code for Erlang is available at https://github.com/vigsachi/erlang",
    "link": "https://www.semanticscholar.org/paper/e19c3a4e2ba51daf44455a1c0aeedef26d5e938f",
    "session_title": "Session J: Scheduling, Shifting and Scaling Workloads",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "0f23dd99-4999-41d2-8050-234d30b89129"
  },
  {
    "title": "On the Limitations of Carbon-Aware Temporal and Spatial Workload Shifting in the Cloud",
    "authors": "Thanathorn  Sukprasert (University of Massachusetts Amherst), Abel  Souza (University of Massachusetts Amherst), Noman  Bashir (Massachusetts Institute of Technology), David  Irwin (University of Massachusetts, Amherst), Prashant  Shenoy (University of Massachusetts Amherst)",
    "abstract": "Cloud platforms have been focusing on reducing their carbon emissions by shifting workloads across time and locations to when and where low-carbon energy is available. Despite the prominence of this idea, prior work has only quantified the potential of spatiotemporal workload shifting in narrow settings, i.e., for specific workloads in select regions. In particular, there has been limited work on quantifying an upper bound on the ideal and practical benefits of carbon-aware spatiotemporal workload shifting for a wide range of cloud workloads. To address the problem, we conduct a detailed data-driven analysis to understand the benefits and limitations of carbon-aware spatiotemporal scheduling for cloud workloads. We utilize carbon intensity data from 123 regions, encompassing most major cloud sites, to analyze two broad classes of workloads---batch and interactive---and their various characteristics, e.g., job duration, deadlines, and SLOs. Our findings show that while spatiotemporal workload shifting can reduce workloads' carbon emissions, the practical upper bounds of these carbon reductions are currently limited and far from ideal. We also show that simple scheduling policies often yield most of these reductions, with more sophisticated techniques yielding little additional benefit. Notably, we also find that the benefit of carbon-aware workload scheduling relative to carbon-agnostic scheduling will decrease as the energy supply becomes \"greener.\"",
    "link": "https://arxiv.org/pdf/2306.06502",
    "session_title": "Session J: Scheduling, Shifting and Scaling Workloads",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "e52145c2-b534-44c9-9f69-c75352ef0ee0"
  },
  {
    "title": "TraceUpscaler: Upscaling Traces to Evaluate Systems at High Load",
    "authors": "Sultan Mahmud  Sajal (The Pennsylvania State University), Timothy  Zhu (The Pennsylvania State University), Bhuvan  Urgaonkar (The Pennsylvania State University), Siddhartha  Sen (Microsoft Research)",
    "abstract": "Trace replay is a common approach for evaluating systems by rerunning historical traffic patterns, but it is not always possible to find suitable real-world traces at the desired level of system load. Experimenting with higher traffic loads requires upscaling a trace to artificially increase the load. Unfortunately, most prior research has adopted ad-hoc approaches for upscaling, and there has not been a systematic study of how the upscaling approach impacts the results. One common approach is to count the arrivals in a predefined time-interval and multiply these counts by a factor, but this requires generating new requests/jobs according to some model (e.g., a Poisson process), which may not be realistic. Another common approach is to divide all the timestamps in the trace by an upscaling factor to squeeze the requests into a shorter time period. However, this can distort temporal patterns within the input trace. This paper evaluates the pros and cons of existing trace upscaling techniques and introduces a new approach, TraceUpscaler, that avoids the drawbacks of existing methods. The key idea behind TraceUpscaler is to decouple the arrival timestamps from the request parameters/data and upscale just the arrival timestamps in a way that preserves temporal patterns within the input trace. Our work applies to open-loop traffic where requests have arrival timestamps that aren't dependent on previous request completions. We evaluate TraceUpscaler under multiple experimental settings using both real-world and synthetic traces. Through our study, we identify the trace characteristics that affect the quality of upscaling in existing approaches and show how TraceUpscaler avoids these pitfalls. We also present a case study demonstrating how inaccurate trace upscaling can lead to incorrect conclusions about a system's ability to handle high load.",
    "link": "https://www.semanticscholar.org/paper/8cebbce6b5dfc778c59caf14a5ba0078ab549808",
    "session_title": "Session J: Scheduling, Shifting and Scaling Workloads",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "bfdabc0b-c57b-46b0-9d4d-e0898aeb717f"
  },
  {
    "title": "Enoki: High Velocity Linux Kernel Scheduler Development",
    "authors": "Samantha  Miller (University of Washington), Anirudh  Kumar (University of Washington), Tanay  Vakharia (University of Washington), Ang  Chen (University of Michigan), Danyang  Zhuo (Duke University), Thomas  Anderson (University of Washington)",
    "abstract": "Kernel task scheduling is important for application performance, adaptability to new hardware, and complex user requirements. However, developing, testing, and debugging new scheduling algorithms in Linux, the most widely used cloud operating system, is slow and difficult. We developed Enoki, a framework for high velocity development of Linux kernel schedulers. Enoki schedulers are written in safe Rust, and the system supports live upgrade of new scheduling policies into the kernel, userspace debugging, and bidirectional communication with applications. A scheduler implemented with Enoki achieved near identical performance (within 1% on average) to the default Linux scheduler CFS on a wide range of benchmarks. Enoki is also able to support a range of research schedulers, specifically the Shinjuku scheduler, a locality aware scheduler, and the Arachne core arbiter, with good performance.",
    "link": "https://www.semanticscholar.org/paper/e4965e2cee55791fcab2f87f0646d96b456c23fc",
    "session_title": "Session J: Scheduling, Shifting and Scaling Workloads",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "d6818cc8-d31d-431f-a1b2-1ed44706bce9"
  },
  {
    "title": "Concealing Compression-accelerated I/O for HPC Applications through In Situ Task Scheduling",
    "authors": "Sian  Jin (Indiana University), Sheng  Di (Argonne National Laboratory), FrÃ©dÃ©ric  Vivien (INRIA, France), Daoce  Wang (Indiana University), Yves  Robert (Ecole Normale SupÃ©rieure de Lyon, France), Dingwen  Tao (Indiana University), Franck  Cappello (Argonne National Laboratory)",
    "abstract": "Lossy compression and asynchronous I/O are two of the most effective solutions for reducing storage overhead and enhancing I/O performance in large-scale high-performance computing (HPC) applications. However, current approaches have limitations that prevent them from fully leveraging lossy compression, and they may also result in task collisions, which restrict the overall performance of HPC applications. To address these issues, we propose an optimization approach for the task scheduling problem that encompasses computation, compression, and I/O. Our algorithm adaptively selects the optimal compression and I/O queue to minimize the performance degradation of the computation. We also introduce an intra-node I/O workload balancing mechanism that evenly distributes the workload across different processes. Additionally, we design a framework that incorporates fine-grained compression, a compressed data buffer, and a shared Huffman tree to fully benefit from our proposed task scheduling. Experimental results with up to 16 nodes and 64 GPUs from ORNL Summit, as well as real-world HPC applications, demonstrate that our solution reduces I/O overhead by up to 3.8× and 2.6× compared to non-compression and asynchronous I/O solutions, respectively.",
    "link": "https://inria.hal.science/hal-04225758/document",
    "session_title": "Session J: Scheduling, Shifting and Scaling Workloads",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "00e4aa3d-6979-47df-ac67-510af027b143"
  },
  {
    "title": "NeuroFlux: Memory-Efficient CNN Training Using Adaptive Local Learning",
    "authors": "Dhananjay  Saikumar (University of St Andrews), Blesson  Varghese (University of St Andrews)",
    "abstract": "Efficient on-device Convolutional Neural Network (CNN) training in resource-constrained mobile and edge environments is an open challenge. Backpropagation is the standard approach adopted, but it is GPU memory intensive due to its strong inter-layer dependencies that demand intermediate activations across the entire CNN model to be retained in GPU memory. This necessitates smaller batch sizes to make training possible within the available GPU memory budget, but in turn, results in substantially high and impractical training time. We introduce NeuroFlux, a novel CNN training system tailored for memory-constrained scenarios. We develop two novel opportunities: firstly, adaptive auxiliary networks that employ a variable number of filters to reduce GPU memory usage, and secondly, block-specific adaptive batch sizes, which not only cater to the GPU memory constraints but also accelerate the training process. NeuroFlux segments a CNN into blocks based on GPU memory usage and further attaches an auxiliary network to each layer in these blocks. This disrupts the typical layer dependencies under a new training paradigm - 'adaptive local learning'. Moreover, NeuroFlux adeptly caches intermediate activations, eliminating redundant forward passes over previously trained blocks, further accelerating the training process. The results are twofold when compared to Backpropagation: on various hardware platforms, NeuroFlux demonstrates training speed-ups of 2.3× to 6.1× under stringent GPU memory budgets, and NeuroFlux generates streamlined models that have 10.9× to 29.4× fewer parameters.",
    "link": "https://research-repository.st-andrews.ac.uk/bitstream/10023/29805/1/NeuroFlux_EuroSys2024_preprint.pdf",
    "session_title": "Session K: Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "95f108c9-490a-4bec-af2e-6ad9ec805700"
  },
  {
    "title": "Model Selection for Latency-Critical Inference Serving",
    "authors": "Daniel  Mendoza (Stanford University), Francisco  Romero (Stanford University), Caroline  Trippel (Stanford University)",
    "abstract": "In an inference service system, model selection and scheduling (MS&S) schemes map inference queries to trained machine learning (ML) models, hosted on a finite set of workers, to solicit accurate predictions within strict latency targets. MS&S is challenged by both varying query load and stochastic query inter-arrival patterns; however, state-of-the-art MS&S approaches conservatively account for load exclusively. In this paper, we first show that explicitly considering inter-arrival patterns creates opportunities to map queries to higher-accuracy (higher-latency) models during intermittent arrival lulls. We then propose RAMSIS, a framework for generating MS&S policies that exploits this finding. RAMSIS leverages a statistical problem model of query load and inter-arrival pattern to produce policies that maximize accuracy given some latency target. We evaluate RAMSIS-generated MS&S policies alongside state-of-the-art approaches. Notably, RAMSIS requires as low as 50.00% (on average 18.77%) fewer resources to achieve the same accuracy for an ImageNet image classification task given 26 trained models.",
    "link": "https://www.semanticscholar.org/paper/2e581de473030592d6f02ed764438730f086ef4b",
    "session_title": "Session K: Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "37333858-bc63-4130-82a2-33ccbef532b0"
  },
  {
    "title": "Optimus: Warming Serverless ML Inference via Inter-Function Model Transformation",
    "authors": "Zicong  Hong (The Hong Kong Polytechnic University), Jian  Lin (Shantou University), Song  Guo (Hong Kong Polytechnic University), Sifu  Luo (Sun Yat-sen University), Wuhui  Chen (Sun Yat-sen University), Roger  Wattenhofer (ETH Zurich), Yue  Yu (Peng Cheng Laboratory)",
    "abstract": "Serverless ML inference is an emerging cloud computing paradigm for low-cost, easy-to-manage inference services. In serverless ML inference, each call is executed in a container; however, the cold start of containers results in long inference delays. Unfortunately, most existing works do not work well because they still need to load models into containers from scratch, which is the bottleneck based on our observations. Therefore, this paper proposes a low-latency serverless ML inference system called Optimus via a new container management scheme. Our key insight is that the loading of a new model can be significantly accelerated when using an existing model with a similar structure in a warm but idle container. We thus develop a novel idea of inter-function model transformation for serverless ML inference, which delves into models within containers at a finer granularity of operations, designs a set of in-container meta-operators for both CNN and transformer model transformation, and develops an efficient scheduling algorithm with linear complexity for a low-cost transformation strategy. Our evaluations on thousands of models show that Optimus reduces inference latency by 24.00% ~ 47.56% in both simulated and real-world workloads compared to state-of-the-art work.",
    "link": "https://www.semanticscholar.org/paper/8c7dcb4477e1b8cde028cb759971920695dbf874",
    "session_title": "Session K: Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "d333af7c-4800-4d3f-8141-3ec03021f5ba"
  },
  {
    "title": "CDMPP: A Device-Model Agnostic Framework for Latency Prediction of Tensor Programs",
    "authors": "Hanpeng  Hu (The University of Hong Kong), Junwei  Su (The University of Hong Kong), Juntao  Zhao (The University of Hong Kong), Yanghua  Peng (ByteDance Inc.), Yibo  Zhu (ByteDance Inc.), Haibin  Lin (ByteDance Inc.), Chuan  Wu (The University of Hong Kong)",
    "abstract": "Deep Neural Networks (DNNs) have shown excellent performance in a wide range of machine learning applications. Knowing the latency of running a DNN model or tensor program on a specific device is useful in various tasks, such as DNN graph- or tensor-level optimization and device selection. Considering the large space of DNN models and devices that impedes direct profiling of all combinations, recent efforts focus on building a predictor to model the performance of DNN models on different devices. However, none of the existing attempts have achieved a cost model that can accurately predict the performance of various tensor programs while supporting both training and inference accelerators. We propose CDMPP, an efficient tensor program latency prediction framework for both cross-model and cross-device prediction. We design an informative but efficient representation of tensor programs, called compact ASTs, and a pre-order-based positional encoding method, to capture the internal structure of tensor programs. We develop a domain-adaption-inspired method to learn domain-invariant representations and devise a KMeans-based sampling algorithm, for the predictor to learn from different domains (i.e., different DNN operators and devices). Our extensive experiments on a diverse range of DNN models and devices demonstrate that CDMPP significantly outperforms state-of-the-art baselines with 14.03% and 10.85% prediction error for cross-model and cross-device prediction, respectively, and one order of magnitude higher training efficiency. The implementation and the expanded dataset are available at https://github.com/joapolarbear/cdmpp.",
    "link": "https://arxiv.org/pdf/2311.09690",
    "session_title": "Session K: Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "4bd9914a-10ef-4938-a4e0-9ec1384ea0c8"
  },
  {
    "title": "Orion: Interference-aware, Fine-grained GPU Sharing for ML Applications",
    "authors": "Foteini  Strati (ETH Zurich), Xianzhe  Ma (ETH Zurich), Ana  Klimovic (ETH Zurich)",
    "abstract": "GPUs are critical for maximizing the throughput-per-Watt of deep neural network (DNN) applications. However, DNN applications often underutilize GPUs, even when using large batch sizes and eliminating input data processing or communication stalls. DNN workloads consist of data-dependent operators, with different compute and memory requirements. While an operator may saturate GPU compute units or memory bandwidth, it often leaves other GPU resources idle. Despite the prevalence of GPU sharing techniques, current approaches are not sufficiently fine-grained or interference-aware to maximize GPU utilization while minimizing interference at the granularity of 10s of μs. We propose Orion, a system that transparently intercepts GPU kernel launches from multiple clients sharing a GPU. Orion schedules work on the GPU at the granularity of individual operators and minimizes interference by taking into account each operator's compute and memory requirements. We integrate Orion in PyTorch and demonstrate its benefits in various DNN workload collocation use cases. Orion significantly improves tail latency compared to state-of-the-art baselines for a high-priority inference job while collocating best-effort inference jobs to increase per-GPU request throughput by up to 7.3×, or while collocating DNN training, saving up to 1.49× in training costs compared to dedicated GPU allocation.",
    "link": "https://www.semanticscholar.org/paper/423d83256f500fa413da9d9d1af474078754ff05",
    "session_title": "Session K: Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "c38772d7-a17e-44e8-911b-23490551b0b4"
  },
  {
    "title": "Blox: A Modular Toolkit for Deep Learning Schedulers",
    "authors": "Saurabh  Agarwal (University of Wisconsin-Madison), Amar  Phanishayee (Microsoft Research), Shivaram  Venkataraman (University of Wisconsin-Madison)",
    "abstract": "Deep Learning (DL) workloads have rapidly increased in popularity in enterprise clusters and several new cluster schedulers have been proposed in recent years to support these workloads. With rapidly evolving DL workloads, it is challenging to quickly prototype and compare scheduling policies across workloads. Further, as prior systems target different aspects of scheduling (resource allocation, placement, elasticity etc.), it is also challenging to combine these techniques and understand the overall benefits. To address these challenges we propose Blox, a modular toolkit which allows developers to compose individual components and realize diverse scheduling frameworks. We identify a set of core abstractions for DL scheduling, implement several existing schedulers using these abstractions, and verify the fidelity of these implementations by reproducing results from prior research. We also highlight how we can evaluate and compare existing schedulers in new settings: different workload traces, higher cluster load, change in DNN workloads and deployment characteristics. Finally, we showcase Blox's extensibility by composing policies from different schedulers, and implementing novel policies with minimal code changes. Blox is available at https://github.com/msr-fiddle/blox.",
    "link": "http://arxiv.org/pdf/2312.12621",
    "session_title": "Session K: Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "0792b56c-8a13-4f6c-918a-ba5f13b21065"
  },
  {
    "title": "Just-In-Time Checkpointing: Low Cost Error Recovery from Deep Learning Training Failures",
    "authors": "Tanmaey  Gupta (Microsoft Research India), Sanjeev  Krishnan (Microsoft Research India), Rituraj  Kumar (Microsoft Research India), Abhishek  Vijeev (Microsoft Research India), Bhargav  Gulavani (Microsoft Research India), Nipun  Kwatra (Microsoft Research India), Ramachandran  Ramjee (Microsoft Research India), Muthian  Sivathanu (Microsoft Research India)",
    "abstract": "Deep Learning training jobs process large amounts of training data using many GPU devices, often running for weeks or months. When hardware or software failures happen, these jobs need to restart, losing the memory state for the Deep Neural Network (DNN) model trained so far, unless checkpointing mechanisms are used to save training state periodically. However, for large models, periodic checkpointing incurs significant steady state overhead, and during recovery, a large number of GPUs need to redo work since the last checkpoint. This is especially problematic when failures are frequent for large DNN (such as Large Language Model) training jobs using many GPUs. In this paper, we present a novel approach of just-in-time checkpointing when failures happen, which enables recovery from failures with just a single minibatch iteration of work replayed by all GPUs. This reduces the cost of error recovery from several minutes to a few seconds per GPU, with nearly zero steady state overhead. This also avoids the guesswork of choosing a checkpointing frequency since failure rates usually have high variance. We discuss how just-in-time checkpointing can be enabled in training code, as well as design of key mechanisms for transparent just-in-time checkpointing without user code change. We analyze the wasted GPU work of just-in-time checkpointing and show that it is less than periodic checkpointing for large numbers of GPUs. We present results from our implementation in modern AI cluster infrastructure.",
    "link": "https://www.semanticscholar.org/paper/e28e266c675acbf131c18af5f6f438a4a1d576a2",
    "session_title": "Session K: Systems for ML",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "af7acf95-2545-4367-bd1e-c333e20a5959"
  },
  {
    "title": "Polynima - Practical Hybrid Recompilation for Multithreaded Binaries",
    "authors": "Chinmay  Deshpande (University of California, Irvine), Fabian  Parzefall (University of California, Irvine), Felicitas  Hetzelt (University of California, Irvine), Michael  Franz (University of California, Irvine)",
    "abstract": "The maintenance of software distributed in its binary form can become challenging over time, due to the lack of vendor support or obsolete build environments. This can be costly when dealing with critical security vulnerabilities that are difficult to fix on a binary level. Moreover, advances in compiler technologies of the past decades remain unavailable to the users of such legacy binaries for performing optimizations and transformations. Binary recompilers aim to bridge this divide by \"lifting\" binary executables to compiler-level intermediate representations (IR) and \"lowering\" them back again. But, current recompilers fail on that promise as they rely on unsound heuristics or impose high tracing overheads. Crucially, no existing recompiler addresses the specific challenges imposed by multithreaded programs that are ubiquitous in the modern software space. To address these challenges, we present Polynima, a binary recompiler that supports the lifting and recompilation of x86/x64 multithreaded binaries while introducing a moderate 1.23x slowdown. We propose a hybrid control flow recovery approach that combines the benefits of static and dynamic techniques while providing an efficient strategy to handle unknown paths. Polynima enables the use of the rich LLVM compiler ecosystem to fix and improve legacy multithreaded binaries, which we demonstrate by mitigating a critical synchronization issue in a FTP server binary. We also leverage its functional IR to introduce a novel dynamic analysis to detect implicit synchronization primitives in binaries, which we use to further improve performance of the output. Finally, we evaluate the generality and correctness of Polynima by recompiling a diverse set of real-world, multithreaded binaries and benchmark suites. To our knowledge, Polynima is the first recompiler to be able to do so.",
    "link": "https://www.semanticscholar.org/paper/e4c8265165419534342583a51a96bb7c938367b9",
    "session_title": "Session L: Distributed Systems + Runtime and Compilers",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "0a205d13-a6fe-4e8d-bc22-5ccefc0b36ce"
  },
  {
    "title": "Transparent Multicore Scaling of Single-Threaded Network Functions",
    "authors": "Lei  Yan (EPFL), Yueyang  Pan (EPFL), Diyu  Zhou (EPFL), George  Candea (EPFL), Sanidhya  Kashyap (EPFL)",
    "abstract": "This paper presents NFOS, a programming model, runtime, and profiler for productively developing software network functions (NFs) that scale on multicore machines. Writing shared-state concurrent systems that are both correct and scalable is still a serious challenge, which is why NFOS insulates developers from writing concurrent code. In the NFOS programming model, developers write their NF as a sequential program, concerning themselves with the NF logic instead of parallelism and shared-state synchronization. The NFOS abstractions are both familiar to the NF programmer and convey to the NFOS runtime crucial information that enables it to correctly execute the NF's packet processing in parallel on multiple cores. Paired with NFOS's domain-specific concurrent data structures, this parallelism scales the NF transparently, obviating the need for developers to write concurrent code. We show that serial, stateful NFs run atop NFOS achieve scalability on par with their concurrent, hand-optimized counterparts in Cisco VPP [8]. Some scalability bottlenecks are inherent to the NF's semantics, and thus cannot be resolved while preserving those semantics. NFOS identifies the root causes of such bottlenecks and provides scalability recipes that guide developers in relaxing the NF's semantics to eliminate these bottlenecks. We present examples where such NFOS-guided relaxation of NF semantics further improves scalability by 2x to 91x.",
    "link": "https://www.semanticscholar.org/paper/bdb998c0ed135789389c6411b317ea8c45042c78",
    "session_title": "Session L: Distributed Systems + Runtime and Compilers",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "0a2e5d1d-bae6-4835-bee7-a43dfc1b27d8"
  },
  {
    "title": "Jade: A High-throughput Concurrent Copying Garbage Collector",
    "authors": "Mingyu  Wu (Shanghai Jiao Tong University), Liang  Mao (Alibaba Group), Yude  Lin (Alibaba Group), Yifeng  Jin (Alibaba Group), Zhe  Li (Shanghai Jiao Tong University), Hongtao  Lyu (Shanghai Jiao Tong University), Jiawei  Tang (Alibaba Group), Xiaowei  Lu (Alibaba Group), Hao  Tang (Alibaba Group), Denghui  Dong (Alibaba Group), Haibo  Chen (Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems), Binyu  Zang (Shanghai Jiao Tong University; Engineering Research Center for Domain-specific Operating Systems)",
    "abstract": "Garbage collection (GC) pauses are a notorious issue threatening the latency of applications. To mitigate this problem, state-of-the-art concurrent copying collectors allow GC threads to run simultaneously with application threads (mutators) in nearly all GC phases. However, the design of concurrent copying collectors does not always lead to low application latency. To this end, this work studies the behaviors of mainstream concurrent copying collectors in OpenJDK and mainly focuses on long application pauses under heavy workloads. By analyzing the design of those collectors, this work uncovers that lengthy pre-reclamation cycles (including GC phases before actual memory release), high GC frequency, and large metadata maintenance overhead are major factors for long pauses. Therefore, this work proposes Jade, a concurrent copying collector aiming to achieve both short pauses and high GC efficiency. Compared with existing collectors, Jade provides a group-wise collection mechanism to shorten pre-reclamation cycles while controlling GC frequency. It also embraces a generational heap layout and a single-phase algorithm to maximize young GC's throughput. The evaluation results on representative latency-critical applications show that Jade can reach sub-millisecond-level pauses even under heavy workloads and significantly improve applications' peak throughput compared with state-of-the-art concurrent collectors.",
    "link": "https://www.semanticscholar.org/paper/c5b5944fbe34c3de6162b0fe7251c51d83bbe721",
    "session_title": "Session L: Distributed Systems + Runtime and Compilers",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "89755545-b0ed-4b52-96b4-944e1078c6fd"
  },
  {
    "title": "Adaptable Runtime Monitoring for Intermittent Systems",
    "authors": "Eren  Yildiz (Ege University), Khakim  Akhunov (University of Trento), Lorenzo Antonio  Riva (University of Trento), Arda  Goknil (SINTEF Digital), Ivan  Kurtev (Eindhoven University of Technology), Kasim Sinan  Yildirim (University of Trento)",
    "abstract": "Batteryless energy harvesting devices compute intermittently due to power failures that frequently interrupt the computational activity and lead to charging delays. To ensure functional correctness in intermittent computing, applications must exhibit several unique properties, such as guarantees for computational progress despite power failures and prevention of stale operations caused by charging delays. We observe that current software support for intermittent computing allows for checking only a fixed set of properties and leads to tightly coupled application and property-checking, thus hampering modularity, scalability, and maintainability. In this paper, we present ARTEMIS, the first framework designed to facilitate flexible property checking of intermittent programs at runtime. ARTEMIS is developed based on techniques from the area of runtime monitoring, offers a specification language for specifying an open set of properties, and provides automatic generation of monitors responsible for checking the properties. Our evaluation showed that ARTEMIS achieves comparable efficiency to state-of-the-art solutions while significantly preventing failure scenarios through its monitoring capabilities.",
    "link": "https://www.semanticscholar.org/paper/472862edb6b8066b1a4c981a9c36d912187aa85b",
    "session_title": "Session L: Distributed Systems + Runtime and Compilers",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "cc334e38-c503-4932-8ac1-06f34fc70f4f"
  },
  {
    "title": "Save the Bruised Striver: A Reliable Live Patching Framework for Protecting Real-World PLCs",
    "authors": "Ming  Zhou (School of Cyber Science and Engineering, Nanjing University of Science and Technology), Haining  Wang (Virginia Tech), Ke  Li (NARI Group Corporation State Grid Electric Power Research Institute), Hongsong  Zhu (School of Cyber Security, University of Chinese Academy of Sciences, Institute of Information Engineering, CAS), Limin  Sun (School of Cyber Security, University of Chinese Academy of Sciences, Institute of Information Engineering, CAS)",
    "abstract": "Industrial Control Systems (ICS), particularly programmable logic controllers (PLCs) responsible for managing underlying physical infrastructures, often operate for extended periods without interruption. Thus, it is challenging to patch security vulnerabilities of ICS in a timely manner after disclosure because it often necessitates waiting for a rare downtime window. While live patching has been introduced to avoid downtime and maintenance costs, conventional live patching methods are not viable for closed-source PLCs. Without the source code, it is difficult to understand the system behaviors and determine binary patch equivalence. To address these challenges, we present a Reliable Live Patching framework called RLPatch for applying live patches to third-party binary without source code. We design RLPatch to capture real-time conditions and dynamic behaviors of PLCs, which enables DevOps engineers to identify major non-recoverable fault (MNRF) vulnerabilities and generate hot patches. The core of RLPatch is an update agent that inserts breakpoints over the original MNRF code and then directs execution to the patches. To ensure system reliability, we use the unique constraints of PLCs to integrate the update processes with the scan cycle. We leverage RLPatch to patch 20 real vulnerabilities in three widely used Rockwell PLCs. We evaluate RLPatch in a real-world gas pipeline, demonstrating its reliability and effectiveness in practice.",
    "link": "https://vtechworks.lib.vt.edu/bitstreams/9fe2b486-7716-48ef-a519-2f91f196d4ae/download",
    "session_title": "Session L: Distributed Systems + Runtime and Compilers",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "aed906a7-340d-478b-883a-992cd5f5832a"
  },
  {
    "title": "Efficient Auditing of Event-driven Web Applications",
    "authors": "Ioanna  Tzialla (NYU and Google), Jeffery  Wang (NYU), Jingyi  Zhu (ETH Zurich), Aurojit  Panda (NYU), Michael  Walfish (NYU)",
    "abstract": "When a deployer of a web application puts that application on a server (on-prem or cloud), how can they be sure that the application is executing as intended? This paper studies how the deployer can efficiently check that the execution is faithful. We seek mechanisms that: (i) work with web applications that are built with modern event-driven web frameworks, (ii) impose tolerable computation and communication overheads on the web server, and (iii) are complete and sound. We exhibit such a mechanism, based on a new record-replay algorithm. We have implemented our algorithm in Karousos, a system that audits Node.js web applications.",
    "link": "https://www.semanticscholar.org/paper/3496fe7782271769fde3cb865513c51a37c5e6f3",
    "session_title": "Session L: Distributed Systems + Runtime and Compilers",
    "conference_name": "EuroSys",
    "date": "2024-04-22",
    "paper_id": "b64b5715-174c-406d-a047-807547a55743"
  }
]