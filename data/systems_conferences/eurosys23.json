[
  {
    "title": "Effective Performance Issue Diagnosis with Value-Assisted Cost Profiling ",
    "authors": "Lingmei Weng (Columbia University), Yigong Hu (Johns Hopkins University), Peng Huang (University of Michigan), Jason Nieh (Columbia University), Junfeng Yang (Columbia University)",
    "abstract": "Diagnosing performance issues is often difficult, especially when they occur only during some program executions. Profilers can help with performance debugging, but are ineffective when the most costly functions are not the root causes of performance issues. To address this problem, we introduce a new profiling methodology, value-assisted cost profiling, and a tool vProf. Our insight is that capturing the values of variables can greatly help diagnose performance issues. vProf continuously records values while profiling normal and buggy program executions. It identifies anomalies in the values and the functions where they occur to pinpoint the real root causes of performance issues. Using a set of 15 real-world performance bugs in four widely used applications, we show that vProf is effective at diagnosing all of the issues while other state-of-the-art tools diagnose only a few of them. We further use vProf to diagnose longstanding performance issues in these applications that have been unresolved for over four years.",
    "link": "https://www.semanticscholar.org/paper/c14e84b7971c696ca605927dd4c8ebc95d333495",
    "session_title": "Debugging 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "701541e3-3208-4afd-a493-7ae8fd976d47"
  },
  {
    "title": "Foxhound: Server-Grade Observability for Network-Augmented Applications ",
    "authors": "Lucas Castanheira (Carnegie Mellon University), Alberto Schaeffer-Filho (Federal University of Rio Grande do Sul (UFRGS)), Theophilus A. Benson (Brown University)",
    "abstract": "There is a growing move to offload functionality, e.g., TCP or key-value stores, into programmable networks - either on SmartNICs or programmable switches. While offloading promises significant performance boosts, these programmable devices often provide little visibility into their performance. Moreover, many existing tools for analyzing and debugging performance problems, e.g., distributed tracing, do not extend into these devices. Motivated by this lack of visibility, we present the design and implementation of an observability framework called Foxhound, which introduces a co-designed query language, compiler, and storage abstraction layer for expressing, capturing and analyzing distributed traces and their performance data across an infrastructure comprising servers and programmable data planes. While general, Foxhound's query language offers optimized constructs which can circumvent limitations of programmable devices by pushing down operations to hardware. We have evaluated Foxhound using a Tofino switch and a large scale simulator. Our evaluations show that our storage layer can support common tracing tasks and detect associated problems at scale.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3567502",
    "session_title": "Debugging 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "d6a80e52-6cda-4bf6-b5cc-2c6d647d390b"
  },
  {
    "title": "OFence: Pairing Barriers to Find Concurrency Bugs in the Linux Kernel (BEST PAPER AWARD) ",
    "authors": "Baptiste Lepers (Université de Neuchâtel), Josselin Giet (ENS), Willy Zwaenepoel (The University of Sydney), Julia Lawall (Inria)",
    "abstract": "",
    "link": "",
    "session_title": "Debugging 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "3b1625d1-8832-48a2-959c-08980ace12ea"
  },
  {
    "title": "Pocket: ML Serving from the Edge ",
    "authors": "Misun Park (Georgia Institute of Technology), Ketan Bhardwaj (Georgia Institute of Technology), Ada Gavrilovska (Georgia Institute of Technology)",
    "abstract": "One of the major challenges in serving ML applications is the resource pressure introduced by the underlying ML frameworks. This becomes a bigger problem at resource-constrained, multi-tenant edge server locations, where it is necessary to scale to a larger number of clients with a fixed resource envelope. Naive approaches which simply minimize the resource budget allocation of each application result in performance degradation that voids the benefits expected from operating at the edge. This paper presents Pocket - a new approach for serving ML applications in settings like the edge, based on a shared ML runtime backend as a service and lightweight ML application pocket containers. Key to realizing Pocket is use of lightweight IPC, support for cross-client isolation, and a novel resource amplification method which inlines resource reallocation with IPC. The latter ensures just-in-time assignment of the limited edge resources where they're most needed, thereby reducing contention effects and boosting overall performance and efficiency. Experimental evaluations demonstrate that Pocket can scale to 1.3--20× more clients with the same amount of resources while reducing response time by 20--80% compared to monolithic designs.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587459",
    "session_title": "Edge",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "01fd8d10-3516-497b-b8ab-ddbb02f530c0"
  },
  {
    "title": "Efficient and Safe I/O Operations for Intermittent Systems ",
    "authors": "Eren Yildiz (Ege University), Saad Ahmed (Georgia Institute of Technology), Bashima Islam (Worcester Polytechnic Institute), Josiah Hester (Georgia Institute of Technology), Kasim Sinan Yildirim (University of Trento)",
    "abstract": "Task-based intermittent software systems always re-execute peripheral input/output (I/O) operations upon power failures since tasks have all-or-nothing semantics. Re-executed I/O wastes significant time and energy and risks memory inconsistency. This paper presents EaseIO, a new task-based intermittent system that remedies these problems. EaseIO programming interface introduces re-execution semantics for I/O operations to facilitate safe and efficient I/O management for intermittent applications. EaseIO compiler front-end considers the programmer-annotated I/O re-execution semantics to preserve the task's energy efficiency and idem-potency. EaseIO runtime introduces regional privatization to eliminate memory inconsistency caused by idempotence bugs. Our evaluation shows that EaseIO reduces the wasted useful I/O work by up to 3× and total execution time by up to 44% by avoiding 76% of the redundant I/O operations, as compared to the state-of-the-art approaches for intermittent computing. Moreover, for the first time, EaseIO ensures memory consistency during DMA-based I/O operations.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587435",
    "session_title": "Edge",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "0b44813f-a3a4-448a-8b28-f73c140efabf"
  },
  {
    "title": "ICE: Collaborating Memory and Process Management for User Experience on Resource-limited Mobile Devices ",
    "authors": "Changlong Li (East China Normal University), Yu Liang (City University of Hong Kong), Rachata Ausavarungnirun (King Mongkut's University of Technology North Bangkok), Zongwei Zhu (University of Science and Technology of China),\n                                            Liang Shi (East China Normal University), Chuan Jason Xue (City University of Hong Kong)",
    "abstract": "Mobile devices with limited resources are prevalent as they have a relatively low price. Providing a good user experience with limited resources has been a big challenge. This paper found that foreground applications are often unexpectedly interfered by background applications' memory activities. Improving user experience on resource-limited mobile devices calls for a strong collaboration between memory and process management. This paper proposes a framework, Ice, to optimize the user experience on resource-limited mobile devices. With Ice, processes that will cause frequent refaults in the background are identified and frozen accordingly. The frozen application will be thawed when memory condition allows. Evaluation of resource-limited mobile devices demonstrates that the user experience is effectively improved with Ice. Specifically, Ice boosts the frame rate by 1.57x on average over the state-of-the-art.",
    "link": "https://www.semanticscholar.org/paper/22ff3dcfa8a6ab01c528b710a4a7dff9f86df3c9",
    "session_title": "Edge",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "faf82a73-9531-460b-97f9-d1f00bf7f88d"
  },
  {
    "title": "Diagnosing Kernel Concurrency Failures with AITIA ",
    "authors": "Dae R. Jeong (KAIST), Minkyu Jung (KAIST), Yoochan Lee (Seoul National University), Byoungyoung Lee (Seoul National University), Insik Shin (KAIST), Youngjin Kwon (KAIST)",
    "abstract": "Kernel concurrency failures are notoriously difficult to identify and diagnose their fundamental reason, the root cause. Kernel concurrency bugs frequently involve challenging patterns such as multi-variable races, data races with asynchronous kernel threads, and pervasive benign races. We perform an in-depth study of real-world kernel concurrency bugs and elicit three requirements: comprehensiveness, pattern-agnostic, and conciseness. To fulfill the requirements, this paper defines the root cause as a chained sequence of data races, called a causality chain. A causality chain is presented as a comprehensive form to explain how a failure eventually happens in the presence of multi-variable races rather than simply pointing out a few instructions related to the root cause. To build a causality chain, this work proposes two practical approaches: Least Interleaving First Search to reproduce a concurrency failure, and Causality Analysis to identify the root cause. Causality Analysis runs the kernel to confirm what data races contribute to the failure among all detected data races. The approach is pattern-agnostic because it dynamically tests data races without counting on pre-defined patterns. While testing data races, Causality Analysis rules out failure-irrelevant data races such as benign races, producing a concise causality chain. Aitia is a system implementing the two approaches. By evaluating Aitia with 22 real-world concurrency failures, we show that Aitia can successfully build their causality chain. With Aitia, we found the root causes of six unfixed bugs; three bugs were concurrently fixed, the root causes of three bugs were confirmed by kernel developers.",
    "link": "https://www.semanticscholar.org/paper/694bba47b91ea3ed3b0b155c809e85607dce9db4",
    "session_title": "Debugging 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "004d9f88-035f-4fa6-8cf6-127abb12c9a3"
  },
  {
    "title": "WAFFLE: Exposing Memory Ordering Bugs Efficiently with Active Delay Injection ",
    "authors": "Bogdan Alexandru Stoica (University of Chicago), Shan Lu (University of Chicago), Madanlal Musuvathi (Microsoft Research), Suman Nath (Microsoft Research)",
    "abstract": "Concurrency bugs are difficult to detect, reproduce, and diagnose, as they manifest under rare timing conditions. Recently, active delay injection has proven efficient for exposing one such type of bug --- thread-safety violations --- with low overhead, high coverage, and minimal code analysis. However, how to efficiently apply active delay injection to broader classes of concurrency bugs is still an open question. We aim to answer this question by focusing on MemOrder bugs --- a type of concurrency bug caused by incorrect timing between a memory access to a particular object and the object's initialization or deallocation. We first show experimentally that the current state-of-the-art delay injection technique leads to high overhead and low detection coverage since MemOrder bugs exhibit particular characteristics that cause high delay density and interference. Based on these insights, we propose Waffle --- a delay injection tool that tailors key design points to better match the nature of MemOrder bugs. Evaluating our tool on 11 popular open-source multi-threaded C# applications shows that Waffle can expose more bugs with less overhead than state-of-the-art techniques.",
    "link": "https://www.semanticscholar.org/paper/60eb986e6742417e3c209feb71579e26755ace16",
    "session_title": "Debugging 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "5cce44e3-0cff-4c00-8f05-1496ffd3dd41"
  },
  {
    "title": "Model Checking Guided Testing for Distributed Systems ",
    "authors": "Dong Wang (Institute of Software Chinese Academy of Sciences), Wensheng Dou (Institute of Software Chinese Academy of Sciences), Yu Gao (Institute of Software Chinese Academy of Sciences), Chenao Wu (Institute of Software Chinese Academy of Sciences),\n                                            Jun Wei (Institute of Software Chinese Academy of Sciences), Tao Huang (Institute of Software Chinese Academy of Sciences)",
    "abstract": "Distributed systems have become the backbone of cloud computing. Incorrect system designs and implementations can greatly impair the reliability of distributed systems. Although a distributed system design modelled in the formal specification can be verified by formal model checking, it is still challenging to figure out whether its corresponding implementation conforms to the verified specification. An incorrect system implementation can violate its verified specification, and causes intricate bugs. In this paper, we propose a novel distributed system testing technique, Model checking guided testing (Mocket), to fill the gap between the specification and its implementation in a distributed system. Specially, we use the state space generated by formal model checking to guide the testing for the system implementation, and unearth bugs in the target distributed system. To evaluate the feasibility and effectiveness of Mocket, we apply Mocket on three popular distributed systems, and find 3 previously unknown bugs in them.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587442",
    "session_title": "Debugging 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "34d6595e-5ccd-4ff9-ba52-3c4ee67c6769"
  },
  {
    "title": "MariusGNN: Resource-Efficient Out-of-Core Training of Graph Neural Networks ",
    "authors": "Roger Waleffe (University of Wisconsin-Madison), Jason Mohoney (University of Wisconsin-Madison), Theodoros Rekatsinas (ETH Zurich), Shivaram Venkataraman (University of Wisconsin-Madison)",
    "abstract": "We study training of Graph Neural Networks (GNNs) for large-scale graphs. We revisit the premise of using distributed training for billion-scale graphs and show that for graphs that fit in main memory or the SSD of a single machine, out-of-core pipelined training with a single GPU can outperform state-of-the-art (SoTA) multi-GPU solutions. We introduce MariusGNN, the first system that utilizes the entire storage hierarchy---including disk---for GNN training. MariusGNN introduces a series of data organization and algorithmic contributions that 1) minimize the end-to-end time required for training and 2) ensure that models learned with disk-based training exhibit accuracy similar to those fully trained in memory. We evaluate MariusGNN against SoTA systems for learning GNN models and find that single-GPU training in MariusGNN achieves the same level of accuracy up to 8× faster than multi-GPU training in these systems, thus, introducing an order of magnitude monetary cost reduction. MariusGNN is open-sourced at www.marius-project.org.",
    "link": "https://arxiv.org/pdf/2202.02365",
    "session_title": "Graph",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "59f8b380-cc61-48c5-a7d3-e82fe2d5b364"
  },
  {
    "title": "Accelerating Graph Mining Systems with Subgraph Morphing ",
    "authors": "Kasra Jamshidi (Simon Fraser University), Harry Xu (UCLA), Keval Vora (Simon Fraser University)",
    "abstract": "Graph mining applications analyze the structural properties of large graphs. These applications are computationally expensive because finding structural patterns requires checking subgraph isomorphism, which is NP-complete. This paper exploits the sub-structural similarities across different patterns by employing Subgraph Morphing to accurately infer the results for a given set of patterns from the results of a completely different set of patterns that are less expensive to compute. To enable Subgraph Morphing in practice, we develop efficient query transformation techniques as well as automatic result conversion strategies for different application scenarios. We have implemented Subgraph Morphing in four state-of-the-art graph mining and subgraph matching systems: Peregrine, AutoMine/- GraphZero, GraphPi, and BigJoin; a thorough evaluation demonstrates that Subgraph Morphing improves the performance of these four systems by 34×, 10×, 18×, and 13×, respectively.",
    "link": "https://www.semanticscholar.org/paper/53c4182dfd692fbf7e35a3f753842db5153b759d",
    "session_title": "Graph",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "7e98818f-9eb7-40ec-bcf8-693a4ba4254e"
  },
  {
    "title": "TEA: A General-Purpose Temporal Graph Random Walk Engine ",
    "authors": "Chengying Huan (Tsinghua University and Baihai Technology Inc.), Shuaiwen Leon Song (University of Sydney), Santosh Pandey (Stevens Institute of Technology), Hang Liu (Stevens Institute of Technology), Yongchao Liu (Ant Group),\n                                            Baptiste Lepers (Université de Neuchâtel), Changhua He (Ant Group), Kang Chen (Tsinghua University), Jinlei Jiang (Tsinghua University), Yongwei Wu (Tsinghua University)",
    "abstract": "Many real-world graphs are temporal in nature, where the temporal information indicates when a particular edge is changed (e.g., edge insertion and deletion). Performing random walks on such temporal graphs is of paramount value. The state-of-the-art sampling strategies are tailored for conventional static graphs and thus cannot effectively tackle the dynamic nature of temporal graphs due to several significant efficiency challenges, i.e., high sampling complexity, gigantic index space, and poor programmability. In this paper, we present TEA, the first highly-efficient general-purpose TEmporal grAph random walk engine. At its core, TEA introduces a new hybrid sampling approach that combines two Monte Carlo sampling methods together to drastically reduce space complexity and achieve high sampling speed. TEA further employs a series of algorithmic and system-level optimizations to remarkably improve the sampling efficiency, as well as provide streaming graph support. Finally, we introduce a temporal-centric programming model to ease the implementation of various random walk algorithms on temporal graphs. Experimental results demonstrate that TEA can achieve up to 3 orders of magnitude speedups over the state-of-the-art random walk engines on large temporal graphs.",
    "link": "https://www.semanticscholar.org/paper/430c1151c3f58149a256fb2c21544031805ed86d",
    "session_title": "Graph",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "eb9154bd-9c36-4f71-b245-4afe5622533f"
  },
  {
    "title": "ALT: Breaking the Wall between Data Layout and Loop Optimizations for Deep Learning Compilation ",
    "authors": "Zhiying Xu (Nanjing University), Jiafan Xu (Nanjing University), Hongding Peng (Nanjing University), Wei Wang (Nanjing University), Xiaoliang Wang (Nanjing University), Haoran Wan (Nanjing University), Haipeng Dai (Nanjing University),\n                                            Yixu Xu (Huawei Technologies), Hao Cheng (Huawei Technologies), Kun Wang (University of California, Los Angeles), Guihai Chen (Nanjing University)",
    "abstract": "Deep learning models rely on highly optimized tensor libraries for efficient inference on heterogeneous hardware. Current deep compilers typically predetermine layouts of tensors and then optimize loops of operators. However, such unidirectional and one-off workflow strictly separates graph-level optimization and operator-level optimization into different system layers, missing opportunities for unified tuning. This paper proposes ALT, a deep compiler that performs joint graph-level layout optimization and operator-level loop optimization. ALT provides a generic transformation module to manipulate layouts and loops with easy-to-use primitive functions. ALT further integrates an auto-tuning module that jointly optimizes graph-level data layouts and operator-level loops while guaranteeing efficiency. Experimental results show that ALT significantly outperforms state-of-the-art compilers (e.g., Ansor) in terms of both single operator performance (e.g., 1.5× speedup on average) and end-to-end inference performance (e.g., 1.4× speedup on average).",
    "link": "https://www.semanticscholar.org/paper/d19156774f1675f07c74e72457483bbd0d9ef258",
    "session_title": "Machine Learning 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "66c58853-6b49-4b4a-b140-81a7c43ff2bd"
  },
  {
    "title": "REFL: Resource-Efficient Federated Learning ",
    "authors": "Ahmed M. Abdelmoniem (Queen Mary University of London), Atal Narayan Sahu (KAUST), Marco Canini (KAUST), Suhaib A. Fahmy (KAUST)",
    "abstract": "Federated Learning (FL) enables distributed training by learners using local data, thereby enhancing privacy and reducing communication. However, it presents numerous challenges relating to the heterogeneity of the data distribution, device capabilities, and participant availability as deployments scale, which can impact both model convergence and bias. Existing FL schemes use random participant selection to improve the fairness of the selection process; however, this can result in inefficient use of resources and lower quality training. In this work, we systematically address the question of resource efficiency in FL, showing the benefits of intelligent participant selection, and incorporation of updates from straggling participants. We demonstrate how these factors enable resource efficiency while also improving trained model quality.",
    "link": "https://arxiv.org/pdf/2111.01108",
    "session_title": "Machine Learning 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "4f5b3077-f9b9-47c3-9110-34e724e47100"
  },
  {
    "title": "Tabi: An Efficient Multi-Level Inference System for Large Language Models ",
    "authors": "Yiding Wang (Hong Kong University of Science and Technology), Kai Chen (Hong Kong University of Science and Technology), Haisheng Tan (University of Science and Technology of China), Kun Guo (Fuzhou University)",
    "abstract": "Today's trend of building ever larger language models (LLMs), while pushing the performance of natural language processing, adds significant latency to the inference stage. We observe that due to the diminishing returns of adding parameters to LLMs, a smaller model could make the same prediction as a costly LLM for a majority of queries. Based on this observation, we design Tabi, an inference system with a multi-level inference engine that serves queries using small models and optional LLMs for demanding applications. Tabi is optimized for discriminative models (i.e., not generative LLMs) in a serving framework. Tabi uses the calibrated confidence score to decide whether to return the accurate results of small models extremely fast or re-route them to LLMs. For re-routed queries, it uses attention-based word pruning and weighted ensemble techniques to offset the system overhead and accuracy loss. We implement and evaluate Tabi with multiple tasks and models. Our result shows that Tabi achieves 21%-40% average latency reduction (with comparable tail latency) over the state-of-the-art while meeting LLM-grade high accuracy targets.",
    "link": "https://repository.hkust.edu.hk/ir/bitstream/1783.1-125990/1/125990-1.pdf",
    "session_title": "Machine Learning 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "2f582fde-0ef1-47cb-81a9-533dd273dee4"
  },
  {
    "title": "Fast and Efficient Model Serving Using Multi-GPUs with Direct-Host-Access ",
    "authors": "Jinwoo Jeong (Ajou University), Seungsu Baek (Ajou University), Jeongseob Ahn (Ajou University)",
    "abstract": "As deep learning (DL) inference has been widely adopted for building user-facing applications in many domains, it is increasingly important for DL inference servers to achieve high throughput while preserving bounded latency. DL inference requests can be immediately served if the corresponding model is already in the GPU memory. Otherwise, it needs to load the model from host to GPU, adding a significant delay to inference. This paper proposes DeepPlan to minimize inference latency while provisioning DL models from host to GPU in server environments. First, we take advantage of the direct-host-access facility provided by commodity GPUs, allowing access to particular layers of models in the host memory directly from GPU without loading. Second, we parallelize model transmission across multiple GPUs to reduce the time for loading models from host to GPU. We show that a single inference can achieve a 1.94× speedup compared with the state-of-the-art pipelining approach for BERT-Base. When deploying multiple BERT, RoBERTa, and GPT-2 instances on a DL inference serving system, DeepPlan shows a significant performance improvement compared to the pipelining technique and stable 99% tail latency.",
    "link": "https://www.semanticscholar.org/paper/ef0785ac346a14198c49189a457d916fa58c7299",
    "session_title": "Machine Learning 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "2f331697-3f9b-4e6b-99df-4a1d51f47a89"
  },
  {
    "title": "DiLOS: Do Not Trade Compatibility for Performance in Memory Disaggregation ",
    "authors": "Wonsup Yoon (KAIST), Jisu Ok (KAIST), Jinyoung Oh (KAIST), Sue Moon (KAIST), Youngjin Kwon (KAIST)",
    "abstract": "Memory disaggregation has replaced the landscape of dat-acenters by physically separating compute and memory nodes, achieving improved utilization. As early efforts, kernel paging-based approaches offer transparent virtual memory abstraction for remote memory with paging schemes but suffer from expensive page fault handling. This paper revisits the paging-based approaches and challenges their performance in paging schemes. We posit that the overhead of the paging-based approaches is not a fundamental limitation. We propose DiLOS, a new library operating system (LibOS) specialized for paging-based memory disaggregation. We have revamped the page fault handler to get away with the swap cache and incorporated known techniques in our prefetcher, page manager, and communication module for performance optimization. Furthermore, we provide APIs to augment the LibOS with application semantics. We present two app-aware guides, app-aware prefetching and bandwidth-reducing memory allocator in DiLOS. Through extensive evaluation of microbenchmarks and applications, we demonstrate that DiLOS outperforms the state-of-the-art kernel paging-based system (Fastswap) up to 2.24× and a recent user-level system (AIFM) 1.54× on a real-world data analytic workload.",
    "link": "https://www.semanticscholar.org/paper/1391450002d6cde195e49892196cf6537175b599",
    "session_title": "Memory",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "623e1988-b5e7-44a8-9bd1-cdd001f0770d"
  },
  {
    "title": "vTMM: Tiered Memory Management for Virtual Machines ",
    "authors": "Sai Sha (Peking University), Chuandong Li (Peking University), Yingwei Luo (Peking University), Xiaolin Wang (Peking University), Zhenlin Wang (Michigan Technological University)",
    "abstract": "The memory demand of virtual machines (VMs) is increasing, while the traditional DRAM-only memory system has limited capacity and high power consumption. The tiered memory system can effectively expand the memory capacity and increase the cost efficiency. Virtualization introduces new challenges for memory tiering, specifically enforcing performance isolation, minimizing context switching, and providing resource overcommit. However, none of the state-of-the-art designs consider virtualization and thus address these challenges; we observe that a VM with tiered memory incurs up to a 2× slowdown compared to a DRAM-only VM. This paper proposes vTMM, a tiered memory management system specifically designed for virtualization. vTMM automatically determines page hotness and migrates pages between fast and slow memory to achieve better performance. A key insight in vTMM is to leverage the unique system characteristics in virtualization to meet the above challenges. Specifically, vTMM tracks memory accesses with page-modification logging (PML) and a multi-level queue design. Next, vTMM quantifies the page \"temperature\" and makes a fine-grained page classification with bucket-sorting. vTMM performs page migration with PML while providing resource overcommit by transparently resizing VM memory through the two-dimensional page tables. In combination, the above techniques minimize overhead, ensure performance isolation and provide dynamic memory partitioning to improve the overall system performance. We evaluate vTMM on a real DRAM+NVM system and a simulated CXL-Memory system. The results show that vTMM outperforms NUMA balancing, Intel Optane memory mode and Nimble (an OS-level tiered memory management system) for VM tiered memory management. Multi-VM co-running results show that vTMM improves the performance of a DRAM+NVM system by 50%--140% and a CXL-Memory system by 16% -- 40%, respectively.",
    "link": "https://www.semanticscholar.org/paper/bbdad4581698c82b8b13364452e9eb7afb8a4bbe",
    "session_title": "Memory",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "3d08e0eb-a7c2-4a8c-931a-a058d269452b"
  },
  {
    "title": "Making Dynamic Page Coalescing Effective on Virtualized Clouds ",
    "authors": "Weiwei Jia (The University of Rhode Island), Jiyuan Zhang (New Jersey Institute of Technology), Jianchen Shan (Hofstra University), Xiaoning Ding (New Jersey Institute of Technology)",
    "abstract": "Using huge pages has become a mainstream method to reduce address translation overhead for big memory workloads in modern computer systems. To create huge pages, system software usually uses page coalescing methods to dynamically combine contiguous base pages. Though page coalescing methods help effectively reduce address translation overhead on native systems, as the paper shows, their effectiveness is substantially undermined on virtualized platforms. The paper identifies this problem and analyzes the causes. It reveals and experimentally confirms that only huge guest pages backed by huge host pages can effectively reduce address translation overhead. Existing page coalescing methods only aim to increase huge pages at each layer, and fail to consider this cross-layer requirement on the alignmentment of huge pages. To address this issue, the paper designs Gemini as a cross-layer solution that guides the formation and allocation of huge pages in the guest and the host. With Gemini, the memory management at one layer is aware of the huge pages at the other layer, and manages carefully the memory regions corresponding to these huge pages. This is to increase the potential of forming and allocating huge pages from these regions and minimize the associated cost. Then, it guides page coalescing and huge page allocation to first consider these regions before other memory regions. Because huge pages are preferentially formed and allocated from these regions and less from other regions, huge guest pages backed by huge host pages can be increased without aggravating the adverse effects incurred by excessive huge pages. Extensive evaluation based on the prototype implementation in Linux/KVM and diverse real-world applications, such as key-value store, web server, and AI workloads, shows that Gemini can reduce TLB misses by up to 83% and improve application performance by up to 126%, compared to state-of-the-art page coalescing methods.",
    "link": "https://www.semanticscholar.org/paper/6fd82720d0e350bd0694f3ba43a901f194ba0820",
    "session_title": "Memory",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "95ac4bf5-3e31-4678-b61f-c6de275c7f16"
  },
  {
    "title": "Omni-Paxos: Breaking the Barriers of Partial Connectivity ",
    "authors": "Harald Ng (KTH Royal Institute of Technology), Seif Haridi (KTH Royal Institute of Technology, RISE Research Institutes of Sweden), Paris Carbone (KTH Royal Institute of Technology, RISE Research Institutes of Sweden)",
    "abstract": "Omni-Paxos is a system for state machine replication that is completely resilient to partial network partitions, a major source of service disruptions in recent years. Omni-Paxos achieves its resilience through a decoupled design that separates the execution and state of leader election from log replication. The leader election builds on the concept of quorum-connected servers, with the sole focus on connectivity. Additionally, by decoupling reconfiguration from log replication, Omni-Paxos provides flexible and parallel log migration that improves the performance and robustness of reconfiguration. Our evaluation showcases two benefits over state-of-the-art protocols: (1) guaranteed recovery in at most four election timeouts under extreme partial network partitions, and (2) up to 8x shorter reconfiguration periods with 46% less I/O at the leader.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587441",
    "session_title": "Misc 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "144b21ba-b209-48c6-ba64-334337d1f6ac"
  },
  {
    "title": "CFS: Scaling Metadata Service for Distributed File System via Pruned Scope of Critical Sections ",
    "authors": "Yiduo Wang (University of Science and Technology of China; Baidu (China) Co., Ltd), Yufei Wu (University of Science and Technology of China), Cheng Li (University of Science and Technology of China,\n                                            Anhui Province Key Laboratory of High Performance Computing), Pengfei Zheng (Baidu (China) Co., Ltd), Biao Cao (Baidu (China) Co., Ltd), Yan Sun (Baidu (China) Co., Ltd), Fei Zhou (Baidu (China) Co., Ltd),\n                                            Yinlong Xu (University of Science and Technology of China; Anhui Province Key Laboratory of High Performance Computing), Yao Wang (Baidu (China) Co., Ltd), Guangjun Xie (Baidu (China) Co., Ltd)",
    "abstract": "There is a fundamental tension between metadata scalability and POSIX semantics within distributed file systems. The bottleneck lies in the coordination, mainly locking, used for ensuring strong metadata consistency, namely, atomicity and isolation. CFS is a scalable, fully POSIX-compliant distributed file system that eliminates the metadata management bottleneck via pruning the scope of critical sections for reduced locking overhead. First, CFS adopts a tiered metadata organization to scale file attributes and the remaining namespace hierarchies independently with appropriate partitioning and indexing methods, eliminating cross-shard distributed coordination. Second, it further scales up the single metadata shard performance by single-shard atomic primitives, shortening the metadata requests' lifespan and removing spurious conflicts. Third, CFS drops the metadata proxy layer but employs the light-weight, scalable client-side metadata resolving. CFS has been running in the production environment of Baidu AI Cloud for three years. Our evaluation with a 50-node cluster and microbenchmarks shows that CFS simultaneously improves the throughput of baselines like HopsFS and InfiniFS by 1.76--75.82× and 1.22--4.10×, and reduces their average latency by up to 91.71% and 54.54%, respectively. Under cases with higher contention and larger directories, CFS' throughput benefits expand by one order of magnitude. For three real-world workloads with data accesses, CFS introduces 1.62--2.55× end-to-end throughput speedups and 35.06--62.47% tail latency reductions over InfiniFS.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587443",
    "session_title": "Misc 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "02aaa2bc-ce7d-4f16-aa3d-04dd4a0e68c4"
  },
  {
    "title": "OLPart: Online Learning based Resource Partitioning for Colocating Multiple Latency-Critical Jobs on Commodity Computers ",
    "authors": "Ruobing Chen (Nankai university), Haosen Shi (Nankai university, NTU), Yusen Li (Nankai university, NTU), Xiaoguang Liu (Nankai University), Gang Wang (Nankai University)",
    "abstract": "Colocating multiple jobs on the same server has been a commonly used approach for improving resource utilization in cloud environments. However, performance interference due to the contention over shared resources makes resource partitioning an important research problem. Partitioning multiple resources coordinately is particularly challenging when multiple latency-critical (LC) jobs are colocated with best-effort (BE) jobs, since the QoS needs to be protected for all the LC jobs. So far, this problem is not well-addressed in the literatures. We propose an online learning based solution, named OL-Part, for partitioning resources among multiple colocated LC jobs and BE jobs. OLPart is designed based on our observation that runtime performance counters can approximately indicate resource sensitivities of jobs. Based on this finding, OLPart leverages contextual multi-armed bandit (CMAB) to design the partitioning solution, which employs the performance counters to enable an intelligent exploration of the search space. Applying CMAB to the resource partitioning problem faces several critical challenges. OLPart proposes several techniques to overcome these challenges. OLPart does not require prior knowledge of jobs and incurs very small overhead. Evaluations demonstrate that OLPart is optimally efficient and robust, which outperforms state-of-the-art solutions with significant margins. OLPart is publicly available at https://github.com/crbnk/OpenOLPart.",
    "link": "https://dl.acm.org/action/downloadSupplement?doi=10.1145%2F3552326.3567490&file=p347-chen-supp.pdf",
    "session_title": "Misc 1",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "5f7bac11-4b09-4f2d-a2b2-40f5d322e607"
  },
  {
    "title": "Palette Load Balancing: Locality Hints for Serverless Functions ",
    "authors": "Mania Abdi (Northeastern University), Samuel Ginzburg (Princeton), Xiayue Charles Lin (Anyscale), Jose Faleiro (Unaffiliated), Gohar Irfan Chaudhry (Azure Systems Research), Inigo Goiri (Azure Systems Research),\n                                            Ricardo Bianchini (Azure Systems Research), Daniel S. Berger (Azure Systems Research), Rodrigo Fonseca (Azure Systems Research)",
    "abstract": "Function-as-a-Service (FaaS) serverless computing enables a simple programming model with almost unbounded elasticity. Unfortunately, current FaaS platforms achieve this flexibility at the cost of lower performance for data-intensive applications compared to a serverful deployment. The ability to have computation close to data is a key missing feature. We introduce Palette load balancing, which offers FaaS applications a simple mechanism to express locality to the platform, through hints we term \"colors\". Palette maintains the serverless nature of the service - users are still not allocating resources - while allowing the platform to place successive invocations related to each other on the same executing node. We compare a prototype of the Palette load balancer to a state-of-the-art locality-oblivious load balancer on representative examples of three applications. For a serverless web application with a local cache, Palette improves the hit ratio by 6x. For a serverless version of Dask, Palette improves run times by 46% and 40% on Task Bench and TPC-H, respectively. On a serverless version of NumS, Palette improves run times by 37%. These improvements largely bridge the gap to serverful implementation of the same systems.",
    "link": "https://dspace.mit.edu/bitstream/1721.1/150837/1/3552326.3567496.pdf",
    "session_title": "Serverless",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "af6c9604-2563-4bd3-8101-41313a8b84e2"
  },
  {
    "title": "With Great Freedom Comes Great Opportunity: Rethinking Resource Allocation for Serverless Functions ",
    "authors": "Muhammad Bilal (Instituto Superior Técnico (ULisboa), INESC-ID, UCLouvain), Marco Canini (KAUST), Rodrigo Fonseca (Azure Systems Research), Rodrigo Rodrigues (Instituto Superior Técnico (ULisboa), INESC-ID)",
    "abstract": "Current serverless offerings give users limited flexibility for configuring the resources allocated to their function invocations. This simplifies the interface for users to deploy server-less computations but creates deployments that are resource inefficient. In this paper, we take a principled approach to the problem of resource allocation for serverless functions, analyzing the effects of automating this choice in a way that leads to the best combination of performance and cost. In particular, we systematically explore the opportunities that come with decoupling memory and CPU resource allocations and also enabling the use of different VM types, and we find a rich trade-off space between performance and cost. The provider can use this in a number of ways, e.g., exposing all these parameters to the user; eliding preferences for performance and cost from users and simply offer the same performance with lower cost; or exposing a small number of choices for users to trade performance for cost. Our results show that, by decoupling memory and CPU allocation, there is the potential to have up to 40% lower execution cost than the preset coupled configurations that are the norm in current serverless offerings. Similarly, making the correct choice of VM instance type can provide up to 50% better execution time. Furthermore, we demonstrate that providers have the flexibility to choose different instance types for the same functions to maximize resource utilization while providing performance within 10--20% of the best resource configuration for each respective function.",
    "link": "https://repository.kaust.edu.sa/bitstream/10754/669332/5/serverless.eurosys23.pdf",
    "session_title": "Serverless",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "454623f9-2cf8-4ac2-88f9-efc13644e394"
  },
  {
    "title": "Groundhog: Efficient Request Isolation in FaaS ",
    "authors": "Mohamed Alzayat (Max Planck Institute for Software Systems (MPI-SWS)), Jonathan Mace (Max Planck Institute for Software Systems (MPI-SWS)), Peter Druschel (Max Planck Institute for Software Systems (MPI-SWS)), Deepak Garg (Max Planck Institute for Software Systems (MPI-SWS))",
    "abstract": "Security is a core responsibility for Function-as-a-Service (FaaS) providers. The prevailing approach isolates concurrent executions of functions in separate containers. However, successive invocations of the same function commonly reuse the runtime state of a previous invocation in order to avoid container cold-start delays. Although efficient, this container reuse has security implications for functions that are invoked on behalf of differently privileged users or administrative domains: bugs in a function's implementation --- or a third-party library/runtime it depends on --- may leak private data from one invocation of the function to a subsequent one. Groundhog isolates sequential invocations of a function by efficiently reverting to a clean state, free from any private data, after each invocation. The system exploits two properties of typical FaaS platforms: each container executes at most one function at a time and legitimate functions do not retain state across invocations. This enables Groundhog to efficiently snapshot and restore function state between invocations in a manner that is independent of the programming language/runtime and does not require any changes to existing functions, libraries, language runtimes, or OS kernels. We describe the design and implementation of Groundhog and its integration with OpenWhisk, a popular production-grade open-source FaaS framework. On three existing benchmark suites, Groundhog isolates sequential invocations with modest overhead on end-to-end latency (median: 1.5%, 95p: 7%) and throughput (median: 2.5%, 95p: 49.6%), relative to an insecure baseline that reuses the container and runtime state.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3567503",
    "session_title": "Serverless",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "b744271b-d5ac-4eba-8274-f5f95895bf5e"
  },
  {
    "title": "Aggregate VM: Why Reduce or Evict VM’s Resources When You Can Borrow Them From Other Nodes? ",
    "authors": "Ho-Ren Chuang (Virginia Tech), Karim Manaouil (The University of Edinburgh), Tong Xing (The University of Edinburgh), Antonio Barbalace (The University of Edinburgh), Pierre Olivier (The University of Manchester), Balvansh Heerekar (Virginia Tech), Binoy Ravindran (Virginia Tech)",
    "abstract": "Hardware resource fragmentation is a common issue in data centers. Traditional solutions based on migration or overcommitment are unacceptably slow, and modern commercial or research solutions like Spot VM may reduce or evict VM's resources anytime. We propose an alternative solution that does not suffer from these drawbacks, the Aggregate VM. We introduce a new distributed hypervisor design, the resource-borrowing hypervisor, which creates Aggregate VMs: distributed VMs that temporarily aggregate fragmented resources belonging to different host machines, which require mobility of virtual CPUs, memory and IO devices. We implement a prototype, FragVisor, which runs guest software transparently. We also propose minimal modifications to the guest OS that can enable significant performance gains. We evaluate FragVisor over a set of microbenchmarks and IaaS-style real applications. Although Aggregate VMs are not a perfect fit for every type of applications, some workloads enjoy significant speedups compared to overcommitted scenarios (up to 3.9x with 4 distributed vCPUs). We further demonstrate that FragVisor is faster than a state-of-the-art competitor, GiantVM (up to 2.5x).",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587452",
    "session_title": "Cloud Computing",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "665f32ce-885d-4bbe-b20b-c7a8546b9502"
  },
  {
    "title": "Understanding and Optimizing Workloads for Unified Resource Management in Large Cloud Platforms ",
    "authors": "Chengzhi Lu (Shenzhen Institute of Advanced Technology, CAS, University of Macau), Huanle Xu (University of Macau, Macau SAR), Kejiang Ye (Shenzhen Institute of Advanced Technology, CAS), Guoyao Xu (Alibaba Group), Liping Zhang (Alibaba Group), Guodong Yang (Alibaba Group), ChengZhong Xu (University of Macau, Macau SAR)",
    "abstract": "To fully utilize computing resources, cloud providers such as Google and Alibaba choose to co-locate online services with batch processing applications in their data centers. By implementing unified resource management policies, different types of complex computing jobs request resources in a consistent way, which can help data centers achieve global optimal scheduling and provide computing power with higher quality. To understand this new scheduling paradigm, in this paper, we first present an in-depth study of Alibaba's unified scheduling workloads. Our study focuses on the characterization of resource utilization, the application running performance, and scheduling scalability. We observe that although computing resources are significantly over-committed under unified scheduling, the resource utilization in Alibaba data centers is still low. In addition, existing resource usage predictors tend to make severe overestimations. At the same time, tasks within the same application behave fairly consistently, and the running performance of tasks can be well-profiled with respect to resource contention on the corresponding physical host. Based on these observations, in this paper, we design Optum, a unified data center scheduler for improving the overall resource utilization while ensuring good performance for each application. Optum formulates an optimization problem to schedule unified task requests, aiming to balance the trade-off between utilization and resource contention. Optum also implements efficient heuristics to solve the optimization problem in a scalable manner. Large-scale experiments demonstrate that Optum can save up to 15% of resources without performance degradation compared to state-of-the-art unified scheduling schemes.",
    "link": "https://www.semanticscholar.org/paper/14cad8da091eb57f4efa0bf859c7c80c0534d07b",
    "session_title": "Cloud Computing",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "a1336096-911a-477b-9aec-2d6d62381408"
  },
  {
    "title": "Fail through the Cracks: Cross-System Interaction Failures in Modern Cloud Systems ",
    "authors": "Lilia Tang (University of Illinois Urbana-Champaign), Chaitanya Bhandari (University of Illinois Urbana-Champaign), Yongle Zhang (Purdue University), Anna Karanika (University of Illinois Urbana-Champaign), Shuyang Ji (University of Illinois Urbana-Champaign), Indranil Gupta (University of Illinois Urbana-Champaign), Tianyin Xu (University of Illinois Urbana-Champaign)",
    "abstract": "Modern cloud systems are orchestrations of independent and interacting (sub-)systems, each specializing in important services (e.g., data processing, storage, resource management, etc.). Hence, cloud system reliability is affected not only by the reliability of each individual system, but also by the interplay between these systems. We observe that many recent production incidents of cloud systems are manifested through interactions across the system boundaries. However, there is a lack of systematic understanding of this emerging mode of failures, which we term as cross-system interaction failures (or CSI failures). This hinders the development of better design, integration practices, and new tooling. In this paper, we discuss cross-system interaction failures based on analyses of (1) 11 CSI-failure-induced cloud incidents of Google, Azure, and AWS, and (2) 120 CSI failure cases of seven widely co-deployed open-source systems. We focus on understanding discrepancies between interacting systems as the root causes of CSI failures---CSI failures cannot be understood by analyzing one single system in isolation. This paper draws attention to this emerging failure mode, provides a comprehensive understanding of CSI failure patterns, and discusses potential approaches for mitigation. We advocate for cross-system testing and verification and demonstrate its potential by cross-testing the Spark-Hive data plane and exposing 15 new discrepancies.",
    "link": "https://www.semanticscholar.org/paper/916557d95f3d95a83d3f3503350c05c42e8c2272",
    "session_title": "Cloud Computing",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "31f659da-050d-43d5-b5ce-b9287e59f00c"
  },
  {
    "title": "LogGrep: Fast and Cheap Cloud Log Storage by Exploiting both Static and Runtime Patterns ",
    "authors": "Junyu Wei (Tsinghua University), Guangyan Zhang (Tsinghua University), Junchao Chen (Tsinghua University), Yang Wang (The Ohio State University), Weimin Zheng (Tsinghua University), Tingtao Sun (Alibaba Group), Jiesheng Wu (Alibaba Group), Jiangwei Jiang (Alibaba Group)",
    "abstract": "In cloud systems, near-line logs are mainly used for debugging, which means they prefer a low query latency for a better user experience, and like any other logs, they also prefer a low overall cost including storage cost to store compressed logs and computation cost to compress logs and execute queries. This paper proposes LogGrep, the first log compression and query tool that structurizes and organizes log data properly in fine-grained units by exploiting both static and runtime patterns. It first parses logs into variable vectors by exploiting static patterns and then extracts runtime pattern(s) automatically within each variable vector with a novel extraction method. Based on these runtime patterns, LogGrep further decomposes the variable vectors into fine-grained units called \"Capsules\" and stamps each Capsule with a summary of its values. During the query process, LogGrep can avoid decompressing and scanning Capsules that cannot possibly match the keywords, with the help of the extracted runtime patterns and the Capsule stamps. We evaluate LogGrep on 21 types of logs from the production environment of Alibaba Cloud, and 16 types of logs from the public datasets. The results show that LogGrep can reduce query latency and overall cost by an order of magnitude compared to state-of-the-art works. Such results have confirmed that exploiting both static and runtime patterns to structurize logs can achieve fast and cheap cloud log storage.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3567484",
    "session_title": "Cloud Computing",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "59d3b77a-31b9-4e69-a7eb-4979420c019b"
  },
  {
    "title": "R2C: AOCR-Resilient Diversity with Reactive and Reflective Camouflage ",
    "authors": "Felix Berlakovich (μCSRL, Research Institute CODE, University of the Bundeswehr Munich), Stefan Brunthaler (μCSRL, Research Institute CODE, University of the Bundeswehr Munich)",
    "abstract": "Address-oblivious code reuse, AOCR for short, poses a substantial security risk, as it remains unchallenged. If neglected, adversaries have a reliable way to attack systems, offering an operational and profitable strategy. AOCR's authors conclude that software diversity cannot mitigate AOCR, because it exposes fundamental limits to diversification. Reactive and reflective camouflage, or R2C for short, is a full-fledged, LLVM-based defense that thwarts AOCR by combining code and data diversification with reactive capabilities through booby traps. R2C includes optimizations using AVX2 SIMD instructions, compiles complex real-world software, such as browsers, and offers full support of C++. R2C thus proves that AOCR poses no fundamental limits to software diversification, but merely indicates that code diversification without data diversification is a dead end. An extensive evaluation along multiple dimensions proves the practicality of R2C. We evaluate the impact of our defense on performance, and find that R2C shows low performance impacts on compute-intensive benchmarks (6.6 -- 8.5% geometric mean on SPEC CPU 2017). A security evaluation indicates R2C's resistance against different types of code-reuse attacks.",
    "link": "https://www.semanticscholar.org/paper/9c284beaab1bb2f87e9325fb6f1de34792d88cf4",
    "session_title": "Security",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "4c9e95e0-a458-46e5-ba7a-e97bcd85e9cd"
  },
  {
    "title": "Safe and Practical GPU Computation in TrustZone ",
    "authors": "Heejin Park (Apple), Felix Lin (University of Virginia)",
    "abstract": "For mobile devices, it is compelling to run sensitive GPU computation within a TrustZone trusted execution environment (TEE). To minimize GPU software deployed in TEE, the replay approach is promising: record CPU/GPU interactions on a full GPU stack outside the TEE; replay the interactions inside the TEE without the GPU stack. A key dilemma is that the recording process must both (1) occur in a safe environment and (2) access the exact GPU models to be used for replay. To this end, we present a novel recording architecture called GR-T: a mobile device possessing the GPU hardware collaborates with a GPU-less cloud service which runs the GPU software; the two parties exercise the GPU hardware/software jointly for recording. To overcome the resultant network delays, GR-T contributes optimizations: register access deferral, speculation, and meta-only synchronization. These techniques reduce the recording delay by 20x, from hundreds of seconds to tens of seconds. Replay-based GPU computation incurs 25% lower delays compared to native execution outside TEE. The code is available at https://github.com/bakhi/GPUReplay.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3567483",
    "session_title": "Security",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "5105a022-16a4-4874-a5bd-a72cb9435611"
  },
  {
    "title": "Dissecting BFT Consensus: In Trusted Components we Trust! (BEST PAPER AWARD) ",
    "authors": "Suyash Gupta (UC Berkeley), Sajjad Rahnama (University of California, Davis), Shubham Pandey (University of California, Davis), Natacha Crooks (UC Berkeley), Mohammad Sadoghi (University of California, Davis)",
    "abstract": "The growing interest in reliable multi-party applications has fostered widespread adoption of Byzantine Fault-Tolerant (bft) consensus protocols. Existing bft protocols need f more replicas than Paxos-style protocols to prevent equivocation attacks. trust-bft protocols seek to minimize this cost by making use of trusted components at replicas. This paper makes two contributions. First, we analyze the design of existing trust-bft protocols and uncover three fundamental limitations that preclude most practical deployments. Some of these limitations are fundamental, while others are linked to the state of trusted components today. Second, we introduce a novel suite of consensus protocols, FlexiTrust, that attempts to sidestep these issues. We show that our FlexiTrust protocols achieve up to 185% more throughput than their trust-bft counterparts.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587455",
    "session_title": "Security",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "b0259432-b0b3-4c4b-8fee-3930878d8b91"
  },
  {
    "title": "Diablo: A Benchmark Suite for Blockchains ",
    "authors": "Vincent Gramoli (University of Sydney), Rachid Guerraoui (EPFL), Andrei Lebedev (University of Sydney), Chris Natoli (University of Sydney), Gauthier Voron (EPFL)",
    "abstract": "With the recent advent of blockchains, we have witnessed a plethora of blockchain proposals. These proposals range from using work to using time, storage or stake in order to select blocks to be appended to the chain. As a drawback it makes it difficult for the application developer to choose the right blockchain to support their applications. In particular, the scalability and performance one can obtain from a specific blockchain is typically unknown. The claimed results are often obtained in isolation by the developers of the blockchain themselves. The experimental conditions corresponding to these results are generally missing and the lack of details make these results irreproducible. In this paper, we propose the most extensive evaluation of blockchain to date. First, we show how the experimental settings impact the performance of 6 state-of-the-art blockchains and argue for more detailed experiments. Second, and to cope with this limitation, we propose a unifying framework to evaluate blockchains on the same ground. The framework includes a suite of 5 realistic Decentralized Applications (DApps), helps deploy the blockchain nodes at different scales and evaluate their performance. Finally, we show that selecting a particular virtual machine or weakening guarantees can help handle computationally demanding workloads but that none of the tested blockchains can yet support the load of these realistic DApps.",
    "link": "https://www.semanticscholar.org/paper/89ffb2143a659e6dfadd7b0e9206c12cfdc4880b",
    "session_title": "Security",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "5370a9d6-d105-4f01-b999-d4e82e502b8f"
  },
  {
    "title": "FrozenHot Cache: Rethinking Cache Management for Modern Hardware ",
    "authors": "Ziyue Qiu (University of Science and Technology of China, Microsoft Research, Carnegie Mellon University), Juncheng Yang (Carnegie Mellon University), Juncheng Zhang (University of Science and Technology of China), Cheng Li (University of Science and Technology of China, Anhui Province Key Laboratory of High Performance Computing),\n                                            Xiaosong Ma (Qatar Computing Research Institute, HBKU), Qi Chen (Microsoft Research), Mao Yang (Microsoft Research), Yinlong Xu (University of Science and Technology of China, Anhui Province Key Laboratory of High Performance Computing)",
    "abstract": "Caching is crucial for accelerating data access, employed as a ubiquitous design in modern systems at many parts of computer systems. With increasing core count, and shrinking latency gap between cache and modern storage devices, hit-path scalability becomes increasingly critical. However, existing production in-memory caches often use list-based management with promotion on each cache hit, which requires extensive locking and poses a significant overhead for scaling beyond a few cores. Moreover, existing techniques for improving scalability either (1) only focus on the indexing structure and do not improve cache management scalability, or (2) sacrifice efficiency or miss-path scalability. Inspired by highly skewed data popularity and short-term hotspot stability in cache workloads, we propose Frozen-Hot, a generic approach to improve the scalability of list-based caches. FrozenHot partitions the cache space into two parts: a frozen cache and a dynamic cache. The frozen cache serves requests for hot objects with minimal latency by eliminating promotion and locking, while the latter leverages the existing cache design to achieve workload adaptivity. We built FrozenHot as a library that can be easily integrated into existing systems. We demonstrate its performance by enabling FrozenHot in two production systems: HHVM and RocksDB using under 100 lines of code. Evaluated using production traces from MSR and Twitter, FrozenHot improves the throughput of three baseline cache algorithms by up to 551%. Compared to stock RocksDB, FrozenHot-enhanced RocksDB shows a higher throughput on all YCSB workloads with up to 90% increase, as well as reduced tail latency.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587446",
    "session_title": "Misc 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "d8e9ba31-b0c7-48ba-96f6-2a9c6b6ae623"
  },
  {
    "title": "Nephele: Extending Virtualization Environments for Cloning Unikernel-based VMs ",
    "authors": "Costin Lupu (University POLITEHNICA of Bucharest), Andrei Albișoru (University POLITEHNICA of Bucharest), Radu Nichita (University POLITEHNICA of Bucharest), Doru-Florin Blânzeanu (University POLITEHNICA of Bucharest), Mihai Pogonaru (University POLITEHNICA of Bucharest), Răzvan Deaconescu (University POLITEHNICA of Bucharest), Costin Raiciu (University POLITEHNICA of Bucharest)",
    "abstract": "Unikernels gained an increasing interest in the recent years because they provide efficient resource allocation and high performance for cloud services by bundling the application with a minimal set of OS services in a guest VM. Although a unikernel is by design small and lightweight, fleets of unikernels based on the same image are not necessarily more efficient than containers because the latter can rely upon OS primitives for sharing memory. Futhermore, porting POSIX applications on top of unikernels brings a new challenge: what does fork() mean in the world of unikernels where there is memory isolation within a VM? Lacking fork() support significantly reduces the applicability of unikernels in popular cloud applications. In this paper we address these shortcomings and show that cloning unikernels makes way to further improvements and enables full functionality of popular cloud applications, such as NGINX and Redis. Our solution, Nephele, extends the Xen virtualization platform and provides autoscaling capabilities to unikernel based VMs. Nephele provides 8x faster instantiation times and can run 3x more active unikernel VMs on the same hardware compared to booting separate unikernels.",
    "link": "https://www.semanticscholar.org/paper/8cba66c24b77ca7bbe9e91e1f0edf27a7aaa689d",
    "session_title": "Misc 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "9ac113f5-b8eb-46d9-9e2a-ccc345796792"
  },
  {
    "title": "Unikernel Linux (UKL) ",
    "authors": "Ali Raza (Boston University), Thomas Unger (Boston University), Matthew Boyd (MIT CSAIL); Eric B. Munson (Boston University), Parul Sohal (Boston University), Ulrich Drepper (Red Hat), Richard Jones (Red Hat), Daniel Bristot de Oliveira (Red Hat), Larry Woodman (Red Hat); Renato Mancuso (Boston University), Jonathan Appavoo (Boston University), Orran Krieger (Boston University)",
    "abstract": "This paper presents Unikernel Linux (UKL), a path toward integrating unikernel optimization techniques in Linux, a general purpose operating system. UKL adds a configuration option to Linux allowing for a single, optimized process to link with the kernel directly, and run at supervisor privilege. This UKL process does not require application source code modification, only a re-link with our, slightly modified, Linux kernel and glibc. Unmodified applications show modest performance gains out of the box, and developers can further optimize applications for more significant gains (e.g. 26% throughput improvement for Redis). UKL retains support for co-running multiple user level processes capable of communicating with the UKL process using standard IPC. UKL preserves Linux's battle-tested codebase, community, and ecosystem of tools, applications, and hardware support. UKL runs both on bare-metal and virtual servers and supports multi-core execution. The changes to the Linux kernel are modest (1250 LOC).",
    "link": "https://dspace.mit.edu/bitstream/1721.1/150839/1/3552326.3587458.pdf",
    "session_title": "Misc 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "539bcbe6-82a4-4d5b-82fa-de647c20a69b"
  },
  {
    "title": "FlexPass: A Case for Flexible Credit-based Transport for Datacenter Networks ",
    "authors": "Hwijoon Lim (KAIST), Jaehong Kim (KAIST), Inho Cho (MIT CSAIL), Keon Jang (MPI-SWS, Rubrik), Wei Bai (Microsoft Research), Dongsu Han (KAIST)",
    "abstract": "Proactive transports explicitly allocate bandwidth to each sender with credits which schedule packet transmission. While promising, existing proactive solutions share a stringent deployment requirement; they assume the perfect control of every link and packet in the network. However, the assumption breaks in practice because new transports are usually deployed gradually over time and legacy traffic always coexists. In this paper, we present FlexPass, a credit-based transport that takes deployment flexibility as a first-class citizen. FlexPass uses a novel combination of network and end-host designs to solve the problem of co-existence and gradual deployment. FlexPass leverages a proactive control loop to send credit-scheduled packets and a complementary reactive control loop to send unscheduled packets to utilize the spare bandwidth. Finally, FlexPass prevents queue buildups of both scheduled and unscheduled packets, and recovers lost packets efficiently. Our evaluation on the testbed shows that FlexPass maintains co-existence with legacy transports (DCTCP), while preserving the high-performance properties of the proactive transport. In large-scale simulations, we show that FlexPass delivers the best incremental benefits during the gradual deployment. We find traffic upgraded to FlexPass benefits from the bounded queue and reduced flow completion time by up to 44% compared to the legacy traffic, while minimizing the side-effect on the legacy flows.",
    "link": "https://dspace.mit.edu/bitstream/1721.1/150838/1/3552326.3587453.pdf",
    "session_title": "Networking",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "eba254b5-b004-4af8-b9f1-8f3f0105329d"
  },
  {
    "title": "Saba: Rethinking Datacenter Network Allocation from Application’s Perspective ",
    "authors": "M.R. Siavash Katebzadeh (University of Edinburgh), Paolo Costa (Microsoft Research), Boris Grot (University of Edinburgh)",
    "abstract": "Today's datacenter workloads increasingly comprise distributed data-intensive applications, including data analytics, graph processing, and machine-learning training. These applications are bandwidth-hungry and often congest the datacenter network, resulting in poor network performance, which hurts application completion time. Efforts made to address this problem generally aim to achieve max-min fairness at the flow or application level. We observe that splitting the bandwidth equally among workloads is sub-optimal for aggregate application-level performance because various workloads exhibit different sensitivity to network bandwidth: for some workloads, even a small reduction in the available bandwidth yields a significant increase in completion time; for others, the completion time is largely insensitive to the available bandwidth. Building on this insight, we propose Saba, an application-aware bandwidth allocation framework that distributes network bandwidth based on application-level sensitivity. Saba combines ahead-of-time application profiling to determine bandwidth sensitivity with runtime bandwidth allocation using lightweight software support with no modifications to network hardware or protocols. Experiments with a 32-server hardware testbed show that Saba improves average completion time by 1.88× (and by 1.27× in a simulated 1,944-server cluster).",
    "link": "https://www.pure.ed.ac.uk/ws/files/333229151/Saba_Rethinking_KATEBZADEH_DOA26012023_AFV_CC_BY.pdf",
    "session_title": "Networking",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "055e0cd2-753e-4f47-9483-8cf5594a42ee"
  },
  {
    "title": "A2TP: Aggregator-aware In-network Aggregation for Multi-tenant Learning ",
    "authors": "Zhaoyi Li (School of Computer Science and Engineering, Central South University), Jiawei Huang (School of Computer Science and Engineering, Central South University), Yijun Li (School of Computer Science and Engineering, Central South University), Aikun Xu (School of Computer Science and Engineering, Central South University),\n                                            Shengwen Zhou (School of Computer Science and Engineering, Central South University), Jingling Liu (School of Computer Science and Engineering, Central South University), Jianxin Wang (School of Computer Science and Engineering, Central South University)",
    "abstract": "Distributed Machine Learning (DML) techniques are widely used to accelerate the training of large-scale machine learning models. However, during training iterations, gradients need to be frequently aggregated across multiple workers, resulting in communication bottleneck. To reduce the communication overhead of DML, several In-Network Aggregation (INA) protocols are proposed to reduce the volume of aggregation traffic by offloading aggregation functions into switches, thus alleviating network bottlenecks. Nevertheless, these protocols couple the congestion control of in-switch aggregator resources and link bandwidth resources, together with the straggler-oblivious manner in aggregator allocation, leading to low aggregation efficiency. To solve the above problem, we propose an Aggregatoraware in-network Aggregation Transmission Protocol (A2TP), which adopts two congestion windows to decouple the congestion control of two resources and combines with the straggling estimation scheme to efficiently allocate aggregator resources according to the straggler degree for multiple jobs, eliminating the impact of straggler jobs on the overall aggregation process. We implement A2TP at P4-programmable switch and a kernel bypass protocol stack at the end-host. The evaluation results show that A2TP reduces the training time by up to 66% than the state-of-the-art INA protocols in real-world benchmark models.",
    "link": "https://www.semanticscholar.org/paper/d3562d9e310873f49a3f10c93dc1db2e4e0b93ec",
    "session_title": "Networking",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "0ba3c275-45fa-478d-ba69-06900817ce7b"
  },
  {
    "title": "Viper: A Fast Snapshot Isolation Checker ",
    "authors": "Jian Zhang (Northeastern University), Ye Ji (Cockroach Labs), Shuai Mu (Stony Brook University), Cheng Tan (Northeastern University)",
    "abstract": "Snapshot isolation (SI) is supported by most commercial databases and is widely used by applications. However, checking SI today---given a set of transactions, checking if they obey SI---is either slow or gives up soundness. We present viper, an SI checker that is sound, complete, and fast. Viper checks black-box databases and hence is transparent to both users and databases. To be fast, viper introduces BC-polygraphs, a new representation of transaction dependencies. A BC-polygraph is acyclic iff transactions are SI, a theorem that we prove. Viper also introduces heuristic pruning, an optimization to accelerate checking SI by leveraging common knowledge of real-world database implementations. Besides vanilla SI, viper supports major SI variants including Strong SI, Generalized SI, and Strong Session SI. Our experiments show that given the same time budget, viper improves over baselines by 15× in the workload sizes being checked.",
    "link": "https://www.semanticscholar.org/paper/9f96a48d2bc0e77004b32960495685be6c301c63",
    "session_title": "Transactions",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "afad7e44-ef38-4d51-b89c-f4df2cc9356a"
  },
  {
    "title": "Integrating Non-Volatile Main Memory in a Deterministic Database ",
    "authors": "Yu Chen Wang (University of Toronto), Angela Demke Brown (University of Toronto), Ashvin Goel (University of Toronto)",
    "abstract": "Deterministic databases provide strong serializability while avoiding concurrency-control related aborts by establishing a serial ordering of transactions before their execution. Recent work has shown that they can also handle skewed and contended workloads effectively. These properties are achieved by batching transactions in epochs and then executing the transactions within an epoch concurrently and deterministically. However, the predetermined serial ordering of transactions makes these databases more vulnerable to long-latency transactions. As a result, they have mainly been designed as main-memory databases, which limits the size of the datasets that can be supported. We show how to integrate non-volatile main memory (NVMM) into deterministic databases to support larger data-sets at a lower cost per gigabyte and faster failure recovery. We describe a novel dual-version checkpointing scheme that takes advantage of deterministic execution, epoch-based processing and NVMM's byte addressability to avoid persisting all updates to NVMM. Our approach reduces NVMM accesses, provides better access locality, and reduces garbage collection costs, thus lowering the performance impact of using NVMM. We show that our design enables scaling the dataset size while reducing the impacts of using NVMM, achieving up to 79% of DRAM performance. Our design supports efficient failure recovery and outperforms alternative failure recovery designs, especially under contended workloads, by up to 56%.",
    "link": "https://www.semanticscholar.org/paper/89bdcd8fb121d9a516d7157da7ae845a48335f0e",
    "session_title": "Transactions",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "4535a95a-ab83-4361-b4c6-03ada9a4f7e2"
  },
  {
    "title": "Morty: Scaling Concurrency Control with Re-Execution ",
    "authors": "Matthew Burke (Cornell University), Florian Suri-Payer (Cornell University), Jeffrey Helt (Princeton University), Lorenzo Alvisi (Cornell University), Natacha Crooks (UC Berkeley)",
    "abstract": "Serializable systems often perform poorly under high contention. In this work, we analyze this performance limitation through a novel take on conflict windows. Through the lens of these windows, we develop a new concurrency control technique that leverages transaction re-execution to improve throughput scalability under high contention. Our system, Morty, achieves up to 1.7x-96x the throughput of state-of-the-art systems, with similar or better latency.",
    "link": "https://www.semanticscholar.org/paper/0ef8a491a7da4b96b4ee38086260408e6c54178e",
    "session_title": "Transactions",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "e539a4c6-8566-43c4-aa9e-dbd36658b611"
  },
  {
    "title": "RIO: Order-Preserving and CPU-Efficient Remote Storage Access ",
    "authors": "Xiaojian Liao (Tsinghua University), Zhe Yang (Tsinghua University), Jiwu Shu (Tsinghua University)",
    "abstract": "Modern NVMe SSDs and RDMA networks provide dramatically higher bandwidth and concurrency. Existing networked storage systems (e.g., NVMe over Fabrics) fail to fully exploit these new devices due to inefficient storage ordering guarantees. Severe synchronous execution for storage order in these systems stalls the CPU and I/O devices and lowers the CPU and I/O performance efficiency of the storage system. We present Rio, a new approach to the storage order of remote storage access. The key insight in Rio is that the layered design of the software stack, along with the concurrent and asynchronous network and storage devices, makes the storage stack conceptually similar to the CPU pipeline. Inspired by the CPU pipeline that executes out-of-order and commits in-order, Rio introduces the I/O pipeline that allows internal out-of-order and asynchronous execution for ordered write requests while offering intact external storage order to applications. Together with merging consecutive ordered requests, these design decisions make for write throughput and CPU efficiency close to that of orderless requests. We implement Rio in Linux NVMe over RDMA stack, and further build a file system named RioFS atop Rio. Evaluations show that Rio outperforms Linux NVMe over RDMA and a state-of-the-art storage stack named Horae by two orders of magnitude and 4.9× on average in terms of throughput of ordered write requests, respectively. RioFS increases the throughput of RocksDB by 1.9× and 1.5× on average, against Ext4 and HoraeFS, respectively.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3567495",
    "session_title": "Persistence",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "48910082-e3c9-4658-80a3-8172eb74e09b"
  },
  {
    "title": "Chipmunk: Investigating Crash-Consistency in Persistent-Memory File Systems (BEST PAPER AWARD) ",
    "authors": "Hayley LeBlanc (University of Texas at Austin), Shankara Pailoor (University of Texas at Austin), Om Saran K. R. E. (University of Texas at Austin), Isil Dillig (University of Texas at Austin), James Bornholt (University of Texas at Austin), Vijay Chidambaram (University of Texas at Austin, VMware Research)",
    "abstract": "",
    "link": "",
    "session_title": "Persistence",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "5dbef9cd-5895-4fea-9996-369707a1a718"
  },
  {
    "title": "Mumak: efficient and black-box bug detection for Persistent Memory ",
    "authors": "João Gonçalves (Instituto Superior Técnico (ULisboa), INESC-ID), Miguel Matos (Instituto Superior Técnico (ULisboa), INESC-ID), Rodrigo Rodrigues (Instituto Superior Técnico (ULisboa), INESC-ID)",
    "abstract": "The advent of Persistent Memory (PM) opens the door to novel application designs that explore its performance and durability benefits. However, there is no free lunch, and to program PM applications, developers need to be aware of potential inconsistent application state upon machine or application crashes. To overcome this difficulty, several tools have been proposed to detect the presence of the so-called crash-consistency bugs. While these are effective in detecting a variety of bugs, they present several key limitations, namely relying on application-specific semantics, requiring the programmer to manually annotate the program or modify the PM library, and relying on techniques with poor scalability, making them impractical for production code. In this paper, we introduce Mumak, a tool that detects bugs in PM applications in an efficient and black-box manner. Our key insight to reduce the search space is to use a two-pronged approach with a first pass that is highly efficient by focusing only on key, error-prone code points without exhaustively testing all possible persistence orderings, and a second pass based on heuristics that try to compensate the shortcomings of the initial approach. Furthermore, we avoid application-specific knowledge or annotations by relying on the application's own recovery procedure as an (imperfect) consistency oracle. Our experimental results, with different applications and libraries, show that Mumak has bug coverage on par with the other state-of-the-art tools, while being up to 25× faster. We also found four new crash-consistency bugs, two in PMDK and two in Montage, three of which have already been acknowledged and fixed by the developers.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587447",
    "session_title": "Persistence",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "d1c4f125-ae31-4b82-8ab6-2be83b6aa48d"
  },
  {
    "title": "NearPM: A Near-Data Processing System for Storage-Class Applications ",
    "authors": "Yasas Seneviratne (University of Virginia), Korakit Seemakhupt (University of Virginia), Sihang Liu (University of Waterloo), Samira Khan (University of Virginia)",
    "abstract": "Persistent Memory (PM) technologies enable both fast memory access and recovery in case of a failure. To ensure crash-consistent behavior, programs need to enforce persist ordering and employ mechanisms that introduce additional data movements such as logging, checkpointing, and shadow-paging. The emerging near-data processing (NDP) architectures can effectively reduce this overhead. In this work, we propose NearPM, a near-data processor that accelerates common, primitive operations that are crucial to crash consistency. Using these primitives, NearPM accelerates commonly-used crash-consistency mechanisms. NearPM further reduces the synchronization overheads between the NDP and the CPU by handling ordering near memory. We propose Partitioned Persist Ordering (PPO) that ensures a correct persist ordering between CPU and NDP devices, as well as among multiple NDP devices. We prototype NearPM on an FPGA platform. NearPM executes the data-intensive operations of crash-consistency mechanisms with correct ordering guarantees, while the rest of the program runs on the CPU. We evaluate nine PM workloads, each implemented in three crash consistency mechanisms: logging, checkpointing, and shadow paging. Overall, NearPM achieves 4.3 -- 9.8× speedup in the NDP-offloaded operations and 1.22 -- 1.35× speedup in the whole applications.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587456",
    "session_title": "Persistence",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "2c9a543b-6124-4235-b757-6b60b7443f56"
  },
  {
    "title": "FlowKV: A Semantic-Aware Store for Large-Scale State Management of Stream Processing Engines ",
    "authors": "Gyewon Lee (FriendliAI), Jaewoo Maeng (Seoul National University), Jinsol Park (Seoul National University), Jangho Seo (NAVER Corp.), Haeyoon Cho (Qualcomm), Youngseok Yang (Mirny Inc.), Taegeon Um (Samsung Research), Jongsung Lee (Samsung Electronics, Seoul National University), Jae W. Lee (Seoul National University), Byung-Gon Chun (FriendliAI, Seoul National University)",
    "abstract": "We propose FlowKV, a persistent store tailored for large-scale state management of streaming applications. Unlike existing KV stores, FlowKV leverages information from stream processing engines by taking a principled approach toward exploiting information about how and when the applications access data. FlowKV categorizes data access patterns of window operations according to how window boundaries are set and how tuples inside a window are aggregated, and deploys customized in-memory and on-disk data structures optimized for each pattern. In addition, FlowKV takes window metadata as explicit arguments of read and write methods to predict the moment when a window is read, and then loads the tuples of windows in batches from storage ahead of time. Using the NEXMark benchmark as workload, our experiments show that Apache Flink on FlowKV outperforms Flink on RocksDB or Faster with up to 4.12× throughput gain.",
    "link": "https://www.semanticscholar.org/paper/edb0c0907548a72220ca375aeda8fe9d2cceea61",
    "session_title": "Key-Value Stores",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "c281612b-5acf-4f12-9d49-37d2e93f6386"
  },
  {
    "title": "All-Flash Array Key-Value Cache for Large Objects ",
    "authors": "Jinhyung Koo (DGIST), Jinwook Bae (DGIST), Minjeong Yuk (DGIST), Seonggyun Oh (DGIST), Jungwoo Kim (DGIST), Jung-Soo Park (WineSOFT), Eunji Lee (Soongsil University), Bryan S. Kim (Syracuse University), Sungjin Lee (DGIST)",
    "abstract": "We present BigKV, a key-value cache specifically designed for caching large objects in an all-flash array (AFA). The design of BigKV is centered around the unique property of a cache: since it contains a copy of the data, exact bookkeeping of what is in the cache is not critical for correctness. By ignoring hash collisions, approximating metadata information, and allowing data loss from failures, BigKV significantly increases the cache hit ratio and keeps more useful objects in the system. Experiments on a real AFA show that our design increases the throughput by 3.1× on average and reduces the average and tail latency by 57% and 81%, respectively.",
    "link": "https://www.semanticscholar.org/paper/2b8f775e8c8d66e35580ae0adf0cad4a89d4c51a",
    "session_title": "Key-Value Stores",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "6641b88b-7ace-42d6-81b3-450e86a1204a"
  },
  {
    "title": "DyTIS: A Dynamic Dataset Targeted Index Structure Simultaneously Efficient for Search, Insert, and Scan ",
    "authors": "Jin Yang (UNIST), Heejin Yoon (UNIST), Gyeongchan Yun (UNIST), Sam H. Noh (Virginia Tech), Young-ri Choi (UNIST)",
    "abstract": "Many datasets in real life are complex and dynamic, that is, their key densities are varied over the whole key space and their key distributions change over time. It is challenging for an index structure to efficiently support all key operations for data management, in particular, search, insert, and scan, for such dynamic datasets. In this paper, we present DyTIS (Dynamic dataset Targeted Index Structure), an index that targets dynamic datasets. DyTIS, though based on the structure of Extendible hashing, leverages the CDF of the key distribution of a dataset, and learns and adjusts its structure as the dataset grows. The key novelty behind DyTIS is to group keys by the natural key order and maintain keys in sorted order in each bucket to support scan operations within a hash index. We also define what we refer to as a dynamic dataset and propose a means to quantify its dynamic characteristics. Our experimental results show that DyTIS provides higher performance than the state-of-the-art learned index for the dynamic datasets considered.",
    "link": "https://vtechworks.lib.vt.edu/bitstreams/86a393e9-1f6b-4267-bba2-1cd8d38485f1/download",
    "session_title": "Key-Value Stores",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "c7e38215-1b04-44c7-8aca-2c322636e9f8"
  },
  {
    "title": "DRAMHiT: A Hash Table Architected for the Speed of DRAM ",
    "authors": "Vikram Narayanan (University of Utah), David Detweiler (University of California, Irvine), Tianjiao Huang (University of California, Irvine), Anton Burtsev (University of Utah)",
    "abstract": "Despite decades of innovation, existing hash tables fail to achieve peak performance on modern hardware. Built around a relatively simple computation, i.e., a hash function, which in most cases takes only a handful of CPU cycles, hash tables should only be limited by the throughput of the memory subsystem. Unfortunately, due to the inherently random memory access pattern and the contention across multiple threads, existing hash tables spend most of their time waiting for the memory subsystem to serve cache misses and coherence requests. DRAMHiT is a new hash table designed to work at the speed of DRAM. Architecting for performance, we embrace the fact that modern machines are distributed systems---while the latency of communication between the cores is much lower than in a traditional network, it is still dominant for the hash table workload. We design DRAMHiT to apply a range of optimizations typical for a distributed system: asynchronous interface, fully-prefetched access, batching with out-of-order completion, and partitioned design with a low-overhead, scalable delegation scheme. DRAMHiT never touches unprefetched memory and minimizes the penalty of coherence requests and atomic instructions. These optimizations allow DRAMHiT to operate close to the speed of DRAM. On uniform key distributions, DRAMHiT achieves 973Mops for reads and 792Mops for writes on 64-thread Intel servers and 1192Mops and 1052Mops on 128-thread AMD machines; hence, outperforming existing lock-free designs by nearly a factor of two.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3552326.3587457",
    "session_title": "Key-Value Stores",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "50472549-0364-46a1-9338-acdd300d8c8f"
  },
  {
    "title": "Lyra: Elastic Scheduling for Deep Learning Clusters ",
    "authors": "Jiamin Li (City University of Hong Kong), Hong Xu (The Chinese University of Hong Kong), Yibo Zhu (Google), Zherui Liu (ByteDance Inc.), Chuanxiong Guo (Unaffiliated), Cong Wang (City University of Hong Kong)",
    "abstract": "Organizations often build separate training and inference clusters for deep learning, and use separate schedulers to manage them. This leads to problems for both: inference clusters have low utilization when the traffic load is low; training jobs often experience long queuing due to a lack of resources. We introduce Lyra, a new cluster scheduler to address these problems. Lyra introduces capacity loaning to loan idle inference servers for training jobs. It further exploits elastic scaling that scales a training job's resource allocation to better utilize loaned servers. Capacity loaning and elastic scaling create new challenges to cluster management. When the loaned servers need to be returned, we need to minimize job preemptions; when more GPUs become available, we need to allocate them to elastic jobs and minimize the job completion time (JCT). Lyra addresses these combinatorial problems with principled heuristics. It introduces the notion of server preemption cost, which it greedily reduces during server reclaiming. It further relies on the JCT reduction value defined for each additional worker of an elastic job to solve the scheduling problem as a multiple-choice knapsack problem. Prototype implementation on a 64-GPU testbed and large-scale simulation with 15-day traces of over 50,000 production jobs show that Lyra brings 1.53x and 1.48x reductions in average queuing time and JCT, and improves cluster usage by up to 25%.",
    "link": "http://arxiv.org/pdf/2202.07896",
    "session_title": "Machine Learning 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "d5ab9601-a569-4e8e-8387-a23464b59e6a"
  },
  {
    "title": "Egeria: Efficient DNN Training with Knowledge-Guided Layer Freezing ",
    "authors": "Yiding Wang (Hong Kong University of Science and Technology), Decang Sun (Hong Kong University of Science and Technology), Kai Chen (Hong Kong University of Science and Technology), Fan Lai (University of Michigan), Mosharaf Chowdhury (University of Michigan)",
    "abstract": "Training deep neural networks (DNNs) is time-consuming. While most existing solutions try to overlap/schedule computation and communication for efficient training, this paper goes one step further by skipping computing and communication through DNN layer freezing. Our key insight is that the training progress of internal DNN layers differs significantly, and front layers often become well-trained much earlier than deep layers. To explore this, we first introduce the notion of training plasticity to quantify the training progress of internal DNN layers. Then we design Egeria, a knowledge-guided DNN training system that employs semantic knowledge from a reference model to accurately evaluate individual layers' training plasticity and safely freeze the converged ones, saving their corresponding backward computation and communication. Our reference model is generated on the fly using quantization techniques and runs forward operations asynchronously on available CPUs to minimize the overhead. In addition, Egeria caches the intermediate outputs of the frozen layers with prefetching to further skip the forward computation. Our implementation and testbed experiments with popular vision and language models show that Egeria achieves 19%-43% training speedup w.r.t. the state-of-the-art without sacrificing accuracy.",
    "link": "https://repository.hkust.edu.hk/ir/bitstream/1783.1-125989/1/125989-1.pdf",
    "session_title": "Machine Learning 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "b239733f-c018-416f-8ed5-7f23cce05e9c"
  },
  {
    "title": "Hi-Speed DNN Training with Espresso: Unleashing the Full Potential of Gradient Compression with Near-Optimal Usage Strategies ",
    "authors": "Zhuang Wang (Rice University), Haibin Lin (ByteDance Inc.), Yibo Zhu (ByteDance Inc.), T. S. Eugene Ng (Rice University)",
    "abstract": "Gradient compression (GC) is a promising approach to addressing the communication bottleneck in distributed deep learning (DDL). It saves the communication time, but also incurs additional computation overheads. The training throughput of compression-enabled DDL is determined by the compression strategy, including whether to compress each tensor, the type of compute resources (e.g., CPUs or GPUs) for compression, the communication schemes for compressed tensor, and so on. However, it is challenging to find the optimal compression strategy for applying GC to DDL because of the intricate interactions among tensors. To fully unleash the benefits of GC, two questions must be addressed: 1) How to express any compression strategies and the corresponding interactions among tensors of any DDL training job? 2) How to quickly select a near-optimal compression strategy? In this paper, we propose Espresso to answer these questions. It first designs a decision tree abstraction to express any compression strategies and develops empirical models to timeline tensor computation, communication, and compression to enable Espresso to derive the intricate interactions among tensors. It then designs a compression decision algorithm that analyzes tensor interactions to eliminate and prioritize strategies and optimally offloads compression from GPUs to CPUs. Experimental evaluations show that Espresso can improve the training throughput over the start-of-the-art compression-enabled system by up to 77% for representative DDL training jobs. Moreover, the computational time needed to select the compression strategy is measured in milliseconds, and the selected strategy is only a few percent from optimal.",
    "link": "https://www.semanticscholar.org/paper/4baeb966780ba90e60622fcabcad4c04f0994659",
    "session_title": "Machine Learning 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "f18a793a-d0d5-4c7d-aaea-c2d82ce1493e"
  },
  {
    "title": "SiloD: A Co-design of Caching and Scheduling for Deep Learning Clusters ",
    "authors": "Hanyu Zhao (Peking University), Zhenhua Han (Microsoft Research), Zhi Yang (Peking University), Quanlu Zhang (Microsoft Research), Mingxia Li (USTC), Fan Yang (Microsoft Research), Qianxi Zhang (Microsoft Research),\n                                            Binyang Li (Microsoft), Yuqing Yang (Microsoft Research), Lili Qiu (Microsoft Research), Lintao Zhang (BaseBit Technologies), Lidong Zhou (Microsoft Research)",
    "abstract": "Deep learning training on cloud platforms usually follows the tradition of the separation of storage and computing. The training executes on a compute cluster equipped with GPUs/TPUs while reading data from a separate cluster hosting the storage service. To alleviate the potential bottleneck, a training cluster usually leverages its local storage as a cache to reduce the remote IO from the storage cluster. However, existing deep learning schedulers do not manage storage resources thus fail to consider the diverse caching effects across different training jobs. This could degrade scheduling quality significantly. To address this issue, we present SiloD, a scheduling framework that co-designs the cluster scheduler and the cache subsystems for deep learning training. SiloD treats cache and remote IO as first-class resources and can integrate different state-of-the-art deep learning scheduling policies in a unified scheduling framework. To achieve this, SiloD develops an enhanced job performance estimator to help different schedulers to jointly consider the impact of storage and compute resource allocation while preserving their respective scheduling objectives. The SiloD-enhanced performance estimator leverages the unique data access pattern of deep learning training to develop a closed-form analytic model that captures the diverse cache / remote IO requirements from different training jobs. Evaluations show that SiloD improves the average job completion time, cluster utilization, and fairness by up to 7.4x, 2.57x, and 1.89x, respectively, compared to different combinations of cache systems and cluster schedulers where they operate independently.",
    "link": "https://www.semanticscholar.org/paper/a8e32285ff09afaa3fcea50a06a421d5b9fe578d",
    "session_title": "Machine Learning 2",
    "conference_name": "EuroSys",
    "date": "2023-05-08",
    "paper_id": "1c503de5-5a87-4002-a1b9-5af9fe9b9417"
  }
]