[
  {
    "title": "Energy-aware Scheduling and Input Buffer Overflow Prevention for Energy-harvesting Systems",
    "authors": "Harsh Desai (Carnegie Mellon University), Xinye Wang (Carnegie Mellon University), Brandon Lucia (Carnegie Mellon University)",
    "abstract": "Modern energy-harvesting devices [23] use on-device compute to discard data uninteresting to the application, improving energy availability. These devices capture data at fixed rate, and process captured data at a rate that varies with environmental factors like input power and event activity. If capture rate exceeds processing rate, new inputs are stored in a small on-device input buffer (hundreds of kBs). When the input buffer fills up, the device discards newer inputs, missing potentially interesting events. Energy-harvesting devices must avoid such input buffer overflows (IBO) to avoid missing interesting events. A static solution to IBOs is impossible given dynamic variations in processing rate, and prior research fails to provide a suitable dynamic solution. We propose Quetzal, a new hardware-software solution targeted at avoiding IBOs. Quetzal’s software has two parts: a new energy-aware scheduler that selects jobs with the lowest end-to-end latency (including energy recharging), and a runtime which uses queueing-theory to predict if the selected job will cause IBOs. Quetzal reacts to imminent IBOs by degrading the scheduled job. Quetzal’s scheduler and runtime use a simple, system-agnostic hardware circuit to measure power at runtime. Quetzal reduces events missed due to IBOs by up to 4 . 2 × compared to several baselines.",
    "link": "https://doi.org/10.1145/3676641.3715995",
    "conference_name": "ASPLOS",
    "session_title": "Edge Computing",
    "date": "2025-03-30",
    "paper_id": "d8f88321-5a31-4a7e-8b47-2ebba8fde9aa"
  },
  {
    "title": "Generalizing Reuse Patterns for Efficient DNN on Microcontrollers",
    "authors": "Jiesong Liu (North Carolina State University), Bin Ren (College of William and Mary), Xipeng Shen (North Carolina State University)",
    "abstract": "Deep Neural Networks (DNNs) face challenges in deployment on resource-constrained devices due to their high computational demands. Leveraging redundancy in input data and activation maps for computation reuse is an effective way to accelerate DNN inference, especially for microcontrollers where the computing power is very limited. This work points out an important limitation in current reuse-based DNN optimizations, the narrow definition of reuse patterns in data. It proposes the concept of generalized reuse and uncovers the relations between generalized reuse patterns and row/column reorder of a matrix view of the input or activation map of a DNN. It revolutionizes the conventional view of explorable reuse patterns, drastically expanding the reuse space. It further develops two novel analytical models for analyzing the impacts of reuse patterns on the accuracy and latency of DNNs, enabling efficient selection of appropriate reuse patterns. Experiments show that generalized reuse consistently brings significant benefits, regardless of the differences among DNNs or microcontroller hardware. It delivers 1.03-2.2 × speedups or 1-8% accuracy improvement over conventional reuse.",
    "link": "https://doi.org/10.1145/3676641.3716257",
    "conference_name": "ASPLOS",
    "session_title": "Edge Computing",
    "date": "2025-03-30",
    "paper_id": "aa73b574-77a0-4156-92a6-dde9388148ab"
  },
  {
    "title": "Earth+: On-Board Satellite Imagery Compression Leveraging Historical Earth Observations",
    "authors": "Kuntai Du (University of Chicago), Yihua Cheng (University of Chicago), Peder Olsen (Microsoft Research), Shadi Noghabi (Microsoft Research), Junchen Jiang (University of Chicago)",
    "abstract": "With the increasing deployment of earth observation satellite constellations, the downlink (satellite-to-ground) capacity often limits the freshness, quality, and coverage of the imagery data available to applications on the ground. To overcome the downlink limitation, we present Earth+, a new satellite imagery compression system that, instead of compressing each image individually, pinpoints and downloads only recent imagery changes with respect to the history reference images. To minimize the amount of changes, it is critical to make reference images as fresh as possible. Earth+ enables each satellite to choose fresh reference images from not only its own history images but also past images of other satellites from an entire satellite constellation. To share reference images across satellites, Earth+ utilizes the limited capacity of the existing uplink (ground-to-satellite) by judiciously selecting and compressing reference images while still allowing accurate change detection. In short, Earth+ is the first to make reference-based compression efficient, by enabling constellation-wide sharing of fresh reference images across satellites. Our evaluation shows that Earth+ can reduce the downlink usage by a factor of 3.3 compared to state-of-the-art on-board image compression techniques while not sacrificing image quality, or using more on-board computing or storage resources, or more uplink bandwidth than currently available.",
    "link": "https://doi.org/10.1145/3669940.3707222",
    "conference_name": "ASPLOS",
    "session_title": "Edge Computing",
    "date": "2025-03-30",
    "paper_id": "8d30f1ee-e83d-4f11-bc51-fcea5e567413"
  },
  {
    "title": "Pirate: No Compromise Low-Bandwidth VR Streaming for Edge Devices",
    "authors": "Yingtian Zhang (The Pennsylvania State University), Yan Kang (The Pennsylvania State University), Ziyu Ying (The Pennsylvania State University), Wanhang Lu (The Pennsylvania State University), Sijie Lan (The Pennsylvania State University), Huijuan Xu (The Pennsylvania State University), Kiwan Maeng (The Pennsylvania State University), Anand Sivasubramaniam (The Pennsylvania State University), Mahmut T. Kandemir (The Pennsylvania State University), Chita R. Das (The Pennsylvania State University)",
    "abstract": "Due to the limited compute power and storage capabilities of edge platforms, ''streaming'' often provides a better VR experience compared to ''rendering''. Yet, achieving high-quality VR streaming faces two significant challenges, namely, bandwidth limitations and the need for real-time operation with high frames per second (FPS). Previous efforts have tended to prioritize either conserving bandwidth without real-time performance or ensuring real-time operation without substantial bandwidth savings. In this work, we incorporate the concept of ''stereo similarity'' to develop a novel real-time stereo video compression framework for streaming, called Pirate. Unlike the previously proposed approaches that rely on large machine learning-based models for synthesizing stereo pairs from both eyes with disparity maps (which can be impractical for most edge platforms due to their high computational cost), Pirate iteratively synthesizes the target eye view using only a single eye view and its corresponding disparity and optical flow information, with alternating left or right eye transmission. This enables us to generate target view at an extremely low computational cost, even under bandwidth constraints as low as 0.1 bits per pixel (bpp), while maintaining a high frame rate of 90 FPS. Our evaluations also reveal that, the proposed approach not only achieves real-time VR streaming with a 20%-40% reduction in bandwidth usage, but also maintains similar superior quality standards.",
    "link": "https://doi.org/10.1145/3676641.3716268",
    "conference_name": "ASPLOS",
    "session_title": "Edge Computing",
    "date": "2025-03-30",
    "paper_id": "13cecc99-245d-4bc9-8151-2b926111cb9d"
  },
  {
    "title": "Nazar: Monitoring and Adapting ML Models on Mobile Devices",
    "authors": "Wei Hao (Columbia University), Zixi Wang (Columbia University), Lauren Hong (Columbia University), Lingxiao Li (Columbia University), Nader Karayanni (Columbia University), AnMei Dasbach-Prisk (University of California San Diego), Chengzhi Mao (Columbia University), Junfeng Yang (Columbia University), Asaf Cidon (Columbia University)",
    "abstract": "ML models are increasingly being pushed to mobile devices, for low-latency inference and offline operation. However, once the models are deployed, it is hard for ML operators to track their accuracy, which can degrade unpredictably (e.g., due to data drift). We design the first end-to-end system for continuously monitoring and adapting models on mobile devices without requiring feedback from users. Our key observation is that often model degradation is due to a specific root cause, which may affect a large group of devices. Therefore, once the system detects a consistent degradation across a large number of devices, it employs a root cause analysis to determine the origin of the problem and applies a cause-specific adaptation. We evaluate the system on two computer vision datasets, and show it consistently boosts accuracy compared to existing approaches. On a dataset containing photos collected from driving cars, our system improves the accuracy on average by 15%.",
    "link": "https://doi.org/10.1145/3669940.3707246",
    "conference_name": "ASPLOS",
    "session_title": "Edge Computing",
    "date": "2025-03-30",
    "paper_id": "4238ffea-c04c-46eb-b4ae-8bdeba5d84d1"
  },
  {
    "title": "Orion: A Fully Homomorphic Encryption Framework for Deep Learning",
    "authors": "Austin Ebel (Tandon School of Engineering, New York University), Karthik Garimella (Tandon School of Engineering, New York University), Brandon Reagen (Tandon School of Engineering, New York University)",
    "abstract": "Fully Homomorphic Encryption (FHE) has the potential to substantially improve privacy and security by enabling computation directly on encrypted data. This is especially true with deep learning, as today, many popular user services are powered by neural networks in the cloud. Beyond its well-known high computational costs, one of the major challenges facing wide-scale deployment of FHE-secured neural inference is effectively mapping these networks to FHE primitives. FHE poses many programming challenges including packing large vectors, managing accumulated noise, and translating arbitrary and general-purpose programs to the limited instruction set provided by FHE. These challenges make building large FHE neural networks intractable using the tools available today. In this paper we address these challenges with Orion, a fully-automated framework for private neural inference using FHE. Orion accepts deep neural networks written in PyTorch and translates them into efficient FHE programs. We achieve this by proposing a novel single-shot multiplexed packing strategy for arbitrary convolutions and through a new, efficient technique to automate bootstrap placement and scale management. We evaluate Orion on common benchmarks used by the FHE deep learning community and outperform state-of-the-art by 2.38x on ResNet-20, the largest network they report. Orion's techniques enable processing much deeper and larger networks. We demonstrate this by evaluating ResNet-50 on ImageNet and present the first high-resolution FHE object detection experiments using a YOLO-v1 model with 139 million parameters. Orion is open-source for all to use at: https://github.com/baahl-nyu/orion",
    "link": "https://doi.org/10.1145/3676641.3716008",
    "conference_name": "ASPLOS",
    "session_title": "Homomorphic Encryption",
    "date": "2025-03-30",
    "paper_id": "80407f92-d3f0-4933-857a-100f6db3534d"
  },
  {
    "title": "CIPHERMATCH: Accelerating Homomorphic Encryption-Based String Matching via Memory-Efficient Data Packing Packing and In-Flash Processing",
    "authors": "Mayank Kabra (ETH Zurich), Rakesh Nadig (ETH Zurich), Harshita Gupta (ETH Zurich), Manos Frouzakis (ETH Zurich), Rahul Bera (ETH Zurich), Vamanan Arulchelvan (ETH Zurich), Yu Liang (ETH Zurich), Haiyu Mao (ETH Zurich), Mohammad Sadrosadati (ETH Zurich), Onur Mutlu (ETH Zurich)",
    "abstract": "Homomorphic encryption (HE) allows secure computation on encrypted data without revealing the original data, providing significant benefits for privacy-sensitive applications. Many cloud computing applications (e.g., DNA read mapping, biometric matching, web search) use exact string matching as a key operation. However, prior string matching algorithms that use homomorphic encryption are limited by high computational latency caused by the use of complex operations and data movement bottlenecks due to the large encrypted data size. In this work, we provide an efficient algorithm-hardware codesign to accelerate HE-based secure exact string matching. We propose CIPHERMATCH, which (i) reduces the increase in memory footprint after encryption using an optimized software-based data packing scheme, (ii) eliminates the use of costly homomorphic operations (e.g., multiplication and rotation), and (iii) reduces data movement by designing a new in-flash processing (IFP) architecture. We demonstrate the benefits of CIPHERMATCH using two case studies: (1) Exact DNA string matching and (2) encrypted database search. Our pure software-based CIPHERMATCH implementation that uses our memory-efficient data packing scheme improves performance and reduces energy consumption by 42.9X and 17.6X, respectively, compared to the state-of-the-art software baseline. Integrating CIPHERMATCH with IFP improves performance and reduces energy consumption by 136.9X and 256.4X, respectively, compared to the software-based CIPHERMATCH implementation.",
    "link": "https://doi.org/10.1145/3676641.3716251",
    "conference_name": "ASPLOS",
    "session_title": "Homomorphic Encryption",
    "date": "2025-03-30",
    "paper_id": "b49253f2-4f7f-4995-90e3-2446a26b21ed"
  },
  {
    "title": "ReSBM: Region-based Scale and Minimal-Level Bootstrapping Management for FHE via Min-Cut",
    "authors": "Yan Liu (Ant Group), Jianxin Lai (Ant Group), Long Li (Ant Group), Tianxiang Sui (Ant Group), Linjie Xiao (Ant Group), Peng Yuan (Ant Group), Xiaojing Zhang (Ant Group), Qing Zhu (Ant Group), Wenguang Chen (Tsinghua University,Ant Group), Jingling Xue (UNSW)",
    "abstract": "The RNS-CKKS scheme in Fully Homomorphic Encryption (FHE) supports crucial features for privacy-preserving machine learning, such as fixed-point arithmetic and SIMD-style vectorization. Yet, managing the escalation of ciphertext scales from homomorphic multiplications, which risks capacity overflow, along with bootstrapping, presents significant challenges. These complexities are exacerbated by the need to efficiently handle scale and bootstrapping at compile time while ensuring rapid encrypted inference.\nIn this paper, we present ReSBM, a novel compiler technique that simultaneously optimizes scale and bootstrapping for encrypted inference under RNS-CKKS. By partitioning a program's data flow graph (DFG) into regions with a uniform multiplicative depth of one, RESBM ensures that placements of Scale Management Operations (SMOs) and bootstraps affect only the latency of a region, not the scales and levels of its live-out ciphertexts. Our region-based approach tackles the NP-hard challenge of optimal bootstrapping placement with hierarchical strategies: (1) optimal intra-region SMO and bootstrapping placement using min-cut, (2) bootstrapping-guided rescaling region identification across a sequence of regions, culminating in tentative bootstrapping at two terminal regions, and (3) minimal-level bootstrap placement across the DFG, elevating ciphertexts only to the necessary minimal level. Validation across a variety of complex models on CPUs shows that ReSBM not only compiles these models more rapidly than a leading method but also boosts encrypted inference efficiency by an average of 12.1% when compared to another leading method. Consequently, ReSBM substantially improves the practical deployment of large models for encrypted inference, surpassing existing methods in terms of both compilation speed and inference performance.",
    "link": "https://doi.org/10.1145/3669940.3707276",
    "conference_name": "ASPLOS",
    "session_title": "Homomorphic Encryption",
    "date": "2025-03-30",
    "paper_id": "5b82affb-2628-4210-8cd6-560d9fd19a4e"
  },
  {
    "title": "HALO: Loop-aware Bootstrapping Management for Fully Homomorphic Encryption",
    "authors": "Seonyoung Cheon (Yonsei University), Yongwoo Lee (Yonsei University), Hoyun Youm (Yonsei University), Dongkwan Kim (Yonsei University), Sungwoo Yun (Yonsei University), Kunmo Jeong (Yonsei University), Dongyoon Lee (Stony Brook University), Hanjun Kim (Yonsei University)",
    "abstract": "Thanks to the computation ability on encrypted data, fully homomorphic encryption (FHE) is an attractive solution for privacy-preserving computation. Despite its advantages, FHE suffers from limited applicability in small programs because repeated FHE multiplications deplete the level of a ciphertext, which is finite. Bootstrapping reinitializes the level, thus allowing support for larger programs. However, its high computational overhead and the risk of level underflow require sophisticated bootstrapping placement, thereby increasing the programming burden. Although a recently proposed compiler automatizes the bootstrapping placement, its applicability is still limited due to lack of loop support.\nThis work proposes the first loop-aware bootstrapping management compiler, called HALO, which optimizes bootstrapping placement in an FHE program with a loop. To correctly support bootstrapping-enabled loops, HALO matches encryption types and levels between live-in and loop-carried ciphertexts in the loops. To reduce the bootstrapping overheads, HALO decreases the number of bootstrapping within a loop body by packing the loop-carried variables to a single ciphertext, reduces wasted levels in a short loop body by unrolling the loop, and optimizes the bootstrapping latency by adjusting the target level of bootstrapping as needed. For seven machine learning programs with flat and nested loops, HALO shows 27% performance speedup compared to the state-of-the-art compiler that places bootstrapping operations on fully unrolled loops. In addition, HALO reduces the compilation time and code size by geometric means of 209.12x and 11.0x compared to the compiler, respectively.",
    "link": "https://doi.org/10.1145/3669940.3707275",
    "conference_name": "ASPLOS",
    "session_title": "Homomorphic Encryption",
    "date": "2025-03-30",
    "paper_id": "85e2b754-d886-43be-b7f1-2461ad1f5812"
  },
  {
    "title": "Affinity-based Optimizations for TFHE on Processing-in-DRAM",
    "authors": "Kevin Nam (Department of Electrical and Computer Engineering (ECE), Seoul National University,Inter-university Semiconductor Research Center (ISRC), Seoul National University), Heonhui Jung (Department of Electrical and Computer Engineering (ECE), Seoul National University,Inter-university Semiconductor Research Center (ISRC), Seoul National University), Hyunyoung Oh (Department of AIÂ·Software, Gachon University), Yunheung Paek (Department of Electrical and Computer Engineering (ECE), Seoul National University,Inter-university Semiconductor Research Center (ISRC), Seoul National University)",
    "abstract": "Processing-in-memory (PIM) architectures are promising for accelerating intensive workloads due to their high internal bandwidth. This paper introduces a technique for accelerating Fully Homomorphic Encryption over the Torus (TFHE), a promising yet intensive application, on a realistic PIM system. Existing TFHE accelerators focus on exploiting parallelism, often overlooking data affinity, which leads to performance degradation in PIM due to excessive remote data accesses (RDAs). To address this, we present an affinity-based approach that optimizes the computation of TFHE on PIM. We apply algorithmic optimizations to TFHE, enabling PIM to effectively leverage its high internal bandwidth. We analyze the affinity patterns in the sub-tasks of TFHE and develop an offline scheduler that exploits our analysis to find optimal scheduling, minimizing RDAs while maintaining sufficient parallelism. To demonstrate the practicality of our work, we design a variant of an existing PIM-HBM device with minimal hardware modifications, and perform evaluations over a real FPGA-based PIM system. Our experiments demonstrate that our affinity-based optimizations outperform prior TFHE accelerators by 4.24-209× for real-world benchmarks.",
    "link": "https://doi.org/10.1145/3676641.3716246",
    "conference_name": "ASPLOS",
    "session_title": "Homomorphic Encryption",
    "date": "2025-03-30",
    "paper_id": "72555c11-76ff-4350-b388-13f6646b27f5"
  },
  {
    "title": "Mosaic: Exploiting Instruction-Level Parallelism on Deep Learning Accelerators with iTex Tessellation",
    "authors": "Jianxing Xu (University of Science and Technology of China,SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Yuanbo Wen (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Zikang Liu (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Ruibai Xu (University of Science and Technology of China,SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Tingfeng Ruan (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Jun Bi (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Rui Zhang (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Di Huang (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Xinkai Song (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Yifan Hao (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Xing Hu (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Zidong Du (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Chongqing Zhao (Tencent), Jie Jiang (Tencent), Qi Guo (SKL of Processors, Institute of Computing Technology, Chinese Academy of Sciences)",
    "abstract": "Deep learning has achieved great success in numerous application areas at the cost of high computational complexity. To meet the ever-increasing computational demand, commodity hardware platforms (e.g., CPUs and GPUs) offer abundant computing resources including scalar, vector, and tensor units for deep learning that could execute in parallel. However, existing top-down tiling-based deep learning compilers often generate a homogeneous mapping from the given tensor computation task to hardware arithmetic instructions, failing to utilize different computing units simultaneously to achieve higher performance.\nIn this paper, we propose Mosaic, a bottom-up tessellation-based deep learning compiler that directly tessellates the given tensor computation task with varying instructions, forming a heterogeneous instruction-to-task mapping to exploit instruction-level parallelism (ILP) across different computing units. The key that enables such tessellation is the iTex abstraction, which models the relationship between the instruction operations and its semantics with formalized affine functions. Based on the iTex, we propose a heuristic approach to efficiently generate various tessellation plans. Further, we propose the iTex scheduling technique to orchestrate the execution of instructions, reducing potential structural hazards and maximizing the exploitable ILP.\nOur extensive evaluation shows that Mosaic achieves an average speedup ranging from 1.08× to 1.28× across multiple hardware platforms compared to highly optimized vendor libraries. Mosaic also achieves an average speedup of 1.34× over the best existing baselines on real-world operators extracted from LLMs. More importantly, Mosaic reaches up to 106% of the GPU Tensor Core theoretical peak throughput, demonstrating its effective exploitation of ILP.",
    "link": "https://doi.org/10.1145/3676641.3716262",
    "conference_name": "ASPLOS",
    "session_title": "ML Acceleration",
    "date": "2025-03-30",
    "paper_id": "f37958ef-e5ce-4897-aab2-716fa42e83de"
  },
  {
    "title": "DynaX: Sparse Attention Acceleration with Dynamic X:M Fine-Grained Structured Pruning",
    "authors": "Xiao Xiong (College of Computer Science, Chongqing University), Zhaorui Chen (College of Computer Science, Chongqing University), Yue Liang (College of Computer Science, Chongqing University), Minghao Tian (College of Computer Science, Chongqing University), Jiaxing Shang (College of Computer Science, Chongqing University), Jiang Zhong (College of Computer Science, Chongqing University), Dajiang Liu (College of Computer Science, Chongqing University)",
    "abstract": "Owning to the mechanism of self-attention, Transformers have exhibited incredible performance in a wide range of artificial intelligence tasks. With the growth of sequence length, attention computation with quadratic complexity becomes the bottleneck, and dynamic sparsity is an effective technique to alleviate this problem. However, dynamic attention sparsity for long-sequence tasks suffers from two challenges, i.e., irregular sparse patterns and heavy prediction overhead. To this end, this paper proposes DynaX, an algorithm-hardware co-design framework that accelerates attention computation via dynamic X:M fine-grained structured pruning. Different from traditional N:M pruning, DynaX dynamically selects variable X (rather than a fixed N) important scores from a group via a 2-step pruning method, which results in high sparsity and less prediction memory overhead while maintaining pattern regularity to a certain extent. After that, DynaX performs block scheduling to reorganize score blocks into hardware blocks that can perfectly match the size of the processing element array (PEA), resulting in a higher utilization rate. Experimental results show that DynaX can achieve average sparsity of 89.54% and 91.77% for short-sequence tasks and long-sequence tasks, respectively, with less than 1% accuracy loss. Compared to Sanger and SALO2, DynaX achieves a speedup of 1.99X and 1.50X on the BERT-base model, and an energy efficiency improvement of 5.16X and 4.20X, respectively.",
    "link": "https://doi.org/10.1145/3676641.3715991",
    "conference_name": "ASPLOS",
    "session_title": "ML Acceleration",
    "date": "2025-03-30",
    "paper_id": "0f927319-3dbc-46cf-a7d3-a8a40ff26eae"
  },
  {
    "title": "Accelerating Retrieval-Augmented Generation",
    "authors": "Derrick Quinn (Cornell University), Mohammad Nouri (Cornell University), Neel Patel (Cornell University), John Salihu (University of Kansas), Alireza Salemi (UMass Amherst), Sukhan Lee (Samsung Electronics), Hamed Zamani (UMass Amherst), Mohammad Alian (Cornell University)",
    "abstract": "Current Retrieval-Augmented Generation (RAG) systems concatenate and process numerous retrieved document chunks for prefill which requires a large volume of computation, therefore leading to significant latency in time-to-first-token (TTFT). To reduce the computation overhead as well as TTFT, we introduce TurboRAG, a novel RAG system that redesigns the inference paradigm of the current RAG system by first pre-computing and storing the key-value (KV) caches of documents offline, and then directly retrieving the saved KV cache for prefill. Hence, online computation of KV caches is eliminated during inference. In addition, we provide a number of insights into the mask matrix and positional embedding mechanisms, plus fine-tune a pretrained language model to maintain model accuracy of TurboRAG. Our approach is applicable to most existing large language models and their applications without any requirement in modification of models and inference systems. Experimental results across a suite of RAG benchmarks demonstrate that TurboRAG reduces TTFT by up to 9.4x compared to the conventional RAG systems (on an average of 8.6x), but reserving comparable performance to the standard RAG systems.",
    "link": "https://doi.org/10.1145/3669940.3707264",
    "conference_name": "ASPLOS",
    "session_title": "ML Acceleration",
    "date": "2025-03-30",
    "paper_id": "12bc839f-378d-45d7-a466-ecbfa6b35200"
  },
  {
    "title": "GUST: Graph Edge-Coloring Utilization for Accelerating Sparse Matrix Vector Multiplication",
    "authors": "Armin Gerami (Computer Science, University of Maryland), Bahar Asgari (Computer Science, University of Maryland)",
    "abstract": "Sparse matrix-vector multiplication (SpMV) plays a vital role in various scientific and engineering fields, from scientific computing to machine learning. Traditional general-purpose processors often fall short of their peak performance with sparse data, leading to the development of domain-specific architectures to enhance SpMV. Yet, these specialized approaches, whether tailored explicitly for SpMV or adapted from matrix-matrix multiplication accelerators, still face challenges in fully utilizing hardware resources as a result of sparsity. To tackle this problem, we introduce GUST, a hardware/software co-design, the key insight of which lies in separating multipliers and adders in the hardware, thereby enabling resource sharing across multiple rows and columns, leading to efficient hardware utilization and ameliorating negative performance impacts from sparsity. Resource sharing, however, can lead to collisions, a problem we address through a specially devised edge-coloring scheduling algorithm. Our comparisons with various prior domain specific architectures using real-world datasets shows the effectiveness of GUST, with an average hardware utilization of $33.67\\%$.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p127-gerami.pdf",
    "conference_name": "ASPLOS",
    "session_title": "ML Acceleration",
    "date": "2025-03-30",
    "paper_id": "0207c5ee-1757-4e1b-b556-8486fd7b0b51"
  },
  {
    "title": "RASSM: Residue-based Acceleration of Single Sparse Matrix Computation via Adaptive Tiling",
    "authors": "Anirudh Jain (Georgia Institute of Technology), Pulkit Gupta (Georgia Institute of Technology), Thomas M. Conte (Georgia Institute of Technology)",
    "abstract": "Single-Sparse-Matrix Kernels (SSMKs) such as SpMM, SDDMM, SpMV, and SpTS form the backbone of applications such as data analytics, graph processing, finite-element analysis, machine learning (including GNNs and LLMs), etc. This paper introduces Residue-based Acceleration of Single Sparse Matrix Computation via Adaptive Tiling (RASSM), an input-dependent, adaptive 2-dimensional tiling technique for SSMKs. The adaptation leverages the concept of a residue matrix: a data structure that compactly captures the pattern of non-zeros in the sparse matrix. With residues, we show it is possible to make intelligent decisions on adaptive tile sizes, resulting in increased cache occupancy. Residues allow for optimizations across both spatial and temporal locality.\nRASSM improves data movement and overall performance as compared to prior techniques. For example, using spatial analysis for SpMM on commodity server CPUs, RASSM has 1.30X speedup over MKL, 1.32X over J-Stream, 1.20X over ASpT, 1.11X over CSF-4 uniform-shape, and 1.10X over CSF-4 uniform-occupancy. RASSM with temporal analysis improves this to 1.36X (vs. MKL), 1.38X (vs. J-Stream), 1.26X (vs. ASpT), 1.17X (vs. CSF-4 uniform-shape), and 1.16X (vs. CSF-4 uniform-occupancy).",
    "link": "https://doi.org/10.1145/3669940.3707219",
    "conference_name": "ASPLOS",
    "session_title": "ML Acceleration",
    "date": "2025-03-30",
    "paper_id": "3daef7e6-5923-4c5e-b02c-d197ea1b8e35"
  },
  {
    "title": "FMCC: Flexible Measurement-based Quantum Computation over Cluster State",
    "authors": "Yingheng Li (University of Pittsburgh), Aditya Pawar (University of Pittsburgh), Zewei Mo (University of Pittsburgh), Youtao Zhang (University of Pittsburgh), Jun Yang (University of Pittsburgh), Xulong Tang (University of Pittsburgh)",
    "abstract": "Measurement-based quantum computing (MBQC) is a promising quantum computing paradigm that performs computation through \"one-way\" measurements on entangled quantum qubits. It is widely used in photonic quantum computing (PQC), where the computation is carried out on photonic cluster states (i.e., a 2-D mesh of entangled photons). In MBQC-based PQC, the cluster state depth (i.e., the length of one-way measurements) plays an important role in the overall execution time and circuit error. In this paper, we propose FMCC, a compilation framework that employs dynamic programming with heuristics to efficiently minimize the cluster state depth. Experimental results on six quantum applications show that FMCC achieves 51.7%, 57.4%, and 56.8% average depth reductions in small, medium, and large qubit counts compared to the state-of-the-art MBQC compilations.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p111-li.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Computing",
    "date": "2025-03-30",
    "paper_id": "5c86c632-af73-4cc1-bf8d-a0de9dbf9e5e"
  },
  {
    "title": "QRCC: Evaluating Large Quantum Circuits on Small Quantum Computers through Integrated Qubit Reuse and Circuit Cutting",
    "authors": "Aditya Pawar (Electrical and Computer Engineering Department, University of Pittsburgh), Yingheng Li (Computer Science Department, University of Pittsburgh), Zewei Mo (Computer Science Department, University of Pittsburgh), Yanan Guo (Electrical and Computer Engineering Department, University of Pittsburgh), Xulong Tang (Computer Science Department, University of Pittsburgh), Youtao Zhang (Computer Science Department, University of Pittsburgh), Jun Yang (Electrical and Computer Engineering Department, University of Pittsburgh)",
    "abstract": "Quantum computing has recently emerged as a promising computing paradigm for many application domains. However, the size of quantum circuits that can be run with high fidelity is constrained by the limited quantity and quality of physical qubits. Recently proposed schemes, such as wire cutting and qubit reuse, mitigate the problem but produce sub-optimal results as they address the problem individually. In addition, gate cutting, an alternative circuit-cutting strategy that is suitable for circuits computing expectation values, has not been fully explored in the field. In this paper, we propose QRCC, an integrated approach that exploits qubit reuse and circuit-cutting (including wire cutting and gate cutting) to run large circuits on small quantum computers. Circuit-cutting techniques introduce non-negligible post-processing overhead, which increases exponentially with the number of cuts. QRCC exploits qubit reuse to find better cutting solutions to minimize the cut numbers and thus the post-processing overhead. Our evaluation results show that on average we reduce the number of cuts by 29% and additional reduction when considering gate cuts.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p236-pawar.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Computing",
    "date": "2025-03-30",
    "paper_id": "4dbe639c-da9e-47d3-a386-71fe5705e37c"
  },
  {
    "title": "Optimizing Quantum Circuits, Fast and Slow",
    "authors": "Amanda Xu (University of Wisconsin-Madison), Abtin Molavi (University of Wisconsin-Madison), Swamit Tannu (University of Wisconsin-Madison), Aws Albarghouthi (University of Wisconsin-Madison)",
    "abstract": "Optimizing quantum circuits is critical: the number of quantum operations needs to be minimized for a successful evaluation of a circuit on a quantum processor. In this paper we unify two disparate ideas for optimizing quantum circuits, rewrite rules, which are fast standard optimizer passes, and unitary synthesis, which is slow, requiring a search through the space of circuits. We present a clean, unifying framework for thinking of rewriting and resynthesis as abstract circuit transformations. We then present a radically simple algorithm, GUOQ, for optimizing quantum circuits that exploits the synergies of rewriting and resynthesis. Our extensive evaluation demonstrates the ability of GUOQ to strongly outperform existing optimizers on a wide range of benchmarks.",
    "link": "https://doi.org/10.1145/3669940.3707240",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Computing",
    "date": "2025-03-30",
    "paper_id": "b9cbec41-c699-4cf6-ab82-4d4277af8142"
  },
  {
    "title": "BQSim: GPU-accelerated Batch Quantum Circuit Simulation using Decision Diagram",
    "authors": "Shui Jiang (The Chinese University of Hong Kong ,University of Wisconsin-Madison), Yi-Hua Chung (University of Wisconsin-Madison), Chih-Chun Chang (University of Wisconsin-Madison), Tsung-Yi Ho (The Chinese University of Hong Kong), Tsung-Wei Huang (University of Wisconsin-Madison)",
    "abstract": "Quantum circuit simulation (QCS) plays an important role in the designs and analysis of a quantum algorithm, as it assists researchers in understanding how quantum operations work without accessing expensive quantum computers. Despite many QCS methods, they are largely limited to simulating one input at a time. However, many simulation-driven quantum computing applications, such as testing and verification, require simulating multiple inputs to reason a quantum algo-rithm under different scenarios. We refer to this type of QCS as batch quantum circuit simulation (BQCS). In this paper, we present BQSim , a GPU-accelerated batch quantum circuit simulator. BQSim is inspired by the state-of-the-art decision diagram (DD) that can compactly represent quantum gate matrices, but overcomes its limitation of CPU-centric simulation. Specifically, BQSim uses DD to optimize a quantum circuit for reduced BQCS computation and converts DD to a GPU-efficient data structure. Additionally, BQSim employs a task graph-based execution strategy to minimize repetitive kernel call overhead and efficiently overlap kernel execution with data movement. Compared with three state-of-the-art quantum circuit simulators, cuQuantum , Qiskit Aer , and FlatDD , BQSim is 3 . 25 × , 159 . 06 × ,",
    "link": "https://doi.org/10.1145/3676641.3715984",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Computing",
    "date": "2025-03-30",
    "paper_id": "d4252a7a-3e04-4f86-b3ca-eb99dc7e3074"
  },
  {
    "title": "Fat-Tree QRAM: A High-Bandwidth Shared Quantum Random Access Memory for Parallel Queries",
    "authors": "Shifan Xu (Yale Quantum Institute, Yale University), Alvin Lu (Yale Quantum Institute, Yale University), Yongshan Ding (Yale Quantum Institute, Yale University)",
    "abstract": "Quantum Random Access Memory (QRAM) is a crucial architectural component for querying classical or quantum data in superposition, enabling algorithms with wide-ranging applications in quantum arithmetic, quantum chemistry, machine learning, and quantum cryptography. In this work, we introduce Fat-Tree QRAM, a novel query architecture capable of pipelining multiple quantum queries simultaneously while maintaining desirable scalings in query speed and fidelity. Specifically, Fat-Tree QRAM performs $O(\\log (N))$ independent queries in $O(\\log (N))$ time using $O(N)$ qubits, offering immense parallelism benefits over traditional QRAM architectures. To demonstrate its experimental feasibility, we propose modular and on-chip implementations of Fat-Tree QRAM based on superconducting circuits and analyze their performance and fidelity under realistic parameters. Furthermore, a query scheduling protocol is presented to maximize hardware utilization and access the underlying data at an optimal rate. These results suggest that Fat-Tree QRAM is an attractive architecture in a shared memory system for practical quantum computing.",
    "link": "https://doi.org/10.1145/3676641.3716256",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Computing",
    "date": "2025-03-30",
    "paper_id": "63bfa2bf-5d81-481d-9b4e-6df2eb22cc90"
  },
  {
    "title": "RANGE-BLOCKS: A Synchronization Facility for Domain-Specific Architectures",
    "authors": "Anagha Molakalmur Anil Kumar (Simon Fraser University), Aditya Prasanna (Simon Fraser University), Arrvindh Shriraman (Simon Fraser University)",
    "abstract": "Current domain-specific architectures (DSAs) work predominantly with static data structures and find it challenging to insert or remove data (they only support in-place updates). However, as DSAs target real-world applications, it is neces- sary to support mutable and dynamically resizable data structures. DSAs cannot support dynamic data structures since they lack a synchronization facility. DSAs are forced to either use address-based atomics or batch updates on the host. Unfortunately, both approaches introduce prohibitive performance penalties and require large caches for the locks. Range-blocks (RBlox) develops a hardware synchronization facility for DSAs to support dynamic data structures. Our idea is to use key ranges to capture synchronization boundaries and tap into the inherent parallelism of the data-structure layout. We make two novel observations that enable a practical hardware implementation: i) Range locks are symbolic and can compactly represent mutexes on multiple nested objects. Thus, any operation requires fewer range locks, and a small on-chip table suffices (2kb) compared to large caches (256kb) for address-based locks [79, 81]. ii) Ranges also explicitly represent the region of interest, and we can instantly achieve mutual exclusion (instead of relying on ordering). On a 128-tile dataflow DSA, we improve performance by 15×, reduce DRAM bandwidth by 4×, save 70% of on-chip traffic, and require 6.6% of on-chip energy.",
    "link": "https://doi.org/10.1145/3669940.3707225",
    "conference_name": "ASPLOS",
    "session_title": "Accelerators",
    "date": "2025-03-30",
    "paper_id": "3db42f23-ca44-4d18-9399-94f4f9f73eca"
  },
  {
    "title": "Enhancing CGRA Efficiency Through Aligned Compute and Communication Provisioning",
    "authors": "Zhaoying Li (National University of Singapore), Pranav Dangi (National University of Singapore), Chenyang Yin (Peking University), Thilini Kaushalya Bandara (National University of Singapore), Rohan Juneja (National University of Singapore), Cheng Tan (Google), Zhenyu Bai (National University of Singapore), Tulika Mitra (National University of Singapore)",
    "abstract": "Coarse-grained Reconfigurable Arrays (CGRAs) are domain-agnostic accelerators that enhance the energy efficiency of resource-constrained edge devices. The CGRA landscape is diverse, exhibiting trade-offs between performance, efficiency, and architectural specialization. However, CGRAs often overprovision communication resources relative to their modest computing capabilities. This occurs because the theoretically provisioned programmability for CGRAs often proves superfluous in practical implementations. In this paper, we propose Plaid, a novel CGRA architecture and compiler that aligns compute and communication capabilities, thereby significantly improving energy and area efficiency while preserving its generality and performance. We demonstrate that the dataflow graph, representing the target application, can be decomposed into smaller, recurring communication patterns called motifs. The primary contribution is the identification of these structural motifs within the dataflow graphs and the development of an efficient collective execution and routing strategy tailored to these motifs. The Plaid architecture employs a novel collective processing unit that can execute multiple operations of a motif and route related data dependencies together. The Plaid compiler can hierarchically map the dataflow graph and judiciously schedule the motifs. Our design achieves a 43% reduction in power consumption and 46% area savings compared to the baseline high-performance spatio-temporal CGRA, all while preserving its generality and performance levels. In comparison to the baseline energy-efficient spatial CGRA, Plaid offers a 1.4x performance improvement and a 48% area savings, with almost the same power.",
    "link": "https://doi.org/10.1145/3669940.3707230",
    "conference_name": "ASPLOS",
    "session_title": "Accelerators",
    "date": "2025-03-30",
    "paper_id": "d81f6d5d-f46b-4100-bbd4-7c1f4bd0b9a0"
  },
  {
    "title": "Squeezing Operator Performance Potential for the Ascend Architecture",
    "authors": "Yuhang Zhou (State Key Laboratory for Novel Software Technology, Nanjing University), Zhibin Wang (State Key Laboratory for Novel Software Technology, Nanjing University), Guyue Liu (Peking University), Shipeng Li (State Key Laboratory for Novel Software Technology, Nanjing University), Xi Lin (State Key Laboratory for Novel Software Technology, Nanjing University), Zibo Wang (State Key Laboratory for Novel Software Technology, Nanjing University), Yongzhong Wang (Huawei Technologies Co., Ltd.), Fuchun Wei (Huawei Technologies Co., Ltd.), Jingyi Zhang (Huawei Technologies Co., Ltd.), Zhiheng Hu (Huawei Technologies Co., Ltd.), Yanlin Liu (Huawei Technologies Co., Ltd.), Chunsheng Li (Huawei Technologies Co., Ltd.), Ziyang Zhang (Huawei Technologies Co., Ltd.), Yaoyuan Wang (Huawei Technologies Co., Ltd.), Bin Zhou (Shandong University), Wanchun Dou (Nanjing University, State Key Laboratory for Novel Software Technology), Guihai Chen (Nanjing University, State Key Laboratory for Novel Software Technology), Chen Tian (Nanjing University, State Key Laboratory for Novel Software Technology)",
    "abstract": "With the rise of deep learning, many companies have developed domain-specific architectures (DSAs) optimized for AI workloads, with Ascend being a representative. To fully realize the operator performance on Ascend, effective analysis and optimization is urgently needed. Compared to GPU, Ascend requires users to manage operations manually, leading to complex performance issues that require precise analysis. However, existing roofline models face challenges of visualization complexity and inaccurate performance assessment. To address these needs, we introduce a component-based roofline model that abstracts components to capture operator performance, thereby effectively identifying bottleneck components. Furthermore, through practical operator optimization case studies, we illustrate a comprehensive process of optimization based on roofline analysis, summarizing common performance issues and optimization strategies. Finally, extensive end-to-end optimization experiments demonstrate significant model speed improvements, ranging from 1.07× to 2.15×, along with valuable insights from practice.",
    "link": "https://doi.org/10.1145/3676641.3716243",
    "conference_name": "ASPLOS",
    "session_title": "Accelerators",
    "date": "2025-03-30",
    "paper_id": "5de29ae0-6bc7-4101-92b2-d5000e733885"
  },
  {
    "title": "PICACHU: Plug-In CGRA Handling Upcoming Nonlinear Operations in LLMs",
    "authors": "Jiajun Qin (New York University,Zhejiang University), Tianhua Xia (New York University), Cheng Tan (Google,Arizona State University), Jeff Zhang (Arizona State University), Sai Qian Zhang (New York University)",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing (NLP) domain by achieving state-of-the-art performance across a range of benchmarks. However, nonlinear operations in LLMs significantly contribute to inference latency and present unique challenges that have not been encountered previously. Addressing these challenges requires accelerators that combine efficiency, flexibility, and support for user-defined precision. Our analysis reveals that Coarse-Grained Reconfigurable Arrays (CGRAs) provide an effective solution, offering a balance of performance and flexibility tailored to domain-specific workloads. This paper introduces PICACHU, a plug-in coarse-grained reconfigurable accelerator tailored to efficiently handle non-linear operations by using custom algorithms and a dedicated compiler toolchain. PICACHU is the first to target all non-linear operations within LLMs and to consider CGRA as a plug-in accelerator for LLM inference. Our evaluation shows that PICACHU achieves speedups of 1 . 86 × and 1 . 55 × over prior state-of-the-art accelerators in LLM inference.",
    "link": "https://doi.org/10.1145/3676641.3716013",
    "conference_name": "ASPLOS",
    "session_title": "Accelerators",
    "date": "2025-03-30",
    "paper_id": "0229ec33-95a2-4425-aec3-9e7406e0eb3f"
  },
  {
    "title": "Coach: Exploiting Temporal Patterns for All-Resource Oversubscription in Cloud Platforms",
    "authors": "Benjamin Reidys (University of Illinois Urbana-Champaign), Pantea Zardoshti (Microsoft), ÃÃ±igo Goiri (Microsoft), Celine Irvene (Microsoft), Daniel S. Berger (Microsoft,University of Washington), Haoran Ma (University of California-Los Angeles), Kapil Arya (Microsoft), Eli Cortez (Microsoft), Taylor Stark (Microsoft), Eugene Bak (Microsoft), Mehmet Iyigun (Microsoft), Stanko NovakoviÄ (Google), Lisa Hsu (Meta), Karel Trueba (Microsoft), Abhisek Pan (Microsoft), Chetan Bansal (Microsoft), Saravan Rajmohan (Microsoft), Jian Huang (University of Illinois Urbana-Champaign), Ricardo Bianchini (Microsoft)",
    "abstract": "Cloud platforms remain underutilized despite multiple proposals to improve their utilization (e.g., disaggregation, harvesting, and oversubscription). Our characterization of the resource utilization of virtual machines (VMs) in Azure reveals that, while CPU is the main underutilized resource, we need to provide a solution to manage all resources holistically. We also observe that many VMs exhibit complementary temporal patterns, which can be leveraged to improve the oversubscription of underutilized resources. Based on these insights, we propose Coach: a system that exploits temporal patterns for all-resource oversubscription in cloud platforms. Coach uses long-term predictions and an efficient VM scheduling policy to exploit temporally complementary patterns. We introduce a new general-purpose VM type, called CoachVM, where we partition each resource allocation into a guaranteed and an oversubscribed portion. Coach monitors the oversubscribed resources to detect contention and mitigate any potential performance degradation. We focus on memory management, which is particularly challenging due to memory's sensitivity to contention and the overhead required to reassign it between CoachVMs. Our experiments show that Coach enables platforms to host up to ~26% more VMs with minimal performance degradation.",
    "link": "https://doi.org/10.1145/3669940.3707226",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 1",
    "date": "2025-03-30",
    "paper_id": "67c6da8f-90cf-4e46-8711-70df4cc0d284"
  },
  {
    "title": "Cooperative Graceful Degradation in Containerized Clouds",
    "authors": "Kapil Agrawal (University of California, Irvine), Sangeetha Abdu Jyothi (University of California, Irvine and VMware Research)",
    "abstract": "Cloud resilience is crucial for cloud operators and the myriad of applications that rely on the cloud. Today, we lack a mechanism that enables cloud operators to perform graceful degradation of applications while satisfying the application's availability requirements. In this paper, we put forward a vision for automated cloud resilience management with cooperative graceful degradation between applications and cloud operators. First, we investigate techniques for graceful degradation and identify an opportunity for cooperative graceful degradation in public clouds. Second, leveraging criticality tags on containers, we propose diagonal scaling -- turning off non-critical containers during capacity crunch scenarios -- to maximize the availability of critical services. Third, we design Phoenix, an automated cloud resilience management system that maximizes critical service availability of applications while also considering operator objectives, thereby improving the overall resilience of the infrastructure during failures. We experimentally show that the Phoenix controller running atop Kubernetes can improve critical service availability by up to $2\\times$ during large-scale failures. Phoenix can handle failures in a cluster of 100,000 nodes within 10 seconds. We also develop AdaptLab, an open-source resilience benchmarking framework that can emulate realistic cloud environments with real-world application dependency graphs.",
    "link": "https://doi.org/10.1145/3669940.3707244",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 1",
    "date": "2025-03-30",
    "paper_id": "c20033bc-0920-4bc5-8724-036dd9fb795f"
  },
  {
    "title": "DarwinGame: Playing Tournaments for Tuning Applications in Noisy Cloud Environments",
    "authors": "Rohan Basu Roy (University of Utah), Vijay Gadepally (Massachusetts Institute of Technology), Devesh Tiwari (Northeastern University)",
    "abstract": "This work introduces a new subarea of performance tuning -- performance tuning in a shared interference-prone computing environment. We demonstrate that existing tuners are significantly suboptimal by design because of their inability to account for interference during tuning. Our solution, DarwinGame, employs a tournament-based design to systematically compare application executions with different tunable parameter configurations, enabling it to identify the relative performance of different tunable parameter configurations in a noisy environment. Compared to existing solutions, DarwinGame achieves more than 27% reduction in execution time, with less than 0.5% performance variability. DarwinGame is the first performance tuner that will help developers tune their applications in shared, interference-prone, cloud environments.",
    "link": "https://doi.org/10.1145/3669940.3707259",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 1",
    "date": "2025-03-30",
    "paper_id": "8f378674-d2c4-40b8-9c49-5e20177436fd"
  },
  {
    "title": "Copper and Wire: Bridging Expressiveness and Performance for Service Mesh Policies",
    "authors": "Divyanshu Saxena (The University of Texas at Austin), William Zhang (The University of Texas at Austin), Shankara Pailoor (The University of Texas at Austin), Isil Dillig (The University of Texas at Austin), Aditya Akella (The University of Texas at Austin)",
    "abstract": "Distributed microservice applications require a convenient means of controlling L7 communication between services. Service meshes have emerged as a popular approach to achieving this. However, current service mesh frameworks are difficult to use – they burden developers in realizing even simple communication policies, lack compatibility with diverse dataplanes, and introduce performance and resource overheads. We identify the root causes of these drawbacks and propose a ground-up new mesh architecture that overcomes them. We develop novel abstractions for mesh communication, a new mesh policy language centered on these abstractions to enable expressive policies, and a novel con-trol plane that enables using minimal dataplane resources for policy enforcement. We develop the precise semantics of our language abstractions and demonstrate how our con-trol plane can use them to execute policies correctly and optimally. We build and evaluate a prototype on realistic workloads and policies and open-source production traces. Our results show that complex policies can be specified in up to 6.75 × fewer lines, enforced with up to 2.6 × smaller tail latencies and up to 39% fewer CPU resources than today.",
    "link": "https://doi.org/10.1145/3669940.3707257",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 1",
    "date": "2025-03-30",
    "paper_id": "df4b3d30-0f5b-4625-94c9-83900f56b259"
  },
  {
    "title": "Composing Distributed Computations Through Task and Kernel Fusion",
    "authors": "Rohan Yadav (Stanford University), Shiv Sundram (Stanford University), Wonchan Lee (NVIDIA), Michael Garland (NVIDIA), Michael Bauer (NVIDIA), Alex Aiken (Stanford University), Fredrik Kjolstad (Stanford University)",
    "abstract": "We introduce Diffuse, a system that dynamically performs task and kernel fusion in distributed, task-based runtime systems. The key component of Diffuse is an intermediate representation of distributed computation that enables the necessary analyses for the fusion of distributed tasks to be performed in a scalable manner. We pair task fusion with a JIT compiler to fuse together the kernels within fused tasks. We show empirically that Diffuse's intermediate representation is general enough to be a target for two real-world, task-based libraries (cuNumeric and Legate Sparse), letting Diffuse find optimization opportunities across function and library boundaries. Diffuse accelerates unmodified applications developed by composing task-based libraries by 1.86x on average (geo-mean), and by between 0.93x--10.7x on up to 128 GPUs. Diffuse also finds optimization opportunities missed by the original application developers, enabling high-level Python programs to match or exceed the performance of an explicitly parallel MPI library.",
    "link": "https://doi.org/10.1145/3669940.3707216",
    "conference_name": "ASPLOS",
    "session_title": "Distributed Computing",
    "date": "2025-03-30",
    "paper_id": "cc49a321-9a49-4127-b090-183ee0fd4b08"
  },
  {
    "title": "CXLfork: Fast Remote Fork over CXL Fabrics",
    "authors": "Chloe Alverti (University of Illinois Urbana-Champaign), Stratos Psomadakis (National Technical University of Athens), Burak Ocalan (University of Illinois Urbana-Champaign), Shashwat Jaiswal (University of Illinois Urbana-Champaign), Tianyin Xu (University of Illinois Urbana-Champaign), Josep Torrellas (University of Illinois Urbana-Champaign)",
    "abstract": "The shared and distributed memory capabilities of the emerging Compute Express Link (CXL) interconnect urge us to rethink the traditional interfaces of system software. In this paper, we explore one such interface: remote fork using CXL-attached shared memory for cluster-wide process cloning. We present CXLfork , a remote fork interface that realizes close to zero-serialization, zero-copy process cloning across nodes over CXL fabrics. CXLfork utilizes globally-shared CXL memory for cluster-wide deduplication of process states. It also enables fine-grained control of state tiering between local and CXL memory. We use CXLfork to develop CXL-porter , an efficient horizontal autoscaler for serverless functions deployed on CXL fabrics. CXLfork minimizes cold-start overhead without sacrificing local memory. CXLfork attains restore latency close to that of a local fork, outperforming state-of-practice by 2.26x on average, and reducing local memory consumption by 87% on average.",
    "link": "https://doi.org/10.1145/3676641.3715988",
    "conference_name": "ASPLOS",
    "session_title": "Distributed Computing",
    "date": "2025-03-30",
    "paper_id": "44557a02-a7f7-41f3-9bcc-6ab276ac6cfe"
  },
  {
    "title": "OS2G: A High-Performance DPU Offloading Architecture for GPU-based Deep Learning with Object Storage",
    "authors": "Zhen Jin (Zhejiang University,Alibaba Group), Yiquan Chen (Alibaba Group), Mingxu Liang (Alibaba Group), Yijing Wang (Alibaba Group), Guoju Fang (Alibaba Group), Ao Zhou (Alibaba Group), Keyao Zhang (Zhejiang University), Jiexiong Xu (Zhejiang University), Wenhai Lin (Zhejiang University), Yiquan Lin (Zhejiang University), Shushu Zhao (Alibaba Group), Wenkai Shi (Alibaba Group), Zhenhua He (Alibaba Group), Shishun Cai (Alibaba Group), Wenzhi Chen (Zhejiang University)",
    "abstract": "Object storage is increasingly attractive for deep learning (DL) applications due to its cost-effectiveness and high scalability. However, it exacerbates CPU burdens in DL clusters due to intensive object storage processing and multiple data movements. Data processing unit (DPU) offloading is a promising solution, but naively offloading the existing object storage client leads to severe performance degradation. Besides, only offloading the object storage client still involves redundant data movements, as data must first transfer from the DPU to the host and then from the host to the GPU, which continues to consume valuable host resources.\nIn this paper, we propose OS2G, a high-performance offloading architecture designed to free up valuable CPU resources while providing high-performance storage services for DL applications. The key idea of OS2G is to offload the object storage client to a DPU and enable direct data transfer between the DPU and GPU. Specifically, we design a high-performance OS2G Client running on the DPU, utilizing asynchrony, pre-reading, and concurrency strategies to provide high-performance object storage services. Additionally, we propose the GPUDirect DPU (GDD) technique for OS2G to optimize the data path, allowing direct data transfer between the DPU-accelerated storage system and the GPU computing system, fully bypassing the host. Results demonstrate that compared to S3FS and S3Connector, OS2G reduces the execution time of the ResNet18 model by 34.3% and 50.4%, and also decreases CPU consumption by 61.9% and 57.7%, respectively.",
    "link": "https://doi.org/10.1145/3676641.3716265",
    "conference_name": "ASPLOS",
    "session_title": "Distributed Computing",
    "date": "2025-03-30",
    "paper_id": "f55cc686-f95d-4f0b-adc6-9649fc6ccdd0"
  },
  {
    "title": "pulse: Accelerating Distributed Pointer-Traversals on Disaggregated Memory",
    "authors": "Yupeng Tang (Yale University), Seung-seob Lee (Yale University), Abhishek Bhattacharjee (Yale University), Anurag Khandelwal (Yale University)",
    "abstract": "Caches at CPU nodes in disaggregated memory architectures amortize the high data access latency over the network. However, such caches are fundamentally unable to improve performance for workloads requiring pointer traversals across linked data structures. We argue for accelerating these pointer traversals closer to disaggregated memory in a manner that preserves expressiveness for supporting various linked structures, ensures energy efficiency and performance, and supports distributed execution. We design pulse, a distributed pointer-traversal framework for rack-scale disaggregated memory to meet all the above requirements. Our evaluation of pulse shows that it enables low-latency, high-throughput, and energy-efficient execution for a wide range of pointer traversal workloads on disaggregated memory that fare poorly with caching alone.",
    "link": "https://doi.org/10.1145/3669940.3707253",
    "conference_name": "ASPLOS",
    "session_title": "Distributed Computing",
    "date": "2025-03-30",
    "paper_id": "a535659f-0492-40aa-ae70-48a96c5ebe84"
  },
  {
    "title": "Salus: A Practical Trusted Execution Environment for CPU-FPGA Heterogeneous Cloud Platforms",
    "authors": "Yu Zou (Alibaba Group), Yiran Li (Alibaba Group), Sheng Wang (Alibaba Group), Le Su (Alibaba Group), Zhen Gu (DAMO Academy, Alibaba Group,Hupan Lab), Yanheng Lu (DAMO Academy, Alibaba Group,Hupan Lab), Yijin Guan (DAMO Academy, Alibaba Group,Hupan Lab), Dimin Niu (DAMO Academy, Alibaba Group,Hupan Lab), Mingyu Gao (Tsinghua University,Shanghai AI Laboratory), Yuan Xie (DAMO Academy, Alibaba Group,Hupan Lab), Feifei Li (Alibaba Group)",
    "abstract": "CPU-FPGA heterogeneous architectures have become increasingly popular in cloud environments for accelerating compute-intensive tasks. Ensuring the protection of sensitive data processed by these architectures requires the presence of a trusted execution environment (TEE). This work highlights the requirements for designing an FPGA TEE, the challenges faced in deploying existing solutions on commercial-off-the-shelf (COTS) cloud FPGA services, and the limitations of previous works that primarily focus on standalone FPGA TEEs. In response to these challenges, Salus introduces an innovative approach by leveraging an enclave running on the host with a TEE-enabled CPU. This approach aims to protect and attest the bitstream loaded on the FPGA side. By repurposing COTS FPGA bitstream utilities in a novel manner and adopting a proposed security-enhanced FPGA IP, Salus presents a practical design for an FPGA TEE, with minor efforts required.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p252-zou.pdf",
    "conference_name": "ASPLOS",
    "session_title": "FPGAs",
    "date": "2025-03-30",
    "paper_id": "886864d3-8a0e-401f-98d5-530863ec305e"
  },
  {
    "title": "Harmonia: A Unified Framework for Heterogeneous FPGA Acceleration in the Cloud",
    "authors": "Luyang Li (Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Heng Pan (Computer Network Information Center, Chinese Academy of Sciences), Xinchen Wan (Hong Kong University of Science and Technology), Kai Lv (Institute of Computing Technology, Chinese Academy of Sciences), Zilong Wang (Hong Kong University of Science and Technology), Qian Zhao (Douyin Co., Ltd.), Feng Ning (Douyin Co., Ltd.), Qingsong Ning (Douyin Co., Ltd.), Shideng Zhang (Douyin Co., Ltd.), Zhenyu Li (Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Layong Luo (Researcher), Gaogang Xie (Computer Network Information Center, Chinese Academy of Sciences,University of Chinese Academy of Sciences)",
    "abstract": "FPGAs are gaining popularity in the cloud as accelerators for various applications. To make FPGAs more accessible for users and streamline system management, cloud providers have widely adopted the shell-role architecture on their homogeneous FPGA servers. However, the increasing heterogeneity of cloud FPGAs poses new challenges for this architecture. Previous studies either focus on homogeneous FPGAs or only partially address the portability issues for roles, while still requiring laborious shell development for providers and ad-hoc software modifications for users. This paper presents Harmonia, a unified framework for heterogeneous FPGA acceleration in the cloud. Harmonia operates on two layers: a platform-specific layer that abstracts",
    "link": "https://doi.org/10.1145/3676641.3716259",
    "conference_name": "ASPLOS",
    "session_title": "FPGAs",
    "date": "2025-03-30",
    "paper_id": "effdb11f-dcda-4f4d-a5c6-50cd9d287a65"
  },
  {
    "title": "PhasePrint: Exposing Cloud FPGA Fingerprints by Inducing Timing Faults at Runtime",
    "authors": "Jubayer Mahmod (Virginia Tech), Matthew Hicks (Virginia Tech)",
    "abstract": "Cloud FPGAs, with their scalable and flexible nature, are rapidly gaining traction as go-to hardware acceleration platforms for compute-intensive workloads. However, their increasing adoption introduces unique security challenges. The hardware-level access that FPGAs provide leads to many vulnerabilities, including the leakage of sensitive information through data remanence and the creation of analog-domain covert channels among users. A foundational requirement in these scenarios is the ability to target an individual FPGA; knowing this, cloud vendors prevent FPGA localization by restricting access to low-level information of the underlying hardware. Beyond aiding adversaries, FPGA localization enables defenders to strategically rotate FPGA usage, preventing prolonged exposure that can lead to confidential data leakage due to long-term data remanence.\nThis paper introduces PhasePrint, a cloud FPGA localization approach using dynamic timing faults in functionally valid circuits. PhasePrint induces timing faults in a specially crafted circuit at runtime and infers delay characteristics from the resulting error pattern---without incorporating information sources blocked by cloud vendors. PhasePrint utilizes an FPGA's internal clock synthesizer to derive a clock pair with a strict phase relationship. By adjusting the phase relationship of these clocks, PhasePrint intentionally causes timing faults at runtime that reveal manufacturing variations among FPGA chips. We transform these fault locations into feature vectors to create device signatures and train a multi-class classifier on a dataset from 300 unique FPGAs across four AWS geographic regions. This entirely on-chip signature extraction method achieves >99% accuracy, operates 13x faster, and costs 92% less than the state-of-the-art.",
    "link": "https://doi.org/10.1145/3676641.3716012",
    "conference_name": "ASPLOS",
    "session_title": "FPGAs",
    "date": "2025-03-30",
    "paper_id": "c4a46444-6e84-42af-94e2-99794e76bae5"
  },
  {
    "title": "Hassert: Hardware Assertion-Based Verification Framework with FPGA Acceleration",
    "authors": "Ziqing Zhang (State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Weijie Weng (Xiamen University of Technology), Yaning Li (University College Dublin), Lijia Cai (Hong Kong University of Science and Technology), Haoyu Wang (Zhejiang University), David Boland (The University of Sydney), Yungang Bao (State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Kan Shi (State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences)",
    "abstract": "Hardware verification is typically the bottleneck of the chip development cycle, mainly due to the time-consuming simulation and debugging process using software simulators. Assertion-Based Verification (ABV) has been widely adopted to provide better visibility into microarchitecture details and automatically detect unexpected behaviors. While ABV significantly improves verification efficiency, checking assertions using software simulators requires extremely long times for large benchmarks. Prototyping designs on an FPGA is a potential alternative to verify hardware, but it lacks fine-grained debugging capabilities for when errors occur. To address these challenges, we present Hassert, an efficient ABV framework that combines high-performance verification on FPGAs with fine-grained debugging in software. Hassert automates the scheduling and mapping of SystemVerilog Assertions (SVAs) to the available FPGA fabric with the design-under-test (DUT), allowing for extensive hardware testing. Hassert also enables dynamic switching between different assertions, either user-specified or based on SVA coverage satisfaction, by partially reconfiguring the FPGA at runtime, eliminating the need to recompile the DUT. To further improve debugging efficiency, we also propose a microarchitecture-guided hardware snapshot scheme. If any assertion is fired, Hassert automatically generates snapshots of the current status of the entire FPGA hardware. These snapshots are then transferred to an external simulator, where the operation is reconstructed in software for further debugging. We demonstrate that these contributions can improve significant verification efficiency over traditional software simulation-based approaches for various hardware benchmarks and RISC-V processor designs whilst maintaining full visibility and debugging capabilities at the cost of only a small area overhead.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p142-zhang.pdf",
    "conference_name": "ASPLOS",
    "session_title": "FPGAs",
    "date": "2025-03-30",
    "paper_id": "f4ad563a-e67d-4caa-b647-9038a26012fc"
  },
  {
    "title": "Control Logic Synthesis: Drawing the Rest of the OWL",
    "authors": "Zachary D. Sisco (University of California, Santa Barbara), Andrew David Alex (University of California, Santa Barbara), Zechen Ma (University of California, Santa Barbara), Yeganeh Aghamohammadi (University of California, Santa Barbara), Boming Kong (University of California, Santa Barbara), Benjamin Darnell (University of Illinois Urbana-Champaign), Timothy Sherwood (University of California, Santa Barbara), Ben Hardekopf (University of California, Santa Barbara), Jonathan Balkind (University of California, Santa Barbara)",
    "abstract": "System-on-chip (SoC) design requires complex reasoning about the interactions between an architectural specification, the microarchitectural datapath (e.g., functional units), and the control logic (which coordinates the datapath) to facilitate the critical computing tasks on which we all depend. Hardware specialization is now the expectation rather than the exception, meaning we need new hardware design tools to bring ideas to reality with both agility and correctness. We introduce a new technique, “control logic synthesis”, which automatically generates control logic given a datapath description and an architectural specification. This enables an entirely new hardware design process where the designer only needs to write a datapath sketch, leaving the control logic as “holes.” Then, guided by an architectural specification, we adapt program synthesis techniques to automatically generate a correct hardware implementation of the control logic, filling the holes and completing the design. We evaluate control logic synthesis over two classes of con-trol (state machines and instruction decoders) and different architectures (embedded-class RISC-V cores and hardware accelerators for cryptography). We demonstrate how agile-oriented SoC developers can iterate over designs without writing control logic by hand yet still",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p63-sisco.pdf",
    "conference_name": "ASPLOS",
    "session_title": "EDA",
    "date": "2025-03-30",
    "paper_id": "f6d4984f-21e6-410f-b7e8-4a7cda86e89e"
  },
  {
    "title": "CRUSH: A Credit-Based Approach for Functional Unit Sharing in Dynamically Scheduled HLS",
    "authors": "Jiahui Xu (ETH Zurich), Lana JosipoviÄ (ETH Zurich)",
    "abstract": "Resource sharing in dynamic HLS-produced circuits is beneficial yet challenging—without a pre-defined schedule, out-of-order access to a shared resource creates resource dependencies between the operations, which may lead to circuit deadlock. In this work, we present CRUSH, a strategy that enables efficient functional unit sharing in dynamically scheduled HLS. CRUSH decouples the interactions of different operations in the shared resource and breaks resource dependencies, thus, deadlock freeness is guaranteed. With-out the need to pessimistically order operation execution for correctness, (1) CRUSH systematically maintains performance and avoids the need for iterative and expensive performance optimization; (2) CRUSH seizes sharing opportunities enabled by out-of-order access to the shared unit; (3) CRUSH avoids complex access control mechanisms, thus, it is more resource-efficient. Compared to a prior strategy, CRUSH achieves a geomean of 10% DSP reduction, 89% reduction in the optimization runtime, and 15% FF reduction, with negligible performance degradation.",
    "link": "https://doi.org/10.1145/3669940.3707273",
    "conference_name": "ASPLOS",
    "session_title": "EDA",
    "date": "2025-03-30",
    "paper_id": "a5c59d40-82a4-4640-8ddc-71d7a5daa0cb"
  },
  {
    "title": "AMuLeT: Automated Design-Time Testing of Secure Speculation Countermeasures",
    "authors": "Bo Fu (University of Toronto), Leo Tenenbaum (University of Toronto), David Adler (University of Toronto), Assaf Klein (Technion - Israel Institute of Technology), Arpit Gogia (IMDEA Software Institute), Alaa R. Alameldeen (Simon Fraser University), Marco Guarnieri (IMDEA Software Institute), Mark Silberstein (Technion - Israel Institute of Technology), Oleksii Oleksenko (Azure Research, Microsoft), Gururaj Saileshwar (University of Toronto)",
    "abstract": "In recent years, several hardware-based countermeasures proposed to mitigate Spectre attacks have been shown to be insecure. To enable the development of effective secure speculation countermeasures, we need easy-to-use tools that can automatically test their security guarantees early-on in the design phase to facilitate rapid prototyping. This paper develops AMuLeT, the first tool capable of testing secure speculation countermeasures for speculative leakage early in their design phase in simulators. Our key idea is to leverage model-based relational testing tools that can detect speculative leaks in commercial CPUs, and apply them to micro-architectural simulators to test secure speculation defenses. We identify and overcome several challenges, including designing an expressive yet realistic attacker observer model in a simulator, overcoming the slow simulation speed, and searching the vast micro-architectural state space for potential vulnerabilities. AMuLeT speeds up test throughput by more than 10x compared to a naive design and uses techniques to amplify vulnerabilities to uncover them within a limited test budget. Using AMuLeT, we launch for the first time, a systematic, large-scale testing campaign of four secure speculation countermeasures from 2018 to 2024--InvisiSpec, CleanupSpec, STT, and SpecLFB--and uncover 3 known and 6 unknown bugs and vulnerabilities, within 3 hours of testing. We also show for the first time that the open-source implementation of SpecLFB is insecure.",
    "link": "https://doi.org/10.1145/3676641.3716247",
    "conference_name": "ASPLOS",
    "session_title": "EDA",
    "date": "2025-03-30",
    "paper_id": "1b6eba1e-11b8-4030-a208-a011ce9a54d0"
  },
  {
    "title": "Don't Repeat Yourself! Coarse-Grained Circuit Deduplication to Accelerate RTL Simulation",
    "authors": "Haoyuan Wang (UC Santa Cruz), Thomas Nijssen (UC Santa Cruz), Scott Beamer (UC Santa Cruz)",
    "abstract": "Designing a digital integrated circuit requires many register transfer level (RTL) simulations for design, debugging, and especially verification. To cope with the slow speed of RTL simulation, industry frequently uses private server farms to run many simulations in parallel. Surprisingly, the implications of parallel runs of different RTL simulations have not been extensively explored. Moreover, in modern digital hardware, there is a growing trend to replicate components to scale out. However, the potential for circuit deduplication has been mostly overlooked. In this work, we pinpoint the shared last-level cache as the primary bottleneck impacting the throughput of RTL simulation. To address this issue, we propose a coarse-grained circuit deduplication strategy integrated into an RTL simulator. Our method involves identifying multiple instances of a single module within a digital circuit and creating shared code that can be applied to all of these instances. Our approach reduces the cache footprint by increasing code reuse, which consequently benefits processor components such as caches and branch predictors. Our experiments demonstrate that deduplication can bring up to 1.95× speedup in a single simulation, and achieve up to 2.09× overall RTL simulation throughput.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p79-wang.pdf",
    "conference_name": "ASPLOS",
    "session_title": "EDA",
    "date": "2025-03-30",
    "paper_id": "26223c4e-af37-434d-aada-ac2c84fabec0"
  },
  {
    "title": "Parendi: Thousand-Way Parallel RTL Simulation",
    "authors": "Mahyar Emami (EPFL), Thomas Bourgeat (EPFL), James R. Larus (EPFL)",
    "abstract": "Hardware development critically depends on cycle-accurate RTL simulation. However, as chip complexity increases, conventional single-threaded simulation becomes impractical due to stagnant single-core performance. Parendi is an RTL simulator that addresses this challenge by exploiting the abundant fine-grained parallelism inherent in RTL simulation and efficiently mapping it onto the massively parallel Graphcore IPU (Intelligence Processing Unit) architecture. Parendi scales up to 5888 cores on 4 Graphcore IPU sockets. It allows us to run large RTL designs up to 4$\\times$ faster than the most powerful state-of-the-art x64 multicore systems. To achieve this performance, we developed new partitioning and compilation techniques and carefully quantified the synchronization, communication, and computation costs of parallel RTL simulation: The paper comprehensively analyzes these factors and details the strategies that Parendi uses to optimize them.",
    "link": "https://doi.org/10.1145/3676641.3716010",
    "conference_name": "ASPLOS",
    "session_title": "EDA",
    "date": "2025-03-30",
    "paper_id": "9225004f-8688-4071-b7c7-508c13a8e789"
  },
  {
    "title": "MetaSapiens: Real-Time Neural Rendering with Efficiency-Aware Pruning and Accelerated Foveated Rendering",
    "authors": "Weikai Lin (University of Rochester), Yu Feng (Shanghai Jiao Tong University), Yuhao Zhu (University of Rochester)",
    "abstract": "Point-Based Neural Rendering (PBNR) is emerging as a promising class of rendering techniques, which are permeating all aspects of society, driven by a growing demand for real-time, photorealistic rendering in AR/VR and digital twins. Achieving real-time PBNR on mobile devices is challenging. This paper proposes MetaSapiens, a PBNR system that for the first time delivers real-time neural rendering on mobile devices while maintaining human visual quality. MetaSapiens combines three techniques. First, we present an efficiency-aware pruning technique to optimize rendering speed. Second, we introduce a Foveated Rendering (FR) method for PBNR, leveraging humans' low visual acuity in peripheral regions to relax rendering quality and improve rendering speed. Finally, we propose an accelerator design for FR, addressing the load imbalance issue in (FR-based) PBNR. Our evaluation shows that our system achieves an order of magnitude speedup over existing PBNR models without sacrificing subjective visual quality, as confirmed by a user study. The code and demo are available at: https://horizon-lab.org/metasapiens/.",
    "link": "https://doi.org/10.1145/3669940.3707227",
    "conference_name": "ASPLOS",
    "session_title": "Graphics",
    "date": "2025-03-30",
    "paper_id": "7734183e-88f3-4d8d-be2c-729c7f204da8"
  },
  {
    "title": "D-VSync: Decoupled Rendering and Displaying for Smartphone Graphics",
    "authors": "Yuanpei Wu (IPADS, Shanghai Jiao Tong University,Engineering Research Center for Domain-specific Operating Systems, Ministry of Education), Dong Du (IPADS, Shanghai Jiao Tong University,Engineering Research Center for Domain-specific Operating Systems, Ministry of Education), Chao Xu (Fields Lab, Huawei Central Software Institute), Yubin Xia (IPADS, Shanghai Jiao Tong University,Engineering Research Center for Domain-specific Operating Systems, Ministry of Education), Ming Fu (Fields Lab, Huawei Central Software Institute), Binyu Zang (IPADS, Shanghai Jiao Tong University,Engineering Research Center for Domain-specific Operating Systems, Ministry of Education), Haibo Chen (IPADS, Shanghai Jiao Tong University,Key Laboratory of System Software (Chinese Academy of Science))",
    "abstract": "Rendering service, which typically orchestrates screen display and UI through Vertical Synchronization (VSync), is an indispensable system service for user experiences of smartphone OSes (e.g., Android, OpenHarmony, and iOS). The recent trend of large high-frame-rate screens, stunning visual effects, and physics-based animations has placed unprecedented pressure on the VSync-based rendering architecture, leading to higher frame drops and longer rendering latency.\nThis paper proposes <u>D</u>ecoupled <u>V</u>ertical <u>Sync</u>hronization (D-VSync), which decouples execution and displaying in the rendering service. D-VSync allows frames to be rendered a number of VSync periods before being physically displayed on the screen. The key insight behind D-VSync to resolve the limitation of VSync is that, the decoupling enables sporadic long frames to utilize the computational power saved by common short frames, therefore providing a larger time window to tolerate workload fluctuations. Evaluation results of 75 common OS use cases and apps on OpenHarmony (Mate 40 Pro, Mate 60 Pro), 25 popular apps on Android (Google Pixel 5), and simulations of 15 mobile games show that compared to VSync, D-VSync on average reduces frame drops by 72.7%, user-perceptible stutters by 72.3%, and rendering latency by 31.1%, with only 0.13%-0.37% more power consumption. D-VSync has been integrated into HarmonyOS NEXT.",
    "link": "https://doi.org/10.1145/3669940.3707235",
    "conference_name": "ASPLOS",
    "session_title": "Graphics",
    "date": "2025-03-30",
    "paper_id": "5ded05f7-7711-4049-aab3-fb85975ef24a"
  },
  {
    "title": "StreamGrid: Streaming Point Cloud Analytics via Compulsory Splitting and Deterministic Termination",
    "authors": "Yu Feng (Shanghai Jiao Tong University,Shanghai Qi Zhi Institute), Zheng Liu (Shanghai Jiao Tong University), Weikai Lin (University of Rochester), Zihan Liu (Shanghai Jiao Tong University,Shanghai Qi Zhi Institute), Jingwen Leng (Shanghai Jiao Tong University,Shanghai Qi Zhi Institute), Minyi Guo (Shanghai Jiaotong University,Shanghai Qi Zhi Institute), Zhezhi He (Shanghai Jiao Tong University), Jieru Zhao (Shanghai Jiao Tong University), Yuhao Zhu (University of Rochester)",
    "abstract": "Point clouds are increasingly important in intelligent applications, but frequent off-chip memory traffic in accelerators causes pipeline stalls and leads to high energy consumption. While conventional line buffer techniques can eliminate off-chip traffic, they cannot be directly applied to point clouds due to their inherent computation patterns. To address this, we introduce two techniques: compulsory splitting and deterministic termination, enabling fully-streaming processing. We further propose StreamGrid, a framework that integrates these techniques and automatically optimizes on-chip buffer sizes. Our evaluation shows StreamGrid reduces on-chip memory by 61.3\\% and energy consumption by 40.5\\% with marginal accuracy loss compared to the baselines without our techniques. Additionally, we achieve 10.0$\\times$ speedup and 3.9$\\times$ energy efficiency over state-of-the-art accelerators.",
    "link": "https://doi.org/10.1145/3676641.3716021",
    "conference_name": "ASPLOS",
    "session_title": "Graphics",
    "date": "2025-03-30",
    "paper_id": "786070d6-64b8-4f89-9683-6bb97b930c00"
  },
  {
    "title": "ARC: Warp-level Adaptive Atomic Reduction in GPUs to Accelerate Differentiable Rendering",
    "authors": "Sankeerth Durvasula (Vector Institute, University of Toronto), Adrian Zhao (Vector Institute, University of Toronto), Fan Chen (University of Toronto), Ruofan Liang (Vector Institute, University of Toronto), Pawan Kumar Sanjaya (Vector Institute, University of Toronto), Yushi Guan (Vector Institute, University of Toronto), Christina Giannoula (Vector Institute, University of Toronto), Nandita Vijaykumar (Vector Institute, University of Toronto)",
    "abstract": "Differentiable rendering is widely used in emerging applications that represent any 3D scene as a model trained using gradient descent from 2D images. Recent works (e.g., 3D Gaussian Splatting) use rasterization to enable rendering photo-realistic imagery at high speeds from these learned 3D models. These rasterization-based differentiable rendering methods have been demonstrated to be very promising, providing state-of-art quality for various important tasks. However, training a model to represent a scene is still time-consuming even on powerful GPUs. In this work, we observe that the gradient computation step during model training is a significant bottleneck due to the large number of atomic operations. These atomics overwhelm the atomic units in the L2 cache of GPUs, causing long stalls.\nTo address this, we leverage the observations that during gradient computation: (1) for most warps, all threads atomically update the same memory locations; and (2) warps generate varying amount of atomic traffic. We propose ARC, a primitive that accelerates atomic operations based on two key ideas: First, we enable warp-level reduction at the GPU cores using registers to leverage the locality in intra-warp atomic updates. Second, we distribute atomic computation between the cores and the L2 atomic units to increase the throughput of atomic computation. We propose two implementations of ARC: ARC-HW, a hardware-based approach and ARC-SW, a software-only approach. We demonstrate significant speedups with ARC of 2.6× on average (up to 5.7×) for widely used differentiable rendering workloads.",
    "link": "https://doi.org/10.1145/3669940.3707238",
    "conference_name": "ASPLOS",
    "session_title": "Graphics",
    "date": "2025-03-30",
    "paper_id": "9a7ccd4f-ae0d-4826-835f-51808b6621c4"
  },
  {
    "title": "Treelet Accelerated Ray Tracing on GPUs",
    "authors": "Yuan Hsi Chou (University of British Columbia), Tor M. Aamodt (University of British Columbia)",
    "abstract": "Despite advances in hardware acceleration, ray tracing use in real-time rendering is limited and often lowers frame rates, leading users such as video game players to disable the feature entirely. Prior work has shown that dividing the BVH tree into smaller subtrees (treelets) and traversing all rays that visit a treelet before switching treelets can significantly reduce memory traffic on a specialized accelerator, but there are many challenges to applying treelets to GPUs. We find that a naive treelet implementation is ineffective and propose optimizations to improve performance. Virtualized Treelet Queues consist of two main components. Ray virtualization increases the number of concurrent rays in flight to create more cache reuse opportunities by terminating raygen shaders that have already issued their trace ray instruction, reclaiming CUDA cores and allowing more raygen shaders to be executed. To take advantage of the increased concurrent rays, we propose a dynamic treelet queue architecture that dynamically switches between traversal modes to increase efficiency. We also find that performing warp repacking boosts SIMT efficiency of warps in the RT unit which is crucial to achieving good traversal performance with treelet queues. Our simulations show virtualized treelet queues achieve on average 95% speedup compared to a baseline GPU with ray tracing acceleration across all scenes in LumiBench rendered with path tracing at one sample per pixel with three max bounces per ray.",
    "link": "https://doi.org/10.1145/3676641.3716279",
    "conference_name": "ASPLOS",
    "session_title": "Graphics",
    "date": "2025-03-30",
    "paper_id": "468e9daf-5570-470f-834a-404b3c22cc21"
  },
  {
    "title": "MPC-Pipe: an Efficient Pipeline Scheme for Semi-honest MPC Machine Learning",
    "authors": "Yongqin Wang (Department of Electrical & Computer Engineering, University of Southern California), Rachit Rajat (Department of Electrical & Computer Engineering, University of Southern California), Murali Annavaram (Department of Electrical & Computer Engineering, University of Southern California)",
    "abstract": "Multi-party computing (MPC) has been gaining popularity as a secure computing model over the past few years. However, prior works have demonstrated that MPC protocols still pay substantial performance penalties compared to plaintext, particularly when applied to ML algorithms. The overhead is due to added computation and communication costs. Prior studies, as well as our own analysis, found that most MPC protocols today sequentially perform communication and computation. The participating parties must compute on their shares first and then perform data communication to allow the distribution of new secret shares before proceeding to the next computation step. In this work, we show that serialization is unnecessary, particularly in the context of ML computations (both in Convolutional neural networks and in Transformer-based models). We demonstrate that it is possible to carefully orchestrate the computation and communication steps to overlap. We propose MPC-Pipe, an efficient MPC system for both training and inference of ML workloads, which pipelines computations and communications in an MPC protocol during the online phase. MPC-Pipe proposes three pipeline schemes to optimize the online phase of ML in the semi-honest majority adversary setting. We implement MPC-Pipe by augmenting a modified version of CrypTen, which separates online and offline phases. We evaluate the end-to-end system performance benefits of the online phase of MPC using deep neural networks (VGG16, ResNet50) and Transformers using different network settings. We show that MPC-Pipe can improve the throughput and latency of ML workloads.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p203-wang.pdf",
    "conference_name": "ASPLOS",
    "session_title": "ML Security",
    "date": "2025-03-30",
    "paper_id": "b9cc0dcc-d438-46a7-a186-64297a874e7f"
  },
  {
    "title": "Cinnamon: A Framework for Scale-Out Encrypted AI",
    "authors": "Siddharth Jayashankar (Carnegie Mellon University), Edward Chen (Carnegie Mellon University), Tom Tang (Carnegie Mellon University), Wenting Zheng (Carnegie Mellon University), Dimitrios Skarlatos (Carnegie Mellon University)",
    "abstract": "Fully homomorphic encryption (FHE) is a promising cryptographic solution that enables computation on encrypted data, but its adoption remains a challenge due to steep performance overheads. Although recent FHE architectures have made valiant efforts to narrow the performance gap, they not only have massive monolithic chip designs but also only target small ML workloads. We present Cinnamon, a framework for accelerating state-of-the-art ML workloads that are encrypted using FHE. Cinnamon accelerates encrypted computing by exploiting parallelism at all levels of a program, using novel algorithms, compilers, and hardware techniques to create a scale-out design for FHE as opposed to a monolithic chip design. Our evaluation of the Cinnamon framework on small programs shows a 2.3 × improvement in performance compared to prior state-of-the-art designs. Further, we use Cinnamon to show for the first time the scalability of large ML models such as the BERT language model in FHE. Cinnamon achieves a speedup of 36 , 600 × compared to a CPU bringing down the inference time from 17 hours to 1.67 seconds thereby enabling new opportunities for privacy-preserving machine learning. Finally, Cinnamon’s parallelization strategies and architectural extensions reduce the required resources per-chip leading to a 5 × and 2.68 × improvement in performance-per-dollar compared to state-of-the-art monolithic and chiplet architectures respectively.",
    "link": "https://doi.org/10.1145/3669940.3707260",
    "conference_name": "ASPLOS",
    "session_title": "ML Security",
    "date": "2025-03-30",
    "paper_id": "13b0beeb-5af5-469e-8dd5-990dc689865b"
  },
  {
    "title": "PipeLLM: Fast and Confidential Large Language Model Services with Speculative Pipelined Encryption",
    "authors": "Yifan Tan (Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University), Cheng Tan (Northeastern University), Zeyu Mi (Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University), Haibo Chen (Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University)",
    "abstract": "Confidential computing on GPUs, like NVIDIA H100, mitigates the security risks of outsourced Large Language Models (LLMs) by implementing strong isolation and data encryption. Nonetheless, this encryption incurs a significant performance overhead, reaching up to 52.8 percent and 88.2 percent throughput drop when serving OPT-30B and OPT-66B, respectively. To address this challenge, we introduce PipeLLM, a user-transparent runtime system. PipeLLM removes the overhead by overlapping the encryption and GPU computation through pipelining - an idea inspired by the CPU instruction pipelining - thereby effectively concealing the latency increase caused by encryption. The primary technical challenge is that, unlike CPUs, the encryption module lacks prior knowledge of the specific data needing encryption until it is requested by the GPUs. To this end, we propose speculative pipelined encryption to predict the data requiring encryption by analyzing the serving patterns of LLMs. Further, we have developed an efficient, low-cost pipeline relinquishing approach for instances of incorrect predictions. Our experiments on NVIDIA H100 GPU show that compared with vanilla systems without confidential computing (e.g., vLLM, PEFT, and FlexGen), PipeLLM incurs modest overhead (less than 19.6 percent in throughput) across various LLM sizes, from 13B to 175B.",
    "link": "https://doi.org/10.1145/3669940.3707224",
    "conference_name": "ASPLOS",
    "session_title": "ML Security",
    "date": "2025-03-30",
    "paper_id": "e318f79f-421f-4e0a-be5b-d20bf2005749"
  },
  {
    "title": "Practical Federated Recommendation Model Learning Using ORAM with Controlled Privacy",
    "authors": "Jinyu Liu (The Pennsylvania State University), Wenjie Xiong (Virginia Tech), G. Edward Suh (NVIDIA,Cornell University), Kiwan Maeng (The Pennsylvania State University)",
    "abstract": "Training high-quality recommendation models requires collecting sensitive user data. The popular privacy-enhancing training method, federated learning (FL), cannot be used practically due to these models' large embedding tables. This paper introduces FEDORA, a system for training recommendation models with FL. FEDORA allows each user to only download, train, and upload a small subset of the large tables based on their private data, while hiding the access pattern using oblivious memory (ORAM). FEDORA reduces the ORAM's prohibitive latency and memory overheads by (1) introducing ε-FDP, a formal way to balance the ORAM's privacy with performance, and (2) placing the large ORAM in a power- and cost-efficient SSD with SSD-friendly optimizations. Additionally, FEDORA is carefully designed to support (3) modern operation modes of FL. FEDORA achieves high model accuracy by using private features during training while achieving up to 24× latency and over 1000× SSD lifetime improvement over the baseline. FEDORA achieves high model accuracy by using private features during training while achieving, on average, 5× latency and 158× SSD lifetime improvement over the baseline.",
    "link": "https://doi.org/10.1145/3676641.3716014",
    "conference_name": "ASPLOS",
    "session_title": "ML Security",
    "date": "2025-03-30",
    "paper_id": "c38b30db-9ffe-45b5-bd0f-4babd211ce6e"
  },
  {
    "title": "Tackling ML-based Dynamic Mispredictions using Statically Computed Invariants for Attack Surface Reduction",
    "authors": "Chris Porter (IBM Research), Sharjeel Khan (Georgia Institute of Technology), Kangqi Ni (Georgia Institute of Technology), Santosh Pande (Georgia Institute of Technology)",
    "abstract": "Recent work has demonstrated the utility of machine learning (ML) in carrying out highly accurate predictions at runtime. One of the major challenges with using ML, however, is that the predictions lack certain guarantees. For such approaches to become practicable in security settings involving debloating and dynamic control flow monitoring, one must distinguish between mispredictions vs. attacks.\nIn this work, we introduce a low overhead framework for tackling mispredictions of ML-based approaches using static invariants. In particular, we tackle the problem of dynamic function call set prediction encountered in program debloating. We first introduce an ML-based prediction technique that works on the whole application, providing high precision and reducing ~90% of code-reuse gadgets useful for staging attacks at runtime. We then propose an effective mechanism for dealing with the ML model's mispredictions: a new static relation called the ensue() of a function, which is a set of functions that could legally follow a given function under any dynamic execution. We develop efficient algorithms to statically compute such a set by modeling the relation in a Datalog-based solver. Upon misprediction, the framework invokes a lightweight mechanism to distinguish between an attack vs. misprediction. We show that the framework triggers misprediction checking in our experiments on a reasonable percentage of predictions invoked at runtime (3.8%, 16%, and 2.3% for SPEC CPU 2017 and low- and high-complexity applications, respectively), of which all cases are validated to conform to the static call relations. We contend that a low runtime overhead (7.5% for SPEC, 3% for real-world applications) and precise ML mechanism, along with the ability to effectively deal with mispredictions, yields a real-world solution.",
    "link": "https://doi.org/10.1145/3676641.3716276",
    "conference_name": "ASPLOS",
    "session_title": "ML Security",
    "date": "2025-03-30",
    "paper_id": "61e0177c-d61d-4361-9456-95083fb1155e"
  },
  {
    "title": "EXIST: Enabling Extremely Efficient Intra-Service Tracing Observability in Datacenters",
    "authors": "Xinkai Wang (Shanghai Jiao Tong University), Xiaofeng Hou (Shanghai Jiao Tong University), Chao Li (Shanghai Jiao Tong University), Yuancheng Li (Shanghai Jiao Tong University), Du Liu (Shanghai Jiao Tong University), Guoyao Xu (Alibaba Group), Guodong Yang (Alibaba Group), Liping Zhang (Alibaba Group), Yuemin Wu (Alibaba Cloud), Xiaopeng Yuan (Alibaba Cloud), Quan Chen (Shanghai Jiao Tong University), Minyi Guo (Shanghai Jiao Tong University)",
    "abstract": "The complexity of online applications is rapidly increasing, bringing more sophisticated performance anomalies in today’s cloud datacenter. To fully understand application behaviors, we should obtain both inter-service communication data via RPC-level tracing and intra-service execution traces via application-level tracing to precisely reason about event causality. However, the average time overhead of existing intra-service tracing schemes on the traced applications is generally about 5-10%, possibly reaching 18% in the worst case. To realize practical intra-service tracing in shared and stressed datacenters, one must achieve extreme tracing efficiency with an overhead at the per-mille level. In this work, we present EXIST, an extremely efficient intra-service tracing system based on off-the-shelf hardware tracing capabilities. EXIST consists of three cooperative modules to pursue optimal trade-offs towards extremely low overhead. Firstly, it identifies and eliminates costly tracing control operations to guarantee the performance of the observed applications. Secondly, it allocates limited trace buffer space dynamically based on application status. Thirdly, it",
    "link": "https://doi.org/10.1145/3676641.3716283",
    "conference_name": "ASPLOS",
    "session_title": "Performance Analysis & Tracing",
    "date": "2025-03-30",
    "paper_id": "4a40df68-b5af-42f2-b09c-aaba2345dcef"
  },
  {
    "title": "Mint: Cost-Efficient Tracing with All Requests Collection via Commonality and Variability Analysis",
    "authors": "Haiyu Huang (Sun Yat-sen University), Cheng Chen (Alibaba Group), Kunyi Chen (Alibaba Group), Pengfei Chen (Sun Yat-sen University), Guangba Yu (Sun Yat-sen University), Zilong He (Sun Yat-sen University), Yilun Wang (Sun Yat-sen University), Huxing Zhang (Alibaba Group), Qi Zhou (Alibaba Group)",
    "abstract": "Distributed traces contain valuable information but are often massive in volume, posing a core challenge in tracing framework design: balancing the tradeoff between preserving essential trace information and reducing trace volume. To address this tradeoff, previous approaches typically used a '1 or 0' sampling strategy: retaining sampled traces while completely discarding unsampled ones. However, based on an empirical study on real-world production traces, we discover that the '1 or 0' strategy actually fails to effectively balance this tradeoff. To achieve a more balanced outcome, we shift the strategy from the '1 or 0' paradigm to the 'commonality + variability' paradigm. The core of 'commonality + variability' paradigm is to first parse traces into common patterns and variable parameters, then aggregate the patterns and filter the parameters. We propose a cost-efficient tracing framework, Mint, which implements the 'commonality + variability' paradigm on the agent side to enable all requests capturing. Our experiments show that Mint can capture all traces and retain more trace information while optimizing trace storage (reduced to an average of 2.7%) and network overhead (reduced to an average of 4.2%). Moreover, experiments also demonstrate that Mint is lightweight enough for production use.",
    "link": "https://doi.org/10.1145/3669940.3707287",
    "conference_name": "ASPLOS",
    "session_title": "Performance Analysis & Tracing",
    "date": "2025-03-30",
    "paper_id": "401a5e57-cc3e-42cd-a8f7-e26035f73e45"
  },
  {
    "title": "Automatic Tracing in Task-Based Runtime Systems",
    "authors": "Rohan Yadav (Stanford University), Michael Bauer (NVIDIA), David Broman (KTH Royal Institute of Technology), Michael Garland (NVIDIA), Alex Aiken (Stanford University), Fredrik Kjolstad (Stanford University)",
    "abstract": "Implicitly parallel task-based runtime systems often perform dynamic analysis to discover dependencies in and extract parallelism from sequential programs. Dependence analysis becomes expensive as task granularity drops below a threshold. Tracing techniques have been developed where programmers annotate repeated program fragments (traces) issued by the application, and the runtime system memoizes the dependence analysis for those fragments, greatly reducing overhead when the fragments are executed again. However, manual trace annotation can be brittle and not easily applicable to complex programs built through the composition of independent components. We introduce Apophenia, a system that automatically traces the dependence analysis of task-based runtime systems, removing the burden of manual annotations from programmers and enabling new and complex programs to be traced. Apophenia identifies traces dynamically through a series of dynamic string analyses, which find repeated program fragments in the stream of tasks issued to the runtime system. We show that Apophenia is able to come between 0.92x--1.03x the performance of manually traced programs, and is able to effectively trace previously untraced programs to yield speedups of between 0.91x--2.82x on the Perlmutter and Eos supercomputers.",
    "link": "https://doi.org/10.1145/3669940.3707237",
    "conference_name": "ASPLOS",
    "session_title": "Performance Analysis & Tracing",
    "date": "2025-03-30",
    "paper_id": "99db8a3e-2c81-4290-8e1e-658f15e12e5a"
  },
  {
    "title": "Enabling Efficient Mobile Tracing with BTrace",
    "authors": "Jiawei Wang (Huawei Dresden Research Center,Huawei Central Software Institute), Nian Liu (Huawei Central Software Institute), Arnau Casadevall-Saiz (Huawei Dresden Research Center,Huawei Central Software Institute), Yutao Liu (Huawei Dresden Research Center,Huawei Central Software Institute), Diogo Behrens (Huawei Dresden Research Center,Huawei Central Software Institute), Ming Fu (Huawei Central Software Institute), Ning Jia (Huawei Central Software Institute), Hermann HÃ¤rtig (Technische UniversitÃ¤t Dresden), Haibo Chen (Huawei Central Software Institute,Shanghai Jiao Tong University)",
    "abstract": "With the growing complexity of smartphone systems, effective tracing becomes vital for enhancing their stability and optimizing the user experience. Unfortunately, existing tracing tools are inefficient in smartphone scenarios. Their distributed designs (with either per-core or per-thread buffers) prioritize performance but lead to missing crucial clues with high probability. While these problems can be overlooked in previous scenarios (e.g., servers), they drastically limit the usefulness of tracing on smartphones. To enable efficient tracing on smartphones, we propose BTrace : a tracing tool that combines the performance benefits of per-core buffers with the capability of retaining longer continuous traces by partitioning a global buffer into multiple",
    "link": "https://doi.org/10.1145/3676641.3715994",
    "conference_name": "ASPLOS",
    "session_title": "Performance Analysis & Tracing",
    "date": "2025-03-30",
    "paper_id": "da88045d-4cdc-4e81-98dd-aa1c90ae36ca"
  },
  {
    "title": "Rethinking Java Performance Analysis",
    "authors": "Stephen M. Blackburn (Google,Australian National University), Zixian Cai (Australian National University), Rui Chen (Unaffiliated-Independent), Xi Yang (IOP Systems), John Zhang (Canva), John Zigman (The University of Sydney)",
    "abstract": "Representative workloads and principled methodologies are the foundation of performance analysis, which in turn provides the empirical grounding for much of the innovation in systems research. However, benchmarks are hard to maintain, methodologies are hard to develop, and our field moves fast. The tension between our fast-moving fields and their need to maintain their methodological foundations is a serious challenge. This paper explores that challenge through the lens of Java performance analysis. Lessons we draw extend to other languages and other fields of computer science.\nIn this paper we: i) introduce a complete overhaul of the DaCapo benchmark suite, [6] characterizing 22 new and/or refreshed workloads across 47 dimensions, using principal components analysis to demonstrate their diversity, ii) demonstrate new methodologies and how they are integrated into an easy to use framework, iii) use this framework to conduct an analysis of the state of the art in production Java performance, and iv) motivate the need to invest in renewed methodologies and workloads, using as an example a review of contemporary production garbage collector performance.\nWe highlight the danger of allowing methodologies to lag innovation and respond with a suite and new methodologies that nudge forward some of our field's methodological foundations. We offer guidance on maintaining the empirical rigor we need to encourage profitable research directions and quickly identify unprofitable ones.",
    "link": "https://doi.org/10.1145/3669940.3707217",
    "conference_name": "ASPLOS",
    "session_title": "Performance Analysis & Tracing",
    "date": "2025-03-30",
    "paper_id": "824e119c-d0c0-4909-93e9-ba8e884a23f8"
  },
  {
    "title": "ReCA: Integrated Acceleration for Real-Time and Efficient Cooperative Embodied Autonomous Agents",
    "authors": "Zishen Wan (Georgia Institute of Technology), Yuhang Du (University of Minnesota, Twin Cities), Mohamed Ibrahim (Georgia Institute of Technology), Jiayi Qian (Georgia Institute of Technology), Jason Jabbour (Harvard University), Yang (Katie) Zhao (University of Minnesota, Twin Cities), Tushar Krishna (Georgia Institute of Technology), Arijit Raychowdhury (Georgia Institute of Technology), Vijay Janapa Reddi (Harvard University)",
    "abstract": "Cooperative embodied systems, where multiple agents collaborate through integrated perception, planning, action, and advanced reasoning powered by large language models (LLMs), show great potential for tackling complex, long-horizon, multi-objective tasks in real-world environments. Despite these algorithmic advancements, deploying embodied agents on current systems remains challenging due to prolonged planning and communication latency, limited scalability, and heightened sensitivity in low-level execution, all of which lead to significant system inefficiencies. This work proposes ReCA, a characterization and co-design framework dedicated to cooperative embodied agent system acceleration, aiming to enhance both task efficiency and system scalability. On the algorithm level, ReCA enables efficient local model processing to alleviate the substantial model costs. On the system level, ReCA presents a dual-memory structure with integrated long-term and short-term memory, hierarchical cooperative planning scheme with centralized and decentralized cooperation, and planning-guided multi-step execution for highly efficient and scalable cooperative embodied agent computation. On the hardware level, ReCA employs a heterogeneous hardware system with high-level planning GPU subsystem and low-level planning accelerator subsystem to ensure efficient and robust task execution. Evaluated across long-horizon multi-objective tasks, ReCA generalizes across application scenarios and system scales, achieving a 4.3% increase in successful missions with 10.2× speedup compared to the state-of-the-art cooperative embodied autonomous agent systems.",
    "link": "https://doi.org/10.1145/3676641.3716016",
    "conference_name": "ASPLOS",
    "session_title": "Autonomous Systems",
    "date": "2025-03-30",
    "paper_id": "02e9d20b-4d0c-4d6d-bb0b-d2d48ff764ad"
  },
  {
    "title": "AnA: An Attentive Autonomous Driving System",
    "authors": "Wonkyo Choe (University of Virginia), Rongxiang Wang (University of Virginia), Felix Xiaozhu Lin (University of Virginia)",
    "abstract": "In an autonomous driving system (ADS), the perception module is crucial to driving safety and efficiency. Unfortunately, the perception in today's ADS remains oblivious to driving decisions, contrasting to how humans drive. Our idea is to refactor ADS so that (1) the ADS guides its perception with the driving knowledge in situ; (2) the perception differentiates between awareness and attention. We propose a system called AnA with three novel mechanisms: (1) a query interface for the planning to express its interest in perception; (2) a query executor that maps queries to an optimal set of perception tasks; (3) a monitor for handling abnormal task executions with driving knowledge. On challenging driving benchmarks, AnA outperforms competitive baselines: it responds to adversarial events timely, reducing collisions by 2x; it reduces compute usage by 44% without compromising driving safety. We attribute AnA's efficacy to its attentive driving, a human-like behavior that improves resource proportionality.",
    "link": "https://doi.org/10.1145/3669940.3707261",
    "conference_name": "ASPLOS",
    "session_title": "Autonomous Systems",
    "date": "2025-03-30",
    "paper_id": "a2adff1c-4428-40fb-aede-abba3a334ae4"
  },
  {
    "title": "SuperNoVA: Algorithm-Hardware Co-Design for Resource-Aware SLAM",
    "authors": "Seah Kim (University of California, Berkeley), Roger Hsiao (University of California, Berkeley), Borivoje NikoliÄ (University of California, Berkeley), James Demmel (University of California, Berkeley), Yakun Sophia Shao (University of California, Berkeley)",
    "abstract": "Simultaneous Localization and Mapping (SLAM) plays a crucial role in robotics, autonomous systems, and augmented and virtual reality (AR/VR) applications by enabling devices to understand and map unknown environments. However, deploying SLAM in AR/VR applications poses significant challenges, including the demand for high accuracy, real-time processing, and efficient resource utilization, especially on compact and lightweight devices. To address these challenges, we propose SuperNoVA, which enables high-accuracy, real-time, large-scale SLAM in resource-constrained settings through a full-stack system, spanning from algo-rithm to hardware. In particular, SuperNoVA dynamically constructs a subgraph to meet the latency target while preserving accuracy, virtualizes hardware resources for efficient graph processing, and implements a novel hardware architecture to accelerate the SLAM backend efficiently. Evaluation results demonstrate that, for a large-scale AR dataset, Super-NoVA reduces full SLAM backend computation latency by 89.5% compared to the baseline out-of-order CPU and 78.6% compared to the baseline embedded GPU, and reduces the maximum pose error by 89% over existing SLAM solutions, while always meeting the latency target.",
    "link": "https://doi.org/10.1145/3669940.3707258",
    "conference_name": "ASPLOS",
    "session_title": "Autonomous Systems",
    "date": "2025-03-30",
    "paper_id": "daceb641-7c50-40eb-8f1b-cd5155c79eaf"
  },
  {
    "title": "OctoCache: Caching Voxels for Accelerating 3D Occupancy Mapping in Autonomous Systems",
    "authors": "Peiqing Chen (University of Maryland), Minghao Li (Harvard University), Zishen Wan (Georgia Institute of Technology), Yushun Hsiao (Harvard University), Minlan Yu (Harvard University), Vijay Janapa Reddi (Harvard University), Zaoxing Liu (University of Maryland)",
    "abstract": "3D mapping systems are crucial for creating digital representations of physical environments, widely used in autonomous robot navigation, 3D visualization, and AR/VR. This paper focuses on OctoMap, a leading 3D mapping framework us-ing an octree-based structure for spatial efficiency. However, OctoMap’s performance is limited by slow updates due to costly memory accesses. We introduce OctoCache, a software system that accelerates OctoMap through (1) optimized cache memory access, (2) refined voxel ordering, and (3) workflow parallelization. OctoCache achieves speedups of 45.63%~88.01% in 3D environment construction tasks compared to standard OctoMap. Deployed in UAV navigation scenarios, OctoCache demonstrates up to 3.02 × speedup and reduces mission completion time by up to 28%. These results highlight OctoCache’s potential to enhance 3D mapping efficiency in autonomous navigation, advancing robotics and environmental modeling. CCS",
    "link": "https://doi.org/10.1145/3676641.3716263",
    "conference_name": "ASPLOS",
    "session_title": "Autonomous Systems",
    "date": "2025-03-30",
    "paper_id": "d694fdfe-9e1b-4ce1-a669-0629806fe4ae"
  },
  {
    "title": "Necro-reaper: Pruning away Dead Memory Traffic in Warehouse-Scale Computers",
    "authors": "Sotiris Apostolakis (Google), Chris Kennelly (Google), Xinliang David Li (Google), Parthasarathy Ranganathan (Google)",
    "abstract": "Memory bandwidth is emerging as a critical bottleneck in warehouse-scale computing (WSC). This work reveals that a significant portion of memory traffic in WSC is surprisingly unnecessary, consisting of needless writebacks of deallocated data and fetches of uninitialized data. This issue is particularly acute in WSC, where short-lived heap allocations bigger than a cache line are prevalent. To address this problem, this work proposes a pragmatic approach tailored to WSC. Leveraging the existing WSC ecosystem of vertical integration, profile-guided compilation flows, and customized memory allocators, this work presents Necro-reaper, a novel software/hardware co-design that avoids dead memory traffic without requiring the hardware tracking of prior work. New ISA instructions enable the hardware to avoid unnecessary dead traffic, while extended software components, including a profile-guided compiler and memory allocator, optimize the utilization of these instructions. Evaluation across a diverse set of 10 WSC workloads demonstrates that Necro-reaper achieves a geomean memory traffic reduction of 26% and a geomean IPC increase of 6%.",
    "link": "https://doi.org/10.1145/3676641.3716007",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 2",
    "date": "2025-03-30",
    "paper_id": "8dfd6204-b7b3-423c-91ca-c4cbb7081b25"
  },
  {
    "title": "Embracing Imbalance: Dynamic Load Shifting among Microservice Containers in Shared Clusters",
    "authors": "Shutian Luo (Yale University), Jianxiong Liao (Sun Yat-sen University,University of Macau), Chenyu Lin (University of Macau), Huanle Xu (University of Macau), Zhi Zhou (Sun Yat-sen University), Chengzhong Xu (University of Macau)",
    "abstract": "In a unified resource scheduling architecture, containers within the same microservice often encounter temporal and spatial performance imbalance when deployed in large-scale shared clusters. As a result, the commonly employed load-balancing approach often leads to substantial resource wastage as applications are frequently over-provisioned to meet service level agreements (SLAs).\nIn this paper, we utilize an alternative approach by leveraging load imbalance. The central concept involves the dynamic load shifting across microservice containers with a focus on imbalance awareness. However, achieving seamless integration between load shifting and resource scaling, while accommodating the demands of partial connection between upstream and downstream containers, remains a challenge. To address this challenge, we introduce Imbres-a new microservice system that optimizes load shifting, connection management, and resource scaling in tandem. One significant advantage of Imbres lies in its rapid responsiveness, relying solely on online gradients of latency, eliminating the need for offline profiling. Evaluation using real microservice benchmarks reveals that Imbres reduces resource allocation by up to 62% and decreases SLA violation probability by up to 82%, compared to state-of-the-art systems.",
    "link": "https://doi.org/10.1145/3676641.3716255",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 2",
    "date": "2025-03-30",
    "paper_id": "2da7bc61-20d6-47a4-b4dc-acbfcee91a77"
  },
  {
    "title": "Tela: A Temporal Load-Aware Cloud Virtual Disk Placement Scheme",
    "authors": "Difan Tan (Huazhong University of Science and Technology), Jiawei Li (Huazhong University of Science and Technology), Hua Wang (Huazhong University of Science and Technology), Xiaoxiao Li (Huazhong University of Science and Technology), Wenbo Liu (Huazhong University of Science and Technology), Zijin Qin (Huazhong University of Science and Technology), Ke Zhou (Huazhong University of Science and Technology), Ming Xie (Tencent Inc.), Mengling Tao (Tencent Inc.)",
    "abstract": "Cloud Block Storage (CBS) relies on Cloud Virtual Disks (CVDs) to provide block interfaces to Cloud Virtual Machines. The process of allocating user-subscribed CVDs to physical storage warehouses in cloud data centers, known as CVD placement, significantly impacts resource utilization, load balancing, and I/O performance. However, previous works have failed to account for temporal fluctuations in cloud loads, resulting in imbalanced loads, low resource utilization, and frequent warehouse overloads.\nTo address these issues, we propose Tela, the first temporal load-aware CVD placement scheme. Using a series of interpretable models, Tela predicts the temporal load characteristics and values of CVDs, as well as potential peak loads in warehouses. Guided by these predictions, TELA places CVDs into warehouses according to their load patterns, aiming for peak shaving and load balancing while preventing overloads. Experimental results show that Tela significantly outperforms the state-of-the-art scheme, reducing overload occurrences by 86.8-93.8%, reducing P99 overload duration by 92.6%, and decreasing load imbalance by 36.7-44.4%.",
    "link": "https://doi.org/10.1145/3669940.3707252",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 2",
    "date": "2025-03-30",
    "paper_id": "c8e4b5c1-b1a4-4aa0-b645-a2bd0c17346d"
  },
  {
    "title": "FleetIO: Managing Multi-Tenant Cloud Storage with Multi-Agent Reinforcement Learning",
    "authors": "Jinghan Sun (UIUC), Benjamin Reidys (UIUC), Daixuan Li (UIUC), Jichuan Chang (Google), Marc Snir (UIUC), Jian Huang (UIUC)",
    "abstract": "Cloud platforms have been virtualizing storage devices like flash-based solid-state drives (SSDs) to make effective use of storage resources. They enable either software-isolated instance or hardware-isolated instance for facilitating the storage sharing between multi-tenant applications. However, for decades, they have to combat the fundamental tussle between the performance isolation and resource utilization. They suffer from either long tail latency caused by weak isolation or low storage utilization caused by strong isolation. In this paper, we present FleetIO, a learning-based storage virtualization framework that employs reinforcement learning (RL) for managing virtualized SSDs. FleetIO explores the unique features of RL to handle the dynamic changes of application workloads and storage states, and integrates the storage scheduling into the RL decision-making process. It achieves both performance isolation and improved storage utilization by enabling dynamic fine-grained storage harvesting across collocated application instances, while minimizing its negative impact on their service-level objectives (SLOs). FleetIO clusters workloads into different types (e.g., latency-sensitive and bandwidth-intensive) based on the collected I/O traces at runtime, and fine-tunes the RL reward functions for each type of workloads. We implement FleetIO on a real programmable SSD board and evaluate it with diverse cloud applications. We show that FleetIO improves the overall storage utilization of the shared SSD by up to 1.4 × , and decreases the tail latency of I/O requests by 1.5 × on average, compared to the state-of-the-art storage sharing approaches",
    "link": "https://doi.org/10.1145/3669940.3707229",
    "conference_name": "ASPLOS",
    "session_title": "Cloud Computing 2",
    "date": "2025-03-30",
    "paper_id": "a681ae13-138b-4e3a-b39b-26927d0af458"
  },
  {
    "title": "ZRAID: Leveraging Zone Random Write Area (ZRWA) for Alleviating Partial Parity Tax in ZNS RAID",
    "authors": "Minwook Kim (Seoul National University), Seongyeop Jeong (Seoul National University), Jin-Soo Kim (Seoul National University)",
    "abstract": "The Zoned Namespace (ZNS) SSD is an innovative technology that aims to mitigate the block interface tax associated with conventional SSDs. However, constructing a RAID system using ZNS SSDs presents a significant challenge in managing partial parity for incomplete stripes. Previous research permanently logs partial parity in a limited number of reserved zones, which not only creates bottlenecks in throughput but also exacerbates write amplification, thereby reducing the device's lifetime. We refer to these inefficiencies as the partial parity tax.\nIn this paper, we present ZRAID, a software ZNS RAID layer that leverages the newly added Zone Random Write Area (ZRWA) feature in the ZNS Command Set, to alleviate partial parity tax. ZRWA enables in-place updates within a confined area near the write pointer. ZRAID temporarily stores partial parity within the ZRWA of data zones. Thus, partial parity writes are distributed across multiple data zones, effectively eliminating throughput bottlenecks. Furthermore, any expired partial parity in the ZRWA is overwritten by subsequent data, avoiding unnecessary flash writes. With the introduction of ZRWA, ZRAID can leverage general schedulers, overcoming the queue depth limitations of ZNS-compatible schedulers. Our evaluation with actual ZNS SSDs demonstrates a significant improvement in write throughput: up to 34.7% in the fio microbenchmark, and an average of 14.5% in db_bench on RocksDB, along with up to a 1.6x reduction in flash write amplification.",
    "link": "https://doi.org/10.1145/3669940.3707248",
    "conference_name": "ASPLOS",
    "session_title": "Memory & Storage",
    "date": "2025-03-30",
    "paper_id": "ec377099-c8c3-4361-af08-715ed6a444b4"
  },
  {
    "title": "Marionette: A RowHammer Attack via Row Coupling",
    "authors": "Seungmin Baek (Seoul National University), Minbok Wi (Seoul National University), Seonyong Park (Seoul National University), Hwayong Nam (Seoul National University), Michael Jaemin Kim (Seoul National University), Nam Sung Kim (University of Illinois), Jung Ho Ahn (Seoul National University)",
    "abstract": "A body of recent work has revealed that two different rows in a DRAM bank, from the perspective of a processor-memory interface, are connected to the same wordline but two separate row buffers (bitline sense amplifiers) in certain DRAM chips. Such a pair of rows is referred to as a ''coupled-row pair.'' Coupled-row pairs pose a substantial security threat as RowHammer bitflips can be caused not only by the conventional, adjacent aggressor rows but also by their coupled rows that are distant in physical address\nWe investigate the impact of a coupled row on both FPGA-based infrastructure and server systems. In RowHammer attacks, coupled rows have hammering strength nearly identical to aggressor rows, with these attacks invisible to conventional, processor-side mitigation solutions. By exploiting these observations, we present Marionette, a new type of RowHammer attack that exploits coupled rows to extend the existing RowHammer attack surface.\nFirst, coupled rows enable an attacker to evade two types of existing software-based RowHammer defenses: tracking- and isolation-based defenses. We induce RowHammer bitflips successfully against tracking-based RowHammer defenses by silently hammering coupled rows. We also identify the feasibility of RowHammer bitflips in an isolation-based inter-VM RowHammer defense by breaking DRAM-subarray-level isolation. Second, we successfully conduct an existing RowHammer exploit in a server under the tracking-based RowHammer defense. In a native server system, Marionette enhances the success rate of the RowHammer exploit by up to 1.66x. Lastly, we explore lightweight mitigation schemes for Marionette by exposing the coupled-row relationship to systems.",
    "link": "https://doi.org/10.1145/3669940.3707242",
    "conference_name": "ASPLOS",
    "session_title": "Memory & Storage",
    "date": "2025-03-30",
    "paper_id": "a4addd93-af3f-4d70-98de-c95196247f60"
  },
  {
    "title": "MOAT: Securely Mitigating Rowhammer with Per-Row Activation Counters",
    "authors": "Moinuddin Qureshi (Georgia Institute of Technology), Salman Qazi (Google)",
    "abstract": "The security vulnerabilities due to Rowhammer have worsened over the last decade, with existing in-DRAM solutions, such as TRR, getting broken with simple patterns. In response, the DDR5 specifications have been extended to support Per-Row Activation Counting (PRAC), with counters inlined with each row, and ALERT-Back-Off (ABO) to stop the memory controller if the DRAM needs more time to mitigate. Although PRAC+ABO represents a strong advance in Rowhammer protection, they are just a framework, and the actual security is dependent on the implementation. In this paper, we first show that a prior work, Panopticon (which formed the basis for PRAC+ABO), is insecure, as our Jailbreak pattern can cause 1150 activations on an attack row for Panopticon configured for a threshold of 128. We then propose MOAT, a provably secure design, which uses two internal thresholds: ETH, an\"Eligibility Threshold\"for mitigating a row, and ATH, an\"ALERT Threshold\"for initiating an ABO. As JEDEC specifications permit a few activations between consecutive ALERTs, we also study how an attacker can exploit such activations to inflict more activations than ATH on an attack row and thus increase the tolerated Rowhammer threshold. Our analysis shows that MOAT configured with ATH=64 can safely tolerate a Rowhammer threshold of 99. Finally, we also study performance attacks and denial-of-service due to ALERTs. Our evaluations, with SPEC and GAP workloads, show that MOAT with ATH=64 incurs an average slowdown of 0.28\\% and 7 bytes of SRAM per bank.",
    "link": "https://doi.org/10.1145/3669940.3707278",
    "conference_name": "ASPLOS",
    "session_title": "Memory & Storage",
    "date": "2025-03-30",
    "paper_id": "b330e5d9-5a1f-457f-bb93-064ab23ab612"
  },
  {
    "title": "HyperHammer: Breaking Free from KVM-Enforced Isolation",
    "authors": "Wei Chen (Peking University), Zhi Zhang (University of Western Australia), Xin Zhang (Peking University), Qingni Shen (Peking University), Yuval Yarom (Ruhr University Bochum), Daniel Genkin (Georgia Institute of Technology), Chen Yan (Peking University), Zhe Wang (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,Zhongguancun Laboratory)",
    "abstract": "Hardware-assisted virtualization is a key enabler of the modern cloud. It decouples virtual machine execution from the hardware it runs on, allowing increased flexibility through services such as dynamic hardware provisioning and live migration. Underlying this flexibility is the security promise that guest virtual machines are isolated from each other. However, due to the level of sharing between VMs, hardware vulnerabilities present a serious threat to this usage. One such vulnerability is Rowhammer, which allows attackers to modify the contents of memory to which they have no access. While the attack has been known for over a decade, published applications against such environments are limited, compromising only co-resident VMs, but not the hypervisor. More-over, due to security concerns, a key component enabling their attack has been disabled. Hence, this attack is no longer applicable in a contemporary virtualized environment. In this paper, we examine how Rowhammer can affect virtualized systems. We present HyperHammer, a Rowhammer attack that breaks hypervisor-enforced memory isolation and further compromises the hypervisor. Due to the highly",
    "link": "https://doi.org/10.1145/3676641.3716002",
    "conference_name": "ASPLOS",
    "session_title": "Memory & Storage",
    "date": "2025-03-30",
    "paper_id": "bfa0845e-551f-43a9-b35f-5d6644ea1b14"
  },
  {
    "title": "Efficient Lossless Compression of Scientific Floating-Point Data on CPUs and GPUs",
    "authors": "Noushin Azami (Department of Computer Science, Texas State University), Alex Fallin (Department of Computer Science, Texas State University), Martin Burtscher (Department of Computer Science, Texas State University)",
    "abstract": "The amount of scientific data being produced, transferred, and processed increases rapidly. Whereas GPUs have made faster processing possible, storage limitations and slow data transfers remain key bottlenecks. Data compression can help, but only if it does not create a new bottleneck. This paper presents four new lossless compression algorithms for single-and double-precision data that compress well and are fast even though they are fully compatible between CPUs and GPUs. Averaged over many SDRBench inputs, our implementations outperform most of the 18 compressors from the literature we compare to in compression ratio, compression throughput, and decompression throughput. Moreover, they outperform all of them in either throughput or compression ratio on the two CPUs and two GPUs we used for evaluation. For example, on an RTX 4090 GPU, our fastest code compresses and decompresses at over 500 GB/s while delivering one of the highest compression ratios.",
    "link": "https://doi.org/10.1145/3669940.3707280",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 1",
    "date": "2025-03-30",
    "paper_id": "f9faf650-e914-49cf-ad24-f6a2d9f23cd2"
  },
  {
    "title": "Data Cache for Intermittent Computing Systems with Non-Volatile Main Memory",
    "authors": "Sourav Mohapatra (Department of Computer Science, TU Delft,ARM Limited), Vito Kortbeek (Department of Computer Science, TU Delft,Synopsys), Marco Antonio van Eerden (Department of Computer Science, TU Delft), Jochem Broekhoff (Department of Computer Science, TU Delft,DSP Innovation B.V.), Saad Ahmed (School of Interactive Computing, Georgia Institute of Technology), PrzemysÅaw PaweÅczak (Department of Computer Science, TU Delft)",
    "abstract": "Intermittently-operating embedded computing platforms powered by energy harvesting must frequently checkpoint their computation state. Using non-volatile memory reduces checkpoint size by eliminating the need to checkpoint volatile memory but increases checkpoint frequency to cover Write After Read (WAR) dependencies. Additionally, non-volatile memory is significantly slower to access - while consuming more energy than its volatile counterpart - suggesting the use of a data cache. Unfortunately, existing data cache solutions do not fit the challenges of intermittent computing and often require additional hardware or software to detect WARs. In this paper, we extend the data cache by integrating it with WAR detection - dropping the need for an additional memory tracker. This idea forms the basis of NACHO: a data cache tailored to intermittent computing. NACHO, on average, reduces intermittent computing runtime overhead by 54% compared to state of the art cache-based systems. It also reduces the number of non-volatile memory writes by 82% compared to a data cache-less system, and 18% on average compared to multiple state of the art cache-based systems.",
    "link": "https://doi.org/10.1145/3676641.3715989",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 1",
    "date": "2025-03-30",
    "paper_id": "cee3eace-a533-4a74-a1e2-73bc971fc151"
  },
  {
    "title": "Fusion: An Analytics Object Store Optimized for Query Pushdown",
    "authors": "Jianan Lu (Princeton University), Ashwini Raina (Princeton University), Asaf Cidon (Columbia University), Michael J. Freedman (Princeton University)",
    "abstract": "The prevalence of disaggregated storage in public clouds has led to increased latency in modern OLAP cloud databases, particularly when handling ad-hoc and highly-selective queries on large objects. To address this, cloud databases have adopted computation pushdown, executing query predicates closer to the storage layer. However, existing pushdown solutions are inefficient in erasure-coded storage. Cloud storage employs erasure coding that partitions analytics file objects into fixed-sized blocks and distributes them across storage nodes. Consequently, when a specific part of the object is queried, the storage system must reassemble the object across nodes, incurring significant network latency. In this work, we present Fusion , an object store for analytics that is optimized for query pushdown on erasure-coded data. It co-designs its erasure coding and file placement topologies, taking into account popular analytics file formats (e.g., Parquet). Fusion employs a novel stripe construction algorithm that prevents fragmentation of computable units within an object, and minimizes storage overhead during erasure coding. Compared to existing erasure-coded stores, Fusion improves median and tail latency by 64% and 81%, respectively, on TPC-H, and up to 40% and 48% respectively, on real-world SQL queries. Fusion achieves this while incurring a modest 1.2% storage overhead compared to the optimal. CCS",
    "link": "https://doi.org/10.1145/3669940.3707234",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 1",
    "date": "2025-03-30",
    "paper_id": "72897497-bf8d-4655-9ebd-8cbb4fba3ebb"
  },
  {
    "title": "VertexSurge: Variable Length Graph Pattern Match on Billion-edge Graphs",
    "authors": "Weiyu Xie (Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University), MingXing Zhang (Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University), Xia Liao (Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University), Kang Chen (Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University), Jinlei Jiang (Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University), YongWei Wu (Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University)",
    "abstract": "Variable-Length Graph Pattern Matching (VLGPM) is a critical functionality in graph databases, pivotal for identifying patterns where the number of connecting edges between two matched vertices is variable. This function plays a vital role in analyzing complex and dynamic networks such as social networks or bank transfers networks, where relationships can vary extensively in both length and structure. However, despite its importance, current graph databases, optimized primarily for single-hop subgraph matching, struggle with VLGPM over large graphs. To bridge this gap between essential user requirements and the lack of efficient support in existing systems, we introduce VertexSurge. Central to VertexSurge is an innovative variable-length expand (VExpand) operator, which incorporates several microarchitecture-friendly optimizations to efficiently compute the reachability matrix between two sets of vertices. These optimizations enable VertexSurge to handle the surge of vertex count due to variable length with high performance. Additionally, VertexSurge combines VExpand with effective multi-set intersection for pattern matching, ruled-based planning, and disk offloading for large datasets, to implement a full-fledged VLGPM engine. Our evaluations with real-world graph datasets and representative patterns demonstrate that VertexSurge significantly outperforms existing systems in VLGPM, validating its efficacy in handling large-scale graph pattern matching challenges.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p361-xie.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 1",
    "date": "2025-03-30",
    "paper_id": "5293420f-13df-493d-8026-72ec5febada7"
  },
  {
    "title": "Formalising CXL Cache Coherence",
    "authors": "Chengsong Tan (Kaihong), Alastair F. Donaldson (Imperial College London), John Wickerson (Imperial College London)",
    "abstract": "We report our experience formally modelling and verifying CXL.cache, the inter-device cache coherence protocol of the Compute Express Link standard. We have used the Isabelle proof assistant to create a formal model for CXL.cache based on the prose English specification. This led to us identifying and proposing fixes to several problems we identified as unclear, ambiguous or inaccurate, some of which could lead to incoherence if left unfixed. Nearly all our issues and proposed fixes have been confirmed and tentatively accepted by the CXL consortium for adoption, save for one which is still under discussion. To validate the faithfulness of our model we performed scenario verification of essential restrictions such as\"Snoop-pushes-GO\", and produced a fully mechanised proof of a coherence property of the model. The considerable size of this proof, comprising tens of thousands of lemmas, prompted us to develop new proof automation tools, which we have made available for other Isabelle users working with similarly cumbersome proofs.",
    "link": "https://doi.org/10.1145/3676641.3715999",
    "conference_name": "ASPLOS",
    "session_title": "CXL Storage",
    "date": "2025-03-30",
    "paper_id": "1e26eef6-7cfb-426f-bd0a-6783eeade1ea"
  },
  {
    "title": "CtXnL: A Software-Hardware Co-Designed Solution for Efficient CXL-Based Transaction Processing",
    "authors": "Zhao Wang (Peking University, School of Integrated Circuits,Peking University, School of Computer Science), Yiqi Chen (Peking University, School of Integrated Circuits), Cong Li (Peking University, School of Integrated Circuits), Yijin Guan (Alibaba Group, DAMO Academy,Hupan Lab), Dimin Niu (Alibaba Group, DAMO Academy,Hupan Lab), Tianchan Guan (Alibaba Group, DAMO Academy,Hupan Lab), Zhaoyang Du (Alibaba Group, DAMO Academy,Hupan Lab), Xingda Wei (Shanghai Jiao Tong University, Institute of Parallel and Distributed Systems, SEIEE), Guangyu Sun (Peking University, School of Integrated Circuits,Beijing Advanced Innovation Center for Integrated Circuits)",
    "abstract": "Transaction processing systems are the crux for modern data-center applications, yet current multi-node systems are slow due to network overheads. This paper advocates for Compute Express Link (CXL) as a network alternative, which enables low-latency and cache-coherent shared memory accesses. However, directly adopting standard CXL primitives leads to performance degradation due to the high cost of maintaining cross-node cache coherence. To address the CXL challenges, this paper introduces CTXNL, a software-hardware co-designed system that implements a novel hybrid coherence primitive tailored to the loosely coherent nature of transactional data. The core innovation of CTXNL is empowering transaction system developers with the ability to selectively achieve data coherence. Our evaluations on OLTP workloads demonstrate that CTXNL enhances performance, outperforming current network-based systems and achieves up to 2.08x greater throughput than vanilla CXL memory sharing architectures across universal transaction processing policies.",
    "link": "https://doi.org/10.1145/3676641.3716244",
    "conference_name": "ASPLOS",
    "session_title": "CXL Storage",
    "date": "2025-03-30",
    "paper_id": "f957cadd-0744-47da-a355-98ac7ba720ca"
  },
  {
    "title": "ByteFS: System Support for (CXL-based) Memory-Semantic Solid-State Drives",
    "authors": "Shaobo Li (University of Illinois Urbana-Champaign), Yirui (Eric) Zhou (University of Illinois Urbana-Champaign), Hao Ren (University of Illinois Urbana-Champaign), Jian Huang (University of Illinois Urbana-Champaign)",
    "abstract": "Unlike non-volatile memory that resides on the processor memory bus, memory-semantic solid-state drives (SSDs) support both byte and block access granularity via PCIe or CXL interconnects. They provide scalable memory capacity using NAND flash at a much lower cost. In addition, they have different performance characteristics for their dual byte/block interface respectively, while offering essential memory semantics for upper-level software. Such a byte-accessible storage device provides new implications on the software system design. In this paper, we develop a new file system, named ByteFS, by rethinking the design primitives of file systems and SSD firmware to exploit the advantages of both byte and block-granular data accesses. ByteFS supports byte-granular data persistence to retain the persistence nature of SSDs. It extends the core data structure of file systems by enabling dual byte/block-granular data accesses. To facilitate the support for byte-granular writes, \\pname{} manages the internal DRAM of SSD firmware in a log-structured manner and enables data coalescing to reduce the unnecessary I/O traffic to flash chips. ByteFS also enables coordinated data caching between the host page cache and SSD cache for best utilizing the precious memory resource. We implement ByteFS on both a real programmable SSD and an emulated memory-semantic SSD for sensitivity study. Compared to state-of-the-art file systems for non-volatile memory and conventional SSDs, ByteFS outperforms them by up to 2.7$\\times$, while preserving the essential properties of a file system. ByteFS also reduces the write traffic to SSDs by up to 5.1$\\times$ by alleviating unnecessary writes caused by both metadata and data updates in file systems.",
    "link": "https://doi.org/10.1145/3669940.3707250",
    "conference_name": "ASPLOS",
    "session_title": "CXL Storage",
    "date": "2025-03-30",
    "paper_id": "f86e20f3-95fa-490f-bd65-fba143d7fcde"
  },
  {
    "title": "M5: Mastering Page Migration and Memory Management for CXL-based Tiered Memory Systems",
    "authors": "Yan Sun (University of Illinois Urbana-Champaign), Jongyul Kim (University of Illinois Urbana-Champaign), Zeduo Yu (University of Illinois Urbana-Champaign), Jiyuan Zhang (University of Illinois Urbana-Champaign), Siyuan Chai (University of Illinois Urbana-Champaign), Michael Jaemin Kim (Seoul National University), Hwayong Nam (Seoul National University), Jaehyun Park (Seoul National University), Eojin Na (Seoul National University), Yifan Yuan (Intel Labs), Ren Wang (Intel Labs), Jung Ho Ahn (Seoul National University), Tianyin Xu (University of Illinois Urbana-Champaign), Nam Sung Kim (University of Illinois Urbana-Champaign)",
    "abstract": "CXL has emerged as a promising memory interface that can cost-effectively expand the capacity and bandwidth of a memory system, complementing the traditional DDR interface. However, CXL DRAM presents 2–3 × longer access latency than DDR DRAM, forming a tiered-memory system that demands an effective and efficient page-migration solution. Although many page-migration solutions have been proposed for past tiered-memory systems, they have achieved limited success. To tackle the challenge of managing tiered-memory systems, this work first presents a CXL-driven profiling so-lution to precisely and transparently count the number of accesses to every 4KB page and 64B word in CXL DRAM. Second, using the profiling solution, this work uncovers that (1) widely used CPU-driven page-migration solutions often identify warm pages as hot pages, and (2) certain applications have sparse hot pages, where only a small percentage of words in each of these pages are frequently accessed.",
    "link": "https://doi.org/10.1145/3676641.3711999",
    "conference_name": "ASPLOS",
    "session_title": "CXL Storage",
    "date": "2025-03-30",
    "paper_id": "a88a82ef-64f4-4b2c-afe2-4c68223cf4ea"
  },
  {
    "title": "Systematic CXL Memory Characterization and Performance Analysis at Scale",
    "authors": "Jinshu Liu (Virginia Tech), Hamid Hadian (Virginia Tech), Yuyue Wang (Virginia Tech), Daniel S. Berger (Microsoft and University of Washington), Marie Nguyen (Samsung), Xun Jian (Virginia Tech), Sam H. Noh (Virginia Tech), Huaicheng Li (Virginia Tech)",
    "abstract": "Compute Express Link (CXL) has emerged as a pivotal inter-connect for memory expansion. Despite its potential, the performance implications of CXL across devices, latency regimes, processors, and workloads remain underexplored. We present Melody, a framework for systematic characterization and analysis of CXL memory performance. Melody builds on an extensive evaluation spanning 265 workloads, 4 real CXL devices, 7 latency levels, and 5 CPU platforms. Melody yields many insights: workload sensitivity to sub-µ s CXL latencies (140-410ns), the first disclosure of CXL tail latencies, CPU tolerance to CXL latencies, a novel approach (Spa) for pinpointing CXL bottlenecks, and CPU prefetcher inefficiencies under CXL.",
    "link": "https://doi.org/10.1145/3676641.3715987",
    "conference_name": "ASPLOS",
    "session_title": "CXL Storage",
    "date": "2025-03-30",
    "paper_id": "48c8a653-99c2-4efe-991f-d771fe190ea0"
  },
  {
    "title": "Fast On-device LLM Inference with NPUs",
    "authors": "Daliang Xu (Key Lab of HCST (PKU), MOE; SCS, Peking University), Hao Zhang (Beijing University of Posts and Telecommunications), Liming Yang (Key Lab of HCST (PKU), MOE; SCS, Peking University), Ruiqi Liu (Key Lab of HCST (PKU), MOE; SCS, Peking University), Gang Huang (Key Lab of HCST (PKU), MOE; SCS, Peking University,National Key Laboratory of Data Space Technology and System), Mengwei Xu (Beijing University of Posts and Telecommunications), Xuanzhe Liu (Key Lab of HCST (PKU), MOE; SCS, Peking University)",
    "abstract": "On-device inference for Large Language Models (LLMs), driven by increasing privacy concerns and advancements of mobile-sized models, has gained significant interest. However, even mobile-sized LLMs (e.g., Gemma-2B) encounter unacceptably high inference latency, often bottlenecked by the prefill stage in tasks like screen UI understanding. We present llm.npu, the first LLM inference system utilizing on-device Neural Processing Unit (NPU) offloading to reduce prefill latency. llm.npu enhances NPU offloading efficiency by re-constructing the prompt and model in three levels: (1) At prompt level, it divides variable-length prompts into multiple fixed-sized chunks while maintaining data dependencies; (2) At tensor level, it identifies and extracts significant outliers to run on the CPU/GPU in parallel with minimal overhead; (3) At block level, it schedules Transformer blocks in an out-of-order manner to the CPU/GPU and NPU based on their hardware affinity and sensitivity to accuracy. Compared to competitive baselines, llm.npu achieves 22.4x faster prefill speed and 30.7$\\times$ energy savings on average, and up to 32.8x speedup in an end-to-end real-world application. For the first time, llm.npu achieves more than 1,000 tokens/sec prefilling for a billion-sized model.",
    "link": "https://doi.org/10.1145/3669940.3707239",
    "conference_name": "ASPLOS",
    "session_title": "Large Language Models",
    "date": "2025-03-30",
    "paper_id": "3f696cd9-3624-4cab-898e-f5966db93d5e"
  },
  {
    "title": "Helix: Serving Large Language Models over Heterogeneous GPUs and Network via Max-Flow",
    "authors": "Yixuan Mei (Carnegie Mellon University), Yonghao Zhuang (Carnegie Mellon University), Xupeng Miao (Carnegie Mellon University), Juncheng Yang (Carnegie Mellon University), Zhihao Jia (Carnegie Mellon University), Rashmi Vinayak (Carnegie Mellon University)",
    "abstract": "This paper introduces Helix, a distributed system for high-throughput, low-latency large language model (LLM) serving in heterogeneous GPU clusters. The key idea behind Helix is to formulate inference computation of LLMs over heterogeneous GPUs and network connections as a max-flow problem on directed, weighted graphs, whose nodes represent GPU instances and edges capture both GPU and network heterogeneity through their capacities. Helix then uses a mixed integer linear programming (MILP) algorithm to discover highly optimized strategies to serve LLMs on heterogeneous GPUs. This approach allows Helix to jointly optimize model placement and request scheduling, two highly entangled tasks in heterogeneous LLM serving. Our evaluation on several heterogeneous clusters ranging from 24 to 42 GPU nodes shows that Helix improves serving throughput by up to 3.3x and reduces prompting and decoding latency by up to 66% and 24%, respectively, compared to existing approaches. Helix is available at https://github.com/Thesys-lab/Helix-ASPLOS25.",
    "link": "https://doi.org/10.1145/3669940.3707215",
    "conference_name": "ASPLOS",
    "session_title": "Large Language Models",
    "date": "2025-03-30",
    "paper_id": "0fd52eed-fb8f-4226-9ba7-4e7f5ef17301"
  },
  {
    "title": "FlexSP: Accelerating Large Language Model Training via Flexible Sequence Parallelism",
    "authors": "Yujie Wang (Peking University), Shiju Wang (Beihang University), Shenhan Zhu (Peking University), Fangcheng Fu (Peking University), Xinyi Liu (Peking University), Xuefeng Xiao (ByteDance Inc.), Huixia Li (ByteDance Inc.), Jiashi Li (ByteDance Inc.), Faming Wu (ByteDance Inc.), Bin Cui (Peking University)",
    "abstract": "Extending the context length (i.e., the maximum supported sequence length) of LLMs is of paramount significance. To facilitate long context training of LLMs, sequence parallelism has emerged as an essential technique, which scatters each input sequence across multiple devices and necessitates communication to process the sequence. In essence, existing sequence parallelism methods assume homogeneous sequence lengths (i.e., all input sequences are equal in length) and therefore leverages a single, static scattering strategy for all input sequences. However, in reality, the sequence lengths in LLM training corpora exhibit substantial variability, often following a long-tail distribution, which leads to workload heterogeneity. In this paper, we show that employing a single, static strategy results in inefficiency and resource under-utilization, highlighting the need for adaptive approaches to handle the heterogeneous workloads across sequences. To address this, we propose a heterogeneity-adaptive sequence parallelism method. For each training step, our approach captures the variability in sequence lengths and assigns the optimal combination of scattering strategies based on workload characteristics. We model this problem as a linear programming optimization and design an efficient and effective solver to find the optimal solution. Furthermore, we implement our method in a high-performance system that supports adaptive parallelization in distributed LLM training. Experimental results demonstrate that our system outperforms state-of-the-art training frameworks by up to 1.98x.",
    "link": "https://doi.org/10.1145/3676641.3715998",
    "conference_name": "ASPLOS",
    "session_title": "Large Language Models",
    "date": "2025-03-30",
    "paper_id": "bbe7bf0c-6d04-4f04-977c-6290e87f1db2"
  },
  {
    "title": "Spindle: Efficient Distributed Training of Multi-Task Large Models via Wavefront Scheduling",
    "authors": "Yujie Wang (Peking University), Shenhan Zhu (Peking University), Fangcheng Fu (Peking University), Xupeng Miao (Purdue University), Jie Zhang (Alibaba Group), Juan Zhu (Alibaba Group), Fan Hong (Alibaba Group), Yong Li (Alibaba Group), Bin Cui (Peking University)",
    "abstract": "Recent foundation models are capable of handling multiple tasks and multiple data modalities with the unified base model structure and several specialized model components. However, efficient training of such multi-task (MT) multi-modal (MM) models poses significant system challenges due to the sophisticated model architecture and the heterogeneous workloads of different tasks and modalities. In this paper, we propose Spindle, a brand new training system tailored for resource-efficient and high-performance training of MT MM models via wavefront scheduling. The key idea of Spindle is to decompose the model execution into waves and address the joint optimization problem sequentially, including both heterogeneity-aware workload parallelization and dependency-driven execution scheduling. We build our system and evaluate it on various MT MM models. Experiments demonstrate the superior performance and efficiency of Spindle, with speedup ratio up to 71% compared to state-of-the-art training systems.",
    "link": "https://doi.org/10.1145/3676641.3715992",
    "conference_name": "ASPLOS",
    "session_title": "Large Language Models",
    "date": "2025-03-30",
    "paper_id": "c3a06c3a-d45b-4ece-b205-efdcdec721e7"
  },
  {
    "title": "Vela: A Virtualized LLM Training System with GPU Direct RoCE",
    "authors": "Apoorve Mohan (IBM Research), Robert Walkup (IBM Research), Bengi Karacali (IBM Research), Ming-Hung Chen (IBM Research), Abdullah Kayi (IBM Research), Liran Schour (IBM), Shweta Salaria (IBM Research), Sophia Wen (IBM Research), IHsin Chung (IBM Research), Abdul Alim (IBM Research), Constantinos Evangelinos (IBM Research), Lixiang Luo (IBM Research), Marc Dombrowa (IBM Research), Laurent Schares (IBM Research), Ali Sydney (IBM Research), Pavlos Maniotis (IBM Research), Sandhya Koteshwara (IBM Research), Brent Tang (IBM), Joel Belog (IBM), Rei Odaira (IBM), Vasily Tarasov (IBM Research), Eran Gampel (IBM Cloud), Drew Thorstensen (IBM), Talia Gershon (IBM Research), Seetharami Seelam (IBM Research)",
    "abstract": "Vela is a cloud-native system designed for LLM training workloads built using off-the-shelf hardware, Linux KVM-based virtualization, and a virtualized RDMA over Converged Ethernet (RoCE) network. Vela virtual machines (VMs) support peer-to-peer DMA between the GPUs and SRIOV-based network interface. In this paper, we share Vela's key architectural aspects with details from an NVIDIA A100 GPU-based deployment in one of the IBM Cloud data centers. Throughout the paper, we share insights and experiences from designing, building, and operating the system over a ~2.5 year timeframe to highlight the capabilities of readily available software and hardware technologies and the improvement opportunities for future AI systems, thereby making AI infrastructure more accessible to a broader community. As we evaluated the system for performance at ~1500 GPU scale, we achieved ~80% of the ideal throughput while training a 50 billion parameter decoder model using model parallelism, and ~70% per GPU FLOPS compared to a single VM with the High-Performance Linpack benchmark.",
    "link": "https://doi.org/10.1145/3676641.3716280",
    "conference_name": "ASPLOS",
    "session_title": "Large Language Models",
    "date": "2025-03-30",
    "paper_id": "fddc3ab1-2793-45d2-a3da-f6da8e04354f"
  },
  {
    "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
    "authors": "Seonho Lee (Georgia Institute of Technology), Amar Phanishayee (Meta), Divya Mahajan (Georgia Institute of Technology)",
    "abstract": "Deep learning kernels exhibit predictable memory accesses and compute patterns, making GPUs' parallel architecture well-suited for their execution. Software and runtime systems for GPUs are optimized to better utilize the stream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As deep learning models and GPUs evolve, access to newer GPUs is often limited, raising questions about the performance of new model architectures on existing GPUs, existing models on new GPUs, and new model architectures on new GPUs. To address these questions, we introduce NeuSight, a framework to predict the performance of various deep learning models, for both training and inference, on unseen GPUs without requiring actual execution. The framework leverages both GPU hardware behavior and software library optimizations to estimate end-to-end performance. Previous work uses regression models that capture linear trends or multilayer perceptrons to predict the overall latency of deep learning kernels on GPUs. These approaches suffer from higher error percentages when forecasting performance on unseen models and new GPUs. Instead, NeuSight decomposes the prediction problem into smaller problems, bounding the prediction through fundamental performance laws. NeuSight decomposes a single deep learning kernel prediction into smaller working sets called tiles, which are executed independently on the GPU. Tile-granularity predictions are determined using a machine learning approach and aggregated to estimate end-to-end latency. NeuSight outperforms prior work across various deep learning workloads and the latest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in predicting the latency of GPT3 model for training and inference on H100, compared to state-of-the-art prior work, where both GPT3 and H100 were not used to train the framework.",
    "link": "https://doi.org/10.1145/3669940.3707265",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 1",
    "date": "2025-03-30",
    "paper_id": "57b43d4a-964e-4c10-8bcc-ae07afc1f945"
  },
  {
    "title": "MVQ: Towards Efficient DNN Compression and Acceleration with Masked Vector Quantization",
    "authors": "Shuaiting Li (Zhejiang University), Chengxuan Wang (Zhejiang University), Juncan Deng (Zhejiang University), Zeyu Wang (Zhejiang University), Zewen Ye (Zhejiang University), Zongsheng Wang (Zhejiang University), Haibin Shen (Zhejiang University), Kejie Huang (Zhejiang University)",
    "abstract": "Vector quantization(VQ) is a hardware-friendly DNN compression method that can reduce the storage cost and weight-loading datawidth of hardware accelerators. However, conventional VQ techniques lead to significant accuracy loss because the important weights are not well preserved. To tackle this problem, a novel approach called MVQ is proposed, which aims at better approximating important weights with a limited number of codewords. At the algorithm level, our approach removes the less important weights through N:M pruning and then minimizes the vector clustering error between the remaining weights and codewords by the masked k-means algorithm. Only distances between the unpruned weights and the codewords are computed, which are then used to update the codewords. At the architecture level, our accelerator implements vector quantization on an EWS (Enhanced weight stationary) CNN accelerator and proposes a sparse systolic array design to maximize the benefits brought by masked vector quantization.\\\\ Our algorithm is validated on various models for image classification, object detection, and segmentation tasks. Experimental results demonstrate that MVQ not only outperforms conventional vector quantization methods at comparable compression ratios but also reduces FLOPs. Under ASIC evaluation, our MVQ accelerator boosts energy efficiency by 2.3$\\times$ and reduces the size of the systolic array by 55\\% when compared with the base EWS accelerator. Compared to the previous sparse accelerators, MVQ achieves 1.73$\\times$ higher energy efficiency.",
    "link": "https://doi.org/10.1145/3669940.3707268",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 1",
    "date": "2025-03-30",
    "paper_id": "4e4c6ce2-838e-41f9-b4a8-f9f5b52e3028"
  },
  {
    "title": "PartIR: Composing SPMD Partitioning Strategies for Machine Learning",
    "authors": "Sami Alabed (Google DeepMind), Daniel Belov (Google DeepMind), Bart Chrzaszcz (Google DeepMind), Juliana Franco (Google DeepMind), Dominik Grewe (Google DeepMind), Dougal Maclaurin (Google DeepMind), James Molloy (Google DeepMind), Tom Natan (Google DeepMind), Tamara Norman (Google DeepMind), Xiaoyue Pan (Google DeepMind), Adam Paszke (Google DeepMind), Norman Alexander Rink (Google DeepMind), Michael Schaarschmidt (Isomorphic Labs), Timur Sitdikov (Google DeepMind), Agnieszka Swietlik (Google DeepMind), Dimitrios Vytiniotis (Google DeepMind), Joel Wee (Google DeepMind)",
    "abstract": "Training of modern large neural networks (NN) requires a combination of parallelization strategies encompassing data, model, or optimizer sharding. When strategies increase in complexity, it becomes necessary for partitioning tools to be 1) expressive, allowing the composition of simpler strategies, and 2) predictable to estimate performance analytically. We present PartIR, our design for a NN partitioning system. PartIR is focused on an incremental approach to rewriting and is hardware-and-runtime agnostic. We present a simple but powerful API for composing sharding strategies and a simulator to validate them. The process is driven by high-level programmer-issued partitioning tactics, which can be both manual and automatic. Importantly, the tactics are specified separately from the model code, making them easy to change. We evaluate PartIR on several different models to demonstrate its predictability, expressibility, and ability to reach peak performance..",
    "link": "https://doi.org/10.1145/3669940.3707284",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 1",
    "date": "2025-03-30",
    "paper_id": "ba73346b-12ff-4c06-b219-0ce63dbc0049"
  },
  {
    "title": "Using Analytical Performance/Power Model and Fine-Grained DVFS to Enhance AI Accelerator Energy Efficiency",
    "authors": "Zibo Wang (State Key Laboratory for Novel Software Technology, Nanjing University), Yijia Zhang (Peng Cheng Laboratory), Fuchun Wei (Huawei Technologies Co., Ltd), Bingqiang Wang (Peng Cheng Laboratory), Yanlin Liu (Huawei Technologies Co., Ltd), Zhiheng Hu (Huawei Technologies Co., Ltd), Jingyi Zhang (Huawei Technologies Co., Ltd), Xiaoxin Xu (Huawei Technologies Co., Ltd), Jian He (Huawei Technologies Co., Ltd), Xiaoliang Wang (State Key Laboratory for Novel Software Technology, Nanjing University), Wanchun Dou (State Key Laboratory for Novel Software Technology, Nanjing University), Guihai Chen (State Key Laboratory for Novel Software Technology, Nanjing University), Chen Tian (State Key Laboratory for Novel Software Technology, Nanjing University)",
    "abstract": "Recent advancements in deep learning have significantly increased AI processors' energy consumption, which is becoming a critical factor limiting AI development. Dynamic Voltage and Frequency Scaling (DVFS) stands as a key method in power optimization. However, due to the latency of DVFS control in AI processors, previous works typically apply DVFS control at the granularity of a program's entire duration or sub-phases, rather than at the level of AI operators.\nThe advent of millisecond-level DVFS capabilities on the latest Ascend NPU platforms enables us to set frequency individually for single or multiple operators, opening up the opportunity for further enhancing energy efficiency through fine-grained DVFS control. To ensure performance is unaffected in DVFS, our work builds performance and power models for each operator. Through in-depth timeline analysis, we demonstrate that the cycle count of an operator can be modeled as a convex piecewise linear function of frequency, resulting in a performance model with an average error of 1.96%. Moreover, we build power models that incorporate temperature-dependent terms, which enhances the model's precision and results in an average error of 4.62%.\nBased on our performance and power models as well as the fine-grained DVFS functionality of Ascend NPU, we propose a DVFS strategy that integrates operator classification, preprocessing, and a genetic algorithm-based search. Experiments on applications including GPT-3 training achieve a reduction in AICore (the computing component within the Ascend NPU) power by 13.44% and NPU chip power by 4.95%, while limiting performance degradation to 1.76%.",
    "link": "https://doi.org/10.1145/3669940.3707231",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 1",
    "date": "2025-03-30",
    "paper_id": "7104e4fb-8117-4005-833d-06dbe63b706d"
  },
  {
    "title": "Early Termination for Hyperdimensional Computing Using Inferential Statistics",
    "authors": "Pu (Luke) Yi (Stanford University), Yifan Yang (Stanford University), Chae Young Lee (Stanford University), Sara Achour (Stanford University)",
    "abstract": "Hyperdimensional Computing (HDC) is a brain-inspired, lightweight computing paradigm that has shown great potential for inference on the edge and on emerging hardware technologies, achieving state-of-the-art accuracy on certain classification tasks. HDC classifiers are inherently error resilient and support early termination of inference to approximate classification results. Practitioners have developed heuristic methods to terminate inference early for individual inputs, reducing the computation of inference at the cost of accuracy. These techniques lack statistical guarantees and may unacceptably degrade classification accuracy or terminate inference later than is needed to obtain an accuracy result.\nWe present Omen, the first dynamic HDC optimizer that uses inferential statistics to terminate inference early while providing accuracy guarantees. To realize Omen, we develop a statistical view of HDC that reframes HD computations as statistical sampling and testing tasks, enabling the use of statistical tests. We evaluate Omen on 19 benchmark instantiations of four classification tasks. Omen is computationally efficient, delivering up to 7.21--12.18× inference speed-ups over an unoptimized baseline while only incurring a 0.0--0.7% drop in accuracy. Omen outperforms heuristic methods, achieving an additional 0.04--5.85× inference speed-up over the unoptimized baseline compared to heuristic methods while maintaining higher or comparable accuracy.",
    "link": "https://doi.org/10.1145/3669940.3707254",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 1",
    "date": "2025-03-30",
    "paper_id": "48dfaf4e-00fb-4dca-b650-77f8de9f69ce"
  },
  {
    "title": "Saving Energy with Per-Variable Bitwidth Speculation",
    "authors": "Tommy McMichen (Northwestern University), David Dlott (Northwestern University), Panitan Wongse-ammat (Northwestern University), Nathan Greiner (Northwestern University), Hussain Khajanchi (Northwestern University), Russ Joseph (Northwestern University), Simone Campanoni (Northwestern University)",
    "abstract": "Tiny devices have become ubiquitous in people’s daily lives. Their applications dictate tight energy budgets, but also require reasonable performance to meet user expectations. To this end, the hardware of tiny devices has been highly optimized, making further optimizations difficult. In this work, we identify a missed opportunity: the bitwidth selection of program variables. Today’s compilers directly translate the bitwidth specified in the source code to the binary. However, we observe that most variables do not utilize the full bitwidth specified in the source code for the majority of execution. To leverage this opportunity, we propose BitSpec: a system that performs fine-grained speculation on the bitwidth of program variables. BitSpec is implemented as a compiler-architecture co-design, where the compiler transparently reduces the bitwidth of program variables to their expected needs and the hardware monitors speculative variables, reporting misspeculation to the software, which re-executes at the original bitwidth, ensuring correctness. BitSpec reduces energy consumption by 9 . 9 % on average, up to 28 . 2 %.",
    "link": "https://doi.org/10.1145/3676641.3716271",
    "conference_name": "ASPLOS",
    "session_title": "Microarchitecture",
    "date": "2025-03-30",
    "paper_id": "0460eca0-f3d6-40e7-b78a-f7f28374630e"
  },
  {
    "title": "ShadowLoad: Injecting State into Hardware Prefetchers",
    "authors": "Lorenz Hetterich (CISPA Helmholtz Center for Information Security), Fabian Thomas (CISPA Helmholtz Center for Information Security), Lukas Gerlach (CISPA Helmholtz Center for Information Security), Ruiyi Zhang (CISPA Helmholtz Center for Information Security), Nils Bernsdorf (CISPA Helmholtz Center for Information Security), Eduard Ebert (CISPA Helmholtz Center for Information Security), Michael Schwarz (CISPA Helmholtz Center for Information Security)",
    "abstract": "Hardware prefetchers are an optimization in modern CPUs that predict memory accesses and preemptively load the corresponding value into the cache. Previous work showed that the internal state of hardware prefetchers can act as a side channel, leaking information across security boundaries such as processes, user and kernel space, and even trusted execution environments. In this paper, we present ShadowLoad, a new attack primitive to bring inaccessible victim data into the cache by injecting state into the hardware prefetcher. ShadowLoad relies on the inner workings of the hardware stride prefetchers, which we automatically reverse-engineer using our tool StrideRE. We illustrate how ShadowLoad extends the attack surface of existing microarchitectural attacks such as Meltdown and software-based power analysis attacks like Collide+Power and how it can partially bypass L1TF mitigations on clouds, such as AWS. We further demonstrate FetchProbe, a stride prefetcher side-channel attack leaking offsets of memory",
    "link": "https://doi.org/10.1145/3676641.3716020",
    "conference_name": "ASPLOS",
    "session_title": "Microarchitecture",
    "date": "2025-03-30",
    "paper_id": "2b1ab89b-f60b-4a56-924e-974809436095"
  },
  {
    "title": "Skia: Exposing Shadow Branches",
    "authors": "Chrysanthos Pepi (Texas A&M University,Intel), Bhargav Reddy Godala (Princeton University,Intel), Krishnam Tibrewala (Texas A&M University), Gino A. Chacon (Intel,AheadComputing), Paul V. Gratz (Texas A&M University), Daniel A. JimÃ©nez (Texas A&M University,Barcelona Supercomputing Center), Gilles A. Pokam (Intel), David I. August (Princeton University)",
    "abstract": "Modern processors implement a decoupled front-end, often using a form of Fetch Directed Instruction Prefetching (FDIP), to avoid front-end stalls. FDIP is driven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and branch target tracking structures to speculatively fetch instructions into the Instruction Cache (L1-I cache). As contemporary data center applications become more complex, their code footprints also grow, resulting in a high number of Branch Target Buffer (BTB) misses. These BTB missing branches typically have previously been decoded and placed in the BTB, but have since been evicted, leading to BTB misses now. FDIP can alleviate L1-I cache misses, but its reliance on the BPU's tracking structures means that when it encounters a BTB miss, the BPU may not identify the current instruction as a branch to FDIP. This can prevent FDIP from prefetching or cause it to speculate down the wrong path, further polluting the L1-I cache.\nWe observe that the vast majority, 75%, of BTB-missing, unidentified branches are actually present in instruction cache lines that FDIP has previously fetched. Nevertheless, these missing branches have not yet been decoded and inserted into the BTB. This is because the instruction line is decoded from an entry point (which is the target of the previous taken branch) till an exit point (taken branch). We call branch instructions present in the ignored portion of the cache line ''Shadow Branches.'' Here we present Skia, a novel shadow branch decoding technique that identifies and decodes unused bytes in cache lines fetched by FDIP, inserting them into a Shadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB, allowing FDIP to speculate despite a BTB miss.\nWith a minimal storage state of 12.25KB, Skia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB) and ~2% versus adding an equal amount of state to the BTB, across 16 front-end bound applications. Since many branches stored in the SBB are distinct compared to those in a similarly sized BTB, we consistently observe greater performance gains with Skia across all examined sizes until saturation.",
    "link": "https://doi.org/10.1145/3676641.3716273",
    "conference_name": "ASPLOS",
    "session_title": "Microarchitecture",
    "date": "2025-03-30",
    "paper_id": "8268753c-586a-4456-a287-020ed6ed132f"
  },
  {
    "title": "Hierarchical Prefetching, A Software-Hardware Instruction Prefetcher for Server Applications",
    "authors": "Tingji Zhang (Tsinghua University), Boris Grot (University of Edinburgh,Huawei Research), Wenjian He (Huawei Technologies Co., Ltd.), Yashuai Lv (Huawei Technologies Co., Ltd.), Peng Qu (Tsinghua University), Fang Su (Huawei Technologies Co., Ltd.), Wenxin Wang (Tsinghua University), Guowei Zhang (Huawei Technologies Co., Ltd.), Xuefeng Zhang (Tsinghua University), Youhui Zhang (Tsinghua University,Zhongguancun National Laboratory)",
    "abstract": "The large working set of instructions in server-side applications causes a significant bottleneck in the front-end, even for high-performance processors equipped with fetch-directed instruction prefetching (FDIP). Prefetchers specifically designed for server scenarios typically rely on a record-and-replay mechanism that exploits the repetitiveness of instruction sequences. However, the efficacy of these techniques is compromised by discrepancies between actual and predicted control flows, resulting in loss of coverage and timeliness. This paper proposes Hierarchical Prefetching, a novel approach that tackles the limitations of existing prefetchers. It identifies common coarse-grained functionality blocks (called Bundles) within the server code and prefetches them as a whole. Bundles are significantly larger than typical prefetch targets, encompassing tens to hundreds of kilobytes of code. The approach combines simple software analysis of code for bundle formation and light-weight hardware for record-and-replay prefetching. The prefetcher requires",
    "link": "https://doi.org/10.1145/3676641.3716260",
    "conference_name": "ASPLOS",
    "session_title": "Microarchitecture",
    "date": "2025-03-30",
    "paper_id": "fd72b613-6fa7-4459-a92c-9d97be10723a"
  },
  {
    "title": "Bounding Speculative Execution of Atomic Regions to a Single Retry",
    "authors": "Eduardo JosÃ© GÃ³mez-HernÃ¡ndez (Computer Engineering Department, University of Murcia), Juan M. Cebrian (Computer Engineering Department, University of Murcia), Stefanos Kaxiras (Department of Information Technology, Uppsala University), Alberto Ros (Computer Engineering Department, University of Murcia)",
    "abstract": "Mutual exclusion has long served as a fundamental construct in parallel programs. Despite a long history of optimizing the lower-level lock and unlock operations used to enforce mutual exclusion, such operations largely dictate performance in parallel programs. Speculative Lock Elision, and more generally Hardware Transactional Memory, allow executing atomic regions (ARs) concurrently and speculatively, and ensure correctness by using conflict detection. However, practical implementations of these ideas are best-effort and, in case of conflicts, the execution of ARs is retried a predetermined number of times before falling back to mutual exclusion. This work explores the opportunities of using cacheline locking to bound the number of retries of speculative solutions. Our key insight is that ARs that access exactly the same set of addresses when re-executing can learn that set in the first execution and execute non-speculatively in the next one by performing an ordered cacheline locking. This way the speculative execution is bounded to a single retry. We first establish the conditions for ARs to be able to re-execute under a cacheline-locked mode. Based on these conditions, we propose cleAR, cacheline-locked executed AR, a novel technique that on the first abort, forces the re-execution to use cacheline locking. The detection and conversion to cacheline-locking mode is transparent to software. Using gem5 running data-structure benchmarks and the STAMP benchmark suite, we show that the average number of ARs that succeed on the first retry grows from 35.4% in our baseline to 64.4% with cleAR, reducing the percentage of fallback (coarse-grain mutual exclusion) execution from 37.2% to 15.4%. These improvements reduce average execution time by 35.0% over a baseline configuration and by 23.3% over more elaborated approaches like PowerTM.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p17-gÃ³mez-hernÃ¡ndez.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Microarchitecture",
    "date": "2025-03-30",
    "paper_id": "c94cec33-419f-462d-8887-7290dd1172f1"
  },
  {
    "title": "Validating JVM Compilers via Maximizing Optimization Interactions",
    "authors": "Zifan Xie (Huazhong University of Science and Technology), Ming Wen (Huazhong University of Science and Technology), Shiyu Qiu (Huazhong University of Science and Technology), Hai Jin (Huazhong University of Science and Technology)",
    "abstract": "This paper introduces the concept of optimization interaction, which refers to the practice in modern compilers where multiple optimization phases, such as inlining , loop unrolling , and dead code elimination , are not completed in a one-off sequential order while being interacted instead. Therefore, while optimizing a certain phase, the compiler needs to ensure that the results of other optimization phases will not be disrupted, as this could lead to compiler crashes or unpredictable results. To verify whether compilers can correctly handle the optimization process across various phases, we propose MopFuzzer , which aims at maximizing runtime optimization interactions during fuzzing. Specifically, it encourages the JVM to perform multi-stage optimizations and verifies the correctness of the compiler’s optimized code through differential testing. Currently, MopFuzzer has implemented 13 mutators, and each is intended to trigger a certain optimization behavior. Such mutators are applied iteratively to the same program point, aiming to maximize",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p345-xie.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Compilers & Languages",
    "date": "2025-03-30",
    "paper_id": "954b59f6-a96d-44c7-aade-9cd7503daf0c"
  },
  {
    "title": "Faster Chaitin-like Register Allocation via Grammatical Decompositions of Control-Flow Graphs",
    "authors": "Xuran Cai (Hong Kong University of Science and Technology), Amir Kafshdar Goharshady (University of Oxford), S. Hitarth (Hong Kong University of Science and Technology), Chun Kit Lam (Hong Kong University of Science and Technology)",
    "abstract": "It is well-known that control-flow graphs (CFGs) of structured programs are sparse. This sparsity has been previously formalized in terms of graph parameters such as treewidth and pathwidth and used to design faster parameterized algorithms for numerous compiler optimization, model checking and program analysis tasks.\nIn this work, we observe that the known graph sparsity parameters fail to exactly capture the kind of sparsity exhibited by CFGs. For example, while all structured CFGs have a treewidth of at most 7, not every graph with a treewidth of 7 or less is realizable as a CFG. As a result, current parameterized algorithms are solving the underlying graph problems over a more general family of graphs than the CFGs.\nTo address this problem, we design a new but natural concept of graph decomposition based on a grammar that precisely captures the set of graphs that can be realized as CFGs of programs. We show that our notion of decomposition enables the same type of dynamic programming algorithms that are often used in treewidth/pathwidth-based methods. As two concrete applications, using our grammatical decomposition of CFGs, we provide asymptotically more efficient algorithms for two variants of the classical problem of register allocation as defined by Chaitin, i.e. assigning program variables to a limited number of registers such that variables with intersecting lifetimes are not assigned to the same register. Note that Chaitin's formulation of register allocation does not allow live-range splitting. Our algorithms are asymptotically faster not only in comparison with the non-parameterized solutions for these problems, but also compared to the state-of-the-art treewidth/pathwidth-based approaches in the literature. For minimum-cost register allocation over a fixed number of registers, we provide an algorithm with a runtime of O(|G| ⋅ |𝕈| 5 ⋅ r) where |G| is the size of the program, 𝕈 is the set of program variables and r is the number of registers. In contrast, the previous treewidth-based algorithm had a runtime of O(|G| ⋅ |𝕈| 16 ⋅ r). For the decision problem of spill-free register allocation, our algorithm's runtime is O(|G| ⋅ r5 ⋅ r + 5) whereas the previous works had a runtime of O(|G| ⋅ r16 ⋅ r).\nFinally, we provide extensive experimental results on spill-free register allocation, showcasing the scalability of our approach in comparison to previous state-of-the-art methods. Most notably, our approach can handle real-world instances with up to 20 registers, whereas previous works could only scale to 8. This is a significant improvement since most ubiquitous architectures, such as the x86 family, have 16 registers. For such architectures, our approach is the first-ever exact algorithm that scales up to solve the real-world instances of spill-free register allocation.",
    "link": "https://doi.org/10.1145/3669940.3707286",
    "conference_name": "ASPLOS",
    "session_title": "Compilers & Languages",
    "date": "2025-03-30",
    "paper_id": "d65778d9-cf9d-478a-82ee-68563b7d7be8"
  },
  {
    "title": "Towards Sound Reassembly of Modern x86-64 Binaries",
    "authors": "Hyungseok Kim (The Affiliated Institute of ETRI), Soomin Kim (KAIST), Sang Kil Cha (KAIST)",
    "abstract": "Reassembly is a promising approach to transparently rewrite binaries without source code. However, sound symbolization remains an open problem as it requires precise identification of all memory references in the binary. In this paper, we systematically study the requirements for sound reassembly of modern x86-64 binaries, and present a novel approach to reassembly that symbolizes all memory references without affecting the original semantics. The key insights are twofold: (1) we find that Control-flow Enhancement Technology (CET), which has increasingly become the default setting for major Linux distributions, adds a unique property to binaries that can be leveraged to precisely symbolize dynamically computed pointers",
    "link": "https://doi.org/10.1145/3676641.3716026",
    "conference_name": "ASPLOS",
    "session_title": "Compilers & Languages",
    "date": "2025-03-30",
    "paper_id": "01b61e21-fb0b-4087-a32d-cf8c9389949c"
  },
  {
    "title": "SmoothE: Differentiable E-Graph Extraction",
    "authors": "Yaohui Cai (Cornell University), Kaixin Yang (Cornell University), Chenhui Deng (Cornell University), Cunxi Yu (University of Maryland, College Park), Zhiru Zhang (Cornell University)",
    "abstract": "E-graphs have gained increasing popularity in compiler optimization, program synthesis, and theorem proving tasks. They enable compact representation of many equivalent expressions and facilitate transformations via rewrite rules without phase ordering limitations. A major benefit of using e-graphs is the ability to explore a large space of equivalent expressions, allowing the extraction of an expression that best meets certain optimization objectives (or cost models). However, current e-graph extraction methods often face unfavorable scalability-quality trade-offs and only support simple linear cost functions, limiting their applicability to more realistic optimization problems. In this work, we propose SmoothE , a differentiable e-graph extraction algorithm designed to handle complex cost models and optimized for GPU acceleration. More specifically, we approach the e-graph extraction problem from a probabilistic perspective, where the original discrete optimization is relaxed to a continuous differentiable form. This formulation supports any differentiable cost functions and enables efficient searching for solutions using gradient descent. We implement SmoothE in PyTorch to leverage the advancements of the modern machine learning ecosystem. Additionally, we introduce performance optimization techniques to exploit sparsity and data parallelism. We evaluate SmoothE on a variety of realistic e-graphs from five different applications using three distinct cost models, including both linear and non-linear ones. Our experiments demonstrate that SmoothE consistently achieves a favorable trade-off between scalability and solution quality.",
    "link": "https://doi.org/10.1145/3669940.3707262",
    "conference_name": "ASPLOS",
    "session_title": "Compilers & Languages",
    "date": "2025-03-30",
    "paper_id": "5c361dd7-da48-4a6f-940b-f20d9d861de1"
  },
  {
    "title": "Exo 2: Growing a Scheduling Language",
    "authors": "Yuka Ikarashi (MIT CSAIL), Kevin Qian (MIT CSAIL), Samir Droubi (MIT CSAIL), Alex Reinking (Adobe), Gilbert Louis Bernstein (University of Washington), Jonathan Ragan-Kelley (MIT CSAIL)",
    "abstract": "User-schedulable languages (USLs) help programmers productively optimize programs by providing safe means of transforming them. Current USLs are designed to give programmers exactly the control they want, while automating all other concerns. However, there is no universal answer for what performance-conscious programmers want to control, how they want to control it, and what they want to automate, even in relatively narrow domains. We claim that USLs should, instead, be designed to grow. We present Exo 2, a scheduling language that enables users to define new scheduling operations externally to the compiler. By composing a set of trusted, fine-grained primitives, users can safely write their own scheduling library to build up desired automation. We identify actions (ways of modifying code), inspection (ways of interrogating code), and references (ways of pointing to code) as essential for any user-extensible USL. We fuse these ideas into a new mechanism called Cursors that enables the creation of scheduling libraries in user code. We demonstrate libraries that amortize scheduling effort across more than 80 high-performance kernels, reducing total scheduling code by an order of magnitude and delivering performance competitive with state-of-the-art implementations on three different platforms.",
    "link": "https://doi.org/10.1145/3669940.3707218",
    "conference_name": "ASPLOS",
    "session_title": "Compilers & Languages",
    "date": "2025-03-30",
    "paper_id": "f2a32aed-01d1-406b-8989-2cbdb8eca45c"
  },
  {
    "title": "Concerto: Automatic Communication Optimization and Scheduling for Large-Scale Deep Learning",
    "authors": "Shenggan Cheng (National University of Singapore), Shengjie Lin (Georgia Institute of Technology), Lansong Diao (Alibaba Group), Hao Wu (George Mason University), Siyu Wang (Alibaba Group), Chang Si (Alibaba Group), Ziming Liu (National University of Singapore), Xuanlei Zhao (National University of Singapore), Jiangsu Du (Sun Yat-sen University), Wei Lin (Alibaba Group), Yang You (National University of Singapore)",
    "abstract": "With the exponential growth of deep learning (DL), there arises an escalating need for scalability. Despite significant advancements in communication hardware capabilities, the time consumed by communication remains a bottleneck during training. The existing various optimizations are coupled within parallel systems to implement specific computation-communication overlap. These approaches pose challenges in terms of performance, programmability, and generality. In this paper, we introduce Concerto, a compiler framework designed to address these challenges by automatically optimizing and scheduling communication. We formulate the scheduling problem as a resource-constrained project scheduling problem and use off-the-shelf solver to get the near-optimal scheduling. And use auto-decomposition to create overlap opportunity for critical (synchronous) communication. Our evaluation shows Concerto can match or outperform state-of-the-art parallel frameworks, including Megatron-LM, JAX/XLA, DeepSpeed, and Alpa, all of which include extensive hand-crafted optimization. Unlike previous works, Concerto decouples the parallel approach and communication optimization, then can generalize to a wide variety of parallelisms without manual optimization.",
    "link": "https://doi.org/10.1145/3669940.3707223",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 2",
    "date": "2025-03-30",
    "paper_id": "56ad79c6-776e-499d-a309-48db36a4d464"
  },
  {
    "title": "Design and Operation of Shared Machine Learning Clusters on Campus",
    "authors": "Kaiqiang Xu (Hong Kong University of Science and Technology), Decang Sun (Hong Kong University of Science and Technology), Hao Wang (Hong Kong University of Science and Technology), Zhenghang Ren (Hong Kong University of Science and Technology), Xinchen Wan (Hong Kong University of Science and Technology), Xudong Liao (Hong Kong University of Science and Technology), Zilong Wang (Hong Kong University of Science and Technology), Junxue Zhang (Hong Kong University of Science and Technology), Kai Chen (Hong Kong University of Science and Technology)",
    "abstract": "Amid the rapid advancements in large machine learning (ML) models, universities worldwide are investing substantial funds and efforts into GPU clusters. However, managing a shared GPU cluster poses a pyramid of challenges, from hardware configuration to resource allocation among users. This paper introduces S ING , a full-stack solution designed to streamline the management of shared GPU clusters in academic institutions. Motivated by the pressing need for efficient resource sharing and the challenges posed by limited staffing, we present a comprehensive view of S ING ’s architecture and design choices, which achieves operational efficiency (i.e., low maintenance cost and high resource utilization). We also share experience and insights from the real-world operations of S ING , including analysis of its usage patterns and management of incidents and failures. This paper is part of our ongoing effort to improve the management of shared ML clusters. We open-source relevant resources to facilitate the development",
    "link": "https://doi.org/10.1145/3669940.3707266",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 2",
    "date": "2025-03-30",
    "paper_id": "7f070080-281a-4476-b137-7f58b9c798b4"
  },
  {
    "title": "PCcheck: Persistent Concurrent Checkpointing for ML",
    "authors": "Foteini Strati (ETH Zurich), Michal Friedman (ETH Zurich), Ana Klimovic (ETH Zurich)",
    "abstract": "Training large-scale machine learning (ML) models is expensive and time-intensive, consuming many hardware accelerators for days or weeks. As the scale of hardware deployments and training time continue to grow, the probability of failures also increases. The desire to use cheaper cloud resources, such as spot VMs, to lower costs also dramatically increases the frequency of failures. The standard approach to deal with failures is to periodically pause training and checkpoint model parameters to persistent storage. Unfortunately, today’s checkpointing mechanisms introduce high overhead when applied at high frequencies, yet frequent checkpointing is necessary to avoid long recovery times. We present a concurrent checkpointing mechanism, PC-check, that allows frequent checkpointing with minimal overhead. Our framework supports persisting checkpoints to SSD and persistent main memory (PMEM) for both single-machine and distributed settings. PCcheck enables check-pointing as frequently as every 10 iterations for detailed monitoring and fast recovery times in case of failures, while maintaining minimal (3%) overhead on training throughput.",
    "link": "https://doi.org/10.1145/3669940.3707255",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 2",
    "date": "2025-03-30",
    "paper_id": "8e8e4d1a-d683-4ac2-aed1-0f73b3d1fd22"
  },
  {
    "title": "Tally: Non-Intrusive Performance Isolation for Concurrent Deep Learning Workloads",
    "authors": "Wei Zhao (Stanford University,CentML), Anand Jayarajan (University of Toronto,Vector Institute), Gennady Pekhimenko (University of Toronto,Vector Institute)",
    "abstract": "GPU underutilization is a significant concern in many production deep learning clusters, leading to prolonged job queues and increased operational expenses. A promising solution to this inefficiency is GPU sharing, which improves resource utilization by allowing multiple workloads to execute concurrently on a single GPU. However, deploying GPU sharing in production settings faces critical obstacles due to the limitations of existing mechanisms, including high integration costs, inadequate performance isolation, and limited application compatibility. To address these issues, we introduce \\emph{Tally}, a non-intrusive GPU sharing mechanism that provides robust performance isolation and comprehensive workload compatibility. The key to Tally's robust performance isolation capability lies in its fine-grained thread-block-level GPU kernel scheduling strategy, which allows the system to effectively mitigate interference caused by workload co-execution. We evaluate Tally on a diverse range of workloads and show that it incurs an average overhead of only $7.2\\%$ on the $99^{th}$-percentile latency of high-priority inference tasks when executed concurrently with best-effort training workloads, compared to $188.9\\%$ overhead exhibited by the state-of-the-art GPU sharing systems like TGS, while achieving over $80\\%$ of TGS's system throughput.",
    "link": "https://doi.org/10.1145/3669940.3707282",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 2",
    "date": "2025-03-30",
    "paper_id": "0f435cd8-108c-43e8-8731-04cf9bc711c7"
  },
  {
    "title": "Load and MLP-Aware Thread Orchestration for Recommendation Systems Inference on CPUs",
    "authors": "Rishabh Jain (The Pennsylvania State University), Teyuh Chou (Advanced Micro Devices, Inc.), Onur Kayiran (Advanced Micro Devices, Inc.), John Kalamatianos (Advanced Micro Devices, Inc.), Gabriel H. Loh (Advanced Micro Devices, Inc.), Mahmut T. Kandemir (The Pennsylvania State University), Chita R. Das (The Pennsylvania State University)",
    "abstract": "Recommendation models can enhance consumer experiences and are one of the most frequently used machine learning models in data centers. The deep learning recommendation model (DLRM) is one such key workload. While DLRMs are often trained using GPUs, CPUs can be a cost-effective solution for inference. Therefore, optimizing DLRM inference for CPUs is an important research problem with significant business value. In this work, we identify several shortcomings of existing DLRM parallelization techniques, which can include load imbalance across CPU chiplets, suboptimal core allocation for embedding tables, and inefficient utilization of memory- level parallelism (MLP) resources. We propose a novel thread scheduler, called ''Balance,'' that addresses those shortcomings by (1) minimizing core allocation per embedding table to maximize core utilization, (2) using MLP-aware task scheduling based on the characteristics of the embedding tables to better utilize memory bandwidth, and (3) combining work stealing and table reordering mechanisms to reduce load imbalance across CPU chiplets. We evaluate Balance on real hardware with production DLRM traces and demonstrate up to a 1.67× higher speedup over prior state-of-the-art DLRM parallelization techniques with 96 cores. Further, Balance consistently achieves 1.22× higher performance over a range of batch sizes.",
    "link": "https://doi.org/10.1145/3676641.3716003",
    "conference_name": "ASPLOS",
    "session_title": "ML Systems 2",
    "date": "2025-03-30",
    "paper_id": "96b73262-9fe7-4191-af52-1d9da70bd781"
  },
  {
    "title": "MoE-Lightning: High-Throughput MoE Inference on Memory-constrained GPUs",
    "authors": "Shiyi Cao (UC Berkeley), Shu Liu (UC Berkeley), Tyler Griggs (UC Berkeley), Peter Schafhalter (UC Berkeley), Xiaoxuan Liu (UC Berkeley), Ying Sheng (Stanford University), Joseph E. Gonzalez (UC Berkeley), Matei Zaharia (UC Berkeley), Ion Stoica (UC Berkeley)",
    "abstract": "Efficient deployment of large language models, particularly Mixture of Experts (MoE) models, on resource-constrained platforms presents significant challenges in terms of computational efficiency and memory utilization. The MoE architecture, renowned for its ability to increase model capacity without a proportional increase in inference cost, greatly reduces the token generation latency compared with dense models. However, the large model size makes MoE models inaccessible to individuals without high-end GPUs. In this paper, we propose a high-throughput MoE batch inference system, MoE-Lightning , that significantly outperforms past work. MoE-Lightning introduces a novel CPU-GPU-I/O pipelining schedule, CGOPipe , with paged weights to achieve high resource utilization, and a performance model, HRM , based on a Hierarchical Roofline Model we introduce to help find policies with higher throughput than existing systems. MoE-Lightning can achieve up to 10 . 3 × higher throughput than state-of-the-art offloading-enabled LLM inference systems for Mixtral 8x7B on a single T4 GPU (16GB). When the theoretical system throughput is bounded by the GPU memory, MoE-Lightning can reach the throughput",
    "link": "https://doi.org/10.1145/3669940.3707267",
    "conference_name": "ASPLOS",
    "session_title": "Mixture of Experts",
    "date": "2025-03-30",
    "paper_id": "529f9e78-0416-4cb5-ad99-0561913cb12e"
  },
  {
    "title": "FSMoE: A Flexible and Scalable Training System for Sparse Mixture-of-Experts Models",
    "authors": "Xinglin Pan (The Hong Kong University of Science and Technology (Guangzhou)), Wenxiang Lin (Harbin Institute of Technology, Shenzhen), Lin Zhang (Hong Kong University of Science and Technology), Shaohuai Shi (Harbin Institute of Technology, Shenzhen), Zhenheng Tang (The Hong Kong University of Science and Technology), Rui Wang (The Hong Kong University of Science and Technology (Guangzhou)), Bo Li (Hong Kong University of Science and Technology), Xiaowen Chu (The Hong Kong University of Science and Technology (Guangzhou),Hong Kong University of Science and Technology)",
    "abstract": "Recent large language models (LLMs) have tended to leverage sparsity to reduce computations, employing the sparsely activated mixture-of-experts (MoE) technique. MoE introduces four modules, including token routing, token communication, expert computation, and expert parallelism, that impact model quality and training efficiency. To enable versatile usage of MoE models, we introduce FSMoE, a flexible training system optimizing task scheduling with three novel techniques: 1) Unified abstraction and online profiling of MoE modules for task scheduling across various MoE implementations. 2) Co-scheduling intra-node and inter-node communications with computations to minimize communication overheads. 3) To support near-optimal task scheduling, we design an adaptive gradient partitioning method for gradient aggregation and a schedule to adaptively pipeline communications and computations. We conduct extensive experiments with configured MoE layers and real-world MoE models on two GPU clusters. Experimental results show that 1) our FSMoE supports four popular types of MoE routing functions and is more efficient than existing implementations (with up to a 1.42$\\times$ speedup), and 2) FSMoE outperforms the state-of-the-art MoE training systems (DeepSpeed-MoE and Tutel) by 1.18$\\times$-1.22$\\times$ on 1458 MoE layers and 1.19$\\times$-3.01$\\times$ on real-world MoE models based on GPT-2 and Mixtral using a popular routing function.",
    "link": "https://doi.org/10.1145/3669940.3707272",
    "conference_name": "ASPLOS",
    "session_title": "Mixture of Experts",
    "date": "2025-03-30",
    "paper_id": "c7ddb088-d30b-4d65-a92a-de098d363e2b"
  },
  {
    "title": "CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory",
    "authors": "Jiashun Suo (State Key Laboratory of CCSE and School of Computer Science and Engineering, Beihang University), Xiaojian Liao (State Key Laboratory of CCSE and School of Computer Science and Engineering, Beihang University), Limin Xiao (State Key Laboratory of CCSE and School of Computer Science and Engineering, Beihang University), Li Ruan (State Key Laboratory of CCSE and School of Computer Science and Engineering, Beihang University), Jinquan Wang (State Key Laboratory of CCSE and School of Computer Science and Engineering, Beihang University), Xiao Su (State Key Laboratory of CCSE and School of Computer Science and Engineering, Beihang University), Zhisheng Huo (State Key Laboratory of CCSE and School of Computer Science and Engineering, Beihang University)",
    "abstract": "Large language models like GPT-4 are resource-intensive, but recent advancements suggest that smaller, specialized experts can outperform the monolithic models on specific tasks. The Collaboration-of-Experts (CoE) approach integrates multiple expert models, improving the accuracy of generated results and offering great potential for precision-critical applications, such as automatic circuit board quality inspection. However, deploying CoE serving systems presents challenges to memory capacity due to the large number of experts required, which can lead to significant performance overhead from frequent expert switching across different memory and storage tiers. We propose CoServe, an efficient CoE model serving system on heterogeneous CPU and GPU with limited memory. CoServe reduces unnecessary expert switching by leveraging expert dependency, a key property of CoE inference. CoServe introduces a dependency-aware request scheduler and dependency-aware expert management for efficient inference. It also introduces an offline profiler to automatically find optimal resource allocation on various processors and devices. In real-world intelligent manufacturing workloads, CoServe achieves 4.5$\\times$ to 12$\\times$ higher throughput compared to state-of-the-art systems.",
    "link": "https://doi.org/10.1145/3676641.3715986",
    "conference_name": "ASPLOS",
    "session_title": "Mixture of Experts",
    "date": "2025-03-30",
    "paper_id": "f31214c9-bd1c-47fd-9831-3c63eb1e0e11"
  },
  {
    "title": "Klotski: Efficient Mixture-of-Expert Inference via Expert-Aware Multi-Batch Pipeline",
    "authors": "Zhiyuan Fang (Sun Yat-sen University), Yuegui Huang (Sun Yat-sen University), Zicong Hong (Hong Kong University of Science and Technology), Yufeng Lyu (Huawei Technologies Co. Ltd), Wuhui Chen (Sun Yat-sen University,Peng Cheng Laboratory), Yue Yu (Peng Cheng Laboratory), Fan Yu (Huawei Technologies Co. Ltd), Zibin Zheng (Sun Yat-sen University)",
    "abstract": "Mixture of Experts (MoE), with its distinctive sparse structure, enables the scaling of language models up to trillions of parameters without significantly increasing computational costs. However, the substantial parameter size presents a challenge for inference, as the expansion in GPU memory cannot keep pace with the growth in parameters. Although offloading techniques utilise memory from the CPU and disk and parallelise the I/O and computation for efficiency, the computation for each expert in MoE models is often less than the I/O, resulting in numerous bubbles in the pipeline. Therefore, we propose Klotski, an efficient MoE inference engine that significantly reduces pipeline bubbles through a novel expert-aware multi-batch pipeline paradigm. The proposed paradigm uses batch processing to extend the computation time of the current layer to overlap with the loading time of the next layer. Although this idea has been effectively applied to dense models, more batches may activate more experts in the MoE, leading to longer loading times and more bubbles. Thus, unlike traditional approaches, we balance computation and I/O time and minimise bubbles by orchestrating their inference orders based on their heterogeneous computation and I/O requirements and activation patterns under different batch numbers. Moreover, to adapt to different hardware environments and models, we design a constraint-sensitive I/O-compute planner and a correlation-aware expert prefetcher for a schedule that minimises pipeline bubbles. Experimental results demonstrate that Klotski achieves a superior throughput-latency trade-off compared to state-of-the-art techniques, with throughput improvements of up to 85.12x.",
    "link": "https://doi.org/10.1145/3676641.3716261",
    "conference_name": "ASPLOS",
    "session_title": "Mixture of Experts",
    "date": "2025-03-30",
    "paper_id": "d96a516b-64ef-46bd-b89c-66f397a7b1e5"
  },
  {
    "title": "MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training",
    "authors": "Weilin Cai (The Hong Kong University of Science and Technology (Guangzhou)), Le Qin (The Hong Kong University of Science and Technology (Guangzhou)), Jiayi Huang (The Hong Kong University of Science and Technology (Guangzhou))",
    "abstract": "As large language models continue to scale up, distributed training systems have expanded beyond 10k nodes, intensifying the importance of fault tolerance. Checkpoint has emerged as the predominant fault tolerance strategy, with extensive studies dedicated to optimizing its efficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model presents new challenges due to the substantial increase in model size, despite comparable computational demands to dense models. In this work, we propose the Mixture-of-Checkpoint System (MoC-System) to orchestrate the vast array of checkpoint shards produced in distributed training systems. MoC-System features a novel Partial Experts Checkpointing (PEC) mechanism, an algorithm-system co-design that strategically saves a selected subset of experts, effectively reducing the MoE checkpoint size to levels comparable with dense models. Incorporating hybrid parallel strategies, MoC-System involves fully sharded checkpointing strategies to evenly distribute the workload across distributed ranks. Furthermore, MoC-System introduces a two-level checkpointing management method that asynchronously handles in-memory snapshots and persistence processes. We build MoC-System upon the Megatron-DeepSpeed framework, achieving up to a 98.9% reduction in overhead for each checkpointing process compared to the original method, during MoE model training with ZeRO-2 data parallelism and expert parallelism. Additionally, extensive empirical analyses substantiate that our methods enhance efficiency while maintaining comparable model accuracy, even achieving an average accuracy increase of 1.08% on downstream tasks.",
    "link": "https://doi.org/10.1145/3676641.3716006",
    "conference_name": "ASPLOS",
    "session_title": "Mixture of Experts",
    "date": "2025-03-30",
    "paper_id": "4510cbee-33cd-4cc5-87be-a387587a4969"
  },
  {
    "title": "Manta: Hybrid-Sensitive Type Inference Toward Type-Assisted Bug Detection for Stripped Binaries",
    "authors": "Chengfeng Ye (The Hong Kong University of Science and Technology), Yuandao Cai (The Hong Kong University of Science and Technology), Anshunkang Zhou (The Hong Kong University of Science and Technology), Heqing Huang (City University of Hong Kong), Hao Ling (The Hong Kong University of Science and Technology), Charles Zhang (The Hong Kong University of Science and Technology)",
    "abstract": "Static binary bug detection has been a prominent approach for ensuring the security of binaries used in our daily lives. However, the type information lost in binaries prevents the improvement opportunity for a static analyzer to utilize type information to prune away infeasible facts and increase analysis precision. To make binary bug detection more practical with higher precision, in this work, we propose the first hybrid-sensitive type inference, Manta, that combines data-flow analysis with different sensitivities to complement each other and infer precise types for many variables. The inferred types are then used to assist with bug detection by pruning infeasible indirect call targets and data dependencies. Our experiments indicate Manta outperforms prior work by inferring types with 78.7% precision and 97.2% recall. Based on the inferred types, we can prune away 63.9% more infeasible indirect-call targets compared to existing type analysis techniques and perform program slicing on binaries with 61.1% similarity to that on source code. Moreover, Manta has led to 86 new developer-confirmed vulnerabilities in many popular IoT firmware, with 64 CVE/PSV IDs assigned.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p170-ye.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Testing",
    "date": "2025-03-30",
    "paper_id": "85008677-65ab-4075-8156-8c117cc6795c"
  },
  {
    "title": "Selectively Uniform Concurrency Testing",
    "authors": "Huan Zhao (National University of Singapore), Dylan Wolff (National University of Singapore), Umang Mathur (National University of Singapore), Abhik Roychoudhury (National University of Singapore)",
    "abstract": "Buggy behaviors in concurrent programs are notoriously elusive, as they may manifest only in few of exponentially many possible thread interleavings. Randomized concurrency testing techniques probabilistically sample from (instead of enumerating) the vast search space and have been shown to be both an effective as well as a scalable class of algorithms for automated discovery of concurrency bugs. In this work we focus on the key desirable characteristic of black-box randomized concurrency testing algorithms — uniformity of exploration. Unfortunately, prior randomized algorithms acutely fall short on uniformity and, as a result, struggle to expose bugs that only manifest in few, infrequent interleavings. Towards this, we show that, indeed, a sampling strategy for uniformly sampling over the interleaving space, is eminently achievable with minimal additional information for broad classes of programs. Moreover, when applied to a carefully selected subset of program events, this interleaving-uniformity strategy allows for an effective exploration of program behaviors. We present an online randomized concurrency testing algorithm named Selectively Uniform Random Walk ( SURW ) that builds on these insights. SURW is the first of its class to achieve interleaving-uniformity for a wide class of programs, or an arbitrary subset of events thereof. This property translates to effective behavioral exploration should a subset with desirable characteristics be selected. Extensive evaluation on leading concurrency benchmarks suggests SURW is able to expose more bugs and significantly faster than comparable randomized algorithms. In addition, we show that SURW is able to explore both the space of interleavings and behaviors more uniformly on real-world programs.",
    "link": "https://doi.org/10.1145/3669940.3707214",
    "conference_name": "ASPLOS",
    "session_title": "Testing",
    "date": "2025-03-30",
    "paper_id": "f3e57fbc-79f3-4e1a-bb74-21941f510a5f"
  },
  {
    "title": "TAOPT: Tool-Agnostic Optimization of Parallelized Automated Mobile UI Testing",
    "authors": "Dezhi Ran (Key Lab of HCST (PKU), MOE; SCS, Peking University), Zihe Song (University of Texas at Dallas), Wenyu Wang (University of Illinois at Urbana-Champaign), Wei Yang (University of Texas at Dallas), Tao Xie (Key Lab of HCST (PKU), MOE; SCS, Peking University)",
    "abstract": "The emergence of modern testing clouds, equipped with a vast array of real testing devices and high-fidelity emulators, has significantly increased the need for parallel automated mobile testing to optimally utilize the resources of testing clouds. Parallel testing aligns perfectly with the characteristic of rapid iteration cycles for mobile app development, where testing time is limited. While numerous tools have been proposed for optimizing the testing effectiveness on a single testing device, it remains an open problem to optimize the parallelization of automated mobile UI testing in terms of resource and time utilization. To optimize the parallelization of automated mobile UI testing, in this paper, we propose TaOPT, a fully automated, tool-agnostic approach, which improves the parallelization effectiveness of any given testing tool without modifying the tool’s internal workflow. In particular, TaOPT conducts online analysis to infer loosely coupled UI subspaces in the App Under Test (AUT). TaOPT then manages access to these subspaces across various testing devices, guiding automated UI testing toward distinct subspaces on different devices without knowing the testing tool’s internal workflow. We apply TaOPT on 18 highly popular mobile apps with three state-of-the-art automated UI testing tools for Android. Evaluation results show that TaOPT helps the tools reach comparable code coverage us-ing 60% less testing duration and 62% less machine time than",
    "link": "https://doi.org/10.1145/3676641.3716282",
    "conference_name": "ASPLOS",
    "session_title": "Testing",
    "date": "2025-03-30",
    "paper_id": "5aa6fc44-8a20-4de9-83da-85b0fcd7637f"
  },
  {
    "title": "Debugger Toolchain Validation via Cross-Level Debugging",
    "authors": "Yibiao Yang (State Key Laboratory for Novel Software Technology, Nanjing University), Maolin Sun (State Key Laboratory for Novel Software Technology, Nanjing University), Jiangchang Wu (State Key Laboratory for Novel Software Technology, Nanjing University), Qingyang Li (State Key Laboratory for Novel Software Technology, Nanjing University), Yuming Zhou (State Key Laboratory for Novel Software Technology, Nanjing University)",
    "abstract": "Ensuring the correctness of debugger toolchains is of paramount importance, as they play a vital role in understanding and resolving programming errors during software development. Bugs hidden within these toolchains can significantly mislead developers. Unfortunately, comprehensive testing of debugger toolchains is lacking due to the absence of effective test oracles. Existing studies on debugger toolchain validation have primarily focused on validating the debug information within optimized executables by comparing the traces between debugging optimized and unoptimized executables (i.e., different executables) in the debugger, under the assumption that the traces obtained from debugging unoptimized executables serve as a reliable oracle. However, these techniques suffer from inherent limitations, as compiler optimizations can drastically alter source code elements, variable representations, and instruction order, rendering the traces obtained from debugging different executables incomparable and failing to uncover bugs in debugger toolchains when debugging unoptimized executables. To address these limitations, we propose a novel concept called Cross-Level Debugging (CLD) for validating the debugger toolchain. CLD compares the traces obtained from debugging the same executable using source-level and instruction-level strategies within the same debugger. The core insight of CLD is that the execution traces obtained from different debugging levels for the same executable should adhere to specific relationships, regardless of whether the executable is generated with or without optimization. We formulate three key relations in CLD: reachability preservation of program locations, order preservation for reachable program locations, and value consistency at program locations, which apply to traces at different debugging levels. We implement Devil, a practical framework that employs these relations for debugger toolchain validation. We evaluate the effectiveness of Devil using two widely used production debugger toolchains, GDB and LLDB. Ultimately, Devil successfully identified 27 new bug reports, of which 18 have been confirmed and 12 have been fixed by developers.",
    "link": "https://doi.org/10.1145/3669940.3707271",
    "conference_name": "ASPLOS",
    "session_title": "Testing",
    "date": "2025-03-30",
    "paper_id": "70a283d7-1c95-4fbe-b805-37796f5f91b3"
  },
  {
    "title": "Dynamic Partial Deadlock Detection and Recovery via Garbage Collection",
    "authors": "Georgian-Vlad Saioc (Aarhus University,Programming Systems Group, Uber Technologies, Inc.), I-Ting Angelina Lee (Washington University in St. Louis), Anders MÃ¸ller (Aarhus University), Milind Chabbi (Programming Systems Group, Uber Technologies, Inc.)",
    "abstract": "A challenge of writing concurrent message-passing programs is ensuring the absence of partial deadlocks, which can cause severe memory leaks in long-running systems. The Go programming language is particularly susceptible to this problem due to its support of message passing and ease of lightweight concurrency creation. We propose a novel dynamic technique to detect partial deadlocks by soundly approximating liveness using the garbage collector’s marking phase. The approach allows systems to not only detect, but also automatically redress partial deadlocks and alleviate their impact on memory. We implement the approach in the tool Golf, as an extension to the garbage collector of the Go runtime system and evaluate its effectiveness in a series of experiments. Preliminary results show that the approach is effective at detecting 94% and 50% of partial deadlocks in a series of microbench-marks and the test suites of a large-scale industrial codebase, respectively. Furthermore, we deployed Golf on a real service used by Uber, and over a period of 24 hours, effectively detected 252 partial deadlocks caused by three programming errors.",
    "link": "https://doi.org/10.1145/3676641.3715990",
    "conference_name": "ASPLOS",
    "session_title": "Testing",
    "date": "2025-03-30",
    "paper_id": "5911b074-17ed-4e65-8ca3-3a0261f8f1bc"
  },
  {
    "title": "The Mutators Reloaded: Fuzzing Compilers with Large Language Model Generated Mutation Operators",
    "authors": "Xianfei Ou (Nanjing University), Cong Li (Ant Group,Zhejiang University), Yanyan Jiang (Nanjing University), Chang Xu (Nanjing University)",
    "abstract": "Crafting high-quality mutators-the core of mutation-based fuzzing that shapes the search space-is challenging. It requires human expertise and creativity, and their implementation demands knowledge of compiler internals. This paper presents MetaMut framework for developing new, useful mutators for compiler fuzzing. It integrates our compiler-domain knowledge into prompts and processes that can best harness the capabilities of a large language model. With MetaMut, we have successfully created 118 semantic-aware mutators at approximately $0.5 each, with only moderate human effort. With these mutators, our fuzzer uncovered 131 bugs in GCC and Clang, 129 of which were confirmed or fixed. The success of MetaMut suggests that the integration of AI into software and system engineering tasks traditionally thought to require expert human intervention could be a promising research direction.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p298-ou.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Fuzz Testing",
    "date": "2025-03-30",
    "paper_id": "7a733a87-d111-4c8f-90c1-e04735601a51"
  },
  {
    "title": "ClosureX: Compiler Support for Correct Persistent Fuzzing",
    "authors": "Rishi Ranjan (Virginia Tech), Ian Paterson (Virginia Tech), Matthew Hicks (Virignia Tech)",
    "abstract": "Fuzzing is a widely adopted and pragmatic methodology for bug hunting as a means of software hardening. Research reveals that increasing fuzzing throughput directly increases bug discovery rate. The highest performance fuzzing strategy is persistent fuzzing, which reuses a single process for all test cases by looping back to the start upon completion, instead of exiting. This eliminates all process creation, initialization, and tear-down costs---which are on-par with execution cost. Unfortunately, persistent fuzzing leads to semantically inconsistent program states because process state changes from one test case remain for subsequent test cases. This semantic inconsistency results in missed crashes, false crashes, and overall incorrectness that undermines fuzzer effectiveness.\nWe observe that existing fuzzing execution mechanisms exist on a continuum, based on the amount of state that gets discarded and restored between test cases. We present ClosureX, a fuzzing execution mechanism that sits at a new spot on this state restoration continuum, where only test-case-execution-specific state is reset. This fine-grain state restoration provides near-persistent performance with the correctness of heavyweight state restoration. We construct ClosureX as a set of LLVM passes that integrate with AFL++. Our evaluation on ten popular open-source fuzzing targets show that ClosureX maintains semantic correctness, while increasing test case execution rate by over 3.5x, on average, compared to AFL++. ClosureX also finds bugs more consistently and 1.9x faster than AFL++, with ClosureX discovering 15 0-day bugs (4 CVEs).",
    "link": "https://doi.org/10.1145/3669940.3707281",
    "conference_name": "ASPLOS",
    "session_title": "Fuzz Testing",
    "date": "2025-03-30",
    "paper_id": "e53123a9-3de1-4691-baa5-2ccfb56ef3b8"
  },
  {
    "title": "Ratte: Fuzzing for Miscompilations in Multi-Level Compilers Using Composable Semantics",
    "authors": "Pingshi Yu (Imperial College London), Nicolas Wu (Imperial College London), Alastair F. Donaldson (Imperial College London)",
    "abstract": "Multi-level intermediate representation (MLIR) is a rapidly growing compiler framework, with its defining feature being an ecosystem of modular language fragments called dialects. Specifying dialect semantics and validating dialect implementations presents novel challenges, as existing techniques do not cater for the modularity and composability required by MLIR. We present Ratte, a framework for specifying composable dialect semantics and modular dialect fuzzers. We introduce a novel technique for the development of semantics and fuzzers for MLIR dialects, enabling a harmonious cycle where the fuzzer validates the semantics via test-case generation, whilst at the same time the semantics allow the generation of high-quality test cases that are free from undefined behaviour. The composability of semantics and fuzzers allows generators to be cheaply derived to test combinations of dialects. We have used Ratte to find 6 previously-unknown miscompilation bugs in the production MLIR implementation. To our knowledge, Ratte is the first MLIR fuzzer capable of finding such bugs. Our work identified several aspects of the MLIR specification that were unclear, for which we proposed fixes that were adopted. Our technique provides composable reference interpreters for important MLIR dialects, validated against the production implementation, which can be used in future compiler development and testing research.",
    "link": "https://doi.org/10.1145/3676641.3716270",
    "conference_name": "ASPLOS",
    "session_title": "Fuzz Testing",
    "date": "2025-03-30",
    "paper_id": "880712ab-8b13-4f9a-9375-15fb559f7d58"
  },
  {
    "title": "Snowplow: Effective Kernel Fuzzing with a Learned White-box Test Mutator",
    "authors": "Sishuai Gong (Purdue University), Wang Rui (Purdue University), Deniz AltinbÃ¼ken (Google DeepMind), Pedro Fonseca (Purdue University), Petros Maniatis (Google DeepMind)",
    "abstract": "Kernel fuzzers rely heavily on program mutation to automatically generate new test programs based on existing ones. In particular, program mutation can alter the test’s control and data flow inside the kernel by inserting new system calls, changing the values of call arguments, or performing other program mutations. However, due to the complexity of the kernel code and its user-space interface, finding the effective mutation that can lead to the desired outcome such as increasing the coverage and reaching a target code location is extremely difficult, even with the widespread use of manually-crafted heuristics. This work proposes Snowplow, a kernel fuzzer that uses a learned white-box test mutator to enhance test mutation. The core of Snowplow is an efficient machine learning model that can learn to predict promising mutations given the test program to mutate, its kernel code coverage, and the desired coverage. Snowplow is demonstrated on argu-ment mutations of the kernel tests, and evaluated on recent Linux kernel releases. When fuzzing the kernels for 24 hours, Snowplow shows a significant speedup of discovering new coverage (4.8 ×∼ 5.2 × ) and achieves higher overall coverage (7.0% ∼ 8.6%). In a 7-day fuzzing campaign, Snow-plow discovers 86 previously-unknown crashes. Furthermore, the learned mutator is shown to accelerate directed kernel fuzzing by reaching 19 target code locations 8.5 × faster and two additional locations that are missed by the state-of-the-art directed kernel fuzzer.",
    "link": "https://doi.org/10.1145/3676641.3716019",
    "conference_name": "ASPLOS",
    "session_title": "Fuzz Testing",
    "date": "2025-03-30",
    "paper_id": "d852577e-16c3-4240-abb3-58079cae96da"
  },
  {
    "title": "KernelGPT: Enhanced Kernel Fuzzing via Large Language Models",
    "authors": "Chenyuan Yang (University of Illinois at Urbana-Champaign), Zijie Zhao (University of Illinois at Urbana-Champaign), Lingming Zhang (University of Illinois at Urbana-Champaign)",
    "abstract": "Bugs in operating system kernels can affect billions of devices and users all over the world. As a result, a large body of research has been focused on kernel fuzzing, i.e., automatically generating syscall (system call) sequences to detect potential kernel bugs or vulnerabilities. Kernel fuzzing aims to generate valid syscall sequences guided by syscall specifications that define both the syntax and semantics of syscalls. While there has been existing work trying to automate syscall specification generation, this remains largely manual work, and a large number of important syscalls are still uncovered. In this paper, we propose KernelGPT, the first approach to automatically synthesizing syscall specifications via Large Language Models (LLMs) for enhanced kernel fuzzing. Our key insight is that LLMs have seen massive kernel code, documentation, and use cases during pre-training, and thus can automatically distill the necessary information for making valid syscalls. More specifically, KernelGPT leverages an iterative approach to automatically infer the specifications, and further debug and repair them based on the validation feedback. Our results demonstrate that KernelGPT can generate more new and valid specifications and achieve higher coverage than state-of-the-art techniques. So far, by using newly generated specifications, KernelGPT has already detected 24 new unique bugs in Linux kernel, with 12 fixed and 11 assigned with CVE numbers. Moreover, a number of specifications generated by KernelGPT have already been merged into the kernel fuzzer Syzkaller, following the request from its development team.",
    "link": "https://doi.org/10.1145/3676641.3716022",
    "conference_name": "ASPLOS",
    "session_title": "Fuzz Testing",
    "date": "2025-03-30",
    "paper_id": "d10f3f40-dde1-45fd-ac02-685dbfc04831"
  },
  {
    "title": "Clapton: Clifford Assisted Problem Transformation for Error Mitigation in Variational Quantum Algorithms",
    "authors": "Lennart Maximilian Seifert (Department of Computer Science, University of Chicago), Siddharth Dangwal (Department of Computer Science, University of Chicago), Frederic T. Chong (Department of Computer Science, University of Chicago), Gokul Subramanian Ravi (Electrical Engineering and Computer Science Department, University of Michigan)",
    "abstract": "Variational quantum algorithms (VQAs) show potential for quantum advantage in the near term of quantum computing, but demand a level of accuracy that surpasses the current capabilities of NISQ devices. To systematically mitigate the impact of quantum device error on VQAs, we propose Clapton: Clifford-Assisted Problem Transformation for Error Mitigation in Variational Quantum Algorithms. Clapton leverages classically estimated good quantum states for a given VQA problem, classical simulable models of device noise, and the variational principle for VQAs. It applies transformations on the VQA problem's Hamiltonian to lower the energy estimates of known good VQA states in the presence of the modeled device noise. The Clapton hypothesis is that as long as the known good states of the VQA problem are close to the problem's ideal ground state and the device noise modeling is reasonably accurate (both of which are generally true), then the Clapton transformation substantially decreases the impact of device noise on the ground state of the VQA problem, thereby increasing the accuracy of the VQA solution. Clapton is built as an end-to-end application-to-device framework and achieves mean VQA initialization improvements of 1.7x to 3.7x, and up to a maximum of 13.3x, over the state-of-the-art baseline when evaluated for a variety of scientific applications from physics and chemistry on noise models and real quantum devices.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p47-seifert.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Error Correction",
    "date": "2025-03-30",
    "paper_id": "d42d21bb-d6a0-4e77-8052-13c5e4f333c5"
  },
  {
    "title": "QECC-Synth: A Layout Synthesizer for Quantum Error Correction Codes on Sparse Architectures",
    "authors": "Keyi Yin (University of California, San Diego), Hezi Zhang (University of California, San Diego), Xiang Fang (University of California, San Diego), Yunong Shi (AWS Quantum Technologies), Travis S. Humble (Oak Ridge National Laboratory), Ang Li (Pacific Northwest National Laboratory), Yufei Ding (University of California, San Diego)",
    "abstract": "Quantum Error Correction (QEC) codes are essential for achieving fault-tolerant quantum computing (FTQC). However, their implementation faces significant challenges due to disparity between required dense qubit connectivity and sparse hardware architectures. Current approaches often either underutilize QEC circuit features or focus on manual designs tailored to specific codes and architectures, limiting their capability and generality. In response, we introduce QECC-Synth, an automated compiler for QEC code implementation that addresses these challenges. We leverage the ancilla bridge technique tailored to the requirements of QEC circuits and introduces a systematic classification of its design space flexibilities. We then formalize this problem using the MaxSAT framework to optimize these flexibilities. Evaluation shows that our method significantly outperforms existing methods while demonstrating broader applicability across diverse QEC codes and hardware architectures.",
    "link": "https://doi.org/10.1145/3669940.3707236",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Error Correction",
    "date": "2025-03-30",
    "paper_id": "00252de2-c064-4e14-b2d2-91ed2c575bde"
  },
  {
    "title": "HetEC: Architectures for Heterogeneous Quantum Error Correction Codes",
    "authors": "Samuel Stein (Future Computing Technologies, Pacific Northwest National Laboratory), Shifan Xu (Yale Quantum Institute, Yale University), Andrew W. Cross (IBM Quantum, IBM T.J Watson Research Center), Theodore J. Yoder (IBM Quantum, IBM T. J. Watson Research Center), Ali Javadi-Abhari (IBM Quantum, IBM T. J. Watson Research Center), Chenxu Liu (Future Computing Technologies, Pacific Northwest National Laboratory), Kun Liu (Yale Quantum Institute, Yale University), Zeyuan Zhou (Yale Quantum Institute, Yale University), Charlie Guinn (Department of Physics, Princeton University), Yufei Ding (Department of Computer Science & Engineering, University of California San Diego), Yongshan Ding (Yale Quantum Institute, Yale University), Ang Li (Future Computing Technologies, Pacific Northwest National Laboratory,Department of Electrical & Computer Engineering, University of Washington)",
    "abstract": "Quantum Error Correction (QEC) is essential for future quantum computers due to its ability to exponentially suppress physical errors. The surface code is a leading error-correcting code candidate because of its local topological structure, experimentally achievable thresholds, and support for universal gate operations with magic states. However, its physical overhead scales quadratically with number of correctable errors. Conversely, quantum low-density parity-check (qLDPC) codes offer superior scaling but lack, on their own, a clear path to universal logical computation. Therefore, it is becoming increasingly evident is becoming that there are significant advantages to designing architectures using multiple codes. Heterogeneous architectures provide a clear path to universal logical computation as well as the ability to access different resource trade offs. To address this, we propose integrating the surface code and gross code using an ancilla bus for inter-code data movement. This approach involves managing trade-offs, including qubit overhead, a constrained instruction set, and gross code (memory) routing and management. While our focus is on the gross-surface code architecture, our method is adaptable to any code combination and the constraints generated by that specific architecture. Motivated by the potential reduction of physical qubit overhead, an ever important feature in the realization of fault tolerant computation, we perform the first full system study of heterogeneous error-correcting codes, discovering architectural trade-offs and optimizing around them. We demonstrate physical qubit reductions of up to 6.42x when executing an algorithm to a specific logical error rate, at the cost of up to a 3.43x increase in execution time.",
    "link": "https://doi.org/10.1145/3676641.3716001",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Error Correction",
    "date": "2025-03-30",
    "paper_id": "7e34b709-91b6-482c-87f9-79d584d22726"
  },
  {
    "title": "Micro Blossom: Accelerated Minimum-Weight Perfect Matching Decoding for Quantum Error Correction",
    "authors": "Yue Wu (Yale University), Namitha Liyanage (Yale University), Lin Zhong (Yale University)",
    "abstract": "Minimum-Weight Perfect Matching (MWPM) decoding is important to quantum error correction decoding because of its accuracy. However, many believe that it is difficult, if possible at all, to achieve the microsecond latency requirement posed by superconducting qubits. This work presents the first publicly known MWPM decoder, called Micro Blossom, that achieves sub-microsecond decoding latency. Micro Blossom employs a heterogeneous architecture that carefully partitions a state-of-the-art MWPM decoder between software and a programmable accelerator with parallel processing units, one of each vertex/edge of the decoding graph. On a surface code with code distance $d$ and a circuit-level noise model with physical error rate $p$, Micro Blossom's accelerator employs $O(d^3)$ parallel processing units to reduce the worst-case latency from $O(d^{12})$ to $O(d^9)$ and reduce the average latency from $O(p d^3+1)$ to $O(p^2 d^2+1)$ when $p \\ll 1$. We report a prototype implementation of Micro Blossom using FPGA. Measured at $d=13$ and $p=0.1\\%$, the prototype achieves an average decoding latency of $0.8 \\mu s$ at a moderate clock frequency of $62 MHz$. Micro Blossom is the first publicly known hardware-accelerated exact MWPM decoder, and the decoding latency of $0.8 \\mu s$ is 8 times shorter than the best latency of MWPM decoder implementations reported in the literature.",
    "link": "https://doi.org/10.1145/3676641.3716005",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Error Correction",
    "date": "2025-03-30",
    "paper_id": "2bf2eaf5-c579-40db-ab76-630e891c8b51"
  },
  {
    "title": "RESCQ: Realtime Scheduling for Continuous Angle Quantum Error Correction Architectures",
    "authors": "Sayam Sethi (Department of Electrical and Computer Engineering, The University of Texas at Austin), Jonathan Mark Baker (Department of Electrical and Computer Engineering, The University of Texas at Austin)",
    "abstract": "In order to realize large scale quantum error correction (QEC), resource states, such as $|T\\rangle$, must be prepared which is expensive in both space and time. In order to circumvent this problem, alternatives have been proposed, such as the production of continuous angle rotation states \\cite{akahoshi2023partially, choi2023fault, toshio2024practicalquantumadvantagepartially}. However, the production of these states is non-deterministic and may require multiple repetitions to succeed. The original proposals suggest architectures which do not account for realtime (or dynamic) management of resources to minimize total execution time. Without a realtime scheduler, a statically generated schedule will be unnecessarily expensive. We propose RESCQ (pronounced rescue), a realtime scheduler for programs compiled onto these continuous angle systems. Our scheme actively minimizes total cycle count by on-demand redistribution of resources based on expected production rates. Depending on the underlying hardware, this can cause excessive classical control overhead. We further address this by dynamically selecting the frequency of our recomputation. RESCQ improves over baseline proposals by an average of $2\\times$ in cycle count.",
    "link": "https://doi.org/10.1145/3676641.3716018",
    "conference_name": "ASPLOS",
    "session_title": "Quantum Error Correction",
    "date": "2025-03-30",
    "paper_id": "7ff2e3c9-2d6a-4b3b-ae4d-9309c65a0c1a"
  },
  {
    "title": "Accelerating LLM Serving for Multi-turn Dialogues with Efficient Resource Management",
    "authors": "Jinwoo Jeong (Korea University), Jeongseob Ahn (Korea University)",
    "abstract": "Although there have been significant efforts to make LLM serving efficient, we observe two limitations of current state-of-the-art serving frameworks in handling multi-turn dialogues between users and assistants, particularly in chat scenarios. First, existing LLM frameworks incur substantial computational overhead in recomputing attention keys and values (KVs) for understanding context across multiple turns of user queries. Second, as the prompt length of user queries is amplified due to multi-turns, a first-come-first-served (FCFS) scheduling policy often causes head-of-line blocking issues, leading to underutilization of GPU resources.\nTo address these limitations, we present FlashGen to rapidly complete multi-turn queries by efficiently utilizing the compute and memory resources of GPUs as well as the host hardware (e.g., DRAM and SSD). We introduce a multi-level KV cache comprised of GPU, CPU, and SSD, to efficiently retain attention KVs from prior turns. Our approach employs low-cost cache restoration techniques to avoid the recomputation burden. Further, we propose a request reordering technique to effectively utilize GPU memory. This scheduling technique carefully adjusts the request order without compromising fairness. Our proposed techniques outperform the vLLM framework in terms of both latency and throughput. For OPT 30B and Llama-2 70B models with the ShareGPT dataset, we achieve 1.63x and 2.85x better throughput, respectively while in a similar latency boundary.",
    "link": "https://doi.org/10.1145/3676641.3716245",
    "conference_name": "ASPLOS",
    "session_title": "Serving LLMs",
    "date": "2025-03-30",
    "paper_id": "e9616312-742c-4453-aeb9-522ad1a0e1d9"
  },
  {
    "title": "COMET: Towards Practical W4A4KV4 LLMs Serving",
    "authors": "Lian Liu (Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Long Cheng (North China Electric Power University), Haimeng Ren (ShanghaiTech University), Zhaohui Xu (ShanghaiTech University), Yudong Pan (Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Mengdi Wang (Institute of Computing Technology, CAS), Xiaowei Li (Institute of Computing Technology, CAS,Zhongguancun Laboratory), Yinhe Han (Institute of Computing Technology, CAS), Ying Wang (Institute of Computing Technology, CAS)",
    "abstract": "Quantization is a widely-used compression technology to reduce the overhead of serving large language models (LLMs) on terminal devices and in cloud data centers. However, prevalent quantization methods, such as 8-bit weight-activation or 4-bit weight-only quantization, achieve limited performance improvements due to poor support for low-precision (e.g., 4-bit) activation. This work, for the first time, realizes practical W4A4KV4 serving for LLMs, fully utilizing the INT4 tensor cores on modern GPUs and reducing the memory bottleneck caused by the KV cache. Specifically, we propose a novel fine-grained mixed-precision quantization algorithm (FMPQ) that compresses most activations into 4-bit with negligible accuracy loss. To support mixed-precision matrix multiplication for W4A4 and W4A8, we develop a highly optimized W4Ax kernel. Our approach introduces a novel mixed-precision data layout to facilitate access and fast dequantization for activation and weight tensors, utilizing the GPU's software pipeline to hide the overhead of data loading and conversion. Additionally, we propose fine-grained streaming multiprocessor (SM) scheduling to achieve load balance across different SMs. We integrate the optimized W4Ax kernel into our inference framework, COMET, and provide efficient management to support popular LLMs such as LLaMA-3-70B. Extensive evaluations demonstrate that, when running LLaMA family models on a single A100-80G-SMX4, COMET achieves a kernel-level speedup of 2.88x over cuBLAS and a 2.02x throughput improvement compared to TensorRT-LLM from an end-to-end framework perspective.",
    "link": "https://doi.org/10.1145/3676641.3716252",
    "conference_name": "ASPLOS",
    "session_title": "Serving LLMs",
    "date": "2025-03-30",
    "paper_id": "cf3da4c9-dc21-454f-aac2-f1a3c2ca58a7"
  },
  {
    "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees",
    "authors": "Ruihao Gong (Beihang University), Shihao Bai (SenseTime), Siyu Wu (Beihang University), Yunqian Fan (SenseTime), Zaijun Wang (SenseTime), Xiuhong Li (Peking University), Hailong Yang (Beihang University), Xianglong Liu (Beihang University)",
    "abstract": "The exploration and application of Large Language Models (LLMs) is thriving. To reduce deployment costs, continuous batching has become an essential feature in current service frameworks. The effectiveness of continuous batching relies on an accurate estimate of the memory requirements of requests. However, due to the diversity in request output lengths, existing frameworks tend to adopt aggressive or conservative schedulers, which often result in significant overestimation or underestimation of memory consumption. Consequently, they suffer from harmful request evictions or prolonged queuing times, failing to achieve satisfactory throughput under strict Service Level Agreement (SLA) guarantees (a.k.a. goodput), across various LLM application scenarios with differing input-output length distributions. To address this issue, we propose a novel Past-Future scheduler that precisely estimates the peak memory resources required by the running batch via considering the historical distribution of request output lengths and calculating memory occupancy at each future time point. It adapts to applications with all types of input-output length distributions, balancing the trade-off between request queuing and harmful evictions, thereby consistently achieving better goodput. Furthermore, to validate the effectiveness of the proposed scheduler, we developed a high-performance LLM serving framework, LightLLM, that implements the Past-Future scheduler. Compared to existing aggressive or conservative schedulers, LightLLM demonstrates superior goodput, achieving up to 2-3× higher goodput than other schedulers under heavy loads. LightLLM is open source to boost the research in such direction (https://github.com/ModelTC/lightllm).",
    "link": "https://doi.org/10.1145/3676641.3716011",
    "conference_name": "ASPLOS",
    "session_title": "Serving LLMs",
    "date": "2025-03-30",
    "paper_id": "2cf17f12-64a2-4247-95a7-7b0f1eba9f2b"
  },
  {
    "title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference",
    "authors": "Aditya K Kamath (Paul G Allen School of Computer Science and Engineering, University of Washington), Ramya Prabhu (Microsoft Research India), Jayashree Mohan (Microsoft Research India), Simon Peter (Paul G Allen School of Computer Science and Engineering, University of Washington), Ramachandran Ramjee (Microsoft Research India), Ashish Panwar (Microsoft Research India)",
    "abstract": "Each request in LLM inference goes through two phases: compute-bound prefill and memory-bandwidth-bound decode. To improve GPU utilization, recent systems use hybrid batching that combines the prefill and decode phases of different requests into the same batch. This approach optimizes linear operations but remains inefficient for attention computation because existing attention kernels specialize execution independently for the prefill and decode phases. In this paper, we present POD-Attention - the first GPU kernel that efficiently computes attention for hybrid batches. POD-Attention aims to maximize the utilization of both compute and memory bandwidth by carefully allocating the GPU's resources such that prefill and decode operations happen concurrently on the same multiprocessor. POD-Attention speeds up attention computation by up to $59\\%$ (mean $28\\%$), enabling higher throughput and lower latency LLM inference compared to the use of independently optimized prefill and decode attention kernels.",
    "link": "https://doi.org/10.1145/3676641.3715996",
    "conference_name": "ASPLOS",
    "session_title": "Serving LLMs",
    "date": "2025-03-30",
    "paper_id": "eef25e55-efca-4def-bec9-ef6afe8326db"
  },
  {
    "title": "TAPAS: Thermal- and Power-Aware Scheduling for LLM Inference in Cloud Platforms",
    "authors": "Jovan Stojkovic (University of Illinois at Urbana-Champaign), Chaojie Zhang (Microsoft Azure Research), ÃÃ±igo Goiri (Microsoft Azure Research), Esha Choukse (Microsoft Azure Research), Haoran Qiu (Microsoft Azure Research), Rodrigo Fonseca (Microsoft Azure Research), Josep Torrellas (University of Illinois at Urbana-Champaign), Ricardo Bianchini (Microsoft Azure)",
    "abstract": "The rising demand for generative large language models (LLMs) poses challenges for thermal and power management in cloud datacenters. Traditional techniques often are inadequate for LLM inference due to the fine-grained, millisecond-scale execution phases, each with distinct performance, thermal, and power profiles. Additionally, LLM inference workloads are sensitive to various configuration parameters (e.g., model parallelism, size, and quantization) that involve trade-offs between performance, temperature, power, and output quality. Moreover, clouds often co-locate SaaS and IaaS workloads, each with different levels of visibility and flexibility. We propose TAPAS, a thermal- and power-aware framework designed for LLM inference clusters in the cloud. TAPAS enhances cooling and power oversubscription capabilities, reducing the total cost of ownership (TCO) while effectively handling emergencies (e.g., cooling and power failures). The system leverages historical temperature and power data, along with the adaptability of SaaS workloads, to: (1) efficiently place new GPU workload VMs within cooling and power constraints, (2) route LLM inference requests across SaaS VMs, and (3) reconfigure SaaS VMs to manage load spikes and emergency situations. Our evaluation on a large GPU cluster demonstrates significant reductions in thermal and power throttling events, boosting system efficiency.",
    "link": "https://doi.org/10.1145/3676641.3716025",
    "conference_name": "ASPLOS",
    "session_title": "Serving LLMs",
    "date": "2025-03-30",
    "paper_id": "2e1b4f16-33ae-46bb-a5ba-b714e1139766"
  },
  {
    "title": "Controlled Preemption: Amplifying Side-Channel Attacks from Userspace",
    "authors": "Yongye Zhu (University of California, Berkeley), Boru Chen (University of California, Berkeley), Zirui Neil Zhao (NVIDIA,UT Austin), Christopher W. Fletcher (University of California, Berkeley)",
    "abstract": "Microarchitectural side channels are an ongoing threat in today's systems. Yet, many side-channel methodologies suffer from low temporal resolution measurement, which can either preclude or significantly complicate an attack.\nThis paper introduces Controlled Preemption an attack primitive enabling a single unprivileged (user-level) attacker thread to repeatedly preempt a victim thread after colocating with that victim thread on the same logical core. Between preemptions, the victim thread executes zero to several instructions---sufficiently few to enable high-resolution side channel measurements.\nThe key idea in Controlled Preemption is to exploit scheduler fairness heuristics. Namely, that modern thread schedulers give a thread A the ability to preempt another thread B until a fairness tripwire (signaling that A is starving B) fires. We show how this idea enables hundreds of short preemptions before tripping the fairness tripwire is robust to noise and applies to both the Linux CFS and EEVDF schedulers. We also develop a technique that helps colocate the attacker and victim threads onto the same logical core, an attacker capability overlooked by prior work.\nOur evaluation tests Controlled Preemption in the context of several different victim programs, victim privilege levels (inside and outside of Intel SGX) and choices of side channel. In each attack, we demonstrate results that are competitive with prior work but make fewer assumptions (e.g., require only user-level privilege or require fewer colocated attacker threads).",
    "link": "https://doi.org/10.1145/3676641.3715985",
    "conference_name": "ASPLOS",
    "session_title": "Side Channels",
    "date": "2025-03-30",
    "paper_id": "4bf24b16-878a-4cdb-8683-67082c968647"
  },
  {
    "title": "Protecting Cryptographic Code Against Spectre-RSB",
    "authors": "Santiago Arranz Olmos (MPI-SP), Gilles Barthe (MPI-SP, IMDEA Software Institute), Chitchanok Chuengsatiansup (University of Melbourne), Benjamin Gregoire (Inria), Vincent Laporte (UniversitÃ© de Lorraine, CNRS, Inria, LORIA), Tiago Oliveira (Sandbox AQ), Peter Schwabe (MPI-SP,Radboud University), Yuval Yarom (Ruhr University Bochum), Zhiyuan Zhang (MPI-SP)",
    "abstract": "Spectre attacks void the guarantees of constant-time cryptographic code by leaking secrets during speculative execution. Recent research shows that such code can be protected from Spectre-v1 attacks with minimal overhead, but leaves open the question of protecting against other Spectre variants. In this work, we design, validate, implement, and verify a new approach to protect cryptographic code against all known classes of Spectre attacks, in particular Spectre-RSB. Our approach combines a new value-dependent information-flow type system that ensures that no secrets leak even under speculative execution and a compiler transformation that enables it on the generated low-level code. We first prove the soundness of the type system and the correctness of the compiler transformation using the Coq proof assistant. We then implement our approach in the Jas-min framework for high-assurance cryptography and demonstrate that the overhead incurred by all Spectre pro-tections is below 2% for most cryptographic primitives and reaches only about 5–7% for the more complex post-quantum key-encapsulation mechanism Kyber.",
    "link": "https://doi.org/10.1145/3676641.3716015",
    "conference_name": "ASPLOS",
    "session_title": "Side Channels",
    "date": "2025-03-30",
    "paper_id": "f7261d5f-148a-4700-aba4-9dcb05990463"
  },
  {
    "title": "Reload+Reload: Exploiting Cache and Memory Contention Side Channel on AMD SEV",
    "authors": "Li-Chung Chiang (National Taiwan University), Shih-Wei Li (National Taiwan University)",
    "abstract": "To enhance the security of virtual machines (VMs) in multi-tenant cloud environments, AMD provides the Secure Encrypted Virtualization (SEV) extension to support encrypted VMs. We discovered two previously unknown side channels from AMD processors with SEV support: cache flush and memory contention side channels. Our findings apply to SEV-SNP and earlier versions of the technology (SEV and SEV-ES). We formulated two Reload+Reload (RR) attacks based on our two respective findings: Reload+Reload-flush-set (RRFS) and Reload+Reload-memory-block (RRMB). We demonstrated the effectiveness of the attacks against SEV-SNP protected VMs: we built a RRFS-based covert channel for a Spectre attack and used RRMB for extracting AES-128 secret keys. Compared to Prime+Probe-based implementations, our RRFS-based covert channel demonstrates superior noise resistance and higher capacity.",
    "link": "https://doi.org/10.1145/3676641.3716017",
    "conference_name": "ASPLOS",
    "session_title": "Side Channels",
    "date": "2025-03-30",
    "paper_id": "1283cb45-4434-487d-a0e5-d02342d709f2"
  },
  {
    "title": "SMaCk: Efficient Instruction Cache Attacks via Self-Modifying Code Conflicts",
    "authors": "Seonghun Son (Iowa State University), Daniel Moghimi (Google), Berk Gulmezoglu (Iowa State University)",
    "abstract": "Self-modifying code (SMC) allows programs to alter their own instructions, optimizing performance and functionality on x86 processors. Despite its benefits, SMC introduces unique microarchitectural behaviors that can be exploited for malicious purposes. In this paper, we explore the security implications of SMC by examining how specific x86 instructions affecting instruction cache lines lead to measurable timing discrepancies between cache hits and misses. These discrepancies facilitate refined cache attacks, making them less noisy and more effective. We introduce novel attack techniques that leverage these timing variations to enhance existing methods such as Prime+Probe and Flush+Reload. Our advanced techniques allow adversaries to more precisely attack cryptographic keys and create covert channels akin to Spectre across various x86 platforms. Finally, we propose a dynamic detection methodology utilizing hardware performance counters to mitigate these enhanced threats.",
    "link": "https://doi.org/10.1145/3676641.3716274",
    "conference_name": "ASPLOS",
    "session_title": "Side Channels",
    "date": "2025-03-30",
    "paper_id": "8a2169ce-89a1-44fc-8638-91e9db346e7f"
  },
  {
    "title": "FlexProf: Flexible, Side-Channel-Free Memory Access",
    "authors": "Jarrett Minton (University of Utah), Rajeev Balasubramonian (University of Utah)",
    "abstract": "Secure processors must defend against a wide array of microarchitecture side-channels, including those induced by a shared memory controller. Multiple studies have proposed techniques that allocate ''turns'' (within the memory controller) to each co-scheduled virtual machine (VM), and introduce gaps between VM turns to prevent resource conflicts and side-channels. In spite of past advancements in secure memory scheduling, the elimination of side-channels imposes a performance slowdown of 2x. We observe that one of the causes of this slowdown is that the memory controller schedule accommodates the worst case, i.e., it is prepared to handle either reads or writes. The key insight in this work is that the schedule can be more efficient if we designate every turn to handle fixed patterns of reads and writes.In particular, we introduce a read-optimized turn and a write-optimized turn. Coarse-grain application profiling helps determine how often the two types of turns are invoked, without leaking sensitive information. We also add flexibility so that a read-optimized turn can opportunistically also issue writes, and vice versa. This provides a good balance between restrictions and flexibility; between throughput and utilization. The proposed FlexProf memory controller improves performance by up to 33% with a geometric mean gain of 8% on mixed workloads, relative to state-of-the-art methods. Over half the memory-intensive programs evaluated exhibit performance gains of over 10%.",
    "link": "https://doi.org/10.1145/3676641.3715997",
    "conference_name": "ASPLOS",
    "session_title": "Side Channels",
    "date": "2025-03-30",
    "paper_id": "9c2ea43d-7a44-4b91-8983-b80f33c0b019"
  },
  {
    "title": "GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism",
    "authors": "Byungsoo Jeon (NVIDIA), Mengdi Wu (Carnegie Mellon Univerisity), Shiyi Cao (UC Berkeley), Sunghyun Kim (Massachusetts Institute of Technology), Sunghyun Park (NVIDIA), Neeraj Aggarwal (Carnegie Mellon University), Colin Unger (Stanford University), Daiyaan Arfeen (Carnegie Mellon University), Peiyuan Liao (Carnegie Mellon University), Xupeng Miao (Carnegie Mellon University), Mohammad Alizadeh (Massachusetts Institute of Technology), Gregory R. Ganger (Carnegie Mellon University), Tianqi Chen (Carnegie Mellon University), Zhihao Jia (Carnegie Mellon University)",
    "abstract": "Deep neural networks (DNNs) continue to grow rapidly in size, making them infeasible to train on a single device. Pipeline parallelism is commonly used in existing DNN systems to support large-scale DNN training by partitioning a DNN into multiple stages, which concurrently perform DNN training for different micro-batches in a pipeline fashion. However, existing pipeline-parallel approaches only consider sequential pipeline stages and thus ignore the topology of a DNN, resulting in missed model-parallel opportunities. This paper presents graph pipeline parallelism (GPP), a new pipeline-parallel scheme that partitions a DNN into pipeline stages whose dependencies are identified by a directed acyclic graph. GPP generalizes existing sequential pipeline parallelism and preserves the inherent topology of a DNN to enable concurrent execution of computationally-independent operators, resulting in reduced memory requirement and improved GPU performance. In addition, we develop GraphPipe, a distributed system that exploits GPP strategies to enable performant and scalable DNN training. GraphPipe partitions a DNN into a graph of stages, optimizes micro-batch schedules for these stages, and parallelizes DNN training using the discovered GPP strategies. Evaluation on a variety of DNNs shows that GraphPipe outperforms existing pipeline-parallel systems such as PipeDream and Piper by up to 1.6X. GraphPipe also reduces the search time by 9-21X compared to PipeDream and Piper.",
    "link": "https://doi.org/10.1145/3669940.3707220",
    "conference_name": "ASPLOS",
    "session_title": "ML Training",
    "date": "2025-03-30",
    "paper_id": "5f43e532-51ff-4268-a41e-30d4629795e9"
  },
  {
    "title": "Cascade: A Dependency-aware Efficient Training Framework for Temporal Graph Neural Network",
    "authors": "Yue Dai (Department of Computer Science, University of Pittsburgh), Xulong Tang (Department of Computer Science, University of Pittsburgh), Youtao Zhang (Department of Computer Science, University of Pittsburgh)",
    "abstract": "Temporal graph neural networks (TGNN) have gained significant momentum in many real-world dynamic graph tasks. These models use graph changes (i.e., events) as inputs to update nodes' status vectors (i.e., memories), which are then exploited to assist predictions. Despite their improved accuracies, the efficiency of TGNN training is significantly limited due to the inherent temporal relationship between the input events. Although larger training batches can improve parallelism and speed up TGNN training, they lead to infrequent memory updates, which cause outdated information and reduced accuracy. This trade-off forces current methods to use small batches, resulting in high latency and underutilized hardware. To address this, we propose an efficient TGNN training framework, Cascade, to adaptively boost TGNN training parallelism based on nodes' spatial and temporal dependencies. Cascade adopts a topology-aware scheduler that includes as many spatial-independent events in the same batches. Moreover, it leverages node memories' similarities to break temporal dependencies on stabilized nodes, enabling it to pack more temporal-independent events in the same batches. Additionally, Cascade adaptively decides nodes' update frequencies based on runtime feedback. Compared to prior state-of-the-art TGNN training frameworks, our approach can averagely achieve 2.3x (up to 5.1x) speed up without jeopardizing the resulted models' accuracy.",
    "link": "https://doi.org/10.1145/3676641.3716250",
    "conference_name": "ASPLOS",
    "session_title": "ML Training",
    "date": "2025-03-30",
    "paper_id": "2588a5a8-3901-4ab5-916a-0c8480387157"
  },
  {
    "title": "Frugal: Efficient and Economic Embedding Model Training with Commodity GPUs",
    "authors": "Minhui Xie (Tsinghua University,Renmin University of China), Shaoxun Zeng (Tsinghua University), Hao Guo (Tsinghua University), Shiwei Gao (Tsinghua University), Youyou Lu (Tsinghua University)",
    "abstract": "Embedding models show superiority in learning representations of massive ID-type features in sparse learning scenarios such as recommendation systems (e.g., user/item IDs) and graph learning (e.g., node/edge IDs). Commodity GPUs are highly favored for their cost-efficient computing power, which is ideally suited for the low computing demand of memory-intensive embedding models. However, directly running embedding model training on commodity GPUs yields poor performance because of their deficient communication resources (including low communication bandwidth and no PCIe P2P support).\nThis paper presents Frugal, an embedding model training system tailored for commodity GPUs. Based on the observation that the communication between commodity GPUs must be bounced on host memory (due to no PCIe P2P support), the key idea of Frugal is proactively flushing, where each GPU proactively flushes its own parameters that other GPUs will access into host memory, thereby decoupling half of the communication overhead to non-critical paths. To alleviate the communication contention of proactively flushing on foreground training processes, Frugal assigns priorities to each flush operation, and prioritizes flushing parameters that GPUs will access while deferring others. Further, Frugal tailors a two-level priority queue to ensure high scalability for operations involving priorities. Frugal has been applied to train embedding models including recommendation models and graph embedding. Experiments indicate that Frugal can significantly increase training throughput on commodity GPUs, and achieve similar throughput compared to existing systems on datacenter GPUs with 4.0-4.3× improvement in cost-effectiveness.",
    "link": "https://doi.org/10.1145/3669940.3707245",
    "conference_name": "ASPLOS",
    "session_title": "ML Training",
    "date": "2025-03-30",
    "paper_id": "4a0f1ad7-4874-4700-bcbb-1dc6ad2d5d9a"
  },
  {
    "title": "FastGL: A GPU-Efficient Framework for Accelerating Sampling-Based GNN Training at Large Scale",
    "authors": "Zeyu Zhu (Institute of Automation, Chinese Academy of Sciences,School of Future Technology, University of Chinese Academy of Sciences), Peisong Wang (Institute of Automation, Chinese Academy of Sciences), Qinghao Hu (Institute of Automation, Chinese Academy of Sciences,AiRiA), Gang Li (Shanghai Jiao Tong University), Xiaoyao Liang (Shanghai Jiao Tong University), Jian Cheng (Institute of Automation, Chinese Academy of Sciences,AiRiA)",
    "abstract": "Graph Neural Networks (GNNs) have shown great superiority on non-Euclidean graph data, achieving ground-breaking performance on various graph-related tasks. As a practical solution to train GNN on large graphs with billions of nodes and edges, the sampling-based training is widely adopted by existing training frameworks. However, through an in-depth analysis, we observe that the efficiency of existing sampling-based training frameworks is still limited due to the key bottlenecks lying in all three phases of sampling-based training, i.e., subgraph sample, memory IO, and computation. To this end, we propose FastGL, a GPU-efficient Framework for accelerating sampling-based training of GNN at Large scale by simultaneously optimizing all above three phases, taking into account both GPU characteristics and graph structure. Specifically, by exploiting the inherent overlap within graph structures, FastGL develops the Match-Reorder strategy to reduce the data traffic, which accelerates the memory IO without incurring any GPU memory overhead. Additionally, FastGL leverages a Memory-Aware computation method, harnessing the GPU memory's hierarchical nature to mitigate irregular data access during computation. FastGL further incorporates the Fused-Map approach aimed at diminishing the synchronization overhead during sampling. Extensive experiments demonstrate that FastGL can achieve an average speedup of 11.8x, 2.2x and 1.5x over the state-of-the-art frameworks PyG, DGL, and GNNLab, respectively.Our code is available at https://github.com/a1bc2def6g/fastgl-ae.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p94-zhu.pdf",
    "conference_name": "ASPLOS",
    "session_title": "ML Training",
    "date": "2025-03-30",
    "paper_id": "2265bb10-f42a-4ba3-84ca-ebae1d35205e"
  },
  {
    "title": "EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation",
    "authors": "Weigao Su (Purdue University), Vishal Shrivastav (Purdue University)",
    "abstract": "Achieving low remote memory access latency remains the primary challenge in realizing memory disaggregation over Ethernet within the datacenters. We present EDM that attempts to overcome this challenge using two key ideas. First, while existing network protocols for remote memory access over the Ethernet, such as TCP/IP and RDMA, are implemented on top of the MAC layer, EDM takes a radical approach by implementing the entire network protocol stack for remote memory access within the Physical layer (PHY) of the Ethernet. This overcomes fundamental latency and bandwidth overheads imposed by the MAC layer, especially for small memory messages. Second, EDM implements a centralized, fast, in-network scheduler for memory traffic within the PHY of the Ethernet switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm, the scheduler dynamically reserves bandwidth between compute and memory nodes by creating virtual circuits in the PHY, thus eliminating queuing delay and layer 2 packet processing delay at the switch for memory traffic, while maintaining high bandwidth utilization. Our FPGA testbed demonstrates that EDM's network fabric incurs a latency of only $\\sim$300 ns for remote memory access in an unloaded network, which is an order of magnitude lower than state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to emerging PCIe-based solutions such as CXL. Larger-scale network simulations indicate that even at high network loads, EDM's average latency remains within 1.3$\\times$ its unloaded latency.",
    "link": "https://doi.org/10.1145/3669940.3707221",
    "conference_name": "ASPLOS",
    "session_title": "Networking",
    "date": "2025-03-30",
    "paper_id": "c7a2664b-8910-4a93-a2dd-5f5fa45c4165"
  },
  {
    "title": "Performance Prediction of On-NIC Network Functions with Multi-Resource Contention and Traffic Awareness",
    "authors": "Shaofeng Wu (The Chinese University of Hong Kong), Qiang Su (The Chinese University of Hong Kong), Zhixiong Niu (Microsoft Research), Hong Xu (The Chinese University of Hong Kong)",
    "abstract": "Network function (NF) offloading on SmartNICs has been widely used in modern data centers, offering benefits in host resource saving and programmability. Co-running NFs on the same SmartNICs can cause performance interference due to contention of onboard resources. To meet performance SLAs while ensuring efficient resource management, operators need mechanisms to predict NF performance under such contention. However, existing solutions lack SmartNIC-specific knowledge and exhibit limited traffic awareness, leading to poor accuracy for on-NIC NFs. This paper proposes Yala, a novel performance predictive system for on-NIC NFs. Yala builds upon the key observation that co-located NFs contend for multiple resources, including onboard accelerators and the memory subsystem. It also facilitates traffic awareness according to the behaviors of individual resources to maintain accuracy as the external traffic attributes vary. Evaluation using BlueField-2 SmartNICs shows that Yala improves the prediction accuracy by 78.8% and reduces SLA violations by 92.2% compared to state-of-the-art approaches, and enables new practical usecases.",
    "link": "https://doi.org/10.1145/3669940.3707232",
    "conference_name": "ASPLOS",
    "session_title": "Networking",
    "date": "2025-03-30",
    "paper_id": "fbb51a3e-304e-417a-8da8-1a812145ba18"
  },
  {
    "title": "Gigaflow: Pipeline-Aware Sub-Traversal Caching for Modern SmartNICs",
    "authors": "Annus Zulfiqar (University of Michigan), Ali Imran (University of Michigan), Venkat Kunaparaju (Purdue University), Ben Pfaff (Feldera Inc.), Gianni Antichi (Politecnico di Milano), Muhammad Shahbaz (University of Michigan)",
    "abstract": "The success of modern public/edge clouds hinges heavily on the performance of their end-host network stacks if they are to support the emerging and diverse tenants' workloads (e.g., distributed training in the cloud to fast inference at the edge). Virtual Switches (vSwitches) are vital components of this stack, providing a unified interface to enforce high-level policies on incoming packets and route them to physical interfaces, containers, or virtual machines. As performance demands escalate, there has been a shift toward offloading vSwitch processing to SmartNICs to alleviate CPU load and improve efficiency. However, existing solutions struggle to handle the growing flow rule space within the NIC, leading to high miss rates and poor scalability.\nIn this paper, we introduce Gigaflow, a novel caching system tailored for deployment on SmartNICs to accelerate vSwitch packet processing. Our core insight is that by harnessing the inherent pipeline-aware locality within programmable vSwitch pipelines-defining policies (e.g., L2, L3, and ACL) and their execution order (e.g., using P4 and OpenFlow)-we can create cache rules for shared segments (sub-traversals) within the pipeline, rather than caching entire flows. These shared segments can be reused across multiple flows, resulting in higher cache efficiency and greater rule-space coverage. Our evaluations show that Gigaflow achieves up to a 51% improvement in cache hit rate (average 25% improvement) over traditional caching solutions (i.e., Megaflow), while capturing up to 450x more rule space within the limited memory of today's SmartNICs-all while operating at line speed.",
    "link": "https://doi.org/10.1145/3676641.3716000",
    "conference_name": "ASPLOS",
    "session_title": "Networking",
    "date": "2025-03-30",
    "paper_id": "fd3ae231-0c75-41d6-8d6b-b8e289860352"
  },
  {
    "title": "TNIC: A Trusted NIC Architecture",
    "authors": "Dimitra Giantsidi (The University of Edinburgh), Julian Pritzi (Technical University of Munich), Felix Gust (Technical University of Munich), Antonios Katsarakis (Huawei Research), Atsushi Koshiba (Technical University of Munich), Pramod Bhatotia (Technical University of Munich)",
    "abstract": "We introduce TNIC, a trusted NIC architecture for building trustworthy distributed systems deployed in heterogeneous, untrusted (Byzantine) cloud environments. TNIC builds a minimal, formally verified, silicon root-of-trust at the network interface level. We strive for three primary design goals: (1) a host CPU-agnostic unified security architecture by providing trustworthy network-level isolation; (2) a minimalistic and verifiable TCB based on a silicon root-of-trust by providing two core properties of transferable authentication and non-equivocation; and (3) a hardware-accelerated trustworthy network stack leveraging SmartNICs. Based on the TNIC architecture and associated network stack, we present a generic set of programming APIs and a recipe for building high-performance, trustworthy, distributed systems for Byzantine settings. We formally verify the safety and security properties of our TNIC while demonstrating its use by building four trustworthy distributed systems. Our evaluation of TNIC shows up to 6x performance improvement compared to CPU-centric TEE systems.",
    "link": "https://doi.org/10.1145/3676641.3716277",
    "conference_name": "ASPLOS",
    "session_title": "Networking",
    "date": "2025-03-30",
    "paper_id": "fe2373d0-d619-4bb3-855c-2d3fef1726c2"
  },
  {
    "title": "Extended User Interrupts (xUI): Fast and Flexible Notification without Polling",
    "authors": "Berk Aydogmus (Purdue University), Linsong Guo (UC San Diego), Danial Zuberi (UC San Diego), Tal Garfinkel (UC San Diego), Dean Tullsen (UC San Diego), Amy Ousterhout (UC San Diego), Kazem Taram (Purdue University)",
    "abstract": "Extended user interrupts (xUI) is a set of processor extensions that builds on Intel’s UIPI model of user interrupts, for enhanced performance and flexibility. This paper de-constructs Intel’s current UIPI design through analysis and measurement, and uses this to develop an accurate model of its timing. It then introduces four novel enhancements to user interrupts: tracked interrupts, hardware safepoints, a kernel bypass timer, and interrupt forwarding. xUI is modeled in gem5 simulation and evaluated on three use cases – preemption in a high-performance user-level runtime, IO notification in a layer3 router using DPDK, and IO notification in a synthetic workload with a streaming accelerator modeled after Intel’s Data Streaming Accelerator. This work shows that xUI offers the performance of shared memory polling with the efficiency of asynchronous notification.",
    "link": "https://doi.org/10.1145/3676641.3716028",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 2",
    "date": "2025-03-30",
    "paper_id": "3e2a10a4-b271-4c53-bc7a-907f598b5859"
  },
  {
    "title": "Stramash: A Fused-kernel Operating System For Cache-Coherent, Heterogeneous-ISA Platforms",
    "authors": "Tong Xing (The University of Edinburgh), Cong Xiong (Imperial College London), Tianrui Wei (UC Berkeley), April Sanchez (Google), Binoy Ravindran (Virginia Tech), Jonathan Balkind (UC Santa Barbara), Antonio Barbalace (The University of Edinburgh)",
    "abstract": "We live in the world of heterogeneous computing. With specialised elements reaching all aspects of our computer systems and their prevalence only growing, we must act to rein in their inherent complexity. One area that has seen significantly less investment in terms of development is heterogeneous-ISA systems, specifically because of complexity. To date, heterogeneous-ISA processors have required significant software overheads, workarounds, and coordination layers, making the development of more advanced software hard, and motivating little further development of more advanced hardware. In this paper, we take a fused approach to heterogeneity, and introduce a new operating system (OS) design, the fused-kernel OS, which goes beyond the multiple-kernel OS design, exploiting cache-coherent shared memory among heterogeneous-ISA CPUs as a first principle -- introducing a set of new OS kernel mechanisms. We built a prototype fused-kernel OS, Stramash-Linux, to demonstrate the applicability of our design to monolithic OS kernels. We profile Stramash OS components on real hardware but tested them on an architectural simulator -- Stramash-QEMU, which we design and build. Our evaluation begins by validating the accuracy of our simulator, achieving an average of less than 4% errors. We then perform a direct comparison between our fused-kernel OS and state-of-the-art multiple-kernel OS designs. Results demonstrate speedups of up to 2.1× on NPB benchmarks. Further, we provide an in-depth analysis of the differences and trade-offs between fused-kernel and multiple-kernel OS designs.",
    "link": "https://doi.org/10.1145/3676641.3716275",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 2",
    "date": "2025-03-30",
    "paper_id": "90bdfa44-b0fa-49c7-a5fd-d70ea63c6191"
  },
  {
    "title": "H-Houdini: Scalable Invariant Learning",
    "authors": "Sushant Dinesh (University of California, Berkeley), Yongye Zhu (University of California, Berkeley), Christopher W. Fletcher (University of California, Berkeley)",
    "abstract": "Formal verification is a critical task in hardware design today. Yet, while there has been significant progress in improving technique automation and efficiency, scaling to large hardware designs remains a significant challenge. We address this challenge by proposing H-Houdini: a new algorithm for (mostly) push-button inductive invariant learning that scales to large hardware designs. H-Houdini combines the strengths of Machine Learning Inspired Synthesis (MLIS) and SAT-based Incremental Learning. The key advance is a method that replaces the monolithic SMT-style checks made by MLIS with a carefully-constructed hierarchy of smaller, incremental SMT checks that can be parallelized, memoized and reassembled into the original ‘monolithic’ invariant in a correct-by-construction fashion. We instantiate H-Houdini as VeloCT, a framework that proves hardware security properties by learning relational invariants. We benchmark VeloCT on the ‘safe instruction set synthesis’ problem in microarchitectural security. Here, VeloCT automatically (with no expert annotations) learns an invariant for the RISC-V Rocketchip in under 10s (2880 × faster than state of the art). Further, VeloCT is the first work to scale to the RISC-V out-of-order BOOM and can (mostly-automatically) verify all BOOM variants (ranging from Small to Mega) in between 6.95 minutes to 199.1 minutes.",
    "link": "https://doi.org/10.1145/3669940.3707263",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 2",
    "date": "2025-03-30",
    "paper_id": "67845f38-adeb-43a1-975f-06eaf33cc7b3"
  },
  {
    "title": "Target-Aware Implementation of Real Expressions",
    "authors": "Brett Saiki (University of Washington), Jackson Brough (University of Utah), Jonas Regehr (University of Utah), Jesus Ponce (University of Utah), Varun Pradeep (University of Washington), Aditya Akhileshwaran (University of Washington), Zachary Tatlock (University of Washington), Pavel Panchekha (University of Utah)",
    "abstract": "New low-precision accelerators, vector instruction sets, and library functions make maximizing accuracy and performance of numerical code increasingly challenging. Two lines of work$\\unicode{x2013}$traditional compilers and numerical compilers$\\unicode{x2013}$attack this problem from opposite directions. Traditional compiler backends optimize for specific target environments but are limited in their ability to balance performance and accuracy. Numerical compilers trade off accuracy and performance, or even improve both, but ignore the target environment. We join aspects of both to produce Chassis, a target-aware numerical compiler. Chassis compiles mathematical expressions to operators from a target description, which lists the real expressions each operator approximates and estimates its cost and accuracy. Chassis then uses an iterative improvement loop to optimize for speed and accuracy. Specifically, a new instruction selection modulo equivalence algorithm efficiently searches for faster target-specific programs, while a new cost-opportunity heuristic supports iterative improvement. We demonstrate Chassis' capabilities on 9 different targets, including hardware ISAs, math libraries, and programming languages. Chassis finds better accuracy and performance trade-offs than both Clang (by 3.5x) or Herbie (by up to 2.0x) by leveraging low-precision accelerators, accuracy-optimized numerical helper functions, and library subcomponents.",
    "link": "https://doi.org/10.1145/3669940.3707277",
    "conference_name": "ASPLOS",
    "session_title": "Potpourri 2",
    "date": "2025-03-30",
    "paper_id": "f7ed93a7-1b37-4fa2-b6e7-858d51eb148b"
  },
  {
    "title": "TaintEMU: Decoupling Tracking from Functional Domains for Architecture-Agnostic and Efficient Whole-System Taint Tracking",
    "authors": "Lei Cui (Guangxi Normal University), Youquan Xian (Guangxi Normal University), Peng Liu (Guangxi Normal University), Longjin Lu (Independent Researcher)",
    "abstract": "Whole-system taint tracking is vital for security analysis. However, existing methods suffer from limited architecture compatibility and significant performance overhead, mainly due to the tight coupling between the functional and tracking domains. This paper introduces TaintEMU, an architecture-agnostic and efficient solution by fully decoupling the two domains. It separates functional and tracking logic at the QEMU TCG layer, mapping shadow registers to host instead of guest registers, ensuring compatibility across guest CPU architectures. At the host layer, it physically isolates the two domains: general-purpose instructions and registers serve the functional domain, while vector resources are dedicated to tracking, avoiding host resource reuse and enhancing tracking performance. Furthermore, it directly generates tracking instructions from TCG operations on the host, bypassing additional translation and further reducing overhead. We implement TaintEMU on an AMD64 host on QEMU 8.2.2. It supports a wide range of guest architectures (x86, MIPS, ARM, AMD, RISC-V, PPC), reduces performance overhead from 301% (DECAF++) to 101% and successfully detects all vulnerabilities in tests with 8 CVEs across 7 applications.",
    "link": "https://doi.org/10.1145/3676641.3716023",
    "conference_name": "ASPLOS",
    "session_title": "Security",
    "date": "2025-03-30",
    "paper_id": "da2a479b-2c52-4ab9-8708-7f49391eb0b2"
  },
  {
    "title": "Segue & ColorGuard: Optimizing SFI Performance and Scalability on Modern Architectures",
    "authors": "Shravan Narayan (UT Austin), Tal Garfinkel (UC San Diego), Evan Johnson (UC San Diego), Zachary Yedidia (Stanford University), Yingchen Wang (UC Berkeley), Andrew Brown (Intel), Anjo Vahldiek-Oberwagner (Intel Labs), Michael LeMay (Intel Labs), Wenyong Huang (Intel), Xin Wang (Intel), Mingqiu Sun (Intel), Dean Tullsen (UC San Diego), Deian Stefan (UC San Diego)",
    "abstract": "Software-based fault isolation (SFI) enables in-process isolation through compiler instrumentation of memory accesses, and is a critical part of WebAssembly (Wasm). We present two optimizations that improve SFI performance and scalability: Segue uses x86-64 segmentation to reduce the cost of instrumentation on memory accesses, e.g., it eliminates 44.7% of Wasm's overhead on a Wasm-compatible subset of SPEC CPU 2006, and reduces overhead of Wasm-sandboxed font rendering in Firefox by 75%; ColorGuard leverages memory tagging (e.g., MPK), to enable up to a 15× increase in the number of Wasm instances that can run concurrently in a single address space, improving efficiency for high scale server-side workloads. We also explore the challenges of deploying these optimizations in three production toolchains: Wasm2c, WAMR and Wasmtime.",
    "link": "https://doi.org/10.1145/3669940.3707249",
    "conference_name": "ASPLOS",
    "session_title": "Security",
    "date": "2025-03-30",
    "paper_id": "68ed923e-d987-4481-85cc-94d9b4016427"
  },
  {
    "title": "Pave: Information Flow Control for Privacy-preserving Online Data Processing Services",
    "authors": "Minkyung Park (University of Texas at Dallas), Jaeseung Choi (Sogang University), Hyeonmin Lee (University of Virginia), Taekyoung Kwon (Seoul National University)",
    "abstract": "In online data-processing services, a user typically hands over personal data to a remote server beyond the user's control. In such environments, the user cannot be assured that the data is protected from potential leaks. We introduce Pave, a new framework to guarantee data privacy while being processed remotely. Pave provides an arbitrary data-processing program with a sandboxed execution environment. The runtime monitor, PaveBox, intercepts all data flows into and out of the sandbox, allowing them only if they do not compromise user data. At the same time, it guarantees that the benign flows will not be hampered to preserve the program's functionality. As the PaveBox is built on top of Intel SGX, a user can verify the integrity and confidentiality of the PaveBox by remote attestation. We provide a formal model of Pave and prove its security and carry out the quantitative analysis with prototype-based experiments.",
    "link": "https://doi.org/10.1145/3676641.3716266",
    "conference_name": "ASPLOS",
    "session_title": "Security",
    "date": "2025-03-30",
    "paper_id": "b82b4e5f-4c68-4248-a66e-89dba4d7e463"
  },
  {
    "title": "Sharing is leaking: blocking transient-execution attacks with core-gapped confidential VMs",
    "authors": "Charly Castes (EPFL,Google), Andrew Baumann (Google)",
    "abstract": "Confidential VMs on platforms such as Intel TDX, AMD SEV and Arm CCA promise greater security for cloud users against even a hypervisor-level attacker, but this promise has been shattered by repeated transient-execution vulnerabilities and CPU bugs. At the root of this problem lies the need to multiplex CPU cores with all their complex microarchitectural state among distrusting entities, with an untrusted hypervisor in control of the multiplexing. We propose core-gapped confidential VMs , a set of software-only modifications that ensure that no distrusting code shares a core, thus removing all same-core side-channels and transient-execution vulnerabilities from the guest’s TCB. We present an Arm-based prototype along with a performance evaluation showing that, not only does core-gapping offer performance competitive with non-confidential VMs, the greater locality achieved by avoiding shared cores can even improve performance for CPU-intensive workloads.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p267-castes.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Security",
    "date": "2025-03-30",
    "paper_id": "74b2fbfa-7a09-438f-a4c6-04f04c083aca"
  },
  {
    "title": "Einsum Trees: An Abstraction for Optimizing the Execution of Tensor Expressions",
    "authors": "Alexander Breuer (Friedrich Schiller University Jena), Mark Blacher (Friedrich Schiller University Jena), Max Engel (Friedrich Schiller University Jena), Joachim Giesen (Friedrich Schiller University Jena), Alexander Heinecke (Intel Corporation), Julien Klaus (Friedrich Schiller University Jena), Stefan Remke (Friedrich Schiller University Jena)",
    "abstract": "Einsum is a declarative language for tensor expressions that specifies an output tensor in terms of several input tensors. However, it does not specify how to compute the output tensor from the input tensors. A typical computational backend for the einsum language comprises two parts: First, a contraction path algorithm that breaks down an einsum expression into a sequence of binary tensor contractions. Second, the execution of the binary contractions. For efficient binary contractions, the data layout of the tensors must be optimized. So far, the computation of contraction paths and the optimization of the data layout for single, that is, local, binary tensor contractions have been studied in isolation. For optimizing the overall execution times of einsum expressions, we introduce Einsum Tree IR, an intermediate representation for globally optimizing the data layout for a given contraction path. We illustrate the effectiveness of the approach on a state-of-the-art Arm server processor, an x86 server processor, and an x86 desktop system.",
    "link": "https://doi.org/10.1145/3676641.3716254",
    "conference_name": "ASPLOS",
    "session_title": "ML Compilers",
    "date": "2025-03-30",
    "paper_id": "e39059e5-1d5b-4d58-a7a2-108827e80599"
  },
  {
    "title": "Optimizing Deep Learning Inference Efficiency through Block Dependency Analysis",
    "authors": "Zhanyuan Di (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Leping Wang (SKLP, Institute of Computing Technology, CAS), En Shao (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Zhaojia Ma (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Ziyi Ren (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Feng Hua (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Lixian Ma (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Jie Zhao (Hunan University), Guangming Tan (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Ninghui Sun (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences)",
    "abstract": "Inter-operator optimization in deep neural networks (DNNs) relies on accurate data dependency analysis. Traditional machine learning compilers (MLCs) perform static data dependency analysis at the element and operator levels, leading to two key limitations: complex dependencies that hinder efficient inter-operator optimizations, and overlooked parallelizable computations that underutilize GPU resources. We introduce BlockDepend, a novel MLC framework that addresses these issues through block-level dependency analysis. By examining the lower-level phases of compilation, BlockDepend extracts crucial block-level dependency information, simplifying complex relationships between operators and uncovering hidden parallelization opportunities. This allows for targeted optimization strategies that enhance memory access efficiency and improve GPU utilization. Our experiments demonstrate BlockDepend's effectiveness, achieving speedups of 1.71× and 2.88× compared to NVIDIA TensorRT and AMD MIGraphX, respectively, across various workloads.",
    "link": "https://doi.org/10.1145/3676641.3716264",
    "conference_name": "ASPLOS",
    "session_title": "ML Compilers",
    "date": "2025-03-30",
    "paper_id": "0248c7c6-d07b-4e02-ba7e-923c2a643980"
  },
  {
    "title": "Pruner: A Draft-then-Verify Exploration Mechanism to Accelerate Tensor Program Tuning",
    "authors": "Liang Qiao (University of Science and Technology of China), Jun Shi (University of Science and Technology of China), Xiaoyu Hao (University of Science and Technology of China), Xi Fang (University of Science and Technology of China), Sen Zhang (University of Science and Technology of China), Minfan Zhao (University of Science and Technology of China), Ziqi Zhu (University of Science and Technology of China), Junshi Chen (University of Science and Technology of China), Hong An (University of Science and Technology of China), Xulong Tang (University of Pittsburgh), Bing Li (NIO), Honghui Yuan (NIO), Xinyang Wang (NIO)",
    "abstract": "Tensor program tuning is essential for the efficient deployment of deep neural networks. Search-based approaches have demonstrated scalability and effectiveness in automatically finding high-performance programs for specific hardware. However, the search process is often inefficient, taking hours or even days to discover optimal programs due to the exploration mechanisms guided by an accurate but slow-learned cost model. Meanwhile, the learned cost model trained on one platform cannot seamlessly adapt online to another, which we call cross-platform online unawareness. In this work, we propose Pruner and MoA-Pruner. Pruner is a\"Draft-then-Verify\"exploration mechanism that accelerates the schedule search process. Instead of applying the complex learned cost model to all explored candidates, Pruner drafts small-scale potential candidates by introducing a naive Symbol-based Analyzer (draft model), then identifies the best candidates by the learned cost model. MoA-Pruner introduces a Momentum online Adaptation strategy to address the cross-platform online unawareness. We incorporate Pruner into the TVM and conduct extensive experiments on three GPU-based platforms. Results show considerable speedup in schedule search time. In online tuning scenarios, Pruner and MoA-Pruner achieve an average speedup of $2.6 \\times$ and $4.82 \\times$ compared to Ansor. In offline tuning scenarios, Pruner achieves an average speedup of $4.75 \\times$ and $4.05\\times$ compared to TenSet and TLP, respectively. Furthermore, Pruner achieves an average speedup of $4.08 \\times$ compared to MetaSchedule on TensorCore.",
    "link": "https://doi.org/10.1145/3676641.3716269",
    "conference_name": "ASPLOS",
    "session_title": "ML Compilers",
    "date": "2025-03-30",
    "paper_id": "deddc730-9543-4094-89ea-b304b293ed91"
  },
  {
    "title": "Relax: Composable Abstractions for End-to-End Dynamic Machine Learning",
    "authors": "Ruihang Lai (Carnegie Mellon University), Junru Shao (OpenAI), Siyuan Feng (Shanghai Jiao Tong University), Steven Lyubomirsky (NVIDIA), Bohan Hou (Carnegie Mellon University), Wuwei Lin (OpenAI), Zihao Ye (University of Washington), Hongyi Jin (Carnegie Mellon University), Yuchen Jin (Hyperbolic), Jiawei Liu (University of Illinois Urbana-Champaign), Lesheng Jin (Hyperbolic), Yaxing Cai (NVIDIA), Ziheng Jiang (ByteDance), Yong Wu (NVIDIA), Sunghyun Park (NVIDIA), Prakalp Srivastava (Netflix), Jared Roesch (NVIDIA), Todd C. Mowry (Carnegie Mellon University), Tianqi Chen (Carnegie Mellon University,NVIDIA)",
    "abstract": "Dynamic shape computations have become critical in modern machine learning workloads, especially in emerging large language models. The success of these models has driven the demand for their universal deployment across a diverse set of backend environments. In this paper, we present Relax, a compiler abstraction for optimizing end-to-end dynamic machine learning workloads. Relax introduces a cross-level abstraction that encapsulates computational graphs, loop-level tensor programs, and external library calls in a single representation. Relax also introduces first-class symbolic shape annotations to track dynamic shape computations globally across the program, enabling dynamic shape-aware cross-level optimizations. We build an end-to-end compilation framework using the proposed approach to optimize dynamic shape models. Experimental results on LLMs show that Relax delivers performance competitive with state-of-the-art systems across various GPUs and enables deployment of emerging models to a broader set of emerging environments, including mobile phones, embedded devices, and web browsers.",
    "link": "https://doi.org/10.1145/3676641.3716249",
    "conference_name": "ASPLOS",
    "session_title": "ML Compilers",
    "date": "2025-03-30",
    "paper_id": "b58eee33-e51e-4308-9877-2f6d629c880e"
  },
  {
    "title": "Towards End-to-End Optimization of LLM-based Applications with Ayo",
    "authors": "Xin Tan (The Chinese University of Hong Kong), Yimin Jiang (Unaffiliated), Yitao Yang (The Chinese University of Hong Kong), Hong Xu (The Chinese University of Hong Kong)",
    "abstract": "Large language model (LLM)-based applications consist of both LLM and non-LLM components, each contributing to the end-to-end latency. Despite great efforts to optimize LLM inference, end-to-end workflow optimization has been overlooked. Existing frameworks employ coarse-grained orchestration with task modules, which confines optimizations to within each module and yields suboptimal scheduling decisions.\nWe propose fine-grained end-to-end orchestration, which utilizes task primitives as the basic units and represents each query's workflow as a primitive-level dataflow graph. This explicitly exposes a much larger design space, enables optimizations in parallelization and pipelining across primitives of different modules, and enhances scheduling to improve application-level performance. We build Ayo, a novel orchestration framework for LLM-based applications that implements this scheme. Comprehensive experiments show that Ayo can achieve up to 2.09x speedup over existing systems across various popular LLM applications.",
    "link": "https://doi.org/10.1145/3676641.3716278",
    "conference_name": "ASPLOS",
    "session_title": "ML Compilers",
    "date": "2025-03-30",
    "paper_id": "8dbd9d93-3c35-4b59-aff9-306815408e48"
  },
  {
    "title": "PAPI: Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System",
    "authors": "Yintao He (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Haiyu Mao (King's College London,ETH ZÃ¼rich), Christina Giannoula (University of Toronto,Vector Institute), Mohammad Sadrosadati (ETH ZÃ¼rich), Juan GÃ³mez-Luna (NVIDIA), Huawei Li (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Xiaowei Li (SKLP, Institute of Computing Technology, CAS,University of Chinese Academy of Sciences), Ying Wang (SKLP, Institute of Computing Technology, CAS), Onur Mutlu (ETH ZÃ¼rich)",
    "abstract": "Large language models (LLMs) are widely used for natural language understanding and text generation. An LLM model relies on a time-consuming step called LLM decoding to generate output tokens. Several prior works focus on improving the performance of LLM decoding using parallelism techniques, such as batching and speculative decoding. State-of-the-art LLM decoding has both compute-bound and memory-bound kernels. Some prior works statically identify and map these different kernels to a heterogeneous architecture consisting of both processing-in-memory (PIM) units and computation-centric accelerators. We observe that characteristics of LLM decoding kernels (e.g., whether or not a kernel is memory-bound) can change dynamically due to parameter changes to meet user and/or system demands, making (1) static kernel mapping to PIM units and computation-centric accelerators suboptimal, and (2) one-size-fits-all approach of designing PIM units inefficient due to a large degree of heterogeneity even in memory-bound kernels. In this paper, we aim to accelerate LLM decoding while considering the dynamically changing characteristics of the kernels involved. We propose PAPI (PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that exploits dynamic scheduling of compute-bound or memory-bound kernels to suitable hardware units. PAPI has two key mechanisms: (1) online kernel characterization to dynamically schedule kernels to the most suitable hardware units at runtime and (2) a PIM-enabled heterogeneous computing system that harmoniously orchestrates both computation-centric processing units and hybrid PIM units with different computing capabilities. Our experimental results on three broadly-used LLMs show that PAPI achieves 1.8$\\times$ and 11.1$\\times$ speedups over a state-of-the-art heterogeneous LLM accelerator and a state-of-the-art PIM-only LLM accelerator, respectively.",
    "link": "https://doi.org/10.1145/3676641.3716009",
    "conference_name": "ASPLOS",
    "session_title": "Processing in Memory",
    "date": "2025-03-30",
    "paper_id": "8c41dcde-8149-4185-a9ce-803774eab052"
  },
  {
    "title": "PIM is All You Need: A CXL-Enabled GPU-Free System for LLM Inference",
    "authors": "Yufeng Gu (University of Michigan), Alireza Khadem (University of Michigan), Sumanth Umesh (University of Michigan), Ning Liang (University of Michigan), Xavier Servot (ETH Zurich), Onur Mutlu (ETH Zurich), Ravi Iyer (Google), Reetuparna Das (University of Michigan)",
    "abstract": "Large Language Model (LLM) inference uses an autoregressive manner to generate one token at a time, which exhibits notably lower operational intensity compared to earlier Machine Learning (ML) models such as encoder-only transformers and Convolutional Neural Networks. At the same time, LLMs possess large parameter sizes and use key-value caches to store context information. Modern LLMs support context windows with up to 1 million tokens to generate versatile text, audio, and video content. A large key-value cache unique to each prompt requires a large memory capacity, limiting the inference batch size. Both low operational intensity and limited batch size necessitate a high memory bandwidth. However, contemporary hardware systems for ML model deployment, such as GPUs and TPUs, are primarily optimized for compute throughput. This mismatch challenges the efficient deployment of advanced LLMs and makes users pay for expensive compute resources that are poorly utilized for the memory-bound LLM inference tasks. We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which harnesses CXL memory expansion capabilities to accommodate substantial LLM sizes, and utilizes near-bank processing units to deliver high memory bandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable CXL network to support peer-to-peer and collective communication primitives across CXL devices. We implement various parallelism strategies to distribute LLMs across these devices. Compared to GPU baselines with maximum supported batch sizes and similar average power, CENT achieves 2.3$\\times$ higher throughput and consumes 2.3$\\times$ less energy. CENT enhances the Total Cost of Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
    "link": "https://doi.org/10.1145/3676641.3716267",
    "conference_name": "ASPLOS",
    "session_title": "Processing in Memory",
    "date": "2025-03-30",
    "paper_id": "a735eb90-88f0-421d-9a6b-5d46049cc0e5"
  },
  {
    "title": "CINM (Cinnamon): A Compilation Infrastructure for Heterogeneous Compute In-Memory and Compute Near-Memory Paradigms",
    "authors": "Asif Ali Khan (Technische UniversitÃ¤t Dresden), Hamid Farzaneh (Technische UniversitÃ¤t Dresden), Karl Friedrich Alexander Friebel (Technische UniversitÃ¤t Dresden), ClÃ©ment Fournier (Technische UniversitÃ¤t Dresden), Lorenzo Chelini (Intel), Jeronimo Castrillon (Technische UniversitÃ¤t Dresden)",
    "abstract": "The rise of data-intensive applications exposed the limitations of conventional processor-centric von-Neumann architectures that struggle to meet the off-chip memory bandwidth demand. Therefore, recent innovations in computer architecture advocate compute-in-memory (CIM) and compute-near-memory (CNM), non-von- Neumann paradigms achieving orders-of-magnitude improvements in performance and energy consumption. Despite significant technological breakthroughs in the last few years, the programmability of these systems is still a serious challenge. Their programming models are too low-level and specific to particular system implementations. Since such future architectures are predicted to be highly heterogenous, developing novel compiler abstractions and frameworks become necessary. To this end, we present CINM (Cinnamon), a first end-to-end compilation flow that leverages the hierarchal abstractions to generalize over different CIM and CNM devices and enable device-agnostic and device-aware optimizations. Cinnamon progressively lowers input programs and performs optimizations at each level in the lowering pipeline. To show its efficacy, we evaluate CINM on a set of benchmarks for the well-known UPMEM CNM system and the memristors-based CIM accelerators. We show that Cinnamon, supporting multiple hardware targets, generates high-performance code comparable to or better than state-of-the-art implementations.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p31-khan.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Processing in Memory",
    "date": "2025-03-30",
    "paper_id": "347065f9-f7e3-417a-81f4-c9407964dc1d"
  },
  {
    "title": "Toleo: Scaling Freshness to Tera-scale Memory Using CXL and PIM",
    "authors": "Juechu Dong (University of Michigan), Jonah Rosenblum (University of Michigan), Satish Narayanasamy (University of Michigan)",
    "abstract": "Trusted hardware's freshness guarantee ensures that an adversary cannot replay an old value in response to a memory read request. They rely on maintaining a version number for each cache block and ensuring their integrity using a Merkle tree. However, these existing solutions protect only a small amount of main memory (few MBs), as the extraneous memory accesses to the Merkle tree increase prohibitively with the protected memory size. We present Toleo, which uses trusted smart memory connected through a secure CXL IDE network to safely store version numbers. Toleo eliminates the need for an unscalable Merkle tree to protect the integrity of version numbers by instead using smart memory as the root of trust. Additionally, Toleo ensures version confidentiality which enables stealth versions that reduce the version storage overhead in half. Furthermore, in the absence of Merkle tree imposed constraints, we effectively exploit version locality at page granularity to compress version number by a factor of 240. These space optimizations make it feasible for one 168 GB Toleo smart memory device to provide freshness to a 28 TB CXL-expanded main memory pool in a rack server for a negligible performance overhead. We analyze the benefits of Toleo using several privacy-sensitive genomics, graph, generative AI, and database workloads.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p313-dong.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Processing in Memory",
    "date": "2025-03-30",
    "paper_id": "65e8f722-d7dd-46ff-9351-3f9292b66a21"
  },
  {
    "title": "Be CIM or Be Memory: A Dual-mode-aware DNN Compiler for CIM Accelerators",
    "authors": "Shixin Zhao (Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Yuming Li (Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Bing Li (Institute of Microelectronics, Chinese Academy of Sciences), Yintao He (Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Mengdi Wang (State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Yinhe Han (State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences), Ying Wang (State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences)",
    "abstract": "Computing-in-memory (CIM) architectures demonstrate superior performance over traditional architectures. To unleash the potential of CIM accelerators, many compilation methods have been proposed, focusing on application scheduling optimization specific to CIM. However, existing compilation methods often overlook CIM's capability to switch dynamically between compute and memory modes, which is crucial for accommodating the diverse memory and computational needs of real-world deep neural network architectures, especially the emerging large language models. To fill this gap, we introduce CMSwitch, a novel compiler to optimize resource allocation for CIM accelerators with adaptive mode-switching capabilities, thereby enhancing the performance of DNN applications. Specifically, our approach integrates the compute-memory mode switch into the CIM compilation optimization space by introducing a new hardware abstraction attribute. Then, we propose a novel compilation optimization pass that identifies the optimal network segment and the corresponding mode resource allocations using dynamic programming and mixed-integer programming. CMSwitch uses the tailored meta-operator to express the compilation result in a generalized manner. Evaluation results demonstrate that CMSwitch achieves an average speedup of 1.31$\\times$ compared to existing SOTA CIM compilation works, highlighting CMSwitch's effectiveness in fully exploiting the potential of CIM processors for a wide range of real-world DNN applications.",
    "link": "https://doi.org/10.1145/3676641.3716248",
    "conference_name": "ASPLOS",
    "session_title": "Processing in Memory",
    "date": "2025-03-30",
    "paper_id": "6deecbd8-f859-4349-844d-795786cfc9ff"
  },
  {
    "title": "TensorTEE: Unifying Heterogeneous TEE Granularity for Efficient Secure Collaborative Tensor Computing",
    "authors": "Husheng Han (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences), Xinyao Zheng (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences ), Yuanbo Wen (SKLP, Institute of Computing Technology, Chinese Academy of Sciences), Yifan Hao (SKLP, Institute of Computing Technology, Chinese Academy of Sciences), Erhu Feng (IPADS, Shanghai Jiao Tong University,Engineering Research Center for Domain-specific Operating Systems (MoE)), Ling Liang (Peking university), Jianan Mu (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences ), Xiaqing Li (SKLP, Institute of Computing Technology, Chinese Academy of Sciences), Tianyun Ma (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,Cambricon Technologies), Pengwei Jin (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,University of Chinese Academy of Sciences ), Xinkai Song (SKLP, Institute of Computing Technology, Chinese Academy of Sciences), Zidong Du (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,Shanghai Innovation Center for Processor Technologies, SHIC), Qi Guo (SKLP, Institute of Computing Technology, Chinese Academy of Sciences), Xing Hu (SKLP, Institute of Computing Technology, Chinese Academy of Sciences,ZGC LAB)",
    "abstract": "Heterogeneous collaborative computing with NPU and CPU has received widespread attention due to its substantial performance benefits. To ensure data confidentiality and integrity during computing, Trusted Execution Environments (TEE) is considered a promising solution because of its comparatively lower overhead. However, existing heterogeneous TEE designs are inefficient for collaborative computing due to fine and different memory granularities between CPU and NPU. 1) The cacheline granularity of CPU TEE intensifies memory pressure due to its extra memory access, and 2) the cacheline granularity MAC of NPU escalates the pressure on the limited memory storage. 3) Data transfer across heterogeneous enclaves relies on the transit of non-secure regions, resulting in cumbersome re-encryption and scheduling. To address these issues, we propose TensorTEE, a unified tensor-granularity heterogeneous TEE for efficient secure collaborative tensor computing. First, we virtually support tensor granularity in CPU TEE to eliminate the off-chip metadata access by detecting and maintaining tensor structures on-chip. Second, we propose tensor-granularity MAC management with predictive execution to avoid computational stalls while eliminating off-chip MAC storage and access. Moreover, based on the unified granularity, we enable direct data transfer without re-encryption and scheduling dilemmas. Our evaluation is built on enhanced Gem5 and a cycle-accurate NPU simulator. The results show that TensorTEE improves the performance of Large Language Model (LLM) training workloads by 4.0x compared to existing work and incurs only 2.1% overhead compared to non-secure training, offering a practical security assurance for LLM training.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p282-han.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Trust",
    "date": "2025-03-30",
    "paper_id": "253495c1-defe-44ad-9f78-0284795d81f9"
  },
  {
    "title": "MDPeek: Breaking Balanced Branches in SGX with Memory Disambiguation Unit Side Channels",
    "authors": "Chang Liu (Tsinghua University), Shuaihu Feng (Zhongguancun Laboratory), Yuan Li (Zhongguancun Laboratory), Dongsheng Wang (Tsinghua University), Wenjian He (Huawei Technologies Co., Ltd.), Yongqiang Lyu (Tsinghua University), Trevor E. Carlson (National University of Singapore)",
    "abstract": "In recent years, control flow attacks targeting Intel SGX have attracted significant attention from the security community due to their potent capacity for information leakage. Although numerous software-based defenses have been developed to counter these attacks, many remain inadequate in fully addressing other, yet-to-be-discovered side channels. In this paper, we introduce MDPeek, a novel control flow attack targeting secret-dependent branches in SGX. To circumvent existing defenses, such as microarchitectural state flushing and branch balancing, we exploit a new leakage source, the Memory Disambiguation Unit (MDU). We present the first comprehensive reverse engineering on the MDU’s enable and update logic. Based on our detailed analysis, we develop a methodology to identify vulnerable workloads in real-world applications. We demonstrate the effectiveness of MDPeek with end-to-end attacks on the latest versions of three SGX-secured applications, including Libjpeg, MbedTLS and WolfSSL. In addition, we propose a low-overhead mitigation technique, store-to-load coupling, which provides a 7 × latency reduction compared to naive techniques like serialization and load aligning.",
    "link": "https://doi.org/10.1145/3676641.3716004",
    "conference_name": "ASPLOS",
    "session_title": "Trust",
    "date": "2025-03-30",
    "paper_id": "15f5a666-3dc2-43e1-a04c-b368b324fa3f"
  },
  {
    "title": "Accelerating Number Theoretic Transform with Multi-GPU Systems for Efficient Zero Knowledge Proof",
    "authors": "Zhuoran Ji (School of Cyber Science and Technology, Shandong University), Jianyu Zhao (School of Cyber Science and Technology, Shandong University), Peimin Gao (School of Cyber Science and Technology, Shandong University), Xiangkai Yin (School of Cyber Science and Technology, Shandong University), Lei Ju (Quan Cheng Laboratory)",
    "abstract": "Zero-knowledge proofs validate statements without revealing any information, pivotal for applications such as verifiable outsourcing and digital currencies. However, their broad adoption is limited by the prolonged proof generation times, mainly due to two operations: Multi-Scalar Multiplication (MSM) and Number Theoretic Transform (NTT). While MSM has been efficiently accelerated using multi-GPU systems, NTT has not, due to the high inter-GPU communication overhead incurred by its permutation data access pattern.\nThis paper identifies the necessity of multi-GPU NTT support for end-to-end proof generation. It introduces UniNTT, an NTT algorithm tailored for multi-GPU systems. The data access pattern of NTT incurs communication across all levels of the multi-GPU hierarchy (i.e., warp, thread block, GPU, and multi-GPU), complicating the implementation of multi-GPU NTT. To this end, UniNTT proposes a novel, overhead-free decomposition approach that recursively decomposes an NTT into smaller NTTs, enabling all hierarchy levels execute the same NTT computations at different scales. It promotes a uniform design of NTT optimizations based on an abstract hardware model, which are then tailored and applied to different levels of the hierarchy. UniNTT not only simplifies the optimization process but also shows that optimizations typically specific to one level can also be effectively generalized to others. Experiments show that UniNTT achieves an average 4.26× speedup compared to leading NTT implementations when both are executed on an 8-GPU system.",
    "link": "https://doi.org/10.1145/3669940.3707241",
    "conference_name": "ASPLOS",
    "session_title": "Trust",
    "date": "2025-03-30",
    "paper_id": "847814c8-1b1f-4053-a4ed-3d36ef4e2c20"
  },
  {
    "title": "BatchZK: A Fully Pipelined GPU-Accelerated System for Batch Generation of Zero-Knowledge Proofs",
    "authors": "Tao Lu (Zhejiang University,National University of Singapore), Yuxun Chen (Zhejiang University), Zonghui Wang (Zhejiang University), Xiaohang Wang (Zhejiang University), Wenzhi Chen (Zhejiang University), Jiaheng Zhang (National University of Singapore)",
    "abstract": "Zero-knowledge proof (ZKP) is a cryptographic primitive that enables one party to prove the validity of a statement to other parties without disclosing any secret information. With its widespread adoption in applications such as blockchain and verifiable machine learning, the demand for generating zero-knowledge proofs has increased dramatically. In recent years, considerable efforts have been directed toward developing GPU-accelerated systems for proof generation. However, these previous systems only explored efficiently generating a single proof by reducing latency rather than batch generation to provide high throughput.\nWe propose a fully pipelined GPU-accelerated system for batch generation of zero-knowledge proofs. Our system has three features to improve throughput. First, we design a pipelined approach that enables each GPU thread to continuously execute its designated proof generation task without being idle. Second, our system supports recent efficient ZKP protocols with their computational modules: sum-check protocol, Merkle tree, and linear-time encoder. We customize these modules to fit our pipelined execution. Third, we adopt a dynamic loading method for the data required for proof generation, reducing the required device memory. Moreover, multi-stream technology enables the overlap of data transfers and GPU computations, reducing overhead caused by data exchanges between host and device memory.\nWe implement our system and evaluate it on various GPU cards. The results show that our system achieves more than 259.5× higher throughput compared to state-of-the-art GPU-accelerated systems. Moreover, we deploy our system in the verifiable machine learning application, where our system generates 9.52 proofs per second, successfully achieving sub-second proof generation for the first time in this field.",
    "link": "https://doi.org/10.1145/3669940.3707270",
    "conference_name": "ASPLOS",
    "session_title": "Trust",
    "date": "2025-03-30",
    "paper_id": "d338953b-48e7-431a-bf8b-e5cab42fd2f6"
  },
  {
    "title": "UniZK: Accelerating Zero-Knowledge Proof with Unified Hardware and Flexible Kernel Mapping",
    "authors": "Cheng Wang (Xi'an Jiaotong University,Institute for Interdisciplinary Information Core Technology), Mingyu Gao (Tsinghua University,Shanghai Qi Zhi Institute)",
    "abstract": "Zero-knowledge proof (ZKP) is an important cryptographic tool that sees wide applications in real-world scenarios where privacy must be protected, including privacy-preserving blockchains and zero-knowledge machine learning. Existing ZKP acceleration approaches using GPUs, FPGAs, and ASICs focus only on classic protocols that rely on expensive elliptic curve arithmetics. Emerging ZKP protocols based on hash functions can greatly reduce the algorithmic complexity, but they also introduce much more diverse computation kernels that cannot be efficiently handled by a single accelerator chip if dedicated units for each kernel are used. Our approach is to leverage a unified hardware architecture that is able to efficiently support the common primitives in ZKP, and then use smart mapping strategies to flexibly map various kernels to such hardware while ensuring high resource utilization. We design UniZK as such a ZKP accelerator, with a systolic-array-based hardware architecture enhanced with extra local links and a new vector processing mode. We propose novel mapping strategies to support diverse kernels including number theoretic transforms, hash functions, and general polynomial computations. UniZK provides 97x and 46x speedups on average compared to the CPU and GPU implementations of the same protocols, and is also 840x faster than previous ZKP accelerators using different protocols.",
    "link": "https://doi.org/10.1145/3669940.3707228",
    "conference_name": "ASPLOS",
    "session_title": "Trust",
    "date": "2025-03-30",
    "paper_id": "85b18fa0-0300-4ddc-b7b1-d0f17c953d1f"
  },
  {
    "title": "RTL Verification for Secure Speculation Using Contract Shadow Logic",
    "authors": "Qinhan Tan (Princeton University), Yuheng Yang (Massachusetts Institute of Technology), Thomas Bourgeat (Ãcole Polytechnique FÃ©dÃ©rale de Lausanne), Sharad Malik (Princeton University), Mengjia Yan (Massachusetts Institute of Technology)",
    "abstract": "Modern out-of-order processors face speculative execution attacks. Despite various proposed software and hardware mitigations to prevent such attacks, new attacks keep arising from unknown vulnerabilities. Thus, a formal and rigorous evaluation of the ability of hardware designs to deal with speculative execution attacks is urgently desired. This paper proposes a formal verification technique called Contract Shadow Logic that can considerably improve RTL verification scalability while being applicable to different defense mechanisms. In this technique, we leverage computer architecture design insights to improve verification performance for checking security properties formulated as software-hardware contracts for secure speculation. Our verification scheme is accessible to computer architects and requires minimal formal-method expertise. We evaluate our technique on multiple RTL designs, including three out-of-order processors. The experimental results demonstrate that our technique exhibits a significant advantage in finding attacks on insecure designs and deriving complete proofs on secure designs, when compared to the baseline and two state-of-the-art verification schemes, LEAVE and UPEC.",
    "link": "https://doi.org/10.1145/3669940.3707243",
    "conference_name": "ASPLOS",
    "session_title": "Verification & Reliability",
    "date": "2025-03-30",
    "paper_id": "3d3c4fb4-a2ca-462a-aa22-939fb607676c"
  },
  {
    "title": "ElasticMiter: Formally Verified Dataflow Circuit Rewrites",
    "authors": "Ayatallah Elakhras (EPFL), Jiahui Xu (ETH Zurich), Martin Erhart (ETH Zurich), Paolo Ienne (EPFL), Lana Josipovic (ETH Zurich)",
    "abstract": "Dataflow circuits have been studied for decades as a way to implement both asynchronous and synchronous designs, and, more recently, have attracted attention as the target of high-level synthesis (HLS) compilers. Yet, little is known about mechanisms to systematically transform and optimize the datapaths of the obtained circuits into functionally equivalent but simpler ones. The main challenge is that of equivalence verification: The latency-insensitive nature of dataflow circuits is incompatible with the standard notion of sequential equivalence, which prevents the direct usage of standard sequential equivalence verification strategies and hinders the development of formally verified dataflow circuit transformations in HLS. In this paper, we devise a generic framework for verifying the equivalence of latency-insensitive circuits. To showcase the practical usefulness of our verification framework, we develop a graph rewriting system that systematically transforms dataflow circuits into simpler ones. We employ our framework to verify our graph rewriting patterns and thus prove that the obtained circuits are equivalent to the original ones. Our work is the first to formally verify dataflow circuit transformations and is a foundation for building formally verified dataflow HLS compilers.",
    "link": "https://doi.org/10.1145/3676641.3715993",
    "conference_name": "ASPLOS",
    "session_title": "Verification & Reliability",
    "date": "2025-03-30",
    "paper_id": "73529709-2450-46b4-a430-c925653b3e6c"
  },
  {
    "title": "Robustness Verification for Checking Crash Consistency of Non-volatile Memory",
    "authors": "Zhilei Han (School of Software, Tsinghua University), Fei He (School of Software, Tsinghua University)",
    "abstract": "The emerging non-volatile memory (NVM) technologies provide competitive performance with DRAM and ensure data persistence in the event of system failure. However, it exhibits weak behaviour in terms of the order in which stores are committed to NVMs, and therefore requires extra efforts from developers to flush pending writes. To ensure correctness of this error-prone task, it is crucial to develop a rigid method to check crash consistency of programs running on NVM devices. Most existing solutions are testing-based and rely on user guidance to dynamically detect such deficiencies. In this paper, we present a fully automated method to verify robustness, a newly established property for ensuring crash consistency of such programs. The method is based on the observation that, reachability of a post-crash non-volatile state under a given pre-crash execution can be reduced to validity of the pre-crash execution with additional ordering constraints. Our robustness verification algorithm employs a search-based framework to explore all partial executions and states, and checks if any non-volatile state is reachable under certain pre-crash execution. Once a reachable non-volatile state is obtained, we further check its reachability under memory consistency model. The algorithm is implemented in a prototype tool PMVerify that leverages symbolic encoding of the program and utilizes an SMT solver to efficiently explore all executions and states. The method is integrated into the DPLL(T) framework to optimize the robustness checking algorithm. Experiments on the PMDK example benchmark show that PMVerify is competitive with the state-of-the-art dynamic tool, PSan, in terms of robustness violation detection.",
    "link": "https://doi.org/10.1145/3669940.3707269",
    "conference_name": "ASPLOS",
    "session_title": "Verification & Reliability",
    "date": "2025-03-30",
    "paper_id": "3b12e8ed-67b9-447f-ae26-6f6f91166ad7"
  },
  {
    "title": "Proactive Runtime Detection of Aging-Related Silent Data Corruptions: A Bottom-Up Approach",
    "authors": "Jiacheng Ma (University of Michigan), Majd Ganaiem (Technion - Israel Institute of Technology), Madeline Burbage (University of Washington), Theo Gregersen (University of Washington), Rachel McAmis (University of Washington), Freddy Gabbay (The Hebrew University of Jerusalem), Baris Kasikci (University of Washington,Google)",
    "abstract": "Recent advancements in semiconductor process technologies have unveiled the susceptibility of hardware circuits to reliability issues, especially those related to transistor aging. Transistor aging gradually degrades gate performance, eventually causing hardware to behave incorrectly. Such misbehaving hardware can result in silent data corruptions (SDCs) in software---a type of failure that comes without logs or exceptions, but causes miscomputing instructions, bitflips, and broken cache coherency. Alas, while design efforts can be made to mitigate transistor aging, complete elimination of this problem during design and fabrication cannot be guaranteed. This emerging challenge calls for a mechanism that not only detects potentially aged hardware in the field, but also triggers software mitigations at application runtime. We propose Vega, a novel workflow that allows efficient detection of aging-related failures at software runtime. Vega leverages the well-studied gate-level modeling of aging effects to identify susceptible signal propagation paths that could fail due to transistor aging. It then utilizes formal verification techniques to generate short test cases that activate these paths and detect any failure within them. Vega integrates the test cases into a user application by directly fusing them together, or by packaging the test cases into a library that the application can invoke. We demonstrate our proposed techniques on the arithmetic logic unit and floating-point unit of a RISC-V CPU. We show that Vega generates effective test cases and integrates them into applications with an average of 0.8% performance overhead.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p220-ma.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Verification & Reliability",
    "date": "2025-03-30",
    "paper_id": "09a1c41a-9bdb-4a35-a2f6-ceff12254b4c"
  },
  {
    "title": "Hardware Sentinel: Protecting Software Applications from Hardware Silent Data Corruptions",
    "authors": "Rhea Dutta (Meta Platforms, Inc.), Harish Dattatraya Dixit (Meta Platforms, Inc.), Rik Van Riel (Meta Platforms, Inc.), Gautham Vunnam (Meta Platforms, Inc.), Sriram Sankar (Meta Platforms, Inc.)",
    "abstract": "Silent Data Corruptions (SDCs) pose a significant challenge in large-scale infrastructures, affecting data center applications unpredictably and reducing service reliability. Primarily caused by silicon defects, traditional hardware testing methods are insufficient to prevent SDC propagation. SDCs are influenced by various factors, including data randomization, workload characteristics, environmental conditions, and aging, necessitating top-down approaches from the application layer. In this paper, we introduce Hardware Sentinel, a novel framework that detects SDCs through typical software failure indicators such as segmentation faults, core dumps, application crashes, and logs. We have validated our framework in a large-scale data center fleet, across diverse application, kernel, and hardware configurations, achieving a high success rate of SDC detection. Hardware Sentinel has uncovered novel instances of SDCs, surpassing the detection capabilities of published testing techniques. Our analysis of over 6 years' worth of application and system failure data within a large-scale infrastructure has successfully identified hundreds of defective CPUs that triggered SDCs. Notably, the Hardware Sentinel flow increases effective coverage over existing hardware-testing methods like Fleetscanner (out-of-production testing) by 1.74x and Ripple (in-production testing) by 1.92x. We share the top kernel exceptions with the highest correlation to silent data corruption failures. We present results spanning 7 CPU generations from multiple semiconductor manufacturers, 13 large-scale workloads, and 27 data center regions, providing insights into the trade-offs involved in detection and fleet deployment.",
    "link": "https://doi.org/10.1145/3676641.3716258",
    "conference_name": "ASPLOS",
    "session_title": "Verification & Reliability",
    "date": "2025-03-30",
    "paper_id": "58f2ed61-fd10-4d81-ac57-ff7263303927"
  },
  {
    "title": "Virgo: Cluster-level Matrix Unit Integration in GPUs for Scalability and Energy Efficiency",
    "authors": "Hansung Kim (University of California, Berkeley), Ruohan Richard Yan (University of California, Berkeley), Joshua You (University of California, Berkeley), Tieliang Vamber Yang (NVIDIA Corporation), Yakun Sophia Shao (University of California, Berkeley)",
    "abstract": "Modern GPUs incorporate specialized matrix units such as Tensor Cores to accelerate GEMM operations, which are central to deep learning workloads. However, existing matrix unit designs are tightly coupled to the SIMT core, restricting operation size due to register file capacity and bandwidth constraints. Such a limitation in scalability makes it difficult to simultaneously improve compute throughput and energy efficiency in GPUs. To address this challenge, we propose Virgo, a GPU microarchitecture that integrates dedicated matrix units at the SIMT core cluster level. By decoupling the matrix unit from the SIMT core, Virgo eliminates scalability constraints imposed by the core microarchitecture. Consequently, Virgo increases operation granularity at the hardware level, reducing energy overhead from core instruction processing. Physical disaggregation also enables a unified matrix unit design and offloading both operand and accumulator accesses from the register file, improving data reuse and energy efficiency. Furthermore, this disaggregation supports efficient concurrent execution of the SIMT core and matrix unit, optimizing mapping for fused DNN workloads. Our evaluations using synthesizable RTL demonstrate that Virgo achieves 67.3% and 24.2% reduction in on-chip active power consumption, compared to the baseline Ampere-style and Hopper-style core-coupled designs.",
    "link": "https://doi.org/10.1145/3676641.3716281",
    "conference_name": "ASPLOS",
    "session_title": "GPGPU",
    "date": "2025-03-30",
    "paper_id": "590b25d2-ac8c-422d-a4e7-ae5b0e7ea28a"
  },
  {
    "title": "Towards Unified Analysis of GPU Consistency",
    "authors": "Haining Tong (University of Helsinki), Natalia Gavrilenko (Huawei Dresden Research Center), Hernan Ponce de Leon (Huawei Dresden Research Center), Keijo Heljanko (University of Helsinki,Helsinki Institute for Information Technology)",
    "abstract": "After more than 30 years of research, there is a solid understanding of the consistency guarantees given by CPU systems. Unfortunately, the same is not yet true for GPUs. The growing popularity of general purpose GPU programming has been a call for action which industry players like Nvidia and Khronos have answered by formalizing their Ptx and Vulkan consistency models. These models give precise answers to questions about program's correctness. However, interpreting them still requires a level of expertise that escapes most developers, and the current tool support is insufficient. To remedy this, we translated and integrated the Ptx and Vulkan models into the Dartagnan verification tool. This makes Dartagnan the first analysis tool for multiple GPU consistency models that can analyze real GPU code. During the validation of the translated models, we discovered two bugs in the original Ptx and Vulkan consistency models.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p329-tong.pdf",
    "conference_name": "ASPLOS",
    "session_title": "GPGPU",
    "date": "2025-03-30",
    "paper_id": "7f86de64-144c-4c4a-bb2b-11d0bc4876f5"
  },
  {
    "title": "Aqua: Network-Accelerated Memory Offloading for LLMs in Scale-Up GPU Domains",
    "authors": "Abhishek Vijaya Kumar (Cornell University), Gianni Antichi (Politecnico di Milano), Rachee Singh (Cornell University)",
    "abstract": "Inference on large-language models (LLMs) is constrained by GPU memory capacity. A sudden increase in the number of inference requests to a cloud-hosted LLM can deplete GPU memory, leading to contention between multiple prompts for limited resources. Modern LLM serving engines deal with the challenge of limited GPU memory using admission control, which causes them to be unresponsive during request bursts. We propose that preemptive scheduling of prompts in time slices is essential for ensuring responsive LLM inference, especially under conditions of high load and limited GPU memory. However, preempting prompt inference incurs a high paging overhead, which reduces inference throughput. We present Aqua, a GPU memory management framework that significantly reduces the overhead of paging inference state, achieving both responsive and high-throughput inference even under bursty request patterns. We evaluate Aqua by hosting several state-of-the-art large generative ML models of different modalities on servers with 8 Nvidia H100 80G GPUs. Aqua improves the responsiveness of LLM inference by 20X compared to the state-of-the-art and improves LLM inference throughput over a single long prompt by 4X.",
    "link": "https://doi.org/10.1145/3676641.3715983",
    "conference_name": "ASPLOS",
    "session_title": "GPGPU",
    "date": "2025-03-30",
    "paper_id": "5e8e7808-6a24-4f95-b756-8d10e9a866bd"
  },
  {
    "title": "Optimizing Datalog for the GPU",
    "authors": "Yihao Sun (Syracuse University), Ahmedur Rahman Shovon (University of Illinois, Chicago), Thomas Gilray (Washington State University), Sidharth Kumar (University of Illinois, Chicago), Kristopher Micinski (Syracuse University)",
    "abstract": "Modern Datalog engines (e.g., LogicBlox, Soufflé, ddlog) enable their users to write declarative queries which compute recursive deductions over extensional facts, leaving high-performance operationalization (query planning, semi-naïve evaluation, and parallelization) to the engine. Such engines form the backbone of modern high-throughput applications in static analysis, network monitoring, and social-media mining. In this paper, we present a methodology for implementing a modern in-memory Datalog engine on data center GPUs, allowing us to achieve significant (up to 45×) gains compared to Soufflé (a modern CPU-based engine) on context-sensitive points-to analysis of httpd. We present GPUlog, a Datalog engine backend that implements iterated relational algebra kernels over a novel range-indexed data structure we call the hash-indexed sorted array (HISA). HISA combines the algorithmic benefits of incremental range-indexed relations with the raw computation throughput of operations over dense data structures. Our experiments show that GPUlog is significantly faster than CPU-based Datalog engines, while achieving favorable memory footprint compared to contemporary GPU-based joins.",
    "link": "https://doi.org/10.1145/3669940.3707274",
    "conference_name": "ASPLOS",
    "session_title": "GPGPU",
    "date": "2025-03-30",
    "paper_id": "85dddf06-c808-4653-a3d7-32bbc19cd1dc"
  },
  {
    "title": "Velosiraptor: Code Synthesis for Memory Translation",
    "authors": "Reto Achermann (University of British Columbia), Em Chu (JuliaHub), Ryan Mehri (Replit), Ilias Karimalis (University of British Columbia), Margo Seltzer (University of British Columbia)",
    "abstract": "Security is among the top concerns of operating system (OS) developers. A secure runtime environment relies on the OS to correctly configure the memory hardware on which it runs. This is mission-critical as it provides essential security-relevant features and abstractions that ensure the integrity and isolation of untrusted applications running alongside each other. Configuring a platform's memory hardware is not a one-off effort as designers constantly develop new mechanisms for translation and protection with different features and means of configuration. Adapting the OS code to the new hardware is not only a manual, repetitive and time-consuming task, it may also introduce subtle, but security-critical bugs that break security and isolation guarantees.\nWe present Velosiraptor, a system that automatically generates correct, low-level OS code that programs the memory hardware of a machine. Velosiraptor leverages software synthesis techniques and exploits the domain specificity of the problem to make the synthesis process efficient. With Velosiraptor, developers write only a high-level description of the memory hardware's mapping behavior and OS environment.\nThe Velosiraptor toolchain transforms this specification into a verified implementation that can be linked directly with the rest of the operating system. Incorporating the OS environment into this process allows porting an OS to new hardware platforms without worrying about writing code to configure the memory hardware. We can also use the same specification to generate hardware components. This enables research in new translation mechanisms, freeing up OS developers from manually writing OS code.",
    "link": "https://doi.org/10.1145/3676641.3711998",
    "conference_name": "ASPLOS",
    "session_title": "Memory Management",
    "date": "2025-03-30",
    "paper_id": "d1bd4243-72fb-42a9-ac4a-09a040239c3e"
  },
  {
    "title": "vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention",
    "authors": "Ramya Prabhu (Microsoft Research), Ajay Nayak (Indian Institute of Science), Jayashree Mohan (Microsoft Research), Ramachandran Ramjee (Microsoft Research), Ashish Panwar (Microsoft Research)",
    "abstract": "PagedAttention is a popular approach for dynamic memory allocation in LLM serving systems. It enables on-demand allocation of GPU memory to mitigate KV cache fragmentation -- a phenomenon that crippled the batch size (and consequently throughput) in prior systems. However, in trying to allocate physical memory at runtime, PagedAttention ends up changing the virtual memory layout of the KV cache from contiguous to non-contiguous. Such a design leads to non-trivial programming and performance overheads. We present vAttention -- an approach that mitigates fragmentation in physical memory while retaining the contiguity of KV cache in virtual memory. We achieve this by decoupling the allocation of virtual and physical memory using CUDA virtual memory management APIs. We also introduce various LLM-specific optimizations to address the limitations of CUDA virtual memory support. Overall, vAttention is a simpler, portable, and performant alternative to PagedAttention: it supports various attention kernels out-of-the-box and improves LLM serving throughput by up to 1.23x compared to the use of PagedAttention-based kernels of FlashAttention and FlashInfer.",
    "link": "https://doi.org/10.1145/3669940.3707256",
    "conference_name": "ASPLOS",
    "session_title": "Memory Management",
    "date": "2025-03-30",
    "paper_id": "1826452f-6379-476b-a3a3-13687b3bf577"
  },
  {
    "title": "Virtuoso: Enabling Fast and Accurate Virtual Memory Research via an Imitation-based Operating System Simulation Methodology",
    "authors": "Konstantinos Kanellopoulos (ETH ZÃ¼rich), Konstantinos Sgouras (ETH ZÃ¼rich), Nisa Bostanci (ETH ZÃ¼rich), Andreas Kosmas Kakolyris (ETH ZÃ¼rich), Berkin Kerim Konar (ETH ZÃ¼rich), Rahul Bera (ETH ZÃ¼rich), Mohammad Sadrosadati (ETH ZÃ¼rich), Rakesh Kumar (Norwegian University of Science and Technology (NTNU)), Nandita Vijaykumar (University of Toronto), Onur Mutlu (ETH ZÃ¼rich)",
    "abstract": "The unprecedented growth in data demand from emerging applications has turned virtual memory (VM) into a major performance bottleneck. Researchers explore new hardware/OS co-designs to optimize VM across diverse applications and systems. To evaluate such designs, researchers rely on various simulation methodologies to model VM components.Unfortunately, current simulation tools (i) either lack the desired accuracy in modeling VM's software components or (ii) are too slow and complex to prototype and evaluate schemes that span across the hardware/software boundary. We introduce Virtuoso, a new simulation framework that enables quick and accurate prototyping and evaluation of the software and hardware components of the VM subsystem. The key idea of Virtuoso is to employ a lightweight userspace OS kernel, called MimicOS, that (i) accelerates simulation time by imitating only the desired kernel functionalities, (ii) facilitates the development of new OS routines that imitate real ones, using an accessible high-level programming interface, (iii) enables accurate and flexible evaluation of the application- and system-level implications of VM after integrating Virtuoso to a desired architectural simulator. We integrate Virtuoso into five diverse architectural simulators, each specializing in different aspects of system design, and heavily enrich it with multiple state-of-the-art VM schemes. Our validation shows that Virtuoso ported on top of Sniper, a state-of-the-art microarchitectural simulator, models the memory management unit of a real high-end server-grade page fault latency of a real Linux kernel with high accuracy . Consequently, Virtuoso models the IPC performance of a real high-end server-grade CPU with 21% higher accuracy than the baseline version of Sniper. The source code of Virtuoso is freely available at https://github.com/CMU-SAFARI/Virtuoso.",
    "link": "https://doi.org/10.1145/3676641.3716027",
    "conference_name": "ASPLOS",
    "session_title": "Memory Management",
    "date": "2025-03-30",
    "paper_id": "dc1b0ca4-35a8-415f-916d-d7f2bb3e0d9c"
  },
  {
    "title": "Instruction-Aware Cooperative TLB and Cache Replacement Policies",
    "authors": "Dimitrios Chasapis (Barcelona Supercomputing Center (BSC)), Georgios Vavouliotis (Unaffiliated), Daniel A. JimÃ©nez (Texas A&M University), Marc Casas (Barcelona Supercomputing Center (BSC),Universitat PolitÃ¨cnica de Catalunya (UPC))",
    "abstract": "Modern server and data center applications are characterized not only by big datasets, but also by large instruction footprints that incur frequent cache and Translation Lookaside Buffer (TLB) misses due to instruction accesses. Instruction TLB misses are particularly problematic since they cause pipeline stalls that significantly harm performance.\nThis paper proposes cooperative last-level TLB (STLB) and L2 cache (L2C) replacement policies targeting workloads with large instruction footprints. We propose the Instruction Translation Prioritization (iTP), an STLB replacement policy that maximizes the number of instruction hits in the STLB at the expense of increasing data page walks. To compensate for the increase of data page walks, we propose the extended Page Table Prioritization (xPTP), a new L2C replacement policy that amplifies the benefits of iTP by effectively reducing L2C misses due to data page walks. Our proposal, iTP+xPTP, combines iTP at STLB and xPTP at L2C. In addition, iTP+xPTP employs an adaptive mechanism that switches between xPTP and LRU policies at L2C based on the pressure placed on the virtual memory subsystem. Our proposal improves single-core geometric mean performance by 18.9% over a baseline that uses the LRU replacement policy at both STLB and L2C across a set of contemporary server workloads. Under SMT co-location, the corresponding performance uplift is 11.4%. Finally, we show that our proposal outperforms the state-of-the-art STLB and cache replacement policies.",
    "link": "https://doi.org/10.1145/3669940.3707247",
    "conference_name": "ASPLOS",
    "session_title": "Memory Management",
    "date": "2025-03-30",
    "paper_id": "d778f077-cf72-4389-8ec1-0867bcd70f54"
  },
  {
    "title": "Litmus: Fair Pricing for Serverless Computing",
    "authors": "Qi Pei (Computer Science / Watson School, The State University of New York at Binghamton), Yipeng Wang (Intel Lab., Intel), Seunghee Shin (Computer Science / Watson School, The State University of New York at Binghamton)",
    "abstract": "Serverless computing has emerged as a market-dominant paradigm in modern cloud computing, benefiting both cloud providers and tenants. While service providers can optimize their machine utilization, tenants only need to pay for the resources they use. To maximize resource utilization, these serverless systems co-run numerous short-lived functions, bearing frequent system condition shifts. When the system gets overcrowded, a tenant's function may suffer from disturbing slowdowns. Ironically, tenants also incur higher costs during these slowdowns, as commercial serverless platforms determine costs proportional to their execution times. This paper argues that cloud providers should compensate tenants for losses incurred when the server is over-provisioned. However, estimating tenants' losses is challenging without pre-profiled information about their functions. Prior studies have indicated that assessing tenant losses leads to heavy overheads. As a solution, this paper introduces a new pricing model that offers discounts based on the machine's state while presuming the tenant's loss under that state. To monitor the machine state accurately, Litmus pricing frequently conducts Litmus tests, an effective and lightweight solution for measuring system congestion. Our experiments show that Litmus pricing can accurately gauge the impact of system congestion and offer nearly ideal prices, with only a 0.2% price difference on average, in a heavily congested system.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p155-pei.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Serverless Computing",
    "date": "2025-03-30",
    "paper_id": "f067be67-a840-4eea-8a8e-6ff1fc64b5c0"
  },
  {
    "title": "Concurrency-Informed Orchestration for Serverless Functions",
    "authors": "Qichang Liu (University of Virginia), Yue Cheng (University of Virginia), Haiying Shen (University of Virginia), Ao Wang (Alibaba Group), Bharathan Balaji (Amazon)",
    "abstract": "Cold start delays are a main pain point for today’s FaaS (Function-as-a-Service) platforms. A widely used mitigation strategy is keeping recently invoked function containers alive in memory to enable warm starts with minimal overhead. This paper identifies new challenges that state-of-the-art FaaS keep-alive policies neglect. These challenges are caused by concurrent function invocations, a common FaaS workload behavior. First, concurrent requests present a trade-off between reusing busy containers (delayed warm starts) versus cold-starting containers. Second, concurrent requests cause imbalanced evictions of containers that will be reused shortly thereafter. To tackle the challenges, we propose a novel serverless function container orchestration algorithm called CIDRE . CIDRE makes informed decisions to speculatively choose between a delayed warm start and a cold start under concurrency-driven function scaling. CIDRE uses both fine-grained container-level and coarse-grained concurrency information to make balanced eviction decisions. We evaluate CIDRE extensively using two production FaaS workloads. Results show that CIDRE reduces the cold start ratio and the average invocation overhead by up to 75.1% and 39.3% compared to state-of-the-art function keep-alive policies.",
    "link": "https://doi.org/10.1145/3676641.3716253",
    "conference_name": "ASPLOS",
    "session_title": "Serverless Computing",
    "date": "2025-03-30",
    "paper_id": "cb2bbe57-d443-4c26-ba18-4e96cb277119"
  },
  {
    "title": "Dilu: Enabling GPU Resourcing-on-Demand for Serverless DL Serving via Introspective Elasticity",
    "authors": "Cunchi Lv (ICT, CAS,UCAS), Xiao Shi (ICT, CAS,Nanjing Institute of InforSuperBahn), Zhengyu Lei (ICT, CAS,UCAS), Jinyue Huang (ICT, CAS, UCAS), Wenting Tan (ICT, CAS), Xiaohui Zheng (ICT, CAS), Xiaofang Zhao (ICT, CAS,IICT, Suzhou, CAS)",
    "abstract": "Serverless computing, with its ease of management, auto-scaling, and cost-effectiveness, is widely adopted by deep learning (DL) applications. DL workloads, especially with large language models, require substantial GPU resources to ensure QoS. However, it is prone to produce GPU fragments (e.g., 15\\%-94\\%) in serverless DL systems due to the dynamicity of workloads and coarse-grained static GPU allocation mechanisms, gradually eroding the profits offered by serverless elasticity. Different from classical serverless systems that only scale horizontally, we present introspective elasticity (IE), a fine-grained and adaptive two-dimensional co-scaling mechanism to support GPU resourcing-on-demand for serverless DL tasks. Based on this insight, we build Dilu, a cross-layer and GPU-based serverless DL system with IE support. First, Dilu provides multi-factor profiling for DL tasks with efficient pruning search methods. Second, Dilu adheres to the resourcing-complementary principles in scheduling to improve GPU utilization with QoS guarantees. Third, Dilu adopts an adaptive 2D co-scaling method to enhance the elasticity of GPU provisioning in real time. Evaluations show that it can dynamically adjust the resourcing of various DL functions with low GPU fragmentation (10\\%-46\\% GPU defragmentation), high throughput (up to 1.8$\\times$ inference and 1.1$\\times$ training throughput increment) and QoS guarantees (11\\%-71\\% violation rate reduction), compared to the SOTA baselines.",
    "link": "https://doi.org/10.1145/3669940.3707251",
    "conference_name": "ASPLOS",
    "session_title": "Serverless Computing",
    "date": "2025-03-30",
    "paper_id": "91b7cc6d-1334-4ede-8ca9-90823424b157"
  },
  {
    "title": "Medusa: Accelerating Serverless LLM Inference with Materialization",
    "authors": "Shaoxun Zeng (Tsinghua University), Minhui Xie (Tsinghua University), Shiwei Gao (Tsinghua University), Youmin Chen (Tsinghua University), Youyou Lu (Tsinghua University)",
    "abstract": "Serverless is a promising paradigm to provide scalable, cost-efficient, and easy-to-use model inference services. However, the cold start of model inference functions requires loading models to the devices, which incurs high latencies and undermines the benefits of serverless computing. In LLMs, things get even worse since two extra stages are introduced: a KV cache initialization stage that profiles and anticipates memory reservation for KV cache, and a capturing stage which dynamically constructs CUDA graphs for different batch sizes. Both stages are paramount to the inference performance, but become the main culprit of cold start latency.\nThis paper proposes Medusa to mitigate the long cold start latency through state materialization. Instead of dynamic profiling and construction in the runtime, Medusa materializes the CUDA graphs as well as the information needed by the KV cache initialization in the offline phase, and restores them efficiently in the online phase. Medusa further introduces two novel techniques -- offline-online cooperated parameters restoration and triggering-kernels enhanced kernel address restoration -- to tackle non-deterministic issues in CUDA graphs. Medusa successfully materializes and restores CUDA graphs across 10 models (with a total of 139364 CUDA graph nodes), and reduces the latency of model loading by 42.5%. Under real-world LLM inference workloads, Medusa reduces the tail latency of the time to first token (TTFT) by 53.0%.",
    "link": "https://doi.org/10.1145/3669940.3707285",
    "conference_name": "ASPLOS",
    "session_title": "Serverless Computing",
    "date": "2025-03-30",
    "paper_id": "64fb917d-e2b7-4c10-a7fe-fda920cda9e3"
  },
  {
    "title": "MaxEmbed: Maximizing SSD bandwidth utilization for huge embedding models serving",
    "authors": "Ruwen Fan (Tsinghua University), Minhui Xie (Tsinghua University), Haodi Jiang (Tsinghua University), Youyou Lu (Tsinghua University)",
    "abstract": "Deep learning recommendation models (DLRMs) have gained widespread application across search, advertising, and e-commerce. Still, DLRMs present notable challenges as they depend heavily on large embedding tables to represent sparse features in recommendation systems. This raises concerns about both memory capacity and cost. Solid-state drives (SSDs) offer a cost-effective solution with a significantly larger capacity, but they introduce read amplification issues because of the mismatch between embedding size and SSD read granularity. Prior SSD embedding storage systems aim to tackle these challenges by employing hypergraph partitioning to co-locate co-appearing embeddings onto the same SSD page, alleviating read amplification. However, this approach has a drawback as it divides embeddings into completely disjoint clusters, limiting potential combinations between embeddings. In response to this limitation, we introduce MaxEmbed. Capitalizing on the extensive storage capacity of SSDs, Max-Embed effectively mines relationships between storage combinations of embeddings with replication, thereby enhancing the effective bandwidth of SSDs. Additionally, MaxEmbed incorporates a corresponding online service module for embedding query request handling, leveraging two key optimizations to reduce the overhead brought by replication. Our evaluations demonstrate that MaxEmbed boosts SSD embedding serving throughput by up to 18.7% under various settings.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p188-fan.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Solid State Storage",
    "date": "2025-03-30",
    "paper_id": "ac86c86b-da47-439f-a3cf-d1fbaf937b48"
  },
  {
    "title": "AnyKey: A Key-Value SSD for All Workload Types",
    "authors": "Chanyoung Park (Hanyang University), Jungho Lee (Hanyang University), Chun-Yi Liu (Micron Technology Inc.), Kyungtae Kang (Hanyang Univeristy), Mahmut Taylan Kandemir (Pennsylvania State University), Wonil Choi (Hanyang University)",
    "abstract": "Key-value solid-state drives (KV-SSDs) are considered as a potential storage solution for large-scale key-value (KV) store applications. Unfortunately, the existing KV-SSD designs are tuned for a specific type of workload, namely, those in which the size of the values are much larger than the size of the keys. Interestingly, there also exists another type of workload, in practice, in which the sizes of keys are relatively large. We re-evaluate the current KV-SSD designs using such unexplored workloads and document their significantly-degraded performance. Observing that the performance problem stems from the increased size of the metadata, we subsequently propose a novel KV-SSD design, called AnyKey, which prevents the size of the metadata from increasing under varying sizes of keys. Our detailed evaluation using a wide range of real-life workloads indicates that AnyKey outperforms the state-of-the-art KV-SSD design under different types of workloads with varying sizes of keys and values.",
    "link": "https://doi.org/10.1145/3669940.3707279",
    "conference_name": "ASPLOS",
    "session_title": "Solid State Storage",
    "date": "2025-03-30",
    "paper_id": "b873889c-af7e-4d2e-af4a-d5c7d1caecea"
  },
  {
    "title": "Simplifying and Accelerating NOR Flash I/O Stack for RAM-Restricted Microcontrollers",
    "authors": "Hao Huang (Harbin Institute of Technology, Shenzhen), Yanqi Pan (Harbin Institute of Technology, Shenzhen), Wen Xia (Harbin Institute of Technology, Shenzhen), Xiangyu Zou (Harbin Institute of Technology, Shenzhen), Darong Yang (Harbin Institute of Technology, Shenzhen), Liang Shi (East China Normal University), Hongwei Du (Harbin Institute of Technology, Shenzhen)",
    "abstract": "NOR flash has been increasingly popular for RAM-restricted microcontrollers due to its small package, high reliability, etc. To satisfy RAM restrictions, existing NOR flash file systems migrate their functionalities, i.e., block-level data organization and wear leveling (WL), from RAM to NOR flash. However, such fine-grained block-level management introduces frequent index updates and NOR flash scanning, leading to severe I/O amplification, which further deteriorates as they are decoupled in existing NOR flash file systems.\nTo address the problem, we propose NF2FS, a NOR flash-friendly file system. Our key insight is that applications running on NOR flash usually have (1) small file sizes, therefore block-based data organization can be converted to flat file layout (for fast file/dir scanning); (2) deterministic I/O patterns, thereby WL can be achieved through coarse file swapping. As a result, NF2FS relaxes data organization and WL to a coarse-grained file level, which are then cooperated within the file system. We implement NF2FS in FreeRTOS using a range of techniques, including the all-logging layout, along with efficient layout management approaches such as dual bitmap space allocator and soft-update-like crash consistency. Experiments suggest that NF2FS significantly outperforms existing works and can prevent quick NOR flash wear-out.",
    "link": "https://doi.org/10.1145/3676641.3716272",
    "conference_name": "ASPLOS",
    "session_title": "Solid State Storage",
    "date": "2025-03-30",
    "paper_id": "cdaa6516-409e-4b2b-8ae8-cdaa749f1b5d"
  },
  {
    "title": "A Software Caching Runtime for Embedded NVRAM Systems",
    "authors": "Harrison Williams (Virginia Tech), Matthew Hicks (Virginia Tech)",
    "abstract": "Increasingly sophisticated low-power microcontrollers are at the heart of millions of IoT and edge computing deployments, with developers pushing large-scale data collection, processing, and inference to end nodes. Advanced workloads on resource-constrained systems depend on emerging technologies to meet performance and lifetime demands. High-performance Non-Volatile RAMs (NVRAMs) are one such technology enabling a new class of systems previously made impossible by memory limitations, including ultra-low-power designs using program state non-volatility and sensing systems storing and processing large blocks of data. Unfortunately, existing NVRAM significantly underperforms SRAM's access latency/energy cost and flash's read performance---condemning systems dependent on NVRAM to pay a steep energy and time penalty for software execution. We observe that this performance penalty stems predominately from instruction fetches into NVRAM, which represent >75% of memory accesses in typical embedded software. To eliminate this performance bottleneck, we propose SwapRAM, a new operating model for NVRAM-based platforms which repurposes underutilized SRAM as an instruction cache, maximizing the proportion of accesses directed towards higher-performance SRAM. SwapRAM consists of a set of compile-time code transformations and a runtime management system that transparently and dynamically copies code into SRAM throughout execution, with an extensible logic to delay eviction of hot code. Across nine embedded benchmarks running on a real FRAM platform, SwapRAM's software-based design increases execution speed by up to 46% (average 26%) and reduces energy consumption by up to 36% (average 24%) compared to a baseline system using the existing hardware cache.",
    "link": "https://simba.cs.stonybrook.edu/pdfs/p1-williams.pdf",
    "conference_name": "ASPLOS",
    "session_title": "Solid State Storage",
    "date": "2025-03-30",
    "paper_id": "d682a0ca-0025-4ef4-8df0-2d20b458320e"
  }
]