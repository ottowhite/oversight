[
  {
    "title": "Autobahn: Seamless High Speed BFT",
    "authors": "Neil Giridharan (UC Berkeley), Florian Suri-Payer (Cornell University), Ittai Abraham (Intel), Lorenzo Alvisi (Cornell University), Natacha Crooks (UC Berkeley)",
    "abstract": "Today's practical, high performance Byzantine Fault Tolerant (BFT) consensus protocols operate in the partial synchrony model. However, existing protocols are inefficient when deployments are indeed partially synchronous. They deliver either low latency during fault-free, synchronous periods (good intervals) or robust recovery from events that interrupt progress (blips). At one end, traditional, view-based BFT protocols optimize for latency during good intervals, but, when blips occur, can suffer from performance degradation (hangovers) that can last beyond the return of a good interval. At the other end, modern DAG-based BFT protocols recover more gracefully from blips, but exhibit lackluster latency during good intervals. To close the gap, this work presents Autobahn, a novel high-throughput BFT protocol that offers both low latency and seamless recovery from blips. By combining a highly parallel asynchronous data dissemination layer with a low-latency, partially synchronous consensus mechanism, Autobahn (i) avoids the hangovers incurred by traditional BFT protocols and (ii) matches the throughput of state of the art DAG-based BFT protocols while cutting their latency in half, matching the latency of traditional BFT protocols.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695942",
    "session_title": "Welcome and Session 1: Distributed Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "SWARM: Replicating Shared Disaggregated-Memory Data in No Time",
    "authors": "Antoine Murat (EPFL), ClÃ©ment Burgelin (EPFL), Athanasios Xygkis (Oracle Labs), Igor Zablotchi (Mysten Labs), Marcos K. Aguilera (VMware Research Group), Rachid Guerraoui (EPFL)",
    "abstract": "Memory disaggregation is an emerging data center architecture that improves resource utilization and scalability. Replication is key to ensure the fault tolerance of applications, but replicating shared data in disaggregated memory is hard. We propose SWARM (Swift WAit-free Replication in disaggregated Memory), the first replication scheme for in-disaggregated-memory shared objects to provide (1) single-roundtrip reads and writes in the common case, (2) strong consistency (linearizability), and (3) strong liveness (wait-freedom). SWARM makes two independent contributions. The first is Safe-Guess, a novel wait-free replication protocol with single-roundtrip operations. The second is In-n-Out, a novel technique to provide conditional atomic update and atomic retrieval of large buffers in disaggregated memory in one roundtrip. Using SWARM, we build SWARM-KV, a low-latency, strongly consistent and highly available disaggregated key-value store. We evaluate SWARM-KV and find that it has marginal latency overhead compared to an unreplicated key-value store, and that it offers much lower latency and better availability than FUSEE, a state-of-the-art replicated disaggregated key-value store.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695945",
    "session_title": "Welcome and Session 1: Distributed Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Efficient Reproduction of Fault-Induced Failures in Distributed Systems with Feedback-Driven Fault Injection",
    "authors": "Jia Pan (Johns Hopkins University), Haoze Wu (Johns Hopkins University), Tanakorn Leesatapornwongsa (Microsoft Research), Suman Nath (Microsoft Research), Peng Huang (University of Michigan)",
    "abstract": "Debugging a failure usually requires reproducing it first. This can be hard for failures in production distributed systems, where bugs are exposed only by some unusual faulty events. While fault injection testing becomes popular, existing solutions are designed for bug finding. They are ineffective and inefficient to reproduce a specific failure during debugging. We explore a new type of fault injection technique for quickly reproducing a given fault-induced production failure in distributed systems. We present a tool, Anduril, that uses static causal analysis and a novel feedback-driven algorithm to quickly search the enormous fault space for the root-cause fault and timing. We evaluate Anduril on 22 real-world complex fault-induced failures from five large-scale distributed systems. Anduril reproduced all failures by identifying and injecting the root-cause faults at the right time, in a median of 8 minutes.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695979",
    "session_title": "Welcome and Session 1: Distributed Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "If At First You Donât Succeed, Try, Try, Again...? Insights and LLM-informed Tooling for Detecting Retry Bugs in Software Systems",
    "authors": "Bogdan Alexandru Stoica (University of Chicago), Utsav Sethi (University of Chicago), Yiming Su (University of Chicago), Cyrus Zhou (University of Chicago), Shan Lu (Microsoft Research), Jonathan Mace (Microsoft Research), Madanlal Musuvathi (Microsoft Research), Suman Nath (Microsoft Research)",
    "abstract": "Retry—the re-execution of a task on failure—is a common mechanism to enable resilient software systems. Yet, despite its commonality and long history, retry remains difficult to implement and test. Guided by our study of real-world retry issues, we propose a novel suite of static and dynamic techniques to detect retry problems in software. We find that the ad-hoc nature of retry implementation in software systems poses challenges for traditional program analysis but can be well handled by large language models; we also find that careful repurposing existing unit tests can, along with fault injection, expose various types of retry problems.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695971",
    "session_title": "Welcome and Session 1: Distributed Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Tiered Memory Management: Access Latency is the Key!",
    "authors": "Midhul Vuppalapati (Cornell University), Rachit Agarwal (Cornell University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695968",
    "session_title": "Session 2: Memory",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Fast & Safe IO Memory Protection",
    "authors": "Benny Rubin (Cornell University), Saksham Agarwal (UIUC), Qizhe Cai (Cornell University), Rachit Agarwal (Cornell University)",
    "abstract": "IO Memory protection mechanisms prevent malicious and/or buggy IO devices from executing errant transfers into memory. Modern servers achieve this using an IOMMU—IO devices operate on virtual addresses, and IOMMU translates virtual addresses to physical addresses (potentially speeding up translations using a cache called IOTLB) before executing memory transfers. Despite their importance, design of memory protection mechanisms that can provide strong safety properties while achieving high performance has remained elusive. Indeed, recent studies from production datacenters demonstrate that ine � ciencies within state-of-the-art memory protection mechanisms result in signi � cant throughput degradation, orders-of-magnitude tail latency in � ation, and violation of isolation guarantees. WepresentFast&Safe(F&S),asimplemodi � cationtoexist-ing memory protection mechanisms that enables them to provide the strongest safety property, and yet, near-completely eliminates their overheads. The key insight in F&S design is that, rather than solely focusing on minimizing IOTLB miss rates, we should focus on reducing the cost of each IOTLB miss.Wedemonstratethatthischangeofperspectiveenablesa simpleF&Sdesignthatrequiresnomodi � cations in host hardware and minimal modi � cations within the operating system.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695943",
    "session_title": "Session 2: Memory",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "CHIME: A Cache-Efficient and High-Performance Hybrid Index on Disaggregated Memory",
    "authors": "Xuchuan Luo (Fudan University), Jiacheng Shen (Duke Kunshan University), Pengfei Zuo (Huawei Cloud), Xin Wang (Fudan University), Michael R. Lyu (The Chinese University of Hong Kong), Yangfan Zhou (Fudan University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695959",
    "session_title": "Session 2: Memory",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Aceso: Achieving Efficient Fault Tolerance in Memory-Disaggregated Key-Value Stores",
    "authors": "Zhisheng Hu (The Chinese University of Hong Kong), Pengfei Zuo (Huawei Cloud), Yizou Chen (The Chinese University of Hong Kong), Chao Wang (The Chinese University of Hong Kong), Junliang Hu (The Chinese University of Hong Kong), Ming-Chang Yang (The Chinese University of Hong Kong)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695951",
    "session_title": "Session 2: Memory",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Reducing Energy Bloat in Large Model Training",
    "authors": "Jae-Won Chung (University of Michigan), Yile Gu (University of Washington), Insu Jang (University of Michigan), Luoxi Meng (University of California, San Diego), Nikhil Bansal (University of Michigan), Mosharaf Chowdhury (University of Michigan)",
    "abstract": "Training large AI models on numerous GPUs consumes a massive amount of energy, making power delivery one of the largest limiting factors in building and operating datacenters for AI workloads. However, we observe that not all energy consumed during training directly contributes to end-to-end throughput; a significant portion can be removed without slowing down training. We call this portion energy bloat. In this work, we identify two independent sources of energy bloat in large model training and propose Perseus, a training system that mitigates both. To do this, Perseus obtains the time--energy tradeoff frontier of a large model training job using an efficient graph cut-based algorithm, and schedules computation energy consumption across time to reduce both types of energy bloat. Evaluation on large models, including GPT-3 and Bloom, shows that Perseus reduces the energy consumption of large model training by up to 30% without any throughput loss or hardware modification.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695970",
    "session_title": "Session 3: Deep Learning and Training",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Uncovering Nested Data Parallelism and Data Reuse in DNN Computation with FractalTensor",
    "authors": "Siran Liu (Peking University), Chengxiang Qi (University of Chinese Academy of Sciences), Ying Cao (Microsoft Research Asia), Chao Yang (Peking University), Weifang Hu (Huazhong University of Science and Technology), Xuanhua Shi (Huazhong University of Science and Technology), Fan Yang (Microsoft Research Asia), Mao Yang (Microsoft Research Asia)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695961",
    "session_title": "Session 3: Deep Learning and Training",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Enabling Parallelism Hot Switching for Efficient Training of Large Language Models",
    "authors": "Hao Ge (Peking University), Fangcheng Fu (Peking University), Haoyang Li (Peking University), Xuanyu Wang (Peking University), Sheng Lin (Peking University), Yujie Wang (Peking University), Xiaonan Nie (Peking University), Hailin Zhang (Peking University), Xupeng Miao (Purdue University), Bin Cui (Peking University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695969",
    "session_title": "Session 3: Deep Learning and Training",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Tenplex: Dynamic Parallelism for Deep Learning using Parallelizable Tensor Collections",
    "authors": "Marcel WagenlÃ¤nder (Imperial College London), Guo Li (Imperial College London), Bo Zhao (Aalto University), Luo Mai (University of Edinburgh), Peter Pietzuch (Imperial College London)",
    "abstract": "Deep learning (DL) jobs use multi-dimensional parallelism, i.e. combining data, model, and pipeline parallelism, to use large GPU clusters efficiently. Long-running jobs may experience changes to their GPU allocation: (i) resource elasticity during training adds or removes GPUs; (ii) hardware maintenance may require redeployment on different GPUs; and (iii) GPU failures force jobs to run with fewer devices. Current DL frameworks tie jobs to a set of GPUs and thus lack support for these scenarios. In particular, they cannot change the multi-dimensional parallelism of an already-running job in an efficient and model-independent way. We describe Scalai, a state management library for DL systems that enables jobs to change their parallelism dynamically after the GPU allocation is updated at runtime. Scalai achieves this through a new abstraction, a parallelizable tensor collection (PTC), that externalizes the job state during training. After a GPU change, Scalai uses the PTC to transform the job state: the PTC repartitions the dataset state under data parallelism and exposes it to DL workers through a virtual file system; and the PTC obtains the model state as partitioned checkpoints and transforms them to reflect the new parallelization configuration. For efficiency, Scalai executes PTC transformations in parallel with minimum data movement between workers. Our experiments show that Scalai enables DL jobs to support dynamic parallelization with low overhead.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695975",
    "session_title": "Session 3: Deep Learning and Training",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "ReCycle: Resilient Training of Large DNNs using Pipeline Adaptation",
    "authors": "Swapnil Gandhi (Stanford University), Mark Zhao (Stanford University), Athinagoras Skiadopoulos (Stanford University), Christos Kozyrakis (Stanford University)",
    "abstract": "Training large Deep Neural Network (DNN) models requires thousands of GPUs over the course of several days or weeks. At this scale, failures are frequent and can have a big impact on training throughput. Utilizing spare GPU servers to mitigate performance loss becomes increasingly costly as model sizes grow. ReCycle is a system designed for efficient DNN training in the presence of failures, without relying on spare servers. It exploits the inherent functional redundancy in distributed training systems -- where servers across data-parallel groups store the same model parameters -- and pipeline schedule bubbles within each data-parallel group. When servers fails, ReCycle dynamically re-routes micro-batches to data-parallel peers, allowing for uninterrupted training despite multiple failures. However, this re-routing can create imbalances across pipeline stages, leading to reduced training throughput. To address this, ReCycle introduces two key optimizations that ensure re-routed micro-batches are processed within the original pipeline schedule's bubbles. First, it decouples the backward pass into two phases: one for computing gradients for the input and another for calculating gradients for the parameters. Second, it avoids synchronization across pipeline stages by staggering the optimizer step. Together, these optimizations enable adaptive pipeline schedules that minimize or even eliminate training throughput degradation during failures. We describe a prototype for ReCycle and show that it achieves high training throughput under multiple failures, outperforming recent proposals for fault-tolerant training such as Oobleck and Bamboo by up to $1.46\\times$ and $1.64\\times$, respectively.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695960",
    "session_title": "Session 3: Deep Learning and Training",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "OZZ: Identifying Kernel Out-of-Order Concurrency Bugs with In-Vivo Memory Access Reordering",
    "authors": "Dae R. Jeong (Georgia Tech), Yewon Choi (KAIST), Byoungyoung Lee (Seoul National University), Insik Shin (KAIST), Youngjin Kwon (KAIST)",
    "abstract": "Kernel concurrency bugs are notoriously difficult to identify, while their consequences severely threaten the reliability and security of the entire system. Especially in the kernel, developers should consider not only locks but also memory barriers to prevent out-of-order execution from breaking the correctness of concurrent execution. Incorrect use of memory barriers may cause non-intuitive concurrency bugs that manifest due to out-of-order execution, which we refer to as O O O bugs. This paper aims to identify O O O bugs in the kernel. We devise a mechanism to emulate out-of-order execution while kernel code is executed, called OEMU. Inspired by how a processor reorders memory accesses, OEMU makes the subtle and non-deterministic behavior of out-of-order execution systematically controllable. Based on OEMU, we propose O ZZ , a new testing tool designed to effectively identify kernel O O O bugs. The key feature of O ZZ is its ability to deterministically control both out-of-order execution and concurrent execution caused by thread interleavings, enabling comprehensive testing of their combined effects. Our evaluation shows that OEMU is effective in reproducing previously-reported kernel O O O bugs, demonstrating its strong capability of controlling out-of-order execution. Furthermore, with O ZZ , we identify 11 new O O O bugs in the latest version of the Linux kernel, subsequently confirmed and patched by kernel developers.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695944",
    "session_title": "Session 4: Kernels",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Fast, Flexible, and Practical Kernel Extensions",
    "authors": "Kumar Kartikeya Dwivedi (EPFL), Rishabh Iyer (UC Berkeley), Sanidhya Kashyap (EPFL)",
    "abstract": "The ability to safely extend OS kernel functionality is a long-standing goal in OS design, with the widespread use of the eBPF framework in Linux and Windows demonstrating the benefits of such extensibility. However, existing solutions for kernel extensibility (including eBPF) are limited and constrain users either in the extent of functionality that they can offload to the kernel or the performance overheads incurred by their extensions. We present KFlex: a new approach to kernel extensibility that strikes an improved balance between the expressivity and performance of kernel extensions. To do so, KFlex separates the safety of kernel-owned resources ( e . g ., kernel memory) from the safety of extension-specific resources ( e . g ., extension memory). This separation enables KFlex to use distinct, bespoke mechanisms to enforce each safety property—automated verification and lightweight runtime checks, respectively—which enables the offload of diverse functionality while incurring low runtime overheads. We realize KFlex in the context of Linux. We demonstrate that KFlex enables users to offload functionality that cannot be offloaded today and provides significant end-to-end performance benefits for applications. Several of KFlex’s proposed mechanisms have been upstreamed into the Linux kernel mainline, with efforts ongoing for full integration.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695950",
    "session_title": "Session 4: Kernels",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Skyloft: A General High-Efficient Scheduling Framework in User Space",
    "authors": "Yuekai Jia (Tsinghua University), Kaifu Tian (Tsinghua University), Yuyang You (Tsinghua University), Yu Chen (Quan Cheng Laboratory and Tsinghua University), Kang Chen (Tsinghua University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695973",
    "session_title": "Session 4: Kernels",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Fast Core Scheduling with Userspace Process Abstraction",
    "authors": "Jiazhen Lin (Tsinghua University), Youmin Chen (Tsinghua University), Shiwei Gao (Tsinghua University), Youyou Lu (Tsinghua University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695976",
    "session_title": "Session 4: Kernels",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "LazyLog: A New Shared Log Abstraction for Low-Latency Applications",
    "authors": "Xuhao Luo (University of Illinois Urbana-Champaign), Shreesha G Bhat (University of Illinois Urbana-Champaign), Jiyu Hu (University of Illinois Urbana-Champaign), Ramnatthan Alagappan (University of Illinois Urbana-Champaign and VMware Research), Aishwarya Ganesan (University of Illinois Urbana-Champaign and VMware Research)",
    "abstract": ". Shared logs offer linearizable total order across storage shards. However, they enforce this order eagerly upon ingestion, leading to high latencies. We observe that in many modern shared-log applications, while linearizable ordering is necessary, it is not required eagerly when ingesting data but only later when data is consumed. Further, readers are naturally decoupled in time from writers in these applications. Based on this insight, we propose LazyLog, a novel shared log abstraction. LazyLog lazily binds records (across shards) to linearizable global positions and enforces this before a log position can be read. Such lazy ordering enables low ingestion latencies. Given the time decoupling, LazyLog can establish the order well before reads arrive, minimizing overhead upon reads. We build two LazyLog systems that provide linearizable total order across shards. Our experiments show that LazyLog systems deliver significantly lower latencies than conventional, eager-ordering shared logs.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695983",
    "session_title": "Session 5: File and Storage Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "BIZA: Design of Self-Governing Block-Interface ZNS AFA for Endurance and Performance",
    "authors": "Shushu Yi (Peking University), Shaocong Sun (Peking University), Li Peng (Peking University), Yingbo Sun (Peking University), Ming-Chang Yang (The Chinese University of Hong Kong), Zhichao Cao (Arizona State University), Qiao Li (Xiamen University), Myoungsoo Jung (KAIST and Panmnesia), Ke Zhou (Huazhong University of Science and Technology), Jie Zhang (Peking University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695953",
    "session_title": "Session 5: File and Storage Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Morph: Efficient File-Lifetime Redundancy Management for Cluster File Systems",
    "authors": "Timothy Kim (Carnegie Mellon University), Sanjith Athlur (Carnegie Mellon University), Saurabh Kadekodi (Google), Francisco Maturana (Carnegie Mellon University), Dax Delvira (Georgia Tech), Arif Merchant (Google), Gregory R. Ganger (Carnegie Mellon University), K. V. Rashmi (Carnegie Mellon University)",
    "abstract": "Many data services tune and change redundancy configurations of files over their lifetimes to address changes in data temperature and latency requirements. Unfortunately, changing redundancy configs ( transcode ) is IO-intensive. The Morph cluster file system introduces new transcode-efficient redundancy schemes to minimize overheads as files progress through lifetime phases. For newly ingested data, commonly stored via 3-way replication, Morph introduces a hybrid redundancy scheme that combines a replica with an erasure-coded (EC) stripe, reducing both ingest IO and capacity over-heads while enabling free transcode to EC by deleting replicas. For subsequent transcodes to wider, more space-efficient EC configs, Morph exploits Convertible Codes , which minimize data read for EC transcode, and introduces new block placement policies to maximize their effectiveness. Analysis of data ingest and transcode activity in Google storage clusters shows the current massive IO load and the potential savings from Morph’s approach—transcode IO can be reduced by over 95%, and total ingest+transcode IO can be reduced by 50–60% while also reducing capacity overheads for newly ingested data by 20%. Experiments evaluating a Morph implementation in HDFS show that these benefits can be realized in a real system without hidden increases in complexity, tail latency, or degraded-mode latency.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695981",
    "session_title": "Session 5: File and Storage Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Reducing Cross-Cloud/Region Costs with the Auto-Configuring MACARON Cache",
    "authors": "Hojin Park (Carnegie Mellon University), Ziyue Qiu (Carnegie Mellon University, Uber), Gregory R. Ganger (Carnegie Mellon University), George Amvrosiadis (Carnegie Mellon University)",
    "abstract": "An increasing demand for cross-cloud and cross-region data access is bringing forth challenges related to high data transfer costs and latency. In response, we introduce Macaron, an auto-configuring cache system designed to minimize cost for remote data access. A key insight behind Macaron is that cloud cache size is tied to cost, not hardware limits, shifting the way we think about cache design and eviction policies. Macaron dynamically configures cache size and utilizes a mix of cloud storage types, in order to adapt to workload changes and reduce cloud costs. We demonstrate that Macaron can reduce cross-cloud workload costs by 65% and cross-region costs by 67%, mainly by reducing outgoing data transfer and by leveraging object storage alongside DRAM to reduce cache capacity cost.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695972",
    "session_title": "Session 5: File and Storage Systems",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Dirigent: Lightweight Serverless Orchestration",
    "authors": "Lazar CvetkoviÄ (ETH Zurich), FranÃ§ois Costa (ETH Zurich), Mihajlo Djokic (ETH Zurich and IBM Research Europe), Michal Friedman (ETH Zurich), Ana Klimovic (ETH Zurich)",
    "abstract": "While Function as a Service (FaaS) platforms can initialize function sandboxes on worker nodes in 10-100s of milliseconds, the latency to schedule functions in real FaaS clusters can be orders of magnitude higher. The current approach of building FaaS cluster managers on top of legacy orchestration systems (e.g., Kubernetes) leads to high scheduling delays when clusters experience high sandbox churn, which is common for FaaS. Generic cluster managers use many hierarchical abstractions and internal components to manage and reconcile cluster state with frequent persistent updates. This becomes a bottleneck for FaaS since the cluster state frequently changes as sandboxes are created on the critical path of requests. Based on our root cause analysis of performance issues in existing FaaS cluster managers, we propose Dirigent, a clean-slate system architecture for FaaS orchestration with three key principles. First, Dirigent optimizes internal cluster manager abstractions to simplify state management. Second, it eliminates persistent state updates on the critical path of function invocations, leveraging the fact that FaaS abstracts sandbox locations from users to relax exact state reconstruction guarantees. Finally, Dirigent runs monolithic control and data planes to minimize internal communication overheads and maximize throughput. We compare Dirigent to state-of-the-art FaaS platforms and show that Dirigent reduces 99th percentile per-function scheduling latency for a production workload by 2.79x compared to AWS Lambda. Dirigent can spin up 2500 sandboxes per second at low latency, which is 1250x more than Knative.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695966",
    "session_title": "Session 6: Serverless",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Unifying Serverless and Microservice Workloads with SigmaOS",
    "authors": "Ariel Szekely (MIT), Adam Belay (MIT), Robert Morris (MIT), M. Frans Kaashoek (MIT)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695947",
    "session_title": "Session 6: Serverless",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Caribou: Fine-Grained Geospatial Shifting of Serverless Applications for Sustainability",
    "authors": "Viktor Gsteiger (ETH ZÃ¼rich), Pin Hong (Daniel) Long (University of British Columbia), Yiran (Jerry) Sun (University of British Columbia), Parshan Javanrood (University of British Columbia), Mohammad Shahrad (University of British Columbia)",
    "abstract": "Sustainability in computing is critical as environmental concerns rise. The cloud industry’s carbon footprint is significant and rapidly growing. We show that dynamic geospatial shifting of cloud workloads to regions with lower carbon emission energy sources, particularly for more portable cloud workloads such as serverless applications, has a high potential to lower operational carbon emissions. To make the case, we build a comprehensive framework called Caribou that offloads serverless workflows across geo-distributed regions. Caribou requires no change in the application logic, nor on the provider side. It dynamically determines the best deployment plans, automatically (re-) deploys functions to appropriate regions, and redirects traffic to new endpoints. In reducing operational carbon through fine-grained, function-level offloading, Caribou does not undermine standard metrics such as performance and cost. We show how this approach can reduce the carbon footprint by an average of 22.9% to 66.6% across the North American continent. We demonstrate how a detailed specification of location constraints (e.g., to ensure compliance of one stage) can allow emission reductions for workflows (e.g., by offloading other stages). By showcasing the feasibility of carbon-aware geospatial application deployment, Caribou aims to push the boundaries of system techniques available to curtail cloud carbon emissions and provide a framework for future research.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695954",
    "session_title": "Session 6: Serverless",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "TrEnv: Transparently Share Serverless Execution Environments Across Different Functions and Nodes",
    "authors": "Jialiang Huang (Tsinghua University, Alibaba Group), MingXing Zhang (Tsinghua University), Teng Ma (Alibaba Group), Zheng Liu (Zhejiang University, Alibaba Group), Sixing Lin (Tsinghua University, Wuhan University), Kang Chen (Tsinghua University), Jinlei Jiang (Tsinghua University), Xia Liao (Tsinghua University), Yingdi Shan (Tsinghua University), Ning Zhang (Alibaba Group), Mengting Lu (Alibaba Group), Tao Ma (Alibaba Group), Haifeng Gong (Intel), YongWei Wu (Tsinghua University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695967",
    "session_title": "Session 6: Serverless",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Verus: A Practical Foundation for Systems Verification",
    "authors": "Andrea Lattuada (MPI-SWS), Travis Hance (Carnegie Mellon University), Jay Bosamiya (Microsoft Research), Matthias Brun (ETH Zurich), Chanhee Cho (Carnegie Mellon University), Hayley LeBlanc (University of Texas at Austin), Pranav Srinivasan (University of Michigan), Reto Achermann (University of British Columbia), Tej Chajed (University of Wisconsin-Madison), Chris Hawblitzel (Microsoft Research), Jon Howell (VMware Research), Jacob R. Lorch (Microsoft Research), Oded Padon (Weizmann Institute of Science), Bryan Parno (Carnegie Mellon University)",
    "abstract": "Formal verification is a promising approach to eliminate bugs at compile time, before they ship. Indeed, our community has verified a wide variety of system software. However, much of this success has required heroic developer effort, relied on bespoke logics for individual domains, or sacrificed expressiveness for powerful proof automation. Building on prior work on Verus, we aim to enable faster, cheaper verification of rich properties for realistic systems. We do so by integrating and optimizing the best choices from prior systems, tuning our design to overcome barriers encountered in those systems, and introducing novel techniques. We evaluate Verus’s effectiveness with a wide variety of case-study systems, including distributed systems, an OS page table, a library for NUMA-aware concurrent data structure replication, a crash-safe storage system, and a concurrent memory allocator, together comprising 6.1K lines of implementation and 31K lines of proof. Verus verifies code 3–61 × faster and with less effort than the state of the art. Our results suggest that Verus offers a platform for exploring the next frontiers in system-verification research. Because Verus builds on Rust, Verus is also positioned for wider use in production by developers who have already adopted Rust in the pursuit of more robust systems.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695952",
    "session_title": "Session 7: Verification and Compilers",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Practical Verification of System-Software Components Written in Standard C",
    "authors": "Can Cebeci (EPFL), Yonghao Zou (EPFL), Diyu Zhou (EPFL), George Candea (EPFL), ClÃ©ment Pit-Claudel (EPFL)",
    "abstract": "Systems code is challenging to verify, because it uses constructs (like raw pointers, pointer arithmetic, and bit twid-dling) that are hard for tools to reason about. Existing approaches either sacrifice programmer friendliness, by demanding significant manual effort and verification expertise, or generality, by restricting the programming language or requiring that the code adapt to the verification tool. We identify a new point in the design space of verifiers that enables a different set of trade-offs, which prove practical in the context of verifying critical system components. We use several novel techniques to develop the TPot verification framework, targeted specifically at systems code written in C. With TPot , developers verify critical components they implement in standard, unrestricted C, using a C-based language to write “proof-oriented tests” (POTs) that specify the desired behavior. TPot then runs the POTs to prove correctness. We evaluate TPot on 6 different systems-code bases, and show that it can verify them at the same level as 4 state-of-the-art verifiers, while consistently reducing the annotation burden, ranging up to more than 3 × . TPot does not require these code bases to be adapted for verification and achieves verification times compatible with typical continuous-integration workflows. TPot is open-source and freely available at https://github. com/dslab-epfl/tpot .",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695980",
    "session_title": "Session 7: Verification and Compilers",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Icarus: Trustworthy Just-In-Time Compilers with Symbolic Meta-Execution",
    "authors": "Naomi Smith (UCSD), Abhishek Sharma (UT Austin), John Renner (UCSD), David Thien (UCSD), Fraser Brown (CMU), Hovav Shacham (UT Austin), Ranjit Jhala (UCSD), Deian Stefan (UCSD)",
    "abstract": "Just-in-time (JIT) compilers make JavaScript run efficiently by replacing slow JavaScript interpreter code with fast machine code. However, this efficiency comes at a cost: bugs in JIT compilers can completely subvert all language-based (memory) safety guarantees, and thereby introduce cata-strophic exploitable vulnerabilities. We present Icarus: a new framework for implementing JIT compilers that are automatically, formally verified to be safe, and which can then be converted to C++ that can be linked into browser runtimes. Crucially, we show how to build a JIT with Icarus such that verifying the JIT implementation statically ensures the security of all possible programs that the JIT could ever generate at run-time, via a novel technique called symbolic meta-execution that encodes the behaviors of all possible JIT-generated programs as a single Boogie meta-program which can be efficiently verified by SMT solvers. We evaluate Icarus by using it to re-implement components of Firefox’s JavaScript JIT. We show that Icarus can scale up to expressing complex JITs, quickly detects real-world JIT bugs and verifies fixed versions, and yields C++ code that is as fast as hand-written code.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695949",
    "session_title": "Session 7: Verification and Compilers",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "SilvanForge: A Schedule Guided Retargetable Compiler for Decision Tree Inference",
    "authors": "Ashwin Prasad (Indian Institute of Science), Sampath Rajendra (Microsoft Research), Kaushik Rajan (Microsoft Research), R. Govindarajan (Indian Institute of Science, Bangalore), Uday Bondhugula (Indian Institute of Science and PolyMage Labs)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695958",
    "session_title": "Session 7: Verification and Compilers",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Scaling Deep Learning Computation over the Inter-Core Connected Intelligence Processor with T10",
    "authors": "Yiqi Liu (University of Illinois Urbana-Champaign), Yuqi Xue (University of Illinois Urbana-Champaign), Yu Cheng (Microsoft Research), Lingxiao Ma (Microsoft Research), Ziming Miao (Microsoft Research), Jilong Xue (Microsoft Research), Jian Huang (University of Illinois Urbana-Champaign)",
    "abstract": "As AI chips incorporate numerous parallelized cores to scale deep learning (DL) computing, inter-core communication is enabled recently by employing high-bandwidth and low-latency interconnect links on the chip (e.g., Graphcore IPU). It allows each core to directly access the fast scratchpad memory in other cores, which enables new parallel computing paradigms. However, without proper support for the scalable inter-core connections in current DL compilers, it is hard for developers to exploit the benefits of this new architecture. We present T10, the first DL compiler to exploit the inter-core communication bandwidth and distributed on-chip memory on AI chips. To formulate the computation and communication patterns of tensor operators in this new architecture, T10 introduces a distributed tensor abstraction rTensor. T10 maps a DNN model to execution plans with a generalized compute-shift pattern, by partitioning DNN computation into sub-operators and mapping them to cores, so that the cores can exchange data following predictable patterns. T10 makes globally optimized trade-offs between on-chip memory consumption and inter-core communication overhead, selects the best execution plan from a vast optimization space, and alleviates unnecessary inter-core communications. Our evaluation with a real inter-core connected AI chip, the Graphcore IPU, shows up to 3.3$\\times$ performance improvement, and scalability support for larger models, compared to state-of-the-art DL compilers and vendor libraries.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695955",
    "session_title": "Session 7: Verification and Compilers",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "FBDetect: Catching Tiny Performance Regressions at Hyperscale through In-Production Monitoring",
    "authors": "Dong Young Yoon (Meta Platforms), Yang Wang (Meta Platforms and The Ohio State University), Miao Yu (Meta Platforms and The Ohio State University), Elvis Huang (Meta Platforms), Juan Ignacio Jones (Meta Platforms), Abhinay Kukkadapu (Meta Platforms), Osman Kocas (Meta Platforms), Jonathan Wiepert (Meta Platforms), Kapil Goenka (Meta Platforms), Sherry Chen (Meta Platforms), Yanjun Lin (Meta Platforms), Zhihui Huang (Meta Platforms), Jocelyn Kong (Meta Platforms), Michael Chow (Meta Platforms), Chunqiang Tang (Meta Platforms)",
    "abstract": "This paper presents Meta’s FBDetect system, which advances the state of the art in performance regression detection by catching regressions as small as 0.005% in noisy production environments. FBDetect monitors around 800,000 time series covering various types of metrics (e.g., throughput, latency, CPU and memory usage) to detect regressions caused by code or configuration changes in hundreds of services running on millions of servers. FBDetect introduces advanced techniques to capture stack traces fleet-wide, measure fine-grained subroutine-level performance differences, filter out deceptive false-positive regressions, deduplicate correlated regressions, and analyze root causes. Beyond these individual techniques, a key strength of FBDetect over prior work is its battle-tested robustness, proven by seven years of production use, and each year catching regressions that would have wasted millions of servers if left undetected.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695977",
    "session_title": "Session 8: Data Center, Cloud, and Virtualization",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "VPRI: Efficient I/O Page Fault Handling via Software-Hardware Co-Design for IaaS Clouds",
    "authors": "Kaijie Guo (Alibaba Group), Dingji Li (Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University), Ben Luo (Alibaba Group), Yibin Shen (Alibaba Group), Kaihuan Peng (Alibaba Group), Ning Luo (Alibaba Group), Shengdong Dai (Alibaba Group), Chen Liang (Alibaba Group), Jianming Song (Alibaba Group), Hang Yang (Alibaba Group), Xiantao Zhang (Alibaba Group), Zeyu Mi (Institute of Parallel and Distributed Systems, SEIEE, Shanghai Jiao Tong University)",
    "abstract": null,
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695957",
    "session_title": "Session 8: Data Center, Cloud, and Virtualization",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "vSoC: Efficient Virtual System-on-Chip on Heterogeneous Hardware",
    "authors": "Jiaxing Qiu (Tsinghua University), Zijie Zhou (Tsinghua University), Yang Li (Tsinghua University), Zhenhua Li (Tsinghua University), Feng Qian (University of Southern California), Hao Lin (Tsinghua University and University of Illinois Urbana-Champaign), Di Gao (Tsinghua University), Haitao Su (Tsinghua University), Xin Miao (Tsinghua University), Yunhao Liu (Tsinghua University), Tianyin Xu (University of Illinois Urbana-Champaign)",
    "abstract": "Emerging mobile apps such as UHD video and AR/VR access diverse high-throughput hardware devices, e.g., video codecs, cameras, and image processors. However, today’s mobile emulators exhibit poor performance when emulating these devices. We pinpoint the major reason to be the discrepancy between the guest’s and host’s memory architectures for hardware devices, i.e., the mobile guest’s centralized memory on a system-on-chip (SoC) versus the PC/server’s separated memory modules on individual hardware. Such a discrepancy makes the shared virtual memory (SVM) architecture of mobile emulators highly inefficient. To address this, we design and implement vSoC, the first virtual mobile SoC that enables virtual devices to efficiently share data through a unified SVM framework. We then build upon the SVM framework a prefetch engine that effectively hides the overhead of coherence maintenance (which guarantees that devices sharing the same virtual memory see the same data), a performance bottleneck in existing emulators. Compared to state-of-the-art emulators, vSoC brings 12%- 49% higher frame rates to top popular mobile apps, while achieving 1.8-9.0 × frame rates and 35%-62% lower motion-to-photon latency for emerging apps. vSoC is adopted by Huawei DevEco Studio, a major mobile IDE.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695946",
    "session_title": "Session 8: Data Center, Cloud, and Virtualization",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Unearthing Semantic Checks for Cloud Infrastructure-as-Code Programs",
    "authors": "Yiming Qiu (University of Michigan), Patrick Tser Jern Kon (University of Michigan), Ryan Beckett (Microsoft), Ang Chen (University of Michigan)",
    "abstract": "Cloud infrastructures are increasingly managed by Infrastruc-ture-as-Code (IaC) frameworks (e.g., Terraform). IaC frameworks enable cloud users to configure their resources in a declarative manner, without having to directly work with low-level cloud API calls. However, with today’s IaC tooling, IaC programs that pass the compilation phase may still incur errors at deployment time, resulting in significant disruption. We observe that this stems from a fundamental semantic gap between IaC-level programs and cloud-level requirements— even a syntactically-correct IaC program may violate cloud-level expectations. To bridge this gap, we develop Zodiac , a tool that can unearth IaC-level semantic checks on cloud-level requirements. It provides an automated pipeline to mine these checks from online IaC repositories and validate them using deployment-based testing. We have applied Zodiac to Terraform resources offered by Microsoft Azure—a leading IaC framework and a leading cloud vendor—where it found 500+ semantic checks where violation would produce deployment failures. With these checks, we have identified 200+ buggy Terraform projects and helped fix errors within official Azure provider usage examples.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695974",
    "session_title": "Session 8: Data Center, Cloud, and Virtualization",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
    "authors": "Yixin Song (Shanghai Jiao Tong University), Zeyu Mi (Shanghai Jiao Tong University), Haotong Xie (Shanghai Jiao Tong University), Haibo Chen (Shanghai Jiao Tong University)",
    "abstract": "This paper introduces PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC) equipped with a single consumer-grade GPU. The key principle underlying the design of PowerInfer is exploiting the high locality inherent in LLM inference, characterized by a power-law distribution in neuron activation. This distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated across inputs, while the majority, cold neurons, vary based on specific inputs. PowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine: hot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed on the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers. PowerInfer further integrates adaptive predictors and neuron-aware sparse operators, optimizing the efficiency of neuron activation and computational sparsity. The evaluation shows that PowerInfer significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU. For the OPT-30B model, PowerInfer achieves performance comparable to that of a high-end server-grade A100 GPU, reaching 82% of its token generation rate on a single consumer-grade RTX 4090 GPU.",
    "link": "",
    "session_title": "Session 9: ML Serving",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving",
    "authors": "Yinwei Dai (Princeton University), Rui Pan (Princeton University), Anand Iyer (Georgia Tech), Kai Li (Princeton University), Ravi Netravali (Princeton University)",
    "abstract": "Machine learning (ML) inference platforms are tasked with balancing two competing goals: ensuring high throughput given many requests, and delivering low-latency responses to support interactive applications. Unfortunately, existing platform knobs (e.g., batch sizes) fail to ease this fundamental tension, and instead only enable users to harshly trade off one property for the other. This paper explores an alternate strategy to taming throughput-latency tradeoffs by changing the granularity at which inference is performed. We present Apparate, a system that automatically applies and manages early exits (EEs) in ML models, whereby certain inputs can exit with results at intermediate layers. To cope with the time-varying overhead and accuracy challenges that EEs bring, Apparate repurposes exits to provide continual feedback that powers several novel runtime monitoring and adaptation strategies. Apparate lowers median response latencies by 40.5--91.5% and 10.0--24.2% for diverse CV and NLP classification workloads, and median time-per-token latencies by 22.6--77.9% for generative scenarios, without affecting throughputs or violating tight accuracy constraints.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695963",
    "session_title": "Session 9: ML Serving",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Improving DNN Inference Throughput Using Practical, Per-Input Compute Adaptation",
    "authors": "Anand Iyer (Georgia Tech), Mingyu Guan (Georgia Tech), Yinwei Dai (Princeton University), Rui Pan (Princeton University), Swapnil Gandhi (Stanford University), Ravi Netravali (Princeton University)",
    "abstract": "Machine learning inference platforms continue to face high request rates and strict latency constraints. Existing solutions largely focus on compressing models to substantially lower compute costs (and time) with mild accuracy degradations. This paper explores an alternate (but complementary) technique that trades off accuracy and resource costs on a per-input granularity: early exit models, which selectively allow certain inputs to exit a model from an intermediate layer. Though intuitive, early exits face fundamental deployment challenges, largely owing to the effects that exiting inputs have on batch size (and resource utilization) throughout model execution. We present 𝐸 3 , the first system that makes early exit models practical for realistic inference deployments. Our key insight is to split and replicate blocks of layers in models in a manner that maintains a constant batch size throughout execution, all the while accounting for resource requirements and communication overheads. Evaluations with NLP and vision models show that 𝐸 3 can deliver up to 1.74 × improvement in goodput (for a fixed cost) or 1.78 × reduction in cost (for a fixed goodput). Additionally, 𝐸 3 ’s goodput wins generalize to autoregressive LLMs (2.8-3.8 × ) and compressed models (1.67 × ).",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695978",
    "session_title": "Session 9: ML Serving",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "LoongServe: Efficiently Serving Long-Context Large Language Models with Elastic Sequence Parallelism",
    "authors": "Bingyang Wu (Peking University), Shengyu Liu (Peking University), Yinmin Zhong (Peking University), Peng Sun (Shanghai AI Lab), Xuanzhe Liu (Peking University), Xin Jin (Peking University)",
    "abstract": "The context window of large language models (LLMs) is rapidly increasing, leading to a huge variance in resource usage between different requests as well as between different phases of the same request. Restricted by static parallelism strategies, existing LLM serving systems cannot efficiently utilize the underlying resources to serve variable-length requests in different phases. To address this problem, we propose a new parallelism paradigm, elastic sequence parallelism (ESP), to elastically adapt to the variance between different requests and phases. Based on ESP, we design and build LoongServe, an LLM serving system that (1) improves computation efficiency by elastically adjusting the degree of parallelism in real-time, (2) improves communication efficiency by reducing key-value cache migration overhead and overlapping partial decoding communication with computation, and (3) improves GPU memory efficiency by reducing key-value cache fragmentation across instances. Our evaluation under diverse real-world datasets shows that LoongServe improves the maximum throughput by up to 3.85$\\times$ compared to the chunked prefill and 5.81$\\times$ compared to the prefill-decoding disaggregation.",
    "link": "",
    "session_title": "Session 9: ML Serving",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Modular Verification of Secure and Leakage-Free Systems: From Application Specification to Circuit-Level Implementation",
    "authors": "Anish Athalye (MIT), Henry Corrigan-Gibbs (MIT), Frans Kaashoek (MIT), Joseph Tassarotti (New York University), Nickolai Zeldovich (MIT)",
    "abstract": "Parfait is a framework for proving that an implementation of a hardware security module (HSM) leaks nothing more than what is mandated by an application specification. Parfait proofs cover the software and the hardware of an HSM, which catches bugs above the cycle-level digital circuit abstraction, including timing side channels. Parfait’s contribution is a scalable approach to proving security and non-leakage by using intermediate levels of abstraction and relating them with transitive information-preserving refinement. This enables Parfait to use different techniques to verify the implementation at different levels of abstraction, reuse existing verified components such as CompCert, and automate parts of the proof, while still providing end-to-end guarantees. We use Parfait to verify four HSMs, including an ECDSA certificate-signing HSM and a password-hashing HSM, on top of the OpenTitan Ibex and PicoRV32 processors. Parfait provides strong guarantees for these HSMs: for instance, it proves that the ECDSA-on-Ibex HSM implementation—2,300 lines of code and 13,500 lines of Verilog—leaks nothing more than what is allowed by a 40-line specification of its behavior.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695956",
    "session_title": "Session 10: Security",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "NOPE: Strengthening Domain Authentication with Succinct Proofs",
    "authors": "Zachary DeStefano (NYU), Jeff J. Ma (NYU), Joseph Bonneau (NYU), Michael Walfish (NYU)",
    "abstract": "Server authentication assures users that they are communicating with a server that genuinely represents a claimed domain. Today, server authentication relies on certification authorities (CAs), third parties who sign statements binding public keys to domains. CAs remain a weak spot in Internet security, as any faulty CA can issue a certificate for any domain. This paper describes the design, implementation, and experimental evaluation of nope , a new mechanism for server authentication that uses succinct proofs (for example, zero-knowledge proofs) to prove that a DNSSEC chain exists that links a public key to a specified domain. The use of DNSSEC dramatically reduces reliance on CAs, and the small size of the proofs enables compatibility with legacy infrastructure, including TLS servers, certificate formats, and certificate transparency. nope proofs add minimal performance overhead to clients, increasing the size of a typical certificate chain by about 10% and requiring just over 1 ms to verify. nope’s core technical contributions (which generalize beyond nope) include efficient techniques for representing parsing and cryptographic operations within succinct proofs, which reduce proof generation time and memory requirements by nearly an order of magnitude.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695962",
    "session_title": "Session 10: Security",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Cookie Monster: Efficient On-Device Budgeting for Differentially-Private Ad-Measurement Systems",
    "authors": "Pierre Tholoniat (Columbia University), Kelly Kostopoulou (Columbia University), Peter McNeely (Columbia University), Prabhpreet Singh Sodhi (Columbia University), Anirudh Varanasi (Columbia University), Benjamin Case (Meta Inc.), Asaf Cidon (Columbia University), Roxana Geambasu (Columbia University), Mathias LÃ©cuyer (University of British Columbia)",
    "abstract": "With the impending removal of third-party cookies from major browsers and the introduction of new privacy-preserving advertising APIs, the research community has a timely opportunity to assist industry in qualitatively improving the Web's privacy. This paper discusses our efforts, within a W3C community group, to enhance existing privacy-preserving advertising measurement APIs. We analyze designs from Google, Apple, Meta and Mozilla, and augment them with a more rigorous and efficient differential privacy (DP) budgeting component. Our approach, called Cookie Monster, enforces well-defined DP guarantees and enables advertisers to conduct more private measurement queries accurately. By framing the privacy guarantee in terms of an individual form of DP, we can make DP budgeting more efficient than in current systems that use a traditional DP definition. We incorporate Cookie Monster into Chrome and evaluate it on microbenchmarks and advertising datasets. Across workloads, Cookie Monster significantly outperforms baselines in enabling more advertising measurements under comparable DP protection.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695965",
    "session_title": "Session 10: Security",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "Sesame: Practical End-to-End Privacy Compliance with Policy Containers and Privacy Regions",
    "authors": "Kinan Dak Albab (Brown University), Artem Agvanian (Brown University), Allen Aby (Brown University), Corinn Tiffany (Brown University), Alexander Portland (Brown University), Sarah Ridley (Brown University), Malte Schwarzkopf (Brown University)",
    "abstract": "Web applications are governed by privacy policies, but developers lack practical abstractions to ensure that their code actually abides by these policies. This leads to frequent over-sights, bugs, and costly privacy violations. Sesame is a practical framework for end-to-end privacy policy enforcement. Sesame wraps data in policy containers that associate data with policies that govern its use. Policy containers force developers to use privacy regions when operating on the data, and Sesame combines sandboxing and a novel static analysis to prevent privacy regions from leaking data. Sesame enforces a policy check before externalizing data, and it supports custom I/O via reviewed, signed code. Experience with four web applications shows that Sesame’s automated guarantees cover 95% of application code, with the remaining 5% needing manual review. Sesame achieves this with reasonable application developer effort and imposes 3–10% performance overhead (10–55% with sandboxes).",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695984",
    "session_title": "Session 10: Security",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  },
  {
    "title": "DNS Congestion Control in Adversarial Settings",
    "authors": "Huayi Duan (ETH Zurich), Jihye Kim (ETH Zurich), Marc Wyss (ETH Zurich), Adrian Perrig (ETH Zurich)",
    "abstract": "We instigate the study of adversarial congestion in the context of the Domain Name System (DNS). By strategically choking inter-server channels, this new type of DoS attack can disrupt a large user group’s access to target DNS servers at a low cost. In reminiscence of classic network congestion control, we propose a DNS congestion control ( DCC ) framework as a fundamental yet practical mitigation measure for such attacks. With an optimized fair-queuing message scheduler, DCC ensures benign clients fair access to inter-server channels regardless of an attacker’s behavior; with a set of extensible anomaly detection and signaling mechanisms, it minimizes collateral damage to innocuous clients. We architect DCC in a non-invasive style so that it can readily augment existing DNS servers. Our prototype evaluation demonstrates that DCC effectively mitigates adversarial congestion while incurring minor performance overheads.",
    "link": "https://dl.acm.org/doi/pdf/10.1145/3694715.3695982",
    "session_title": "Session 10: Security",
    "conference_name": "SOSP",
    "date": "2024-11-04"
  }
]