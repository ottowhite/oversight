[
  {
    "id": "8feefe64-5127-4962-85d3-1513ecebcf96",
    "title": "GpJSON: High-performance JSON Data Processing on GPUs",
    "abstract": "The JavaScript Object Notation (JSON) format is ubiquitous, and countless applications depend on it to store and exchange high volumes of data. Despite its great popularity, JSON is nevertheless a very inefficient data format: decoding and querying JSON data is often a major bottleneck for many data-intensive applications. \nIn this paper, we explore how Graphics Processing Units (GPUs) can be used to parallelize  both  JSON de-serialization and querying. We show how JSON parsing can be implemented on GPUs by means of parallel structural index construction, and we describe how JSON data can then be queried  in situ  using a lightweight query engine designed to run on GPUs. We present the design and implementation of GpJSON, a GPU-based JSON data processing library. The library can be used from high-level languages such as JavaScript or Python, and features bindings for the GraalVM language runtime. Our evaluation on real-world datasets shows that, on a single NVIDIA Ampere A100, GpJSON achieves at least 2.9√ó speedup on end-to-end performance (de-serialization plus querying) over state-of-the-art parallel JSON parsers and query engines, and 6-8√ó over NVIDIA RAPIDS.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GpJSON%3A%20High-performance%20JSON%20Data%20Processing%20on%20GPUs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3216-bonetta.pdf",
    "session": "Research 24: Database Engines I",
    "authors": [
      {
        "Name": "Sacheendra Talluri",
        "Affiliation": "Vrije Universiteit Amsterdam"
      },
      {
        "Name": "Guido Walter Di Donato",
        "Affiliation": "Politecnico di Milano"
      },
      {
        "Name": "Luca Danelutti",
        "Affiliation": "Politecnico di Milano"
      },
      {
        "Name": "Koen Vlaswinkel",
        "Affiliation": "Eindhoven University of Technology"
      },
      {
        "Name": "Marco Arnaboldi",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Arnaud Delamare",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Marco Domenico Santambrogio",
        "Affiliation": "Politecnico di Milano"
      },
      {
        "Name": "Daniele Bonetta",
        "Affiliation": "VU Amsterdam"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a798b7cf-31ad-4db4-b9e1-28c1a335add0",
    "title": "ELEET: Efficient Learned Query Execution over Text and Tables",
    "abstract": "In this paper, we present ELEET, a novel execution engine that allows one to seamlessly query and process text as a first-class citizen along with tables. To enable such a seamless integration of text and tables, ELEET leverages learned multi-modal operators (MMOps) such as joins and unions that seamlessly combine structured with unstructured textual data. While large language models (LLM) such as GPT-4 are interesting candidates to enable such learned multimodal operations, we deliberately do not follow this trend to enable MMOps, since it would result in high overhead at query runtime. Instead, to enable MMOps, ELEET comes with a more efficient small language model (SLM) that is targeted to extract structured data from text. Thanks to our novel architecture and pre-training procedure, the ELEET-model enables high-accuracy extraction with low overheads. In our evaluation, we compare query execution based on ELEET to baselines leveraging LLMs such as GPT-4 and show that ELEET can speed up multi-modal queries over tables and text by up to 575√ó without sacrificing accuracy.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/ELEET%3A%20Efficient%20Learned%20Query%20Execution%20over%20Text%20and%20Tables",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4867-urban.pdf",
    "session": "Research 29: Learned Database Systems",
    "authors": [
      {
        "Name": "Matthias Urban",
        "Affiliation": "Technical University of Darmstadt"
      },
      {
        "Name": "Carsten Binnig",
        "Affiliation": "TU Darmstadt"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "25dca334-f631-4871-9c8e-cafc95120e83",
    "title": "Where Does Academic Database Research Go From Here?",
    "abstract": "An open forum to discuss and debate the future of database research in the context of industry, other research communities, and AI.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Where%20Does%20Academic%20Database%20Research%20Go%20From%20Here%EF%BC%9F",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5510-eugene.pdf",
    "session": "Panel 2: Where Does Academic Database Research Go From Here?",
    "authors": [
      {
        "Name": "Eugene Wu",
        "Affiliation": "Columbia University"
      },
      {
        "Name": "Raul Castro Fernandez",
        "Affiliation": "The University of Chicago"
      },
      {
        "Name": "Shreya Shankar",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Natacha Crooks",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Jiannan Wang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Gustavo Alonso",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Divesh Srivastava",
        "Affiliation": "AT&T Research"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "93b121ac-7ccf-4158-bc12-b04435059dca",
    "title": "Revisiting CNNs for Trajectory Similarity Learning",
    "abstract": "Similarity search is a fundamental but expensive operator in querying trajectory data, due to its quadratic complexity of distance computation. To mitigate the computational burden for long trajectories, neural networks have been widely employed for similarity learning and each trajectory is encoded as a high-dimensional vector for similarity search with linear complexity. Given the sequential nature of trajectory data, previous efforts have been primarily devoted to the utilization of RNNs or Transformers. \nIn this paper, we argue that the common practice of treating trajectory as sequential data results in excessive attention to capturing long-term global dependency between two sequences. Instead, our investigation reveals the pivotal role of local similarity, prompting a revisit of simple CNNs for trajectory similarity learning. We introduce ConvTraj, incorporating both 1D and 2D convolutions to capture sequential and geo-distribution features of trajectories, respectively. In addition, we conduct a series of theoretical analyses to justify the effectiveness of ConvTraj. Experimental results on four real-world large-scale datasets demonstrate that ConvTraj achieves state-of-the-art accuracy in trajectory similarity search. Owing to the simple network structure of ConvTraj, the training and inference speed on the Porto dataset with 1.6 million trajectories are increased by at least  240 x and  2 . 16 x, respectively.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Revisiting%20CNNs%20for%20Trajectory%20Similarity%20Learning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1013-chang.pdf",
    "session": "Research 37: Applied ML and AI for Graph, Temporal, and Trajectory Data",
    "authors": [
      {
        "Name": "Zhihao Chang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Linzhu Yu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Huan Li",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Sai Wu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Dongxiang Zhang",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "05295165-0063-405b-ab5b-5e8a556e18f8",
    "title": "Environmental Footprints of Query Processing: A Vision for Sustainable Database Architectures",
    "abstract": "Database systems underpin modern computing infrastructure, yet their environmental impact remains a significant blind spot in both industry and research. As data volumes grow exponentially, the energy consumption, carbon emissions, and water usage of database operations increasingly threaten global sustainability goals. Our paper explores this multidimensional environmental footprint and proposes a vision where sustainability becomes a first-class design criterion alongside traditional performance metrics. We reimagine database architectures that incorporate environmental awareness throughout both hardware and software layers. By identifying critical research challenges, we establish a foundation for database systems that can deliver high performance while meeting the environmental demands of our resource-constrained world.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Environmental%20Footprints%20of%20Query%20Processing%3A%20A%20Vision%20for%20Sustainable%20Database%20Architectures",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4064-bachras.pdf",
    "session": "Research 64: Database Engines and Architectures",
    "authors": [
      {
        "Name": "Michail Bachras",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Hans-Arno Jacobsen",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4c382874-f4bf-4568-95c5-9da706f3d099",
    "title": "Streaming Time Series Subsequence Anomaly Detection: A Glance and Focus Approach",
    "abstract": "Subsequence anomaly detection for time series is a crucial problem in various real-world applications. However, existing methods proposed so far design the anomaly score functions solely based on either local neighborhood or global patterns, leading to unsatisfactory detection accuracy. In addition, these methods either cannot adapt, or yield insufficient accuracy and efficiency in streaming scenario. Therefore, we propose  Sirloin , an accurate and efficient streaming time series subsequence anomaly detection framework. First,  Sirloin  proposes a glance and focus anomaly score function that takes both global and local information into consideration, contributing to an accurate anomaly detection. Second, Sirloin dynamically maintains an inverted file index and product quantization codebooks to index and compress the subsequences, hence is able to cope with the time series evolution and to process streaming batches efficiently. In addition, a dual index optimization strategy is put forward that further improves the efficiency. An experimental study in 11 different datasets from 5 domains offers insight into the performance of  Sirloin , showing that it improves throughput on average 4 √ó  and enhances accuracy 58.02% compared to the state-of-the-art streaming method.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Streaming%20Time%20Series%20Subsequence%20Anomaly%20Detection%3A%20A%20Glance%20and%20Focus%20Approach",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1892-zheng.pdf",
    "session": "Research 12: Time Series Data I",
    "authors": [
      {
        "Name": "Wenjing Wang",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "ziyang yue",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Bolong Zheng",
        "Affiliation": "Huazhong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7c60bfdd-3c51-4a97-93a5-62f80729e047",
    "title": "Efficient and Accurate Subgraph Counting: A Bottom-up Flow-learning Based Approach",
    "abstract": "Subgraph counting is a fundamental problem in graph analytics with broad applications, yet remains computationally intractable due to its #P-hardness. To address this, numerous approximate solutions have been proposed, though they often suffer from limited efficiency and accuracy. In this paper, we introduce  FlowSC , a novel approach that achieves both high accuracy and efficiency in subgraph counting. Our method starts with an enhanced candidate filtering algorithm, which significantly improves the pruning capability of bipartite graph-based techniques with minimal overhead. Building on this, we propose a bottom-up flow-learning model based on a new Graph Neural Network (GNN) architecture. By employing a carefully designed message-passing mechanism, the model explicitly controls the direction, range, and iterations of information flow, enabling a simulation of the candidate tree-based counting process. This mechanism is further empowered by a customized message aggregation technique, alongside a pretraining strategy that facilitates model training. Extensive experiments show that  FlowSC  can achieve up to 4 orders of magnitude improvement in accuracy and 3 √ó  improvement in efficiency over the baselines across datasets, while scaling to billion-edge graphs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20and%20Accurate%20Subgraph%20Counting%3A%20A%20Bottom-up%20Flow-learning%20based%20Approach",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2695-yang.pdf",
    "session": "Research 9: Query Processing and Optimization I",
    "authors": [
      {
        "Name": "Qiuyu Guo",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Jianye Yang",
        "Affiliation": "Guangzhou University"
      },
      {
        "Name": "Wenjie Zhang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Hanchen Wang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Ying Zhang",
        "Affiliation": "University of Technology Sydney"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiao Tong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fa6d714b-d04a-4728-8d85-96993f806f6f",
    "title": "Esc: An Early-Stopping Checker for Budget-aware Index Tuning",
    "abstract": "Index tuning is a time-consuming process. One major performance bottleneck in existing index tuning systems is the large amount of ‚Äúwhat-if‚Äù query optimizer calls that estimate the cost of a given pair of query and index configuration without materializing the indexes. There has been recent work on budget-aware index tuning that limits the amount of what-if calls allowed in index tuning. Existing budget-aware index tuning algorithms, however, typically make fast progress early on in terms of the best configuration found but slow down when more and more what-if calls are allocated. This observation of ‚Äúdiminishing return‚Äù on index quality leads us to introduce early stopping for budget-aware index tuning, where user specifies a threshold on the tolerable loss of index quality and we stop index tuning if the projected loss with the remaining budget is below the threshold. We further propose Esc, a low-overhead early-stopping checker that realizes this new functionality. Experi- mental evaluation on top of both industrial benchmarks and real customer workloads demonstrates that Esc can significantly reduce the number of what-if calls made during budget-aware index tun- ing while incurring little or zero improvement loss and little extra computational overhead compared to the overall index tuning time.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Esc%3A%20An%20Early-Stopping%20Checker%20for%20Budget-aware%20Index%20Tuning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1278-wu.pdf",
    "session": "Research 44: Database Engine Performance and Manageability I",
    "authors": [
      {
        "Name": "Xiaoying Wang",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Wentao Wu",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Vivek Narasayya",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Surajit Chaudhuri",
        "Affiliation": "Microsoft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f8a2f337-0c96-4775-bbed-25e2834ad63e",
    "title": "Shifting Transaction Isolation on Graphs: From Systems to Data",
    "abstract": "Processing long-running read-write transactions on graphs is an open challenge, primarily due to the need for serializability to maintain basic structural consistency of graphs. We identify that a fundamental impediment to a solution arises from the homogeneous database-wide notion of transaction isolation developed for relations, which fails to capture the heterogeneous consistency semantics on graphs. We propose Ddi, a notion of Ô¨Åne-grained isolation for graph transactions that advocates per-operation isolation allocation. It extracts concurrency for graph transactions that traditional isolation cannot, by assigning one or multiple isolation levels to each traversal operation, while maintaining graph consistency as serializability does. We develop formal semantics for Ddi and prove the consistency guarantees of its transaction schedules. We also develop  DD-OCC , an optimistic concurrency control protocol for Ddi isolation, and implement it on a state-of-the-art graph storage. Experiments over LDBC graphs conÔ¨Årm the eÔ¨Äectiveness of Ddi.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Shifting%20Transaction%20Isolation%20on%20Graphs%3A%20From%20Systems%20to%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3784-cao.pdf",
    "session": "Research 35: Database Engines for Graphs",
    "authors": [
      {
        "Name": "Wenzhi Fu",
        "Affiliation": "University of Edinburgh"
      },
      {
        "Name": "Yang Cao",
        "Affiliation": "University of Edinburgh"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bd00a919-d359-486c-a6eb-2adc613ae5e9",
    "title": "VerIso: Verifiable Isolation Guarantees for Database Transactions",
    "abstract": "Isolation bugs, stemming especially from design-level defects, have been repeatedly found in carefully designed and extensively tested production databases over decades. In parallel, various frameworks for modeling database transactions and reasoning about their iso- lation guarantees have been developed. What is missing however is a mathematically rigorous and systematic framework with tool support for formally verifying a wide range of such guarantees for all possible system behaviors. We present the first such framework, VerIso, developed within the theorem prover Isabelle/HOL. To showcase its use in verification, we model the strict two-phase lock- ing concurrency control protocol and verify that it provides strict serializability isolation guarantee. Moreover, we show how VerIso helps identify isolation bugs during protocol design. We derive new counterexamples for the TAPIR protocol from failed attempts to prove its claimed strict serializability. In particular, we show that it violates a much weaker isolation level, namely, atomic visibility.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/VerIso%3A%20Verifiable%20Isolation%20Guarantees%20for%20Database%20Transactions",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1362-ghasemirad.pdf",
    "session": "Research 52: Transaction Management",
    "authors": [
      {
        "Name": "Shabnam Ghasemirad",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Si Liu",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Christoph Sprenger",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Luca Multazzu",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "David Basin",
        "Affiliation": "ETH Zurich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b7336208-8726-42ab-a622-20fce1c438fb",
    "title": "SQL Engines Excel at the Execution of Imperative Programs",
    "abstract": "SQL query engines can act as eÔøøcient runtime environments for the execution of imperative programs over database-resident tabular data. To make this point, we lay out the details of a compilation strategy that maps the basic blocks of arbitrarily branching and looping control Ôøøow graphs into plain‚Äîpossibly recursive‚ÄîSQL:1999 common table expressions. The compiler does not stumble when faced with imperative programs of several hundred lines and emits SQL code that can execute such programs over entire batches of input arguments. These batches create opportunities for parallel program evaluation which contemporary query decorrelation techniques exploit automatically. SQL engines that already support UDFs may Ôøønd the present program execution approach to outperform their native implementation‚ÄîSQL engines without such support may gain UDF capabilities without the need to build a dedicated interpreter.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/SQL%20Engines%20Excel%20at%20the%20Execution%20of%20Imperative%20Programs",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4696-fischer.pdf",
    "session": "Research 54: Database Engines II",
    "authors": [
      {
        "Name": "Tim Fischer",
        "Affiliation": "Universit√§t T√ºbingen"
      },
      {
        "Name": "Denis Hirn",
        "Affiliation": "Universit√§t T√ºbingen"
      },
      {
        "Name": "Torsten Grust",
        "Affiliation": "Universit√§t T√ºbingen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bbde5df9-b693-4a4c-af0a-7d6577f762c9",
    "title": "TSB-AutoAD: Towards Automated Solutions for Time-Series Anomaly Detection",
    "abstract": "Despite decades of research on time-series anomaly detection, the eÔ¨Äectiveness of existing anomaly detectors remains constrained to speciÔ¨Åc domains - a model that performs well on one dataset may fail on another. Consequently, developing  automated  solutions for anomaly detection remains a pressing challenge. However, the AutoML community has predominantly focused on supervised learning solutions, which are impractical for anomaly detection due to the lack of labeled data and the absence of a well-deÔ¨Åned objective function for model evaluation. While recent studies have evaluated standalone anomaly detectors, no study has ever evaluated automated solutions for selecting or generating scores in an automated manner. In this study, we (i) provide a systematic review and taxonomy of automated solutions for time-series anomaly detection, categorizing them into selection, ensembling, and generation methods; (ii) introduce TSB-AutoAD, a comprehensive benchmark encompassing 20 standalone methods and 70 variants; and (iii) conduct the most extensive evaluation in this area to date. Our benchmark includes state-of-the-art methods across all three categories, evaluated on TSB-AD, a recently curated heterogeneous testbed from nine domains. Our Ô¨Åndings reveal a signiÔ¨Åcant gap, where over half of the existing solutions do not statistically outperform a simple random choice. Foundation models that claim to oÔ¨Äer generalized, one-size-Ô¨Åts-all solutions have yet to deliver on this promise. While naive ensembling achieves high accuracy, it comes at a substantial computational overhead. Conversely, methods leveraging historical datasets enable fast inference but suÔ¨Äer under out-of-distribution conditions. To address this trade-oÔ¨Ä, we propose a selective ensembling solution, which combines model selection with ensembling to oÔ¨Äer a lightweight, practical balance between accuracy and eÔ¨Éciency. We open-source TSB-AutoAD and highlight the need for more robust and eÔ¨Écient solutions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TSB-AutoAD%3A%20Towards%20Automated%20Solutions%20for%20Time-Series%20Anomaly%20Detection%20%5BE%2C%20A%20%26%20B%5D",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4364-liu.pdf",
    "session": "Research 12: Time Series Data I",
    "authors": [
      {
        "Name": "Qinghua Liu",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "Seunghak Lee",
        "Affiliation": "Meta"
      },
      {
        "Name": "Paparrizos John",
        "Affiliation": "The Ohio State University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "379a31b8-a096-43e4-94d8-a47a003fcc02",
    "title": "Effective and Efficient Attributed Hypergraph Embedding on Nodes and Hyperedges",
    "abstract": "An attributed hypergraph comprises nodes with attributes and hyperedges that connect varying numbers of nodes.  Attributed hypergraph node and hyperedge embedding  (AHNEE) maps nodes and hyperedges to compact vectors for use in important tasks such as node classification, hyperedge link prediction, and hyperedge classification. Generating high-quality embeddings is challenging due to the complexity of attributed hypergraphs and the need to embed both nodes and hyperedges, especially in large-scale data. Existing solutions often fall short by focusing only on nodes or lacking native support for attributed hypergraphs, leading to inferior quality, and struggle with scalability on large attributed hypergraphs. \nWe propose  SAHE , an efficient and effective approach that unifies node and hyperedge embeddings for AHNEE computation, advancing the state of the art via comprehensive embedding formulations and algorithmic designs. First, we introduce two higher-order similarity measures, HMS-N and HMS-E, to capture similarities between node pairs and hyperedge pairs, respectively. These measures consider multi-hop connections and global topology within an extended hypergraph that incorporates attribute-based hyperedges. SAHE  formulates the AHNEE objective to jointly preserve all-pair HMS-N and HMS-E similarities. Direct optimization is computationally expensive, so we analyze and unify core approximations of allpair HMS-N and HMS-E to solve them simultaneously. To enhance efficiency, we design several non-trivial optimizations that avoid iteratively materializing large dense matrices while maintaining high-quality results. Extensive experiments on diverse attributed hypergraphs and 3 downstream tasks, compared against 11 baselines, show that  SAHE  consistently outperforms existing methods in embedding quality and is up to orders of magnitude faster.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Effective%20and%20Efficient%20Attributed%20Hypergraph%20Embedding%20on%20Nodes%20and%20Hyperedges",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4269-shi.pdf",
    "session": "Research 34: Graph Data Management IV",
    "authors": [
      {
        "Name": "Yiran Li",
        "Affiliation": "The Hong Kong Polytechnic University"
      },
      {
        "Name": "Gongyao Guo",
        "Affiliation": "The Hong Kong Polytechnic University"
      },
      {
        "Name": "Chen Feng",
        "Affiliation": "The Hong Kong Polytechnic University"
      },
      {
        "Name": "Jieming Shi",
        "Affiliation": "The Hong Kong Polytechnic University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "616b010a-f1f2-46a5-bb6e-56c615e82b2d",
    "title": "Robust Plan Evaluation based on Approximate Probabilistic Machine Learning",
    "abstract": "Query optimizers in RDBMSs search for execution plans expected to be optimal for given queries. They use parameter estimates, often inaccurate, and make assumptions that may not hold in practice. Consequently, they may select plans that are suboptimal at runtime, if estimates and assumptions are not valid. Therefore, they do not sufficiently support robust query optimization. Using ML to improve data systems has shown promising results for query optimization. Inspired by this, we propose   Ro bust   Q uery Optimizer, (Roq), a holistic framework based on a risk-aware learning approach. Roq includes a novel formalization of the notion of robustness in the context of query optimization and a principled approach for its quantification and measurement based on approximate probabilistic ML. It also includes novel strategies and algorithms for query plan evaluation and selection. Roq includes a novel learned cost model that is designed to predict the cost of query execution and the associated risks and performs query optimization accordingly. We demonstrate that Roq provides significant improvements in robust query optimization compared with the state-of-the-art.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Robust%20Plan%20Evaluation%20based%20on%20Approximate%20Probabilistic%20Machine%20Learning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2626-kamali.pdf",
    "session": "Research 61: Learned Query Optimization",
    "authors": [
      {
        "Name": "Amin Kamali",
        "Affiliation": "University of Ottawa"
      },
      {
        "Name": "Verena Kantere",
        "Affiliation": "University of Ottawa"
      },
      {
        "Name": "Calisto Zuzarte",
        "Affiliation": "IBM"
      },
      {
        "Name": "Vincent Corvinelli",
        "Affiliation": "IBM"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "da2021db-04a2-4c59-ae46-2b62d4a48f64",
    "title": "OpenMEL: Unsupervised Multimodal Entity Linking Using Noise-Free Expanded Queries and Global Coherence",
    "abstract": "Multimodal Entity Linking (MEL), which involves disambiguating a mention composed of multimodal inputs to a multimodal knowledge base (KB), has gained increasing attention. Although existing MEL approaches using supervised learning show promising performance, they depend heavily on large-scale labeled training data, which is expensive to obtain for each new scenario. Unsupervised learning MEL methods, on the other hand, typically consist of two main steps. In the first multimodal data encoding step, these methods either assume that the multimodal data inputs are of high quality or attempt to filter out the noisy modality. In the second entity ranking step, they employ a bipartite graph to model the relationships only between mentions and entities. However, unsupervised methods face challenges in both steps. In the first step, data quality issues arise, including limited context in textual inputs and noise in the corresponding images. Moreover, in the second step, the bipartite graph fails to capture coherence between highly correlated entities within the KB, which offers clues on shared domains among entities. This limitation hinders effective retrieval of the target entity. To address these issues, we propose a novel unsupervised learning framework, OpenMEL, for solving the MEL task. We enhance the textual modality contextual information by incorporating full context comprehension and general knowledge, and generates three levels of visual inputs for further adaptive selection to handle noise. To capture global entity coherence, we construct a tree cover structure, defining it as a maximum spanning tree with bounded nodes to meet the MEL objective. We then introduce a greedy algorithm with theoretical guarantees to solve this problem. Experimental results on three public benchmark datasets show that OpenMEL outperforms various state-of-the-art baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/OpenMEL%3A%20Unsupervised%20Multimodal%20Entity%20Linking%20Using%20Noise-Free%20Expanded%20Queries%20and%20Global%20Coherence",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2454-zhu.pdf",
    "session": "Research 63: Knowledge Graph Management",
    "authors": [
      {
        "Name": "Xinyi ZHU",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Yongqi Zhang",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e5e32073-6e9e-4eec-98d6-e257bd535201",
    "title": "QPET: A Versatile and Portable Quantity-of-Interest-Preservation Framework for Error-Bounded Lossy Compression",
    "abstract": "Error-bounded lossy compression has been widely adopted in many scientiÔ¨Åc domains because it can address the challenges in storing, transferring, and analyzing unprecedented amounts of scientiÔ¨Åc data. However, general error-bounded lossy compressors may fail to meet additional quality requirements for downstream analysis, a.k.a. Quantities of Interest (QoIs). This may lead to uncertainties and even misinterpretations in scientiÔ¨Åc discoveries, signiÔ¨Åcantly limiting the use of lossy compression in practice. In this paper, we propose QPET, a novel, versatile, and portable framework for QoIpreserving error-bounded lossy compression, which overcomes the challenges of modeling diverse QoIs by leveraging numerical strategies. QPET features (1) high portability to multiple existing lossy compressors, (2) versatile preservation to most diÔ¨Äerentiable univariate and multivariate QoIs, and (3) signiÔ¨Åcant compression improvements in QoI-preservation tasks. Experiments with six realworld datasets demonstrate that integrating QPET into state-of-theart error-bounded lossy compressors can gain 2x to 10x compression speedups of existing QoI-preserving error-bounded lossy compression solutions, up to 1000% compression ratio improvements to general-purpose compressors, and up to 133% compression ratio improvements to existing QoI-integrated scientiÔ¨Åc compressors.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/QPET%3A%20A%20Versatile%20and%20Portable%20Quantity-of-Interest-preservation%20Framework%20for%20Error-Bounded%20Lossy%20Compression",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2440-liang.pdf",
    "session": "Research 21: Specialized and Domain-Specific Data Management",
    "authors": [
      {
        "Name": "Jinyang Liu",
        "Affiliation": "University of Houston"
      },
      {
        "Name": "Pu Jiao",
        "Affiliation": "University of Kentucky"
      },
      {
        "Name": "Kai Zhao",
        "Affiliation": "Florida State University"
      },
      {
        "Name": "Xin Liang",
        "Affiliation": "University of Kentucky"
      },
      {
        "Name": "Sheng Di",
        "Affiliation": "Argonne National Laboratory, Lemont, IL"
      },
      {
        "Name": "Franck Cappello",
        "Affiliation": "Argonne National Laboratory, Lemont, IL"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "362684bf-e66f-4113-862f-8ca49dc59f9b",
    "title": "SPECIAL: Synopsis Assisted Secure Collaborative Analytics",
    "abstract": "Secure collaborative analytics (SCA) enables the processing of analytical SQL queries across data from multiple owners, even when direct data sharing is not possible. While traditional SCA provides strong privacy through data-oblivious methods, the signiÔ¨Åcant overhead has limited its practical use. Recent SCA variants that allow controlled leakages under diÔ¨Äerential privacy (DP) strike balance between privacy and eÔ¨Éciency but still face challenges like unbounded privacy loss, costly execution plan, and lossy processing. \nTo address these challenges, we introduce SPECIAL, the Ô¨Årst SCA system that simultaneously ensures bounded privacy loss, advanced query planning, and lossless processing. SPECIAL employs a novel  synopsis-assisted secure processing model , where a one-time privacy cost is used to generate private synopses from owner data. These synopses enable SPECIAL to estimate compaction sizes for secure operations (e.g., Ô¨Ålter, join) and index encrypted data without additional privacy loss. These estimates and indexes can be prepared before runtime, enabling eÔ¨Écient query planning and accurate cost estimations. By leveraging one-sided noise mechanisms and private upper bound techniques, SPECIAL guarantees lossless processing for complex queries (e.g., multi-join). Our comprehensive benchmarks demonstrate that SPECIAL outperforms state-of-the-art SCAs, with up to 80 √ó  faster query times, 900 √ó smaller memory usage for complex queries, and up to 89 √ó  reduced privacy loss in continual processing",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SPECIAL%3A%20Synopsis%20Assisted%20Secure%20Collaborative%20Analytics",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1035-wang.pdf",
    "session": "Research 66: Access Control and Privacy II",
    "authors": [
      {
        "Name": "Chenghong Wang",
        "Affiliation": "Indiana University"
      },
      {
        "Name": "Lina Qiu",
        "Affiliation": "Boston University"
      },
      {
        "Name": "Johes Bater",
        "Affiliation": "Tufts University"
      },
      {
        "Name": "Yukui Luo",
        "Affiliation": "University of Massachusetts Dartmouth"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7f50f34c-2e8e-4a3d-bdca-69530bb62ce9",
    "title": "VStream: A Distributed Streaming Vector Search System",
    "abstract": "Vector search is widely employed in recommendation systems, search engines, etc. With the explosive growth of online data and streaming processing engines, streaming vector search has attracted increasing research attention. However, prevailing vector search systems like Vearch, Vespa, and Milvus typically operate as external batch services for streaming processing requirements, resulting in sub-optimal performance for streaming processing scenarios. \nIn this paper, we propose VStream, a distributed streaming vector search system. Implementing such a system is non-trivial, raising three technical challenges in streaming adaptability, system scalability, and real-time response. Specifically, VStream offers a dynamic partitioner that adapts to data distribution changes in vector streams. Additionally, VStream features an effective hierarchical storage architecture facilitated by streaming state management, enabling a hybrid of four-level storage media with diverse access speeds and targets. Furthermore, VStream utilizes dynamic hot-cold patterns, such as access frequency, in the streaming vector data, incorporating a specialized hot-cold separation mechanism to enhance query efficiency. Extensive experiments prove that VStream outperforms existing vector search systems, e.g., achieving 251‚Äì 373 √ó  improvements in query efficiency, 2.2‚Äì2.5 √ó  savings in CPU usage, and 1.5‚Äì2.0 √ó  reductions in memory overhead.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/VStream%3A%20A%20Distributed%20Streaming%20Vector%20Search%20System",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1593-gao.pdf",
    "session": "Research 46: Vector Data Management III",
    "authors": [
      {
        "Name": "Shenghao Gong",
        "Affiliation": "Zhejiang University, China"
      },
      {
        "Name": "Haobo Sun",
        "Affiliation": "Zhejiang Universiy"
      },
      {
        "Name": "Ziquan Fang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Liu Liu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lu Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "183d244d-66b4-452b-b7d6-17bfbc421574",
    "title": "A CPU-GPU Hybrid Labelling Algorithm for Massive Shortest Distance Queries on Road Networks",
    "abstract": "Shortest distance computation is a fundamental operation in graph-Shortest distance computation is a fundamental operation in graphrelated applications, especially in location-based services. The most efficient method is hop-labeling, which can answer queries in mi-efficient method is hop-labeling, which can answer queries in microseconds. However, when the traffic condition changes dynami-croseconds. However, when the traffic condition changes dynamically, they need a long time to maintain or an even longer time to re-construct, making it hard to catch up with numerous or frequent updates. As a result, real-life applications still rely on slow graph searching algorithms. To improve the hop labeling construction efficiency, we resort to GPU for its high parallelism power and pro-efficiency, we resort to GPU for its high parallelism power and propose the G2H index. Specifically, we first analyze the relation of the graph partitions, index performance, and parallelism to identify the most suitable partition scheme for G2H, with a hybrid scheme and optimized node ordering for faster contraction. Then, we propose a label-pruning method to reduce the label construction workload with several strategies designed to balance and improve the parallel label construction. Finally, experiments on real-life networks show that our G2H can finish construction within seconds for large urban networks and under one minute for large region networks with 6M vertices, which is several times faster than the state-of-the-art methods. Besides, G2H can answer hundreds of millions of queries per second, achieving two orders of magnitude acceleration. KEYWORDS Shortest Distance, Tree Decomposition, GPU, Road Network",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20CPU-GPU%20Hybrid%20Labelling%20Algorithm%20for%20Massive%20Shortest%20Distance%20Queries%20on%20Road%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p770-li.pdf",
    "session": "Research 20: Road Networks, Social Networks",
    "authors": [
      {
        "Name": "Li Jiajia",
        "Affiliation": "Shenyang Aerospace University"
      },
      {
        "Name": "YongZhi Chen",
        "Affiliation": "Shenyang Aerospace University"
      },
      {
        "Name": "Mengxuan Zhang",
        "Affiliation": "Australian National University"
      },
      {
        "Name": "Lei Li",
        "Affiliation": "The Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6d89460e-7481-443c-807a-0c100898c336",
    "title": "Explaining GNN-based Recommendations in Logic",
    "abstract": "This paper proposes  Makex  (MAKE senSE), a logic approach to ex-This paper proposes  Makex  (MAKE senSE), a logic approach to explaining why a GNN-based model  M( ùêø,ùëÄ )  recommends item  ùëÄ to user ùêø . It proposes a class of Rules for ExPlanations, denoted as  REPs and de!ned with a graph pattern  Q  and dependency  ùëÅ ‚ÜíM( ùêø,ùëÄ ) , where  ùëÅ is a collection of predicates, and the model  M( ùêø,ùëÄ )  is treated as the consequence of the rule. Intuitively, given  M( ùêø,ùëÄ ) , we discover pattern  Q  to identify relevant topology, and precon-we discover pattern  Q  to identify relevant topology, and precondition  ùëÅ to disclose correlations, interactions and dependencies of vertex features; together they provide rationals behind prediction M( ùêø,ùëÄ ) , identifying what features are decisive for  M  to make pre-M( ùêø,ùëÄ ) , identifying what features are decisive for  M  to make predictions and under what conditions the decision can be made. We (a) de!ne  REPs  with 1-WL test, on which most GNN models for recommendation are based; (b) develop an algorithm for discov-recommendation are based; (b) develop an algorithm for discovering  REPs  for  M  as global explanations, and (c) provide a top- ùëÇ algorithm to compute top-ranked local explanations. Using real-life graphs, we empirically verify that  Makex  outperforms previous explanation methods in terms of fidelity, sparsity and effciency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Explaining%20GNN-based%20Recommendations%20in%20Logic",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p715-lin.pdf",
    "session": "Research 14: User Interfaces for Data Exploration and Explainable AI",
    "authors": [
      {
        "Name": "Wenfei Fan",
        "Affiliation": "Univ. of Edinburgh"
      },
      {
        "Name": "Lihang Fan",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Dandan LIN",
        "Affiliation": "Shenzhen Institute of Computing Sciences"
      },
      {
        "Name": "Min Xie",
        "Affiliation": "Shenzhen Institute of Computing Sciences"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "214d6d5d-a013-4351-9070-f7a6d35c8d5b",
    "title": "Magnus: A Holistic Approach to Data Management for Large-Scale Machine Learning Workloads",
    "abstract": "Machine learning (ML) has become a cornerstone of key applications at ByteDance. As model complexity and data volumes surge, data management for large-scale ML workloads faces substantial challenges, particularly with recent advances in large recommendation models (LRMs) and large multimodal models (LMMs). Traditional approaches exhibit limitations in storage efficiency, metadata scalability, update mechanisms, and integration with ML frameworks. To address these challenges, we propose  Magnus , a holistic data management system built upon Apache Iceberg.  Magnus integrates innovative optimizations across resource-efficient storage formats optimized for large wide tables and multimodal data, built-in support for vector and inverted indexes to accelerate data retrieval, scalable metadata planning with Git-like branching and tagging capabilities, and high-performance update/upsert based on lightweight merge-on-read (MOR) strategies. Additionally,  Magnus provides native support and specialized enhancement for LRM and LMM training workloads. Experimental results demonstrate significant performance gains in real-world ML scenarios.  Magnus  has been deployed at ByteDance for over five years, enabling robust and efficient data infrastructure for large-scale ML workloads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Magnus%3A%20A%20Holistic%20Approach%20to%20Data%20Management%20for%20Large-Scale%20Machine%20Learning%20Workloads",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4964-song.pdf",
    "session": "Industry 4: Machine Learning, AI, and Databases",
    "authors": [
      {
        "Name": "Jun Song",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Jingyi Ding",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Irshad Kandy",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Yanghao Lin",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Zhongjia Wei",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Zilong Zhou",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Zhiwei Peng",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Jixi Shan",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Hongyue Mao",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Xiuqi Huang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Xun Song",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Cheng Chen",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Yanjia Li",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Tianhao Yang",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Wei Jia",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Xiaohong Dong",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Kang Lei",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Rui Shi",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Pengwei Zhao",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Wei Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1b6fbebd-0975-491e-be7d-0c0ab651c62c",
    "title": "When Entity/Relationship Models Meet Graph Databases",
    "abstract": "This tutorial shows how traditional Entity/Relationship modeling and modern graph data modeling can be combined to bring forward well-designed graph data models that process workloads and maintain data integrity e!ciently.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/When%20Entity-slash-Relationship%20Models%20Meet%20Graph%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5444-link.pdf",
    "session": "Tutorial 4: When Entity/Relationship Models Meet Graph Databases",
    "authors": [
      {
        "Name": "Philipp Skavantzos",
        "Affiliation": "The University of Auckland"
      },
      {
        "Name": "Sebastian Link",
        "Affiliation": "University of Auckland"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5f82c319-de10-4c0b-ae22-1c5ba8db32c1",
    "title": "UmbraPerf - Profiling Results Tailored for DBMS Developers",
    "abstract": "Developing a code-generating Database Management System requires tight proÔ¨Åling and performance-tuning iterations. However, existing proÔ¨Ålers report results at instruction or function level, making it challenging to correlate them with constructs like query plan operators to derive actionable insights. \nIn this demonstration, we show how to solve this issue, building on our previous work, Tailored ProÔ¨Åling. We introduce UmbraPerf, a novel proÔ¨Åling tool that combines proÔ¨Åling samples with metadata to map proÔ¨Åling results to a DBMS‚Äôs internal abstractions, from query plan components down to generated code. Its interactive frontend visualizes data across multiple abstraction levels, enabling developers to narrow performance bottlenecks from operators down to individual instructions. We demonstrate its utility through two scenarios using our publicly available frontend.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/UmbraPerf%20-%20Profiling%20Results%20Tailored%20for%20DBMS%20Developers",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5291-beischl.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Alexander Beischl",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Thomas Neumann",
        "Affiliation": "Technical University of Munich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2fd5c565-fc22-4fee-86d0-338170d03fc2",
    "title": "MLN-geeWhiz: A Dashboard for Supporting Complete Life-Cycle of Complex Data Analysis using Multilayer Networks",
    "abstract": "Over the last few decades, simple graphs have been extensively used for studying complex systems of interacting entities from diverse disciplines, such as social networks, transportation, epidemiology, etc. However, when studying data with multiple types of entities, relationships, and features, simple (or even attributed) graphs are not always suÔ¨Écient. For example, to study accident patterns to take mitigating actions, one needs to explore accident patterns based on factors like weather (rain, sunny, sleet, etc.), light, and road surface conditions in diÔ¨Äerent geographical regions. As another example, to Ô¨Ånd individuals who are inÔ¨Çuential across multiple social media, a single graph approach is not well-suited. Indeed, to model such multiple relationships, multiple related graphs are useful. This can be done using multilayer networks (MLNs). \nAny complex data analysis can immensely beneÔ¨Åt from interactive graphic tools rather than working with raw data in command prompt mode. This is especially true as data and models become increasingly complex. To interpret and understand the results of analysis, drill-down, and visualization become critical. The MLNDashboard (called  MLN-geeWhiz ) presented in this demo paper aims to facilitate all aspects of MLN layer generation, analysis, and visualization through an intuitive, interactive web-based dashboard. In this paper, we discuss the dashboard, its architecture, the functionality currently supported, and some use cases.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/MLN-geeWhiz%3A%20A%20Dashboard%20for%20Supporting%20Complete%20Life-Cycle%20of%20Complex%20Data%20Analysis%20using%20Multilayer%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5323-santra.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Amey Shinde",
        "Affiliation": "The University of Texas at Arlington"
      },
      {
        "Name": "Viraj Sabhaya",
        "Affiliation": "The University of Texas at Arlington"
      },
      {
        "Name": "Kevin Farokhrouz",
        "Affiliation": "The University of Texas at Arlington"
      },
      {
        "Name": "Fariba Irany",
        "Affiliation": "University of North Texas"
      },
      {
        "Name": "Ali Khan",
        "Affiliation": "University of North Texas"
      },
      {
        "Name": "Sanjukta Bhowmick",
        "Affiliation": "University of North Texas"
      },
      {
        "Name": "Abhishek Santra",
        "Affiliation": "The Department of Computer Science and Engineering, University of Texas at Arlington"
      },
      {
        "Name": "Sharma Chakravarthy",
        "Affiliation": "The University of Texas at Arlington"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "57d761b8-2b71-4d41-b491-22df4a80f261",
    "title": "Cardinality Estimation for Having-Clauses",
    "abstract": "We present several methods for estimating the result cardinality of single table queries with a having clause. More specifically, we provide cardinality estimates for predicates using the aggregate functions count(*), sum(B), avg(B), min(B), and max(B). We do so for queries with and without a where-clause. Finally, we show how to handle conjunctions and disjunctions in the having-clause.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cardinality%20Estimation%20for%20Having-Clauses",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p28-moerkotte.pdf",
    "session": "Research 9: Query Processing and Optimization I",
    "authors": [
      {
        "Name": "Guido Moerkotte",
        "Affiliation": "University of Mannheim"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b6df9e2c-5d9f-42ce-af46-7d997433fd3a",
    "title": "TxnSails: Achieving Serializable Transaction Scheduling with Self-Adaptive Isolation Level Selection",
    "abstract": "Achieving the serializable isolation level is costly. Recent studies have revealed that adjusting specific query patterns within the workload can still achieve serializability, even at lower isolation levels. Nevertheless, these studies typically overlook the trade-off between the performance advantages of lower isolation levels and the overhead required to maintain serializability, potentially leading to suboptimal isolation level choices that fail to maximize performance. In this paper, we present TxnSails, a middle-tier solution designed to achieve serializable scheduling with self-adaptive isolation level selection. First, TxnSails incorporates a unified concurrency control algorithm that achieves serializability at lower isolation levels with minimal overhead. Second, TxnSails employs a deep learning method to characterize the trade-off between the performance benefits and overhead associated with lower isolation levels, thus predicting the optimal isolation level. Finally, TxnSails implements a cross-isolation validation mechanism to ensure serializability during real-time isolation level transitions. Extensive experiments demonstrate that TxnSails outperforms existing solutions by up to 26.7 √ó  and PostgreSQL ‚Äôs serializable isolation level by up to 4.8 √ó .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TxnSails%3A%20Achieving%20Serializable%20Transaction%20Scheduling%20with%20Self-Adaptive%20Isolation%20Level%20Selection",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4227-lu.pdf",
    "session": "Research 52: Transaction Management",
    "authors": [
      {
        "Name": "QiYu Zhuang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Wei Lu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Shuang Liu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yuxing Chen",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Xinyue Shi",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Zhanhao Zhao",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yipeng Sun",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Anqun Pan",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Xiaoyong Du",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "20380547-7449-471c-8059-45e77db624ab",
    "title": "Data Discovery in Data Lakes: Operations, Indexes, Systems",
    "abstract": "Data discovery has gained significant traction in the database community resulting in various discovery operations, index schemes, and discovery systems. This tutorial explores the architecture and components of data discovery systems, focusing on indexing structures and scalable algorithms for typical operations, such as join and union discovery. While giving insights into individual algorithms, we point out open challenges for holistic systems, data discovery evaluation, and discovery in federated setups.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Data%20Disovery%20in%20Data%20Lakes%3A%20Operations%2C%20Indexes%2C%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5455-abedjan.pdf",
    "session": "Tutorial 3: Data Discovery in Data Lakes: Operations, Indexes, Systems",
    "authors": [
      {
        "Name": "Ziawasch Abedjan",
        "Affiliation": "BIFOLD/TU Berlin"
      },
      {
        "Name": "Mahdi Esmailoghli",
        "Affiliation": "HU Berlin"
      },
      {
        "Name": "Sainyam Galhorta",
        "Affiliation": "Cornell University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "36983119-ca08-4eee-8922-fc4b6293a642",
    "title": "Natural Language to SQL: State of the Art and Open Problems",
    "abstract": "Translating users‚Äô natural language queries (nl) into sql queries ( i.e.,  nl2sql) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of nl2sql has been greatly improved with the emergence of large language models (LLMs). In this context, it is crucial to assess our current position, determine the nl2sql solutions that should be adopted for specific scenarios by practitioners, and identify the research topics that researchers should explore next. \nIn this tutorial, we will provide a comprehensive overview of nl2sql techniques, covering every aspect of its lifecycle, from the collection and synthesis of training data, recent advancements in nl2sql translation techniques using LLMs and agents, debugging ‚àó Yuyu Luo is the corresponding author. \nnl2sql processes, to multi-angle and scenario-based evaluation of nl2sql methods. We conclude by highlighting the research challenges and open problems in nl2sql.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Natural%20Language%20to%20SQL%3A%20State%20of%20the%20Art%20and%20Open%20Problems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5466-luo.pdf",
    "session": "Tutorial 9: Natural Language to SQL: State of the Art and Open Problems",
    "authors": [
      {
        "Name": "Yuyu Luo",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Ju Fan",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Chengliang Chai",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Nan Tang",
        "Affiliation": "HKUST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8bf30267-b6d1-4450-b8ba-9e6c3ef1ccbf",
    "title": "A Comprehensive Study of Shapley Value in Data Analytics",
    "abstract": "Over the recent years, Shapley value (SV), a solution concept from cooperative game theory, has found numerous applications in data analytics (DA). This paper presents the first comprehensive study of SV used throughout the DA workflow, clarifying the key variables in defining DA-applicable SV and the essential functionalities that SV can provide for data scientists. We condense four primary challenges of using SV in DA, namely computation efficiency, approximation error, privacy preservation, and interpretability, disentangle the resolution techniques from existing arts in this field, then analyze and discuss the techniques w.r.t. each challenge and the potential conflicts between challenges. We also implement  SVBench , a modular and extensible open-source framework for developing SV applications in different DA tasks, and conduct extensive evaluations to validate our analyses and discussions. Based on the qualitative and quantitative results, we identify the limitations of current efforts for applying SV to DA and highlight the directions of future research and engineering.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Comprehensive%20Study%20of%20Shapley%20Value%20in%20Data%20Analytics",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3077-xie.pdf",
    "session": "Research 14: User Interfaces for Data Exploration and Explainable AI",
    "authors": [
      {
        "Name": "Hong Lin",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Shixin Wan",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Zhongle Xie",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Ke Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Meihui Zhang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Lidan Shou",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "08b61771-7eb8-41c4-b127-bdf95faa2300",
    "title": "Not Small Enough? SegPQ: A Learned Approach to Compress Product Quantization Codebooks",
    "abstract": "The rapid advancements of generative artificial intelligence (GenAI) have recently led to renewed attention towards approximate nearest neighbor (ANN) search and vector databases (VectorDB). Among various ANN methodologies, vector quantization techniques like product quantization (PQ) are widely used to generate space-efficient representations for large-scale dense vectors. However, the codebooks generated by PQ often reach several gigabytes in size, making them impractical for web-scale, high-dimensional vectors in resource-constrained environments like mobile devices. \nIn this study, we propose  SegPQ , a simple yet effective framework for losslessly compressing codebooks generated by  any  PQ variants, enabling efficient  in-memory  vector search on devices with limited memory. SegPQ represents the raw PQ codewords as a trained error-bounded piecewise linear approximation model ( ùúñ PLA) and pre-computed low-bit residuals. We theoretically demonstrate that, with high probability, the number of bits per compressed codeword is 1 . 721  + ‚åà log 2  ùúñ OPT ‚åâ , where  ùúñ OPT   is the optimal error parameter that can be determined by data characteristics. To accelerate query execution, we further design SIMD-aware query processing algorithms on compressed codebooks to fully exploit the hardware parallelism offered by modern architectures. Extensive experimental studies on real datasets showcase that, for  1 billion  vectors, SegPQ reduces PQ codebook memory consumption by up to  4.7√ó  (approx.  851 MB ) while incurring only  3.3% additional query processing overhead caused by decompression.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Not%20Small%20Enough%EF%BC%9F%20SegPQ%3A%20A%20Learned%20Approach%20to%20Compress%20Product%20Quantization%20Codebooks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3730-liu.pdf",
    "session": "Research 40: Time Series and Vector Data",
    "authors": [
      {
        "Name": "Qiyu LIU",
        "Affiliation": "Southwest University"
      },
      {
        "Name": "Yanlin Qi",
        "Affiliation": "Harbin Institute of Technology, shenzhen"
      },
      {
        "Name": "Siyuan HAN",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Jingshu Peng",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Jin Li",
        "Affiliation": "Harvard University"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "HKUST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b2574299-3e42-414c-8ce3-b28d239dce78",
    "title": "Quantum Data Management in the NISQ Era",
    "abstract": "Quantum computing has emerged as a transformative force in the evolution of computing technology. Recent eÔ¨Äorts have applied quantum techniques to classical database challenges, such as query optimization, data integration, index selection, and transaction management. In this paper, we shift focus to a critical yet underexplored area:  data management for quantum computing . We are currently in the noisy intermediate-scale quantum (NISQ) era, where qubits, while promising, are fragile and still limited in scale. After diÔ¨Äerentiating quantum data from classical data, we outline current and future data management paradigms in the NISQ era and beyond. We address the data management challenges arising from the emerging demands of near-term quantum computing. Our goal is to chart a clear course for future quantum-oriented data management research, establishing it as a cornerstone for the advancement of quantum computing in the NISQ era.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Quantum%20Data%20Management%20in%20the%20NISQ%20Era",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1720-hai.pdf",
    "session": "Research 21: Specialized and Domain-Specific Data Management",
    "authors": [
      {
        "Name": "Rihan Hai",
        "Affiliation": "TU Delft"
      },
      {
        "Name": "Shih-Han Hung",
        "Affiliation": "National Taiwan University"
      },
      {
        "Name": "Tim Coopmans",
        "Affiliation": "TU Delft"
      },
      {
        "Name": "Tim Littau",
        "Affiliation": "TU Delft"
      },
      {
        "Name": "Floris Geerts",
        "Affiliation": "University of Antwerp"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "03ed0785-7408-4218-bf1b-0cc2ae329095",
    "title": "Hermes: Off-the-Shelf Real-Time Transactional Analytics",
    "abstract": "Many modern applications require real-time analytics, where analytical processing (AP) workloads needs access to the latest data updates from a transactional processing (TP) engine. However, managing separate TP and AP engines across teams complicates achieving real-time analytics without switching to specialized HTAP systems. To address this challenge, we introduce  oÔ¨Ä-the-shelf real-time analytics , a system design that leverages the existing TP and AP engines to provide (1) the latest transactional updates for analytical queries and (2) support for eÔ¨Écient  transactional analytics ‚Äìtransactions that combine transactional logic and analytical queries within a single ACID transaction‚Äìat various isolation levels. We demonstrate this concept with a new service called  Hermes , which acts as a middleware that merges log records with analytical reads without altering existing engines. Our evaluation utilizes two AP engines, FlexPushdownDB  and  DuckDB , with  MySQL  as the TP engine. Using the  HATtrick  benchmark and a new workload called  Transactional Analytics Workload  (TAW), we compare Hermes with the leading HTAP solution,  TiDB . Our results indicate that Hermes performs comparably to current HTAP solutions for real-time analytics and surpasses them by  3 √ó  in transactional analytics performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Hermes%3A%20Off-the-Shelf%20Real-Time%20Transactional%20Analytics",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2334-milkai.pdf",
    "session": "Research 64: Database Engines and Architectures",
    "authors": [
      {
        "Name": "Elena Milkai",
        "Affiliation": "UW Madison"
      },
      {
        "Name": "Xiangyao Yu",
        "Affiliation": "University of Wisconsin-Madison"
      },
      {
        "Name": "Jignesh Patel",
        "Affiliation": "Carnegie Mellon University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cf28b716-009d-4118-940c-0ff392f1d2d8",
    "title": "FB+-tree: A Memory-Optimized B+-tree with Latch-Free Update",
    "abstract": "B + -trees are prevalent in traditional database systems due to their versatility and balanced structure. While binary search is typically utilized for branch operations, it may lead to inefficient cache utilization in main-memory scenarios. In contrast, trie-based index structures drive branch operations through prefix matching. While these structures generally produce fewer cache misses and are thus increasingly popular, they may underperform in range scans because of frequent pointer chasing. \nThis paper proposes a new high-performance B + -tree variant called  Feature B + -tree (FB + -tree) . Similar to employing bit or byte for branch operation in tries, FB + -tree progressively considers several bytes following the common prefix on each level of its inner nodes‚Äîreferred to as features, which allows FB + -tree to benefit from prefix skewness. FB + -tree blurs the lines between B + -trees and tries, while still retaining balance. In the best case, FB + -tree almost becomes a trie, whereas in the worst case, it continues to function as a B + -tree. Meanwhile, a crafted synchronization protocol that combines the link technique and optimistic lock is designed to support efficient concurrent index access. Distinctively, FB + -tree leverages subtle atomic operations seamlessly coordinated with optimistic lock to facilitate latch-free updates, which can be easily extended to other structures. Intensive experiments on multiple workloaddataset combinations demonstrate that FB + -tree shows comparable lookup performance to state-of-the-art trie-based indexes and outperforms popular B + -trees by 2.3x  ‚àº 3.7x under 96 threads. FB + -tree also exhibits significant potential on other workloads, especially update workloads under contention and scan workloads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/FB%2B-tree%3A%20A%20Memory-Optimized%20B%2B-tree%20with%20Latch-Free%20Update",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1579-li.pdf",
    "session": "Research 54: Database Engines II",
    "authors": [
      {
        "Name": "Yuan Chen",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Ao Li",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Wenhai Li",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Lingfeng Deng",
        "Affiliation": "Wuhan University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6e810dc5-7a75-4ef0-b842-1c6b1a6a94eb",
    "title": "Fremer: Lightweight and Effective Frequency Transformer for Workload Forecasting in Cloud Services",
    "abstract": "Workload forecasting is pivotal in cloud service applications, such as auto-scaling and scheduling, with profound implications for operational efficiency. Although Transformer-based forecasting models have demonstrated remarkable success in general tasks, their computational efficiency often falls short of the stringent requirements in large-scale cloud environments. Given that most workload series exhibit complicated periodic patterns, addressing these challenges in the frequency domain offers substantial advantages. To this end, we propose  Fremer , an efficient and effective deep forecasting model.  Fremer  fulfills three critical requirements: it demonstrates superior efficiency, outperforming most Transformer-based forecasting models; it achieves exceptional accuracy, surpassing all state-of-the-art (SOTA) models in workload forecasting; and it exhibits robust performance for multi-period series. Furthermore, we collect and open-source four high-quality, open-source workload datasets derived from ByteDance‚Äôs cloud services, encompassing workload data from thousands of computing instances. Extensive experiments on both our proprietary datasets and public benchmarks demonstrate that  Fremer  consistently outperforms baseline models, achieving average improvements of 5.5% in MSE, 4.7% in MAE, and 8.6% in SMAPE over SOTA models, while simultaneously reducing parameter scale and computational costs. Additionally, in a proactive auto-scaling test based on Kubernetes,  Fremer  improves average latency by 18.78% and reduces resource consumption by 2.35%, underscoring its practical efficacy in real-world applications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Fremer%3A%20Lightweight%20and%20Effective%20Frequency%20Transformer%20for%20Workload%20Forecasting%20in%20Cloud%20Services",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3812-gao.pdf",
    "session": "Research 59: Distributed and Streaming Data Processing",
    "authors": [
      {
        "Name": "Hengyu Ye",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Jiadong Chen",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Fuxin Jiang",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Xiao He",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Tieying Zhang",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Jianjun Chen",
        "Affiliation": "ByteDance Inc."
      },
      {
        "Name": "Xiaofeng Gao",
        "Affiliation": "Shanghai Jiao Tong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3861a920-24a8-41fb-84a6-cfaf4da57a3a",
    "title": "Buckaroo: A Direct Manipulation Visual Data Wrangler",
    "abstract": "Preparing datasets‚Äîa critical phase known as data wrangling‚Äî constitutes the dominant phase of data science development, consuming upwards of 80% of the total project time. This phase encompasses a myriad of tasks: parsing data, restructuring it for analysis, repairing inaccuracies, merging sources, eliminating duplicates, and ensuring overall data integrity. Traditional approaches, typically through manual coding in languages such as Python or using spreadsheets, are not only laborious but also error-prone. These issues range from missing entries and formatting inconsistencies to data type inaccuracies, all of which can affect the quality of downstream tasks if not properly corrected. To address these challenges, we present Buckaroo, a visualization system to highlight discrepancies in data and enable on-the-spot corrections through direct manipulations of visual objects. Buckaroo (1) automatically finds ‚Äúinteresting‚Äù data groups that exhibit anomalies compared to the rest of the groups and recommends them for inspection; (2) suggests wrangling actions that the user can choose to repair the anomalies; and (3) allows users to visually manipulate their data by displaying the effects of their wrangling actions and offering the ability to undo or redo these actions, which supports the iterative nature of data wrangling.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Buckaroo%3A%20A%20Direct%20Manipulation%20Visual%20Data%20Wrangler",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5423-rezig.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Annabelle Warner",
        "Affiliation": "University of Utah"
      },
      {
        "Name": "Andrew McNutt",
        "Affiliation": "University of Utah"
      },
      {
        "Name": "Paul Rosen",
        "Affiliation": "University of Utah"
      },
      {
        "Name": "El Kindi Rezig",
        "Affiliation": "University of Utah"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dcf9105c-a57e-49db-80c6-b11366271015",
    "title": "Analytics Are Heavy. The DBMS Is Busy. When Will My Mission-Critical Transaction Start Running?",
    "abstract": "Conventional non-preemptive scheduling strategies struggle to meet the latency requirements of mixed workloads: low-priority, long-running analytics can dominate CPU cores while short, highpriority transactions wait a long time to be scheduled. Although preemptive scheduling appears to be a natural solution, it has long been discouraged in DBMSs by conventional wisdom due to concerns about deadlocks and interrupt-handling overheads. In this demonstration, we highlight that this is no longer the case with PreemptDB, a modern memory-optimized DBMS that we built around (1) optimistic concurrency and (2) userspace interrupts that recently became available in x86 CPUs. PreemptDB proposes user-interruptassisted context switching to renew preemptive scheduling in modern DBMSs. Through a set of demonstration scenarios, we show that preemptive scheduling is practical and prioritizes high-priority transactions while preserving throughput and fairness.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Analytics%20Are%20Heavy.%20The%20DBMS%20Is%20Busy.%20When%20Will%20My%20Mission-Critical%20Transaction%20Start%20Running%EF%BC%9F",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5299-zhou.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Jiatang Zhou",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Kaisong Huang",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Zhuoyue Zhao",
        "Affiliation": "University at Buffalo"
      },
      {
        "Name": "Dong Xie",
        "Affiliation": "Penn State University"
      },
      {
        "Name": "Tianzheng Wang",
        "Affiliation": "Simon Fraser University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "086d6ca0-5209-4821-b3c2-2e5c9137787a",
    "title": "SkyStore: Cost-Optimized Object Storage Across Regions and Clouds",
    "abstract": "Modern applications span multiple clouds to reduce costs, avoid vendor lock-in, and leverage low-availability resources in another cloud. However, standard object stores operate within a single cloud, forcing users to manually manage data placement across clouds, i.e., navigate their diverse APIs and handle heterogeneous costs for network and storage. This is often a complex choice: users must either pay to store objects in a remote cloud, or pay to transfer them over the network based on application access patterns and cloud provider cost offerings. To address this, we present SkyStore, a unified object store that addresses cost-optimal data management across regions and clouds. SkyStore introduces a virtual object and bucket API to hide the complexity of interacting with multiple clouds. At its core, SkyStore has a novel TTL-based data placement policy that dynamically replicates and evicts objects according to application access patterns while optimizing for lower cost. Our evaluation shows that across various workloads, SkyStore reduces the overall cost by up to 6 √ó  over academic baselines and commercial alternatives like AWS multi-region buckets. SkyStore also has comparable latency, and its availability and fault tolerance are on par with standard cloud offerings.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SkyStore%3A%20Cost-Optimized%20Object%20Storage%20Across%20Regions%20and%20Clouds",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2084-liu.pdf",
    "session": "Research 1: Cloud Data Management",
    "authors": [
      {
        "Name": "Shu Liu",
        "Affiliation": "University of California Berkeley"
      },
      {
        "Name": "Xiangxi Mo",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Moshik Hershcovitch",
        "Affiliation": "IBM Research"
      },
      {
        "Name": "Henric Zhang",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Audrey Cheng",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Guy Girmonsky",
        "Affiliation": "IBM"
      },
      {
        "Name": "Gil Vernik",
        "Affiliation": "IBM"
      },
      {
        "Name": "Michael Factor",
        "Affiliation": "IBM Research"
      },
      {
        "Name": "Tiemo Bang",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Soujanya Ponnapalli",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Natacha Crooks",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Joseph Gonzalez",
        "Affiliation": "U.C. Berkeley"
      },
      {
        "Name": "Danny Harnik",
        "Affiliation": "IBM Research"
      },
      {
        "Name": "Ion Stoica",
        "Affiliation": "UC Berkeley"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "da94a453-6928-4a98-8f15-037cfcd020be",
    "title": "MSGNN: Masked Schema based Graph Neural Networks",
    "abstract": "Heterogeneous graph representation learning aims to extract low-Heterogeneous graph representation learning aims to extract lowdimensional node representations from complex networks with different types of entities and relationships. With the prevalence of heterogeneous information networks (HINs) in real-world sce-of heterogeneous information networks (HINs) in real-world scenarios, it is of vital significance for a network embedding model to handle heterogeneity and capture as much semantic information as possible. Existing works can be roughly categorized into meta-as possible. Existing works can be roughly categorized into metapath-based and adjacent matrix-based methods depending on their definition of node neighborhoods. Meta-path-based methods aim to capture semantic similarities but require manual design. Adjacent matrix-based methods focus on structural information but may risk losing semantic context. In this work, we propose using schema instances representing node minimal complete contexts to embed HINs, aiming to integrate the advantages of both methods and avoid their deficiencies. We introduce Masked Schema based Graph Neural Networks (MSGNN), which combines schema instances with bi-level self-supervised learning and mask technique to ac-with bi-level self-supervised learning and mask technique to acquire effective context representations. Furthermore, we propose a decomposition-reconstruction schema instance retrieval strategy to ensure efficient instance searching. Comprehensive experiments demonstrate that MSGNN outperforms state-of-the-art models. In the link prediction task, the F1-score has improved by up to 16.08% compared to the suboptimal method.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/MSGNN%3A%20Masked%20Schema%20based%20Graph%20Neural%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p571-liu.pdf",
    "session": "Research 10: Data Mining and Analytics",
    "authors": [
      {
        "Name": "Hao Liu",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Qianwen Yang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Taoyong Cui",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Wei Wang",
        "Affiliation": "\" Fudan University, China\""
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c2edc43d-344b-45a6-a2e3-d00f942fc59a",
    "title": "GeoBloom: Revisiting Lightweight Models for Geographic Information Retrieval",
    "abstract": "Geographic Information Retrieval (GIR) systems process text queries with geographic location to identify relevant geographic objects for users. Although recent advancements have leveraged Pre-trained Language Models (PLMs) for their robust semantic comprehension, these models typically depend on extensive labeled queries and re- quire considerable computational resources. Deviating from this prevailing trend, we propose GeoBloom, a lightweight framework that surpasses the effectiveness of PLMs with fewer or no labeled queries, with remarkable efficiency in both time and space.\nGeoBloom tackles critical challenges such as the lack of labeled queries, low data (labeled) efficiency, and high computational de- mands. At its core, it employs Bloom filters to encode text at a fine-grained term level and uses intersecting bits to create a ro- bust unsupervised text similarity metric. A specialized Bloom Filter Evaluator is proposed to assess the importance of each intersect- ing bit, focusing on those associated with ground truth, improving effectiveness with fewer training labels. For enhanced search effi- ciency, the evaluator exploits the inherent sparsity of Bloom filters, achieving remarkably low time and space complexities. This effi- ciency is further boosted by a tree-based index that partitions the search space while preserving effectiveness. Extensive experiments show that GeoBloom surpasses state-of-the-art baselines in both unsupervised (up to 15.66% improvement) and supervised settings (up to 10.94% improvement) on real datasets in terms of NDCG@5. Furthermore, GeoBloom operates up to 80x faster and saves up to 74.72% memory and 87.64% disk space over PLM-based alternatives, rendering it highly potent for real-world applications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GeoBloom%3A%20Revisiting%20Lightweight%20Models%20for%20Geographic%20Information%20Retrieval",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1348-li.pdf",
    "session": "Research 11: Graph Data Privacy, Text and Semi-Structured Data Management",
    "authors": [
      {
        "Name": "YI LI",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Gao Cong",
        "Affiliation": "Nanyang Technological Univesity"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "62e15fdc-60ae-4ae4-a733-c83ff4231b3b",
    "title": "Fucci: Database Transaction Fuzzing via Random Conflict Construction and Multilevel Constraint Solving",
    "abstract": "Ensuring the ACID properties of transactions is the fundamental functionality of transactional DBMSs. However, through our study on existing solutions on transaction management, we found that transaction implementations in some mainstream databases, such as MySQL, MariaDB and TiDB, may violate what they claim in their documentation, in the form of incorrect database state or query results. Since there is still a lack of efficient and comprehensive testing methods to detect bugs within transaction management implementation for off-the-shelf DBMSs at present, we propose Fucci, a fuzzing framework, to solve the problem. Given a target DBMS, Fucci improves the efficiency of detecting transaction bugs through three key components: Random Conflict Construction (RCC), Multilevel Constraint Solving (MCS), and Experience-driven Automatic Simplification (EAS). RCC addresses the issue of inadequate case validity by ensuring the presence of read-write or write-write conflicts between transactions. MCS enhances the accuracy and efficiency of the transaction oracle by employing an external multi-version control system to solve data visibility. EAS is ultimately adopted to improve the efficiency of simplification and the readability of the identified bug cases. All of the above strategies are tested on commercial databases such as MySQL, MariaDB and TiDB. Accordingly, 6 previously unknown transaction bugs and 14 known duplicate transaction bugs have been newly discovered, most of which have been officially acknowledged.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Fucci%3A%20Database%20Transaction%20Fuzzing%20via%20Random%20Conflict%20Construction%20and%20Multilevel%20Constraint%20Solving",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1879-li.pdf",
    "session": "Research 44: Database Engine Performance and Manageability I",
    "authors": [
      {
        "Name": "Xiyue Gao",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "zhuang liu",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Yiran Shen",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Hui Li",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Yingfan Liu",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "hongjun xiao",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Yanguo Peng",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Jiangtao Cui",
        "Affiliation": "Xidian University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c84c63ea-a099-40cb-b8e9-a1f11044147a",
    "title": "AnyBlox: A Framework for Self-Decoding Datasets",
    "abstract": "Research advancements in storage formats continuously produce more efficient encodings and better compression rates. Despite this, new formats are not adopted due to high implementation cost and existing formats cannot evolve because they need to maintain compatibility across systems. Can this problem be solved by introducing a new abstraction? We answer affrmatively with AnyBlox, a framework for reading arbitrary datasets using lightweight WebAssembly decoders bundled with the data. By decoupling decoders from both systems and file format specifications, AnyBlox allows transparent format evolution, instance-optimized encodings, and enables mainstream adoption of research advancements. It integrates seamlessly with modern systems like DuckDB, Spark, and Umbra, while delivering solid performance and security guarantees.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/AnyBlox%3A%20A%20Framework%20for%20Self-Decoding%20Datasets",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4017-gienieczko.pdf",
    "session": "Research 64: Database Engines and Architectures",
    "authors": [
      {
        "Name": "Mateusz Gienieczko",
        "Affiliation": "TUM"
      },
      {
        "Name": "Maximilian Kuschewski",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Thomas Neumann",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Viktor Leis",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Jana Giceva",
        "Affiliation": "Technical University of Munich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0f2cd037-1d53-4d2c-96e4-387ee38e03e2",
    "title": "TableCopilot: A Table Assistant Empowered by Natural Language Conditional Table Discovery",
    "abstract": "The rise of LLM has enabled natural language-based table assistants, but existing systems assume users already have a well-formed table, neglecting the challenge of table discovery in large-scale table pools. To address this, we introduce TableCopilot, an LLM-powered assistant for interactive, precise, and personalized table discovery and analysis. We define a novel scenario, nlcTD, where users provide both a natural language condition and a query table, enabling intuitive and flexible table discovery for users of all expertise levels. To handle this, we propose Crofuma, a cross-fusion-based approach that learns and aggregates single-modal and cross-modal matching scores. Experimental results show Crofuma outperforms SOTA single-input methods by at least 12% on NDCG@5. We also release an instructional video, codebase, datasets, and other resources on GitHub to encourage community contributions. TableCopilot sets a new standard for interactive table assistants, making advanced table discovery accessible and integrated.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TableCopilot%3A%20A%20Table%20Assistant%20Empowered%20by%20Natural%20Language%20Conditional%20Table%20Discovery",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5399-cui.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Lingxi Cui",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Guanyu Jiang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Huan Li",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Ke Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lidan Shou",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "03841ca8-6da1-42ca-9aa8-6a16a17d0b79",
    "title": "Database Perspective on LLM Inference Systems",
    "abstract": "Large language models (LLMs) are powering a new wave of languagebased applications, including database applications, leading to new techniques and systems for dealing with the enormous compute and memory needs of LLMs, coupled with advances in computing hardware. In this tutorial, we review how these techniques lower inference costs by managing uncertain request lifecycles, exploiting specialized hardware, and scaling over distributed inference devices and machines. We present these techniques from the database perspective of request processing, model execution and optimization, and memory management. Following these discussion, we review how inference systems combine these techniques in diverse architectures to achieve application or performance objectives.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Database%20Perspective%20on%20LLM%20Inference%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5504-li.pdf",
    "session": "Tutorial 13: Database Perspective on LLM Inference Systems",
    "authors": [
      {
        "Name": "James Pan",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fe0c0c73-1f5b-436b-ba57-ca3e83c21d82",
    "title": "Noise Matters: Cross Contrastive Learning for Flink Anomaly Detection",
    "abstract": "Flink clusters often suffer from hotspot issues where the monitored job delay and CPU usage keep rising and remain high. This necessitates the detection of anomalous time series to pinpoint the hotspot machines. However, the state-of-the-art unsupervised time series anomaly detection (UTAD) methods are ineffective in this scenario. We identify two main reasons for this. First, the hotspot scenario requires us to pay particular attention to Flink-specific anomalies, e.g., slow-rising and high-level anomalies, which the existing methods struggle to address. Second, the state-of-the-art anomaly detection methods often assume that training datasets do not contain anomalies, but the data collected from the running Flink clusters contains noise, which causes these methods to learn anomalous patterns as normal patterns. In this paper, we first conduct experiments to analyze why existing methods fail in the Flink scenario. To tackle these challenges, we propose a cross-contrastive approach to learn the context information for each timestamp to enable Flink-specific anomaly detection. Then, to address noisy anomalies, we incorporate prior knowledge to set an anomaly boundary to prevent the model from learning anomalous patterns. Extensive experiments show that our method not only outperforms existing methods in the Flink scenario but also achieves state-of-the-art results on public benchmark datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Noise%20Matters%3A%20Cross%20Contrastive%20Learning%20for%20Flink%20Anomaly%20Detection",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1159-zhuang.pdf",
    "session": "Research 25: Time Series Data III",
    "authors": [
      {
        "Name": "zhihao zhuang",
        "Affiliation": "ECNU"
      },
      {
        "Name": "Yingying Zhang",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "kai zhao",
        "Affiliation": "AAU"
      },
      {
        "Name": "Chenjuan Guo",
        "Affiliation": "ECNU"
      },
      {
        "Name": "Bin Yang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Qingsong Wen",
        "Affiliation": "Alibaba Group U.S."
      },
      {
        "Name": "Lunting Fan",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0a5613ac-327e-4caf-9479-18a2471cc63a",
    "title": "Learned Cost Models for Query Optimization: From Batch to Streaming Systems",
    "abstract": "Learned cost models (LCMs) have recently gained traction as a promising alternative to traditional cost estimation techniques in data management, offering improved accuracy by capturing complex interactions between queries, data, and runtime behavior. While initially developed for batch systems, LCMs are now increasingly applied to stream processing as well, where real-time demands pose new challenges. This tutorial presents the first unified overview of LCMs across both batch and stream processing systems, examining their role as essential components in modern query optimizers. We explore key aspects of LCM design‚Äîincluding input representations and model architectures‚Äîand highlight how these models deal with query optimization tasks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Learned%20Cost%20Models%20for%20Query%20Optimization%3A%20From%20Batch%20to%20Streaming%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5482-li.pdf",
    "session": "Tutorial 8: Learned Cost Models for Query Optimization: From Batch to Streaming Systems",
    "authors": [
      {
        "Name": "Roman Heinrich",
        "Affiliation": "TU Darmstadt & DFKI"
      },
      {
        "Name": "Xiao Li",
        "Affiliation": "IT University of Copenhagen"
      },
      {
        "Name": "Manisha Luthra",
        "Affiliation": "TU Darmstadt & DFKI"
      },
      {
        "Name": "Zoi Kaoudi",
        "Affiliation": "IT University of Copenhagen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ddd00571-e05c-44ff-9d2d-be4ca8310996",
    "title": "Unleashing Graph Partitioning for Large-Scale Nearest Neighbor Search",
    "abstract": "We consider the fundamental problem of decomposing a largescale approximate nearest neighbor search (ANNS) problem into smaller sub-problems. The goal is to partition the input points into neighborhood-preserving  shards , so that the nearest neighbors of any point are contained in only a few shards. When a query arrives, a  routing  algorithm is used to identify the shards which should be searched for its nearest neighbors. This approach forms the backbone of distributed ANNS, where the dataset is so large that it must be split across multiple machines. \nIn this paper, we design simple and highly efficient routing methods based on clustering and locality-sensitive hashing. We prove strong theoretical guarantees for the LSH-based method, whereas the clustering-based method exhibits better empirical performance. A crucial characteristic of our routing algorithms is that they are inherently modular, and can be used with  any  partitioning method. This addresses a key drawback of prior approaches, where the routing algorithms are inextricably linked to their associated partitioning method. In particular, due to their modular structure, our routing methods enable the use of  balanced graph partitioning , which is a high-quality partitioning method without a naturally associated routing algorithm. Prior routing methods compatible with graph partitioning are too slow to train on large-scale data. \nWe provide the first routing methods that are simultaneously compatible with graph partitioning, fast to train, admit low latency, and achieve high recall. In a comprehensive evaluation of our partitioning and routing on billion-scale datasets, we show that our methods outperform existing scalable partitioning methods by significant margins, achieving up to 1 . 72 √ó  higher QPS at 90% 10-recall than the best competitor and 1 . 27 √ó  in the geometric mean. Through fast and modular routing we establish graph partitioning as the new method of choice for partitioning large-scale ANNS datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Unleashing%20Graph%20Partitioning%20for%20Large-Scale%20Nearest%20Neighbor%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1649-gottesbueren.pdf",
    "session": "Research 31: Vector Data Management II",
    "authors": [
      {
        "Name": "Lars Gottesbueren",
        "Affiliation": "Google"
      },
      {
        "Name": "Laxman Dhulipala",
        "Affiliation": "University of Maryland, College Park"
      },
      {
        "Name": "Rajesh Jayaram",
        "Affiliation": "Google Research"
      },
      {
        "Name": "Jakub ≈ÅƒÖcki",
        "Affiliation": "Google"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eb13928b-bb09-4310-bfad-07368da60d88",
    "title": "RICH: Real-time Identification of negative Cycles for High-efficiency Arbitrage",
    "abstract": "Arbitrage is a challenging data science problem characterized by rapidly fluctuating price discrepancies across multiple markets, necessitating real-time solutions. To overcome the challenge, we model it as a k-hop negative cycle detection problem in graphs and introduce  RICH :  Real-time  Identification of negative  Cycles for High-efficiency arbitrage.  RICH  is a novel framework that leverages color-coding and dynamic programming to accelerate the identification of negative-weight cycles without exhaustive graph traversal. Additionally,  RICH  incorporates encoding techniques and graph reduction to minimize computational overhead while maintaining probabilistic guarantees. Our extensive experiments on real-world datasets demonstrate that  RICH  is up to  32.69 √ó  faster than state-of-the-art methods, enabling timely arbitrage execution while outperforming existing methods in both speed and accuracy. We further validate its effectiveness in identifying arbitrage opportunities in cryptocurrency markets and foreign exchange markets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RICH%3A%20Real-time%20Identification%20of%20negative%20Cycles%20for%20High-efficiency%20Arbitrage",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4081-luo.pdf",
    "session": "Research 65: Graph Data Management VIII",
    "authors": [
      {
        "Name": "Bingqiao Luo",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Jiaxin Jiang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Yuhang Chen",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Junyi Hou",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Cheng Jun Tey",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Ziyang Qiu",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Bingsheng He",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Spencer Xiao",
        "Affiliation": "Tokka Labs"
      },
      {
        "Name": "Dominic Ong",
        "Affiliation": "Tokka Labs"
      },
      {
        "Name": "Wee Howe Ang",
        "Affiliation": "Tokka Labs"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "02032aa4-16c6-46e9-8ba2-e661ba556dc1",
    "title": "DECK: Experiences on Delta Checkpointing for Industrial Recommendation Systems",
    "abstract": "In large-scale industrial recommendation systems, model checkpoints are instrumental in maintaining training goodput and numerical correctness during system failures and job preemptions. The increasing prevalence of multi-terabyte models has rendered frequent regular model checkpoints impractical, resulting in substantial lost progress when recovering from failures. As model sizes continue to grow, researchers and practitioners are compelled to investigate more e!cient and scalable solutions. This paper presents DECK, a novel approach to delta model checkpointing designed for real-world industrial systems. Specifically, DECK focuses on extracting delta states with near-zero overhead, staging and streaming delta checkpoints without interrupting the training process, and merging delta checkpoints in an optimal and decoupled manner. Experimental results demonstrate that DECK achieves a 12-fold increase in checkpoint frequency while maintaining negligible impact on training throughput, thereby attaining state-of-the-art (SOTA) production performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DECK%3A%20Experiences%20on%20Delta%20Checkpointing%20for%20Industrial%20Recommendation%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4978-gao.pdf",
    "session": "Industry 4: Machine Learning, AI, and Databases",
    "authors": [
      {
        "Name": "Xin Gao",
        "Affiliation": "Meta"
      },
      {
        "Name": "Sibasish Acharya",
        "Affiliation": "Meta"
      },
      {
        "Name": "Sihui Han",
        "Affiliation": "Meta"
      },
      {
        "Name": "Yongxiong Ren",
        "Affiliation": "Meta"
      },
      {
        "Name": "Yanli Zhao",
        "Affiliation": "Meta"
      },
      {
        "Name": "Liang Luo",
        "Affiliation": "Meta"
      },
      {
        "Name": "Chucheng Wang",
        "Affiliation": "Meta"
      },
      {
        "Name": "Pradeep Fernando",
        "Affiliation": "Meta"
      },
      {
        "Name": "Saurabh Mishra",
        "Affiliation": "Meta"
      },
      {
        "Name": "Siqi Yan",
        "Affiliation": "Meta"
      },
      {
        "Name": "Yicong Du",
        "Affiliation": "Meta"
      },
      {
        "Name": "Elzbieta Krepska",
        "Affiliation": "Meta"
      },
      {
        "Name": "Intaik Park",
        "Affiliation": "Meta"
      },
      {
        "Name": "Min Ni",
        "Affiliation": "Meta"
      },
      {
        "Name": "Qunshu Zhang",
        "Affiliation": "Meta"
      },
      {
        "Name": "Shen Li",
        "Affiliation": "Meta"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3a316f98-5459-4793-a927-494b30fe780e",
    "title": "UniClean: A Scalable Data Cleaning Solution for Mixed Errors based on Unified Cleaners and Optimized Cleaning Workflow",
    "abstract": "Data cleaning is an essential technique to enhance data quality. Despite the proposal of various algorithms with diÔ¨Äerent cleaning strategies, current automated cleaning technologies still fall short of practical requirements when dealing with large-scale data containing mixed errors. This paper presents  UniClean  to eÔ¨Éciently solve the mixed error cleaning problem with three key technical contributions. (1) A uniÔ¨Åed construction and extension method for cleaners, enabling cleaning methods to easily utilize various cleaners to perform cleaning tasks. (2) Three optimization strategies to achieve eÔ¨Éciency-oriented cleaning preparation. (3) A cleaning algorithm based on an optimized cleaning process to effectively clean mixed errors.  UniClean  achieves a time complexity of  O(| D error | 4 ¬∑ | Op| + | D | ¬∑ | D error |) , signiÔ¨Åcantly enhancing scalability. Experiments on public and large-scale enterprise datasets demonstrate that  UniClean  achieves over 40% improvement across Ô¨Åve metrics, compared to Ô¨Åve state-of-the-art cleaning methods, and delivers more than 30% gains in  F1  and  REDR  on complex datasets, while completing the cleaning process within hours even for millions of records.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/UniClean%3A%20A%20Scalable%20Data%20Cleaning%20Solution%20for%20Mixed%20Errors%20based%20on%20Unified%20Cleaners%20and%20Optimized%20Cleaning%20Workflow",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4117-wang.pdf",
    "session": "Research 5: Data Cleaning and Preparation I",
    "authors": [
      {
        "Name": "Xiaoou Ding",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "zekai qian",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Hongzhi Wang",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Siying Chen",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Yafeng Tang",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Hongbin Su",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Huan Hu",
        "Affiliation": "Huawei Cloud Computing Technologies"
      },
      {
        "Name": "Chen Wang",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "83e138ba-2628-4e5e-8b61-4099b7623067",
    "title": "CUTTANA: Scalable Graph Partitioning for Faster Distributed Graph Databases and Analytics",
    "abstract": "Graph partitioning plays a pivotal role in various distributed graph processing applications, including graph analytics, graph neural network training, and distributed graph databases. A ‚Äúgood‚Äù graph partitioner reduces workload execution time, worker imbalance, and network overhead. Graphs that require distributed settings are often too large to fit in the main memory of a single machine. This challenge renders traditional in-memory graph partitioners infea- sible, leading to the emergence of streaming solutions. Streaming partitioners produce lower-quality partitions, because they work from partial information and must make premature decisions be- fore they have a complete view of a vertex‚Äôs neighborhood. We introduce CUTTANA, a streaming graph partitioner that partitions massive graphs (Web/Twitter scale) with superior quality compared to existing streaming solutions. CUTTANA uses a novel buffering technique that prevents the premature assignment of vertices to partitions and a scalable coarsening and refinement technique that enables a complete graph view, improving the intermediate assign- ment made by a streaming partitioner. We implemented a parallel version for CUTTANA that offers nearly the same partitioning latency as existing streaming partitioners.\nOur experimental analysis shows that CUTTANA consistently yields better partitioning quality than state-of-the-art streaming vertex partitioners in terms of both edge-cut and communication volume metrics. We also evaluate the workload latencies that re- sult from using CUTTANA and other partitioners in distributed graph analytics and databases. CUTTANA outperforms the other methods in most scenarios (algorithms, datasets). In analytics ap- plications, CUTTANA improves runtime performance by up to 59% compared to various streaming partitioners (i.e., HDRF, Fennel, Ginger, HeiStream). In graph database tasks, CUTTANA results in higher query throughput by up to 23%, without hurting tail latency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/CUTTANA%3A%20Scalable%20Graph%20Partitioning%20for%20Faster%20Distributed%20Graph%20Databases%20and%20Analytics",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p14-hajidehi.pdf",
    "session": "Research 57: Graph Data Management VII",
    "authors": [
      {
        "Name": "Milad Rezaei Hajidehi",
        "Affiliation": "University of British Columbia"
      },
      {
        "Name": "Sraavan Sridhar",
        "Affiliation": "University of British Columbia"
      },
      {
        "Name": "Margo Seltzer",
        "Affiliation": "University of British Columbia"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a493b001-900b-4eb1-ad3b-4f2aa10d0d7f",
    "title": "Generalizable Data Cleaning of Tabular Data in Latent Space",
    "abstract": "In this paper, we present a new method for learned data cleaning. In contrast to existing methods, our method learns to clean data in the latent space. The main idea is that we (1) shape the latent space such that we know the area where clean data resides and (2) learn latent operators trained on error repair (Lopster) which shift erroneous data (e.g., table rows with noise, outliers, or missing values) in their latent representation back to a ‚Äúclean‚Äù region, thus abstracting the complexities of the input domain. When formulating data cleaning as a simple shift operation in latent space, we can repair all types of errors using the same method which makes it more robust than other methods. Importantly, with our method, we can handle errors that are unseen during the training of our error repair model. We do not rely on an external error detection method as seen in the state-of-the-art, instead, we handle both detection and repair within the Lopster framework. In our evaluation, we show that our approach outperforms existing cleaning methods even when trained on only a subset of the errors that occur in the dirty data.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Generalizable%20Data%20Cleaning%20of%20Tabular%20Data%20in%20Latent%20Space",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4786-reis.pdf",
    "session": "Research 30: Information Integration and Data Quality I",
    "authors": [
      {
        "Name": "Eduardo S Reis",
        "Affiliation": "TU Darmstadt"
      },
      {
        "Name": "Mohamed Abdelaal",
        "Affiliation": "Software AG"
      },
      {
        "Name": "Carsten Binnig",
        "Affiliation": "TU Darmstadt"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "107c2af9-4112-4f15-883e-ad3118034069",
    "title": "Robust Recursive Query Parallelism in Graph Database Management Systems",
    "abstract": "Recursive Query Parallelism in Graph Database",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Robust%20Recursive%20Query%20Parallelism%20in%20Graph%20Database%20Management%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4465-chakraborty.pdf",
    "session": "Research 9: Query Processing and Optimization I",
    "authors": [
      {
        "Name": "Anurag Chakraborty",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Semih Salihoglu",
        "Affiliation": "University of Waterloo"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "24d2ee42-382f-4b55-a9cb-3d8e16a19231",
    "title": "AQETuner: Reliable Query-level Configuration Tuning for Analytical Query Engines",
    "abstract": "Modern analytical query engines ( AQEs ) are essential for large-scale data analysis and processing. These systems usually provide numerous query-level tunable knobs that signi!cantly a\"ect individual query performance. While several studies have explored automatic DBMS con!guration tuning, they have several limitations to handle query-level tuning. Firstly, they fail to capture how knobs influence query plans, which directly affect query performance. Secondly, they overlook query failures during the tuning processing, resulting in low tuning efficiency. Thirdly, they struggle with cold-start problems for new queries, leading to prolonged tuning time. To address these challenges, we propose  AQETuner , a novel Bayesian Optimization-based system tailored for  reliable  query-level knob tuning in  AQEs .  AQETuner  !rst applies the attention mechanisms to jointly encode the knobs and plan query, effectively identifying the impact of knobs on plan nodes. Then,  AQETuner  employs a dual-task Neural Process to predict both query performance and failures, leveraging their interactions to guide the tuning process. Furthermore,  AQETuner  utilizes Particle Swarm Optimization to ef!ciently generate high-quality samples in parallel during the initial tuning stage for the new queries. Experimental results show that AQETuner  signi!cantly outperforms existing methods, reducing query latency by up to 23.7% and query failures by up to 51.2%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/AQETuner%3A%20Reliable%20Query-level%20Configuration%20Tuning%20for%20Analytical%20Query%20Engines",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2709-han.pdf",
    "session": "Research 23: Applied ML and AI for Data Management III",
    "authors": [
      {
        "Name": "Lixiang Chen",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Yuxing Han",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Yu Chen",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Xing Chen",
        "Affiliation": "Bytedance Inc"
      },
      {
        "Name": "Chengcheng Yang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Weining Qian",
        "Affiliation": "East China Normal University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ff3f5f14-1fe7-46cb-b745-610c05f9e59f",
    "title": "GORAM: Graph-oriented ORAM for Efficient Ego-centric Queries on Federated Graphs",
    "abstract": "Ego-centric  queries, focusing on a target vertex and its direct neighbors, are essential for various applications. Enabling such queries on graphs owned by mutually distrustful data providers without breaching privacy holds promise for more comprehensive results. \nIn this paper, we propose  GORAM , a graph-oriented data structure that enables efficient ego-centric queries on federated graphs with strong privacy guarantees.  GORAM  leverages  secure multiparty computation (MPC)  and ensures that no information about the graphs or the querying keys is exposed during the process. For practical performance,  GORAM  partitions the federated graph and constructs an  Oblivious RAM (ORAM) -inspired index atop these partitions. This design enables each ego-centric query to process only a single partition, which can be accessed fast and securely. \nUtilizing  GORAM , we develop a prototype querying engine on a real-world MPC framework. We then conduct a comprehensive evaluation using five commonly used queries similar to the LinkBench workload description [ 11 ] on both synthetic and real-world graphs. Our evaluation shows that all five queries can be completed in just 58.1 milliseconds to 35.7 seconds, even on graphs with up to 41.6 million vertices and 1.4 billion edges. To the best of our knowledge, this represents the first instance of processing billion-scale graphs with practical performance on MPC.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GORAM%3A%20Graph-oriented%20ORAM%20for%20Efficient%20Ego-centric%20Queries%20on%20Federated%20Graphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3601-fan.pdf",
    "session": "Research 11: Graph Data Privacy, Text and Semi-Structured Data Management",
    "authors": [
      {
        "Name": "Xiaoyu Fan",
        "Affiliation": "IIIS, Tsinghua University"
      },
      {
        "Name": "Kun Chen",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Jiping Yu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xiaowei Zhu",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Yunyi Chen",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Huanchen Zhang",
        "Affiliation": "IIIS, Tsinghua University"
      },
      {
        "Name": "Wei Xu",
        "Affiliation": "IIIS, Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2525e283-fd10-409b-90f2-801d2c33eb49",
    "title": "PS-MI: Accurate, Efficient, and Private Data Valuation in Vertical Federated Learning",
    "abstract": "Vertical federated learning (VFL) trains models when multiple databases (a.k.a participants) hold different features of the same set of samples. By quantifying each participant‚Äôs contribution to model training,  data valuation  can prevent hitch-riders and reward the instrumental parties. However, vertical federated data valuation (VFDV) is challenging because it needs to be accurate and efficient while protecting participant data privacy. In this paper, we propose a method meeting all three requirements by using  projection  and sampling  for  mutual information  estimation (thus dubbed PS-MI). In particular, we first show that the utility of a participant set (a.k.a a coalition ) can be expressed as the mutual information (MI) between their features and the target labels. MI is favorable because it does not depend on the model to train (i.e.,  model-agnostic ) and can be estimated via ùëò -nearest neighbor (KNN). To run KNN, instead of using costly homomorphic encryption to protect data privacy, we apply simple  random projection  to participant features before distance computation. We prove that random projection ensures differential privacy and preserves unbiased distance estimates. Since the contribution of a participant involves many coalitions, we adopt  stratified sampling  to reduce the number of coalitions while controlling estimation variance. To further improve efficiency, we incorporate optimizations including using locality sensitive hashing (LSH) to prune kNN candidates, batching kNN candidate checking for multiple coalitions, and adaptive early termination for utility evaluation. We compare PS-MI with 5 state-of-the-art VFDV methods. The results show that PS-MI yields higher accuracy and shorter running time than the baselines, and the maximum speedup can be 592 √ó .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PS-MI%3A%20Accurate%2C%20Efficient%2C%20and%20Private%20Data%20Valuation%20in%20Vertical%20Federated%20Learning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3559-zhou.pdf",
    "session": "Research 30: Information Integration and Data Quality I",
    "authors": [
      {
        "Name": "Xiaokai Zhou",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Xiao Yan",
        "Affiliation": "Centre for Perceptual and Interactive Intelligence"
      },
      {
        "Name": "Fangcheng Fu",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Ziwen Fu",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Tieyun Qian",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Yuanyuan Zhu",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Qinbo Zhang",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Bin Cui",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Jiawei Jiang",
        "Affiliation": "Wuhan Univeristy"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d2c68156-e042-4408-b871-40eb5e811d70",
    "title": "CXL Memory Performance for In-Memory Data Processing",
    "abstract": "The Compute Express Link (CXL) standard enables new forms of memory management and access across devices and servers. Based on PCIe, it enables cache-coherent access to remote memory. This widens the design space for database systems by expanding the available memory beyond memory local to the CPU. Efficiently utilizing CXL-attached memory requires conscious decisions by data systems about data placement and management. In this paper, we provide an in-depth analysis of database operation performance with data interleaved across multiple CXL memory devices. We experimentally evaluate the memory access performance for basic access patterns, the performance impact of placing data across multiple CXL memory devices for in-memory column scans and inmemory B+tree operations, and the performance impact of placing data in CXL memory for an in-memory database system when running the analytical TPC-H workload. Our experiments show that access to CXL-attached memory does not have to penalize performance over local access, but careful workload-aware data management is required. Our TPC-H evaluation shows that placing table columns based on access frequencies allows storing over 80% of the table data in CXL memory with a performance of 85% of a local-memory-only solution.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/CXL%20Memory%20Performance%20for%20In-Memory%20Data%20Processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3119-weisgut.pdf",
    "session": "Research 38: Data Management on Novel Hardware",
    "authors": [
      {
        "Name": "Marcel Weisgut",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Daniel Ritter",
        "Affiliation": "SAP"
      },
      {
        "Name": "Pinar T√∂z√ºn",
        "Affiliation": "IT University of Copenhagen"
      },
      {
        "Name": "Lawrence Benson",
        "Affiliation": "Technische Universit√§t Munich"
      },
      {
        "Name": "Tilmann Rabl",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9fcaffe5-c476-4ee5-94ca-90896e64d098",
    "title": "RGS-Sketch: An Accurate, Invertible, and Mergeable Sketch for Online Super Spreader Detection in High-speed Data Streams",
    "abstract": "Super spreader detection in high-speed data streams is crucial for numerous applications. Although many methods have emerged, existing works can hardly concurrently achieve high memory efficiency, support online detection, enable merging data from different measurement points/periods, and offer invertibility. This makes them unable to satisfy flexible application requirements. This paper proposes RGS-Sketch, a novel sketch designed to address this problem. The core of RGS-Sketch lies in a new mergeable memory sharing design called register group sharing. This design organizes registers into groups as basic memory sharing units, accommodating the high skewness of real-world data streams and offering high memory efficiency. Besides, it enables online detection through the real-time acquisition of a group‚Äôs state, which also facilitates invertibility. To enhance detection accuracy further, we propose a limited register update strategy. It blocks small flows from updating registers, thereby reducing memory overhead and estimation noises. Extensive experimental results based on four real-world datasets show that RGS-Sketch significantly outperforms the most accurate baselines in accuracy while maintaining a high throughput. Specifically, it improves the F1 scores by up to 0.643 for measurements at a single point/period and up to 0.472 across multiple points/periods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RGS-Sketch%3A%20An%20Accurate%2C%20Invertible%2C%20and%20Mergeable%20Sketch%20for%20Online%20Super%20Spreader%20Detection%20in%20High-speed%20Data%20Streams",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1237-zhang.pdf",
    "session": "Research 33: Data Stream Mining",
    "authors": [
      {
        "Name": "Boyu Zhang",
        "Affiliation": "Soochow University"
      },
      {
        "Name": "He Huang",
        "Affiliation": "Soochow University"
      },
      {
        "Name": "Yu-E Sun",
        "Affiliation": "Soochow University"
      },
      {
        "Name": "Guoju Gao",
        "Affiliation": "Soochow University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b582a780-5017-42b1-910b-469fbd459974",
    "title": "Balancing Privacy and Utility in Correlated Data: A Study of Bayesian Differential Privacy",
    "abstract": "Privacy risks in differentially private (DP) systems increase significantly when data is correlated, as standard DP metrics often underestimate the resulting privacy leakage, leaving sensitive information vulnerable. Given the ubiquity of dependencies in realworld databases, this oversight poses a critical challenge for privacy protections. Bayesian differential privacy (BDP) extends DP to account for these correlations, yet current BDP mechanisms indicate a notable utility loss, limiting its adoption. \nIn this work, we address whether BDP can be realistically implemented in common data structures without sacrificing utility‚Äîa key factor for its applicability. By analyzing arbitrary and structured correlation models, including Gaussian multivariate distributions and Markov chains, we derive practical utility guarantees for BDP. Our contributions include theoretical links between DP and BDP and a novel methodology to adapt DP mechanisms to meet the requirements of BDP. Through evaluations on real-world databases, we demonstrate that our novel theorems enable the design of BDP mechanisms that maintain competitive utility, paving the way for practical privacy-preserving data practices in correlated settings.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Balancing%20Privacy%20and%20Utility%20in%20Correlated%20Data%3A%20A%20Study%20of%20Bayesian%20Differential%20Privacy",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4090-guerra-balboa.pdf",
    "session": "Research 11: Graph Data Privacy, Text and Semi-Structured Data Management",
    "authors": [
      {
        "Name": "Martin Lange",
        "Affiliation": "Karlsruhe Institute of Technology"
      },
      {
        "Name": "Patricia Guerra-Balboa",
        "Affiliation": "Karlsruhe Institute of Technology"
      },
      {
        "Name": "Javier Parra-Arnau",
        "Affiliation": "Universitat Polit√®cnica de Catalunya"
      },
      {
        "Name": "Thorsten Strufe",
        "Affiliation": "Karlsruhe Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7e3f24f4-e3e2-4b08-b3a7-87173f927593",
    "title": "Kishu: Time-Traveling for Computational Notebooks",
    "abstract": "Computational notebooks (e.g., Jupyter, Google Colab) are widely used by data scientists. A key feature of notebooks is the interactive computing model of iteratively executing  cells  (i.e., a set of statements) and observing the result (e.g., model or plot). Unfortunately, existing notebook systems do not offer  time-traveling to past states : when the user executes a cell, the notebook  session state  consisting of user-defined variables can be  irreversibly modified √êe.g., the user cannot ‚Äôun-drop‚Äô a dataframe column. This is because, unlike DBMS, existing notebook systems do not keep track of the session state. Existing techniques for checkpointing and restoring session states, such as OS-level memory snapshot or application-level session dump, are insufficient: checkpointing can incur prohibitive storage costs and may fail, while restoration can only be inefficiently performed from scratch by fully loading checkpoint files. \nIn this paper, we introduce a new notebook system,  Kishu , that offers time-traveling to and from arbitrary notebook states using an efficient and fault-tolerant incremental checkpoint and checkout mechanism.  Kishu  creates incremental checkpoints that are small and correctly preserve complex inter-variable dependencies at a novel  Co-variable  granularity. Then, to return to a previous state, Kishu  accurately identifies the  state difference  between the current and target states to perform incremental checkout at sub-second latency with minimal data loading.  Kishu  is compatible with 146 object classes from popular data science libraries (e.g., Ray, Spark, PyTorch), and reduces checkpoint size and checkout time by up to 4.55 √ó  and 9.02 √ó , respectively, on a variety of notebooks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Kishu%3A%20Time-Traveling%20for%20Computational%20Notebooks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p970-li.pdf",
    "session": "Research 32: Data-centric Machine Learning",
    "authors": [
      {
        "Name": "Zhaoheng Li",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "Name": "Supawit Chockchowwat",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "Name": "Ribhav Sahu",
        "Affiliation": "University of Illinois Urbana-Champaign"
      },
      {
        "Name": "Areet Sheth",
        "Affiliation": "Univeristy of Illinois Urbana-Champaign"
      },
      {
        "Name": "Yongjoo Park",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e02de334-2a48-4953-aa5f-6ef45e0c4e23",
    "title": "Pistis: A Decentralized Knowledge Graph Platform Enabling Ownership-Preserving SPARQL Querying",
    "abstract": "Decentralized Knowledge Graph (DKG) platforms allow the sharing of knowledge with multiple owners. While data owners can share their data with others by encrypting their data before sharing it, this na√Øve approach prevents data encrypted by different owners from being queried together, as it compromises query verifiability, an essential DKG platform feature. We propose Pistis, the first DKG platform capable of preserving ownership while also enabling verifiable SPARQL queries. Two novel techniques facilitate this: owner-managed end-to-end encryption  and  collaborative query verification . In Pistis, data owners thus encrypt their data individually and collaborate to construct an authenticated data structure (ADS) with a global key by means of secret sharing and secure multi-party computation. Then, by indexing KG data as ciphertext over the ADS, Pistis offers a cryptographic scheme called VO-SPARQL that facilitates verifiable queries on encrypted KG data with multiple owners. Pistis provides succinct proofs for two-stage SPARQL queries, including subgraph queries based on the ADS and aggregation on encrypted intermediate results based on a key-aggregate cryptographic primitive. A theoretical analysis and an empirical study provide detailed insight into the performance of Pistis while offering provable security.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Pistis%3A%20A%20Decentralized%20Knowledge%20Graph%20Platform%20Enabling%20Ownership-Preserving%20SPARQL%20Querying",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4602-zhou.pdf",
    "session": "Research 63: Knowledge Graph Management",
    "authors": [
      {
        "Name": "Enyuan Zhou",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Song Guo",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Zicong Hong",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Christian Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Yang Xiao",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Jinwen Liang",
        "Affiliation": "Hong Kong Polytechnic University"
      },
      {
        "Name": "Dalin Zhang",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ac385052-b0e4-4b88-bfa8-e26ae3ebe4ee",
    "title": "Towards Pattern-aware Data Augmentation for Temporal Knowledge Graph Completion",
    "abstract": "Predicting missing facts for temporal knowledge graphs (TKGs) is a fundamental task, called temporal knowledge graph completion (TKGC). One key challenge in this task is the imbalance in data distribution, where facts are unevenly spread across entities and timestamps. This imbalance can lead to poor completion performance for long-tail entities and timestamps, and unstable training due to the introduction of false negative samples. Unfortunately, few previous studies have investigated how to mitigate these effects. Moreover, for the first time, we found that existing methods suffer from model preferences, revealing that entities with specific properties (e.g., recently active) are favored by different models. Such preferences will lead to error accumulation and further exacerbate the effects of imbalanced data distribution. To alleviate the impacts of imbalanced data and model preferences, we introduce  Booster , the first data augmentation strategy for TKGs. The unique requirements here lie in generating new samples that fit the complex semantic and temporal patterns within TKGs, and identifying hard-learning samples specific to models. Therefore, we propose a hierarchical scoring algorithm based on triadic closures within TKGs. By incorporating both global semantic patterns and local time-aware structures, the algorithm enables pattern-aware validation for new samples. Meanwhile, we propose a two-stage training approach to identify samples that deviate from the model‚Äôs preferred patterns. With a frequency-based filtering strategy, this approach also helps to avoid the misleading of false negatives. Experiments justify that Booster  can seamlessly adapt to existing TKGC models and achieve on average 4.5% performance improvement.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Pattern-aware%20Data%20Augmentation%20for%20Temporal%20Knowledge%20Graph%20Completion",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3573-ouyang.pdf",
    "session": "Research 62: Information Integration and Data Quality IV",
    "authors": [
      {
        "Name": "Jiasheng Zhang",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Deqiang Ouyang",
        "Affiliation": "Chongqing University"
      },
      {
        "Name": "Shuang Liang",
        "Affiliation": "University of Electronic Science and Technology of China"
      },
      {
        "Name": "Jie Shao",
        "Affiliation": "University of Electronic Science and Technology of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "77b0a7ed-794e-4e7e-962b-813a25d176c0",
    "title": "Sort it Like You Mean It: Discovering Semantically Interesting Attribute Augmentations to Sort Tables",
    "abstract": "Sorting is a fundamental operation in table analysis. Data scientists frequently sort tables to uncover key insights‚Äîfor example, identifying the top 10 products by sales. However, this process is largely manual. Data scientists must (1) understand the semantics behind the sorting they wish to apply, and (2) ensure the necessary attributes are present‚Äîoften requiring manual augmentation of the table. But what if data scientists could receive suggestions for semantically meaningful ways to sort a table, powered by automatic augmentations from a data lake? In this demo, we present InsightSort, an end-to-end system that recommends attribute augmentations to enable richer, more insightful sorting for table exploration. InsightSort works by: (1) discovering potential augmentations by linking the input table with relevant data lake tables, and (2) leveraging a Large Language Model (LLM) to synthesize the top-k diverse sorting attributes based on their semantics. A companion video is available at [1].",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Sort%20it%20Like%20You%20Mean%20It%3A%20Discovering%20Semantically%20Interesting%20Attribute%20Augmentations%20to%20Sort%20Tables",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5427-rezig.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Akash Khatri",
        "Affiliation": "University of Utah"
      },
      {
        "Name": "Mahathir Mohammad",
        "Affiliation": "University of Utah"
      },
      {
        "Name": "El Kindi Rezig",
        "Affiliation": "University of Utah"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b079942b-19b3-45f1-ab4e-e90bc583d381",
    "title": "Practical and Accurate Local Edge Differentially Private Graph Algorithms",
    "abstract": "The rise of massive networks across diverse domains necessitates sophisticated graph analytics, often involving sensitive data and raising privacy concerns. This paper addresses these challenges using  local differential privacy (LDP) , which enforces privacy at the individual level, where  no third-party entity is trusted , unlike centralized models that assume a trusted curator. \nWe introduce novel LDP algorithms for two fundamental graph statistics:  ùëò -core decomposition and triangle counting. Our approach leverages input-dependent private graph properties‚Äîspecifically degeneracy and maximum degree‚Äîto improve theoretical utility. Unlike prior methods, our error bounds depend on the maximum degree rather than the total edge count, yielding significantly tighter guarantees. For triangle counting, we improve on the work of Imola, Murakami, and Chaudhury [USENIX Security ‚Äô21, ‚Äô22], which bounds error in terms of edge count. Our algorithm instead achieves bounds based on degeneracy by leveraging a private out-degree orientation, a refined variant of Eden et al.‚Äôs randomized response technique [ICALP ‚Äô23], and a novel analysis, yielding stronger guarantees than prior work. \nBeyond theoretical gains, we are the first to evaluate local DP algorithms in a distributed simulation, unlike prior work tested on a single processor. Experiments on real-world graphs show substantial accuracy gains: our  ùëò -core decomposition achieves errors within  3 x  of exact values, far outperforming the  131 x  error in the baseline of Dhulipala et al. [FOCS ‚Äò22]. Our triangle counting algorithm reduces multiplicative approximation errors by up to  six orders of magnitude , while maintaining competitive runtime.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Practical%20and%20Accurate%20Local%20Edge%20Differentially%20Private%20Graph%20Algorithms",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4199-mundra.pdf",
    "session": "Research 28: Social Networks",
    "authors": [
      {
        "Name": "Pranay Mundra",
        "Affiliation": "Yale University"
      },
      {
        "Name": "Charalampos Papamanthou",
        "Affiliation": "Yale University"
      },
      {
        "Name": "Julian Shun",
        "Affiliation": "MIT CSAIL"
      },
      {
        "Name": "Quanquan Liu",
        "Affiliation": "Yale University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9a9b1420-b370-4efa-b202-c89240baa20f",
    "title": "Federated Data Shift Distance Estimation",
    "abstract": "As data is increasingly held at the edge of the network, new methods are needed to perform analysis over distributed inputs. This has led to the emergence of the federated model of distributed computation, which places emphasis on privacy and scalability. A central problem is to analyze data distributions where the data is spread across a large number of distributed clients. This supports a number of tasks within federated learning and federated analytics. We present techniques to measure the similarity of distributions of data in the federated model. We deÔ¨Åne sketches for this task that allow eÔ¨Écient estimation of the diÔ¨Äerence between two distributions based on the total variation distance ( Ìêø 1 ) metric. These have accuracy and privacy guarantees, and can be computed incrementally over dynamic data. Our experimental study shows that these are practical to implement and provide accurate estimates.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Federated%20Data%20Shift%20Distance%20Estimation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2399-cormode.pdf",
    "session": "Research 33: Data Stream Mining",
    "authors": [
      {
        "Name": "Graham Cormode",
        "Affiliation": "University of Warwick"
      },
      {
        "Name": "Daniel Ting",
        "Affiliation": "Tableau Software"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c7ae878b-8ee0-4c57-b476-b17632bbb60e",
    "title": "Vadacode: A Logician-friendly IDE for Datalog+/-",
    "abstract": "Languages, namely, fragments, of the Datalog+/- family are attracting interest in both academia and industry because of their possibility to balance high expressive power and computational complexity. However, understanding the differences among the fragments, mastering them to achieve scalable industrial applications, and communicating their peculiarities to a non-expert audience is challenging for researchers, developers, logicians, and educators. \nIn this demo, we introduce Vadacode, an IDE for Datalog+/designed to support a broad category of users. The tool offers advanced features, including fragment detection, syntax highlighting, code completion, error diagnostics, schema inference, debugging support, and AI-assisted coding capabilities. Thanks to our experience in the financial context, our demo will guide the audience in modeling financial Datalog+/- programs, showcasing a seamless and effective coding experience.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Vadacode%3A%20A%20Logician-friendly%20IDE%20for%20Datalog%2B-slash--",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5411-gentili.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Luigi Bellomarini",
        "Affiliation": "Banca d'Italia"
      },
      {
        "Name": "Andrea Gentili",
        "Affiliation": "Banca d'Italia"
      },
      {
        "Name": "Davide Magnanimi",
        "Affiliation": "Banca d'Italia"
      },
      {
        "Name": "Emanuel Sallinger",
        "Affiliation": "TU Wien"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "50b5ffb7-2d11-4636-85d4-b43e5ad86d32",
    "title": "DocDB: A Database for Unstructured Document Analysis",
    "abstract": "Recent studies have developed LLM-powered data systems that enable database-like analysis of unstructured text documents. While LLMs excel at attribute extraction from documents, their high computational costs and latency make extraction operations the primary performance bottleneck. Existing systems typically adopt traditional relational database query optimization strategies, which prove ineffective in minimizing LLM-related expenses. To fill this gap, we propose  DocDB , a prototype system that features a bunch of novel optimization strategies designated to unstructured document analysis. First, we employ a two-level index to reduce LLM extraction costs by selectively retrieving and processing only text segments relevant to target attributes. Second,  DocDB  employs adaptive execution, generating document-specific plans to minimize LLM extraction frequency based on varying per-document attribute extraction costs. With a real-life scenario, we demonstrate that  DocDB allows users to analyze unstructured documents accurately and affordably using SQL-like queries. The corresponding video is available at https://youtu.be/8yDIKOBHIOg.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DocDB%3A%20A%20Database%20for%20Unstructured%20Document%20Analysis",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5387-chai.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Zequn Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Yuanhao Zhong",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Chengliang Chai",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Zhaoze Sun",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Yuhao Deng",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Ye Yuan",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guoren Wang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Lei Cao",
        "Affiliation": "University of Arizona/MIT"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bce31e6f-6256-4886-a2aa-f12f863bf821",
    "title": "GalaxyWeaver: Autonomous Table-to-Graph Conversion and Schema Optimization with Large Language Models",
    "abstract": "Most enterprise graph data derives from relational databases, yet transforming relational tables into query-optimized graph schemas remains challenging. Existing approaches have notable limitations: (1) transformations based on primary and foreign keys often fail to generate schemas optimized for query performance; (2) manual schema design, although flexible, is costly and requires domain expertise; and (3) machine learning methods predict graph structures based on data patterns but heavily depend on large, high-quality training datasets. To address these challenges, we propose GalaxyWeaver, a framework to automate query-aware graph schema generation. GalaxyWeaver utilizes the reasoning power of Large Language Models (LLMs) to align graph schema designs with specific query requirements, e#ectively integrating domain knowledge with optimization strategies. The framework employs prompt-guided analysis to enhance the decision-making accuracy of LLM agents, facilitating iterative schema refinement. Experiments across diverse domains show that GalaxyWeaver simplifies transformation while improving query performance and reducing storage costs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GalaxyWeaver%3A%20Autonomous%20Table-to-Graph%20Conversion%20and%20Schema%20Optimization%20with%20Large%20Language%20Models",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5100-tong.pdf",
    "session": "Industry 3: Document, Graph, and Vector Databases",
    "authors": [
      {
        "Name": "Bing Tong",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Yan Zhou",
        "Affiliation": "Zhejiang CreateLink Technology"
      },
      {
        "Name": "Chen Zhang",
        "Affiliation": "Zhejiang CreateLink Technology"
      },
      {
        "Name": "Jianheng Tang",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Jia Li",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "The Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6d2a1967-063a-4014-9504-d61607036e72",
    "title": "Cabinet: Dynamically Weighted Consensus Made Fast",
    "abstract": "Conventional consensus algorithms, such as Paxos and Raft, encounter inefficiencies when applied to large-scale distributed systems due to the requirement of waiting for replies from a majority of nodes. To address these challenges, we propose Cabinet, a novel consensus algorithm that introduces dynamically weighted con- sensus, allocating distinct weights to nodes based on any given failure thresholds. Cabinet dynamically adjusts nodes‚Äô weights according to their responsiveness, assigning higher weights to faster nodes. The dynamic weight assignment maintains an optimal system performance, especially in large-scale and heterogeneous systems where node responsiveness varies. We evaluate Cabinet against Raft with distributed MongoDB and PostgreSQL databases using YCSB and TPC-C workloads. The evaluation results show that Cabinet outperforms Raft in throughput and latency under increasing system scales, complex networks, and failures in both homogeneous and heterogeneous clusters, offering a promising high-performance consensus solution.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cabinet%3A%20Dynamically%20Weighted%20Consensus%20Made%20Fast",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1439-zhang.pdf",
    "session": "Research 26: Distributed Transactions I",
    "authors": [
      {
        "Name": "Gengrui Zhang",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Shiquan Zhang",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Michail Bachras",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Yuqiu Zhang",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Hans-Arno Jacobsen",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2ea4410e-534f-4701-9184-1dd5738f3f4a",
    "title": "Relational Data Models for Genetic VCF data",
    "abstract": "The Variant Call Format (VCF) and its binary counterpart (BCF) are commonly used in bioinformatics for storing gene sequence data. While VCF oles provide compact storage, they require specioc tools and scripts for querying, thereby missing the rich functionality arsenal of database management systems and their potential for integration in multiomics pipelines. In this paper, we leverage Relational Database Management Systems (RDBMS) to enhance eociency and nexibility in storing and querying large-scale genetic datasets. We map the VCF ole structure to narrow, wide, and array-based data models that are further reoned using JSON data structures, resulting in eight data models. Our experimental evaluation shows that RDBMS provide competitive performance in comparison with specialized state-of-the-art tools while making full-nedged database capabilities available for genetic data analysis.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Relational%20Data%20Models%20for%20Genetic%20VCF%20data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4045-hafidi.pdf",
    "session": "Research 21: Specialized and Domain-Specific Data Management",
    "authors": [
      {
        "Name": "Mohamed Sabri Hafidi",
        "Affiliation": "unibz"
      },
      {
        "Name": "Ozan Kahramanoƒüullarƒ±",
        "Affiliation": "unibz"
      },
      {
        "Name": "Anton Dign√∂s",
        "Affiliation": "unibz"
      },
      {
        "Name": "Johann Gamper",
        "Affiliation": "unibz"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "edaf41ac-2f95-4861-b1da-f29cc3f3e43e",
    "title": "Select Edges Wisely: Monotonic Path Aware Graph Layout Optimization for Disk-based ANN Search",
    "abstract": "Approximate nearest neighbor (ANN) search is a critical problem in various real-world applications. However, as one of the most promising solutions to ANN search, graph-based indexes often suffer from high memory consumption. Although a few studies attempt to alleviate this issue by storing the index on inexpensive disk storage, they still face challenges such as insufficient data locality and low efficiency when optimizing the graph layout on disk. Therefore, we propose  MARGO , a monotonic path-aware graph layout optimization method for disk-based ANN search. First, we present the essence of graph layout optimization in disk-based ANN search, and design a monotonic path-aware objective function that weighs the edges based on their importance in monotonic paths, supported by rigorous theoretical analysis. Second, we propose a greedy algorithm that prioritizes high-weight edges to accommodate more monotonic paths. To enhance efficiency,  MARGO  introduces a two stage decoupling method that processes intra-cluster edges in parallel first, followed by inter-cluster edges. Third, we develop a weight computation strategy that computes edge weights on-the-fly during index construction with almost no additional overhead. A comprehensive experimental study demonstrates that  MARGO  improves search efficiency by up to 26.6% while maintaining the same recall, and accelerates the graph layout optimization by up to 5.5 √ó .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Select%20Edges%20Wisely%3A%20Monotonic%20Path%20Aware%20Graph%20Layout%20Optimization%20for%20Disk-based%20ANN%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4337-zheng.pdf",
    "session": "Research 3: Vector Data Management I",
    "authors": [
      {
        "Name": "Ziyang Yue",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Bolong Zheng",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Ling Xu",
        "Affiliation": "Shuyi Technology Co., Ltd."
      },
      {
        "Name": "Kanru Xu",
        "Affiliation": "Shuyi Technology Co., Ltd."
      },
      {
        "Name": "Shuhao Zhang",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Yajuan Du",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Xiaofang Zhou",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Christian Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "04677bc4-9515-4404-bb92-fb507f8cd3fb",
    "title": "What If: Causal Analysis with Graph Databases",
    "abstract": "Graphs are powerful abstractions for modeling relationships and enabling data science tasks. In causal inference, Directed Acyclic Graphs (DAGs) serve as a key formalism, but they are typically handcrafted by experts and rarely treated as first-class data artifacts in graph data management systems. This paper presents a novel vision to align causal analysis with property graphs‚Äîthe foundation of modern graph databases‚Äîby rethinking graph models to incorporate hypernodes, structural equations, and causality-aware query semantics. By unifying graph databases with causal reasoning, our approach enables the declarative expression of DAG manipulation operations along with interventions and counterfactuals, combining expressiveness with computational efficiency. We validate this vision through a proof-of-concept implementation supporting scalable causal queries over DAGs, ultimately aiming to make graph databases causally aware and support data-driven, personalized decision-making across several scientific domains.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/What%20If%3A%20Causal%20Analysis%20with%20Graph%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4009-pachera.pdf",
    "session": "Research 65: Graph Data Management VIII",
    "authors": [
      {
        "Name": "Amedeo Pachera",
        "Affiliation": "Universit√© Claude Bernard Lyon 1"
      },
      {
        "Name": "Mattia Palmiotto",
        "Affiliation": "Universit√© Claude Bernard Lyon 1"
      },
      {
        "Name": "Angela Bonifati",
        "Affiliation": "Universit√© Claude Bernard Lyon 1"
      },
      {
        "Name": "Andrea Mauri",
        "Affiliation": "Universit√© Claude Bernard Lyon 1"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cab5c32c-8c69-4da0-8bd7-f00c897abe89",
    "title": "InBox: Recommendation with Knowledge Graph using Interest Box Embedding",
    "abstract": "Knowledge graphs (KGs) have become vitally important in modern recommender systems, effectively improving performance and interpretability. Fundamentally, recommender systems aim to identify user interests based on historical interactions and recommend suitable items. However, existing works overlook two key challenges: (1) an interest corresponds to a potentially large set of related items, and (2) the lack of explicit, fine-grained exploitation of KG information and interest connectivity. This leads to an inability to reflect distinctions between entities and interests when modeling them in a single way. Additionally, the granularity of concepts in the knowledge graphs used for recommendations tends to be coarse, failing to match the fine-grained nature of user interests. This homogenization limits the precise exploitation of knowledge graph data and interest connectivity. To address these limitations, we introduce a novel embedding-based model called InBox. Specifically, various knowledge graph entities and relations are embedded as points or boxes, while user interests are modeled as boxes encompassing interaction history. Representing interests as boxes enables containing collections of item points related to that interest. We further propose that an interest comprises diverse basic concepts, and box intersection naturally supports concept combination. Across three training steps, InBox significantly outperforms state-of-the-art methods like HAKG and KGIN on recommendation tasks. Further analysis provides meaningful insights into the variable value of different KG data for recommendations.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/InBox%3A%20Recommendation%20with%20Knowledge%20Graph%20using%20Interest%20Box%20Embedding",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4641-xu.pdf",
    "session": "Research 63: Knowledge Graph Management",
    "authors": [
      {
        "Name": "Zezhong Xu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yincen Qu",
        "Affiliation": "Alibaba"
      },
      {
        "Name": "Wen Zhang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lei Liang",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Huajun Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2adcc6c4-969c-4c84-a864-c00faf7e75ef",
    "title": "MLP-Mixer based Masked Autoencoders Are Effective, Explainable and Robust for Time Series Anomaly Detection",
    "abstract": "Time series anomaly detection remains one of the most active re-Time series anomaly detection remains one of the most active research areas in data mining due to its wide range of real-world ap-search areas in data mining due to its wide range of real-world applications. In recent years, numerous deep learning-based methods have been proposed for this task. However, deep learning-based methods fail to detect subsequence anomalies with long durations, lack explainability, and are vulnerable to training set contamina-lack explainability, and are vulnerable to training set contamination. This paper addresses these issues by proposing a novel deep learning framework for effective, explainable, and robust time se-learning framework for effective, explainable, and robust time series anomaly detection. Our framework,  MMA , incorporates the M LP-Mixer backbone with a  M asked  A utoencoder-based anomaly detection approach to allow for a significantly larger input win-detection approach to allow for a significantly larger input window size (10 to 20 times larger than the input window sizes of cur-dow size (10 to 20 times larger than the input window sizes of current models). This larger input window enables our model to detect challenging subsequence anomalies. Meanwhile, a contrast learn-challenging subsequence anomalies. Meanwhile, a contrast learning module is proposed to aid in detecting subtle anomalies that fail to be identified by residual errors. Furthermore, a dynamic anom-to be identified by residual errors. Furthermore, a dynamic anomaly filtering method is introduced to mitigate the impact of sub-aly filtering method is introduced to mitigate the impact of subsequence anomalies on the reconstruction of surrounding normal regions to reduce false alarms. Extensive experiments on univari-regions to reduce false alarms. Extensive experiments on univariate and multivariate time series datasets demonstrate that our pro-ate and multivariate time series datasets demonstrate that our proposed framework significantly outperforms state-of-the-art meth-posed framework significantly outperforms state-of-the-art methods across rigorous evaluation metrics. Additionally, MMA has a strong ability to reconstruct potential normal patterns in anoma-strong ability to reconstruct potential normal patterns in anomalous regions, providing high levels of explainability. Moreover, MMA demonstrates high robustness to various types of training set pollution.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/MLP-Mixer%20based%20Masked%20Autoencoders%20Are%20Effective%2C%20Explainable%20and%20Robust%20for%20Time%20Series%20Anomaly%20Detection",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p798-qideng.pdf",
    "session": "Research 39: Analytics over Different Data Types II",
    "authors": [
      {
        "Name": "Tang qideng",
        "Affiliation": "National University of Defense Technology"
      },
      {
        "Name": "Dai Chaofan",
        "Affiliation": "NUDT"
      },
      {
        "Name": "Wu Yahui",
        "Affiliation": "National University of Defense Technology"
      },
      {
        "Name": "Zhou Haohao",
        "Affiliation": "National University of Defense Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0cb627d3-cffb-4e69-b3a9-22df457d19fd",
    "title": "Wolverine: Highly Efficient Monotonic Search Path Repair for Graph-based ANN Index Updates",
    "abstract": "Approximate nearest neighbor (ANN) search on high-dimensional vector data is core functionality in an increasing number of realworld applications. However, most existing methods only focus on accelerating search by means of indexing that assumes that the data is static. The few methods capable of contending with dynamic data often face challenges such as decreased query accuracy following updates and low update efficiency. In this study, we propose Wolverine , the first proposal that, to our knowledge, enables efficient monotonic search path repair, thereby solving the graph-based ANN index update problem.  Wolverine  repairs disrupted monotonic search paths by adding in-edges to the out-neighbors of a point to be deleted. To improve efficiency,  Wolverine+  restricts the search space to be within the 2-hop neighbors of the point to be deleted. In addition,  Wolverine++  employs a sophisticated candidate selection policy to find high-quality candidates in the reduced search space, simultaneously improving accuracy and efficiency. An experimental study on 9 real-world datasets demonstrates that  Wolverine is capable of accelerating the deletion throughput by up to 11 √ó and achieving more stable recall during updates compared to the state-of-the-art dynamic ANN search method.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Wolverine%3A%20Highly%20Efficient%20Monotonic%20Search%20Path%20Repair%20for%20Graph-based%20ANN%20Index%20Updates",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2268-zheng.pdf",
    "session": "Research 3: Vector Data Management I",
    "authors": [
      {
        "Name": "dawei liu",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Bolong Zheng",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "ziyang yue",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "fuhao ruan",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Xiaofang Zhou",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "72bc3bd8-160e-4389-82df-0841ca7b3f21",
    "title": "Efficient Top-k Frequent Subgraph Mining Using Tight Upper and Lower Bounds",
    "abstract": "Frequent subgraph mining is an important and well-studied prob- lem with numerous applications such as the prediction of protein functionalities and graph indexing. Many studies use the minimum- image-based support (MNI) to measure the frequency of subgraphs in single graph mining. Given a graph G and an integer k, top-k frequent subgraph mining is to Ôøønd top-: frequent subgraphs in the graph ‚åß based on MNI. However, there are two main challenges in top-k frequent subgraph mining. (1) Computing MNI is time- consuming. (2) The number of subgraphs for which MNI should be computed is large. In this paper, we propose a novel algorithm Minting to address these challenges. We propose a method to significantly reduce the number of subgraphs for which MNI computation is required by using a tight upper bound of the MNI value. We also improve the computation of MNI itself by utilizing both a lower bound and an upper bound of the MNI value. Experiments shows that our algorithm outperforms the state-of-the-art algorithms by up to three orders of magnitude in terms of the elapsed time. Our algorithm is also a feasible solution for this challenging problem, even for large k.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Top-k%20Frequent%20Subgraph%20Mining%20Using%20Tight%20Upper%20and%20Lower%20Bounds",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p557-park.pdf",
    "session": "Research 34: Graph Data Management IV",
    "authors": [
      {
        "Name": "Seonho Lee",
        "Affiliation": "Seoul National University"
      },
      {
        "Name": "Yeunjun Lee",
        "Affiliation": "Seoul National University"
      },
      {
        "Name": "Kunsoo Park",
        "Affiliation": "Seoul National University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "48c1f349-bfeb-4e5f-80a6-a1cb20975bc4",
    "title": "The Limits of Graph Samplers for Training Inductive Recommender Systems",
    "abstract": "Inductive Recommender Systems are capable of recommending for new users and with new items thus avoiding the need to retrain after new data reaches the system. However, these methods are still trained on all the data available, requiring multiple days to train a single model, without counting hyperparameter tuning. In this work we focus on graph-based recommender systems, i.e., systems that model the data as a heterogeneous network. In other applications, graph sampling allows to study a subgraph and generalize the findings to the original graph. Thus, we investigate the applicability of sampling techniques for this task. We test on three real world datasets, with three state-of-the-art inductive methods, and using six different sampling methods. We find that its possible to maintain performance using only 50% of the training data with up to 86% percent decrease in training time; however, using less training data leads to far worse performance. Further, we find that when it comes to data for recommendations, graph sampling should also account for the temporal dimension. Therefore, we find that if higher data reduction is needed, new graph based sampling techniques should be studied and new inductive methods should be designed.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20Limits%20of%20Graph%20Samplers%20for%20Training%20Inductive%20Recommender%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2496-jendal.pdf",
    "session": "Research 58: User Interfaces for Data Exploration and Recommender Systems",
    "authors": [
      {
        "Name": "Theis Jendal",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Matteo Lissandrini",
        "Affiliation": "University of Verona"
      },
      {
        "Name": "Peter Dolog",
        "Affiliation": "University of Aalborg"
      },
      {
        "Name": "Katja Hose",
        "Affiliation": "TU Wien"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7d2f0f20-607a-4f3b-b83f-a09b9f2b9e05",
    "title": "OasisDB: An Oblivious and Scalable System for Relational Data",
    "abstract": "An Oblivious and Scalable System for Relational Data",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/OasisDB%3A%20An%20Oblivious%20and%20Scalable%20System%20for%20Relational%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4478-ahmed.pdf",
    "session": "Research 66: Access Control and Privacy II",
    "authors": [
      {
        "Name": "Haseeb Ahmed",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Nachiket Rao",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Abdelkarim Kati",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Florian Kerschbaum",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Sujayya Maiyya",
        "Affiliation": "University of Waterloo"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dbd60722-1e92-4acd-a0fc-5d9625119612",
    "title": "eXpath: Explaining Knowledge Graph Link Prediction with Ontological Closed Path Rules",
    "abstract": "Link prediction (LP) is crucial for Knowledge Graphs (KG) completion but commonly suffers from interpretability issues. While several methods have been proposed to explain embedding-based LP models, they are generally limited to local explanations on KG and are deficient in providing human interpretable semantics. Based on real-world observations of the characteristics of KGs from multiple domains, we propose to explain LP models in KG with pathbased explanations. An integrated framework, namely eXpath, is introduced which incorporates the concept of relation path with ontological closed path rules to enhance both the efficiency and effectiveness of LP interpretation. Notably, the eXpath explanations can be fused with other single-link explanation approaches to achieve a better overall solution. Extensive experiments across benchmark datasets and LP models demonstrate that introducing eXpath can boost the quality of resulting explanations by about 20% on two key metrics and reduce the required explanation time by 61.4%, in comparison to the best existing method. Case studies further highlight eXpath‚Äôs ability to provide more semantically meaningful explanations through path-based evidence.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/eXpath%3A%20Explaining%20Knowledge%20Graph%20Link%20Prediction%20with%20Ontological%20Closed%20Path%20Rules",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2818-sun.pdf",
    "session": "Research 63: Knowledge Graph Management",
    "authors": [
      {
        "Name": "Ye Sun",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Lei Shi",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Yongxin Tong",
        "Affiliation": "Beihang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1df28075-924a-4bbc-8643-e1e3c590c6cd",
    "title": "Optimized Batch Prompting for Cost-effective LLMs",
    "abstract": "Large Language Models (LLMs) have recently demonstrated exceptional performance in various real-world data management tasks through in-context learning (ICL), which involves structuring prompts with task descriptions and several demonstrations. However, most LLMs are not free and charge based on the number of input tokens. Specifically, for data management tasks, there may be massive related questions, leading to high inference cost due to redundant prompt content (i.e., overlapping demonstrations and repeated task descriptions). In this paper, we investigate the idea of batch prompting in leveraging LLMs for data management, which leads to cost-effective LLMs by grouping questions and demonstrations to perform inferences in batches. Current studies on batch prompting are preliminary and mostly based on heuristics, making it difficult to generalize to various types of tasks and adapt to different grouping strategies. To address these challenges, in this work we first formalize the batch prompting problem in general setting. Then, we study the hardness of this problem and propose efficient algorithms for adaptive grouping. Finally, we conduct comprehensive experiments on 14 datasets. Extensive experimental results demonstrate that our solution consistently outperforms the state-of-the-art baselines while consuming lower cost.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Optimized%20Batch%20Prompting%20for%20Cost-effective%20LLMs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2172-zhang.pdf",
    "session": "Research 30: Information Integration and Data Quality I",
    "authors": [
      {
        "Name": "Zhaoxuan Ji",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "XinLu Wang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "ZHAOJING LUO",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Zhongle Xie",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Meihui Zhang",
        "Affiliation": "Beijing Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2437c94e-05e7-420f-bebd-bd136188d0e9",
    "title": "The UDFBench Benchmark for General-purpose UDF Queries",
    "abstract": "User-defined functions (UDFs) extend the expressiveness of declarative SQL with functional capabilities, but also pose a core bottleneck in query processing due to the impedance mismatch between the UDF and SQL execution environments, and the limitations of the query optimizers to consistently produce good plans for UDF queries. Research and commercial approaches propose remedies for performant UDF query execution ranging from logical optimization and heuristics to physical optimization and compilation techniques. Each work however follows a different path to evaluate their proposed techniques. Despite the practical significance of optimizing UDF queries, UDFs have not been so far the focus of the database benchmarks. In this paper, we present UDFBench, a UDF-centric database benchmark based on real-world schema and data. We identify the core overheads in UDF query execution and design the UDFBench UDFs and queries to enable experimentation with these overheads, alone or in tandem with others. Finally, to showcase the portability and scope of UDFBench, we present an experimental analysis on five popular databases with different characteristics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20UDFBench%20Benchmark%20for%20General-purpose%20UDF%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2804-foufoulas.pdf",
    "session": "Research 8: Database Engines for OLAP",
    "authors": [
      {
        "Name": "Yannis Foufoulas",
        "Affiliation": "Athena Research Center"
      },
      {
        "Name": "Theoni Palaiologou",
        "Affiliation": "Athena Research Center"
      },
      {
        "Name": "Alkis Simitsis",
        "Affiliation": "Athena Research Center"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "96a2d460-3aaa-45e8-a966-eed3ccc0b521",
    "title": "The Vadalog Parallel System: Distributed Reasoning with Datalog+/-",
    "abstract": "",
    "conference_link": "",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4614-benedetto.pdf",
    "session": "Research 63: Knowledge Graph Management",
    "authors": [
      {
        "Name": "Luigi Bellomarini",
        "Affiliation": "Banca d'Italia"
      },
      {
        "Name": "Davide Benedetto",
        "Affiliation": "Prometheux & Universit√† Roma Tre"
      },
      {
        "Name": "Matteo Brandetti",
        "Affiliation": "TU Wien"
      },
      {
        "Name": "Emanuel Sallinger",
        "Affiliation": "TU Wien"
      },
      {
        "Name": "Adriano Vlad",
        "Affiliation": "Prometheux, TU Wien and University of Oxford"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Missing conference link",
      "Empty or missing abstract"
    ]
  },
  {
    "id": "75d5e74f-c9d9-4afb-a01d-6d08c00f0038",
    "title": "Lighter-X: An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness in recommendation systems. However, conventional graph-based recommenders, such as LightGCN, require maintaining embeddings of size  ùëë for each node, resulting in a parameter complexity of  O( ùëõ √ó ùëë ) , where ùëõ represents the total number of users and items. This scaling pattern poses significant challenges for deployment on large-scale graphs encountered in real-world applications. To address this scalability limitation, we propose  Lighter-X , an efficient and modular framework that can be seamlessly integrated with existing GNN-based recommender architectures. Our approach substantially reduces both parameter size and computational complexity while preserving the theoretical guarantees and empirical performance of the base models, thereby enabling practical deployment at scale. Specifically, we analyze the original structure and inherent redundancy in their parameters, identifying opportunities for optimization. Based on this insight, we propose an efficient compression scheme for the sparse adjacency structure and high-dimensional embedding matrices, achieving a parameter complexity of  O( ‚Ñé √ó  ùëë ) , where  ‚Ñé ‚â™ ùëõ . Furthermore, the model is optimized through a decoupled framework, reducing computational complexity during the training process and enhancing scalability. Extensive experiments demonstrate that Lighter-X achieves comparable performance to baseline models with significantly fewer parameters. In particular, on large-scale interaction graphs with millions of edges, we are able to attain even better results with only 1% of the parameter over LightGCN.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Lighter-X%3A%20An%20Efficient%20and%20Plug-and-play%20Strategy%20for%20Graph-based%20Recommendation%20through%20Decoupled%20Propagation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3721-zheng.pdf",
    "session": "Research 65: Graph Data Management VIII",
    "authors": [
      {
        "Name": "Yanping Zheng",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Zhewei Wei",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Frank de Hoo",
        "Affiliation": "Data 61, CSIRO"
      },
      {
        "Name": "Xu Chen",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Hongteng Xu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yuhang Ye",
        "Affiliation": "Huawei Poisson Lab"
      },
      {
        "Name": "Jiadeng Huang",
        "Affiliation": "Huawei Poisson Lab"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bc241911-568f-4dfc-8065-c29030a9f853",
    "title": "A Practical Theory of Generalization in Selectivity Learning",
    "abstract": "Query-driven machine learning models  have emerged as a promising estimation technique for query selectivities. Yet, surprisingly little is known about the efficacy of these techniques from a theoretical perspective, as there exist substantial gaps between practical solutions and state-of-the-art (SOTA) theory based on the Probably Approximately Correct (PAC) learning framework. In this paper, we aim to bridge the gaps between theory and practice. First, we demonstrate that selectivity predictors induced by  signed measures are learnable, which relaxes the reliance on  probability measures  in SOTA theory. More importantly, beyond the PAC learning framework (which only allows us to characterize how the model behaves when both training and test workloads are drawn from the  same distribution), we establish, under mild assumptions, that selectivity predictors from this class exhibit favorable  out-of-distribution (OOD) generalization error bounds. \nThese theoretical advances provide us with a better understanding of both the in-distribution and OOD generalization capabilities of query-driven selectivity learning, and facilitate the design of two general strategies to improve OOD generalization for existing query-driven selectivity models. We empirically verify that our techniques help query-driven selectivity models generalize significantly better to OOD queries both in terms of prediction accuracy and query latency performance, while maintaining their superior in-distribution generalization performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Practical%20Theory%20of%20Generalization%20in%20Selectivity%20Learning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1811-wu.pdf",
    "session": "Research 2: Applied ML and AI for Data Management I",
    "authors": [
      {
        "Name": "Peizhi Wu",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Haoshu Xu",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Ryan Marcus",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Zack Ives",
        "Affiliation": "University of Pennsylvania"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "39d059eb-ce91-4ae1-901d-91a851e425e8",
    "title": "Chimera: A system design of dual storage and traversal-join unified query processing for SQL/PGQ",
    "abstract": "",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Chimera%3A%20A%20system%20design%20of%20dual%20storage%20and%20traversal-join%20unified%20query%20processing%20for%20SQL%2FPGQ",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p279-lee.pdf",
    "session": "Research 35: Database Engines for Graphs",
    "authors": [
      {
        "Name": "Geonho Lee",
        "Affiliation": "Korea Advanced Institute of Science and Technology"
      },
      {
        "Name": "Jeongho Park",
        "Affiliation": "Korea Advanced Institute of Science and Technology"
      },
      {
        "Name": "Min-Soo Kim",
        "Affiliation": "KAIST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Empty or missing abstract"
    ]
  },
  {
    "id": "543ac33b-f6c3-4c47-9af1-3922e8e151af",
    "title": "COLOR: A Framework for Applying Graph Coloring to Subgraph Cardinality Estimation",
    "abstract": "Graph workloads pose a particularly challenging problem for query optimizers. They typically feature large queries made up of entirely many-to-manyjoinswithcomplexcorrelations.Thisputssignificant stress on traditional cardinality estimation methods which generally see catastrophic errors when estimating the size of queries with only a handful of joins. To overcome this, we propose COLOR, a frame-a handful of joins. To overcome this, we propose COLOR, a framework for subgraph cardinality estimation which applies insights from graph compression theory to produce a compact summary that captures the global topology of the data graph. Further, we identify several key optimizations that enable tractable estimation over this summary even for large query graphs. We then evaluate several de-summary even for large query graphs. We then evaluate several designs within this framework and find that they improve accuracy by up to 10¬≥√ó  over all competing methods while maintaining fast infer-up to 10¬≥√ó  over all competing methods while maintaining fast inference, a small memory footprint, efficient construction, and graceful degradation under updates. KEYWORDS Cardinality Estimation, Graph Databases, Graph Summarization, Query Optimization",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/COLOR%3A%20A%20Framework%20for%20Applying%20Graph%20Coloring%20to%20Subgraph%20Cardinality%20Estimation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p130-deeds.pdf",
    "session": "Research 35: Database Engines for Graphs",
    "authors": [
      {
        "Name": "Kyle Deeds",
        "Affiliation": "University of Washington"
      },
      {
        "Name": "Diandre Miguel Sabale",
        "Affiliation": "University of Washington"
      },
      {
        "Name": "Moe Kayali",
        "Affiliation": "University of Washington"
      },
      {
        "Name": "Dan Suciu",
        "Affiliation": "University of Washington"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2cf96267-2cfc-40dd-8c52-eb67bd510081",
    "title": "Making CRDTs Not So Eventual",
    "abstract": "Conflict-free replicated data types (CRDTs) are highly available and performant data replication solutions for distributed applica-and performant data replication solutions for distributed applications. However, their eventual consistency guarantees are often insufficient for ensuring application correctness, especially in the presence of Byzantine failures. Naively applying traditional consen-presence of Byzantine failures. Naively applying traditional consensus and Byzantine fault tolerance (BFT) protocols to CRDT updates for stronger guarantees, while intuitive, negates the performance benefits of CRDTs. \nWe introduce a novel programming model called  reliable CRDTs that expands CRDTs with additional guarantees: users can query strongly or eventually consistent values, enforce a total order among selected operations, and define data-type level invariants while re-selected operations, and define data-type level invariants while remaining operational in the presence of Byzantine failures. Reliable CRDTs enable the use of CRDTs in scenarios where strong consis-CRDTs enable the use of CRDTs in scenarios where strong consistency is needed while maintaining their performance advantages. \nWe present an implementation of reliable CRDTs named  Janus . It enhances CRDTs with the aforementioned features by functioning as a middleware that facilitates CRDT communication and asyn-as a middleware that facilitates CRDT communication and asynchronously runs a BFT consensus protocol. Our evaluation demon-chronously runs a BFT consensus protocol. Our evaluation demonstrates that  Janus  achieves 21√ó  higher throughput than naively applying state-of-the-art BFT protocols such as HotStuff achieves, and it remains responsive even under heavy loads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Making%20CRDTs%20Not%20So%20Eventual",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p349-mao.pdf",
    "session": "Research 42: Data Models and Query Languages",
    "authors": [
      {
        "Name": "Yunhao Mao",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Gengrui Zhang",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Zongxin Liu",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Pezhman Nasirifard",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Sofia Tijanic",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Hans-Arno Jacobsen",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f0eaf56d-85ee-4f7f-833c-4db8e0214472",
    "title": "PipeTGL: (Near) Zero Bubble Memory-based Temporal Graph Neural Network Training via Pipeline Optimization",
    "abstract": "Memory-based Temporal Graph Neural Networks  (M-TGNNs) demonstrate superior performance in dynamic graph learning tasks. Their success attributes to a memory module, which captures historical information for each node and implicitly creates a memory dependency constraint among chronologically ordered minibatches. This unique characteristic of M-TGNN introduces new challenges for parallel training that have not been encountered before. Existing parallelism strategies for M-TGNN either sacriÔ¨Åce memory accuracy (minibatch parallelism and epoch parallelism) or compromise space eÔ¨Éciency (memory parallelism) to optimize runtime. This paper proposes a pipeline parallel approach for multi-GPU M-TGNN training that eÔ¨Äectively addresses both inter-minibatch memory dependencies and intra-minibatch task dependencies, based on a runtime analysis DAG for M-TGNNs. We further optimize pipeline eÔ¨Éciency by incorporating improved scheduling, Ô¨Åner-grained operation reorganization, and targeted communication optimizations tailored to the speciÔ¨Åc training properties of M-TGNN. These enhancements signiÔ¨Åcantly reduce GPU waiting and idle time caused by memory dependencies and frequent communication and result in zero pipeline bubbles for common training conÔ¨Ågurations. Extensive evaluations demonstrate that PipeTGL achieves a speedup of 1.27x to 4.74x over other baselines while also improving the accuracy of M-TGNN training across multiple GPUs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PipeTGL%3A%20(Near)%20Zero%20Bubble%20Memory-based%20Temporal%20Graph%20Neural%20Network%20Training%20via%20Pipeline%20Optimization",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2722-du.pdf",
    "session": "Research 10: Data Mining and Analytics",
    "authors": [
      {
        "Name": "Jun Liu",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Bingqian Du",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Ziyue Luo",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "Sitian Lu",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Qiankun Zhang",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Hai Jin",
        "Affiliation": "Huazhong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c481a616-325a-46cc-9dbf-0f81374cf582",
    "title": "Diva: Dynamic Range Filter for Var-Length Keys and Queries",
    "abstract": "Range filters are compact probabilistic data structures that answer approximate range emptiness queries. They are used in many domains, e.g., in key-value stores, to quickly rule out the existence of keys in a given query range and avoid having to search for them in storage. However, all existing range filters exhibit at least one of the following shortcomings: (1) they do not provide robust false positive rate and performance guarantees, (2) they do not support variable-length keys and query ranges, and (3) they do not allow dynamic operations such as insertions, deletions, or expansions. \nWe introduce Diva, the first range filter to address all the above challenges simultaneously. Diva learns the dataset‚Äôs distribution by sampling keys and storing them in a cache-efficient trie. It compresses the keys in-between samples by removing their longest common prefix and truncating their suffixes while leaving enough bits in the middle (i.e., an infix) to allow differentiating between the keys in the sorted order. It stores infixes in constant time dynamic data blocks, which it splits to handle insertions and expansions. It processes a range query by traversing the trie and checking for the inclusion of infixes in the target query range. \nWe show, theoretically and empirically, that Diva achieves a false positive rate on par with the state of the art on real-world datasets while supporting dynamicity and variable-length queries and keys.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Diva%3A%20Dynamic%20Range%20Filter%20for%20Var-Length%20Keys%20and%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3923-eslami.pdf",
    "session": "Research 15: Views, Indexing, and Search I",
    "authors": [
      {
        "Name": "Navid Eslami",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Ioana Bercea",
        "Affiliation": "KTH Royal Institute of Technology"
      },
      {
        "Name": "Niv Dayan",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eb38f683-fd0f-4704-bdb1-cb35c0293e5f",
    "title": "Jodes: Efficient Oblivious Join in the Distributed Setting",
    "abstract": "Trusted execution environment (TEE) has provided an isolated and secure environment for building cloud-based analytic systems, but it still suffers from access pattern leakages caused by side- channel attacks. To better secure the data, computation inside TEE enclave should be made oblivious, which introduces significant overhead and severely slows down the computation. A natural way to speed up is to build the analytic system with multiple servers in the distributed setting. However, this setting raises a new security concern‚Äîthe volumes of the transmissions among these servers can leak sensitive information to a network adversary. Existing works have designed specialized algorithms to address this concern, but their supports for equi-join, one of the most important but non- trivial database operators, are either inefficient, limited, or under a weak security assumption.\nIn this paper, we present Jodes, an efficient oblivious join al- gorithm in the distributed setting. Jodes prevents the leakage on both the network and enclave sides, supports a general equi-join operation, and provides a high security level protection that only publicizes the input sizes and the output size. Meanwhile, it achieves both communication cost and computation cost asymptotically su- perior to existing algorithms. To demonstrate the practicality of Jodes, we conduct experiments in the distributed setting compris- ing 16 servers. Empirical results show that Jodes achieves up to a sixfold performance improvement over state-of-the-art join algo- rithms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Jodes%3A%20Efficient%20Oblivious%20Join%20in%20the%20Distributed%20Setting",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1291-wang.pdf",
    "session": "Research 18: Data Privacy and Security I",
    "authors": [
      {
        "Name": "Yilei Wang",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Xiangdong Zeng",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Sheng Wang",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ec9d89e2-6a2a-48de-b6e1-e38ebc046238",
    "title": "TARImpute: Task-Aware Auto-Recommender System for Missing Value Imputation Algorithms with Clustering Case Studies",
    "abstract": "Missing data prevalent in information systems impacts data diversity and Ô¨Ådelity, which systematically degrade clustering performance through biased similarity measures and unstable cluster boundaries. Current large-scale environments lack standardized imputation-clustering pipelines, as existing methods operate independently of downstream tasks without analyzing error propagation eÔ¨Äects, leading to unreliable results. To address this, we propose  TARImpute , a   T ask - A ware auto - R ecommender system for missing value   imput ation for clustering. It owns three integrated features:  Imputation Impact Profiler  for quantitative evaluation of imputation-clustering interactions,  Error Propagation Interpreter  enabling explainable modeling of imputation error diÔ¨Äusion, and  Adaptive Strategy Optimizer  for dynamic selection of optimal imputation methods.  TARImpute  provides state-ofthe-art imputation methods to evaluate their eÔ¨Äects on clustering tasks.  TARImpute  also provides robust, interpretable solutions for low-quality data and shows extensibility to other analytical tasks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TARImpute%3A%20Task-Aware%20Auto-Recommender%20System%20for%20Missing%20Value%20Imputation%20Algorithms%20with%20Clustering%20Case%20Studies",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5343-wang.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Xiaoou Ding",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Yanshuo Liu",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Zhounan Chen",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Hongzhi Wang",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Chen Wang",
        "Affiliation": "Tsinghua Unversity"
      },
      {
        "Name": "Jianmin Wang",
        "Affiliation": "Tsinghua Unversity"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "21462065-9e15-4da3-b66c-ddd55a62d92f",
    "title": "No Cap, This Memory Slaps: Breaking Through the Memory Wall of Transactional Database Systems with Processing-in-Memory",
    "abstract": "Memory channel bandwidth imposes an upper bound on the performance of online transaction processing (OLTP) on in-memory database management systems (DBMS). Emerging processing-inmemory (PIM) hardware has the potential to overcome this barrier by using small cores in DRAM chips that can read and process data in situ, thereby avoiding moving these data across memory channels. However, na√Øvely offoading all database components to PIM does not solve the problem due to the characteristics of software components and the limitations of PIM hardware. \nIn this paper, we present  OLTPim , the first end-to-end OLTP DBMS designed for PIM systems. We build a formalized model for the affinity of each database operation towards PIM and use it to decide the partitioning of components on different types of memory. We also design a lightweight batching algorithm to overcome the large PIM control latency while minimizing the batching overhead. We implement and evaluate  OLTPim  on the latest PIM system from UPMEM with 64 worker threads and 2048 PIM modules. Our results show that  OLTPim  achieves up to 1 . 71 ‚Üí throughput and up to 6 . 14 ‚Üí less per-transaction memory channel traffic over MosaicDB, a state-of-the-art in-memory system.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/No%20Cap%2C%20This%20Memory%20Slaps%3A%20Breaking%20Through%20the%20Memory%20Wall%20of%20Transactional%20Database%20Systems%20with%20Processing-in-Memory",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4241-kim.pdf",
    "session": "Research 38: Data Management on Novel Hardware",
    "authors": [
      {
        "Name": "Hyoungjoo Kim",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Yiwei Zhao",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Andrew Pavlo",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Phillip B. Gibbons",
        "Affiliation": "Carnegie Mellon University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1a5ce080-6fc4-4e4b-9a67-0c60496ec704",
    "title": "Revisiting the Index Construction of Proximity Graph-Based Approximate Nearest Neighbor Search",
    "abstract": "Proximity graphs (PG) have gained increasing popularity as the state-of-the-art solutions to  ùëò -approximate nearest neighbor ( ùëò ANN) search on high-dimensional data, which serves as a fundamental function in various fields, e.g., retrieval-augmented generation. Although PG-based approaches have the best  ùëò -ANN search performance, their index construction cost is superlinear to the number of points. Such superlinear cost substantially limits their scalability in the era of big data. Hence, the goal of this paper is to accelerate the construction of PG-based methods without compromising their  ùëò -ANN search performance. \nTo achieve this goal, two mainstream categories of PG are revisited: relative neighborhood graph (RNG) and navigable small world graph (NSWG). By revisiting their construction process, we find the issues of construction efficiency. To address these issues, we propose a new construction framework with a novel pruning strategy for edge selection, which accelerates RNG construction while keeping its  ùëò -ANN search performance. Then, we integrate this framework into NSWG construction to enhance both the construction efficiency and  ùëò -ANN search performance of NSWG. Extensive experiments are conducted to validate our construction framework for both RNG and NSWG, and that it significantly reduces the PG construction cost, achieving up to 5.6x speedup, while not compromising the  ùëò -ANN search performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Revisiting%20the%20Index%20Construction%20of%20Proximity%20Graph-Based%20Approximate%20Nearest%20Neighbor%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1825-liu.pdf",
    "session": "Research 3: Vector Data Management I",
    "authors": [
      {
        "Name": "Shuo Yang",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Jiadong Xie",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Yingfan Liu",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Jeffrey Xu Yu",
        "Affiliation": "Chinese University of Hong Kong"
      },
      {
        "Name": "Xiyue Gao",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Qianru Wang",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Yanguo Peng",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Jiangtao Cui",
        "Affiliation": "Xidian University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "20acfec9-742d-4e31-ac33-b0a8c5a46c8d",
    "title": "Towards Principled, Practical Document Database Design",
    "abstract": "Relational database design is a well-understood process enabled by a combination of database theory (e.g., normal forms) as well as conceptual modeling (e.g., ER-based design). In contrast, database design for NoSQL databases, notably document databases, is often approached in a much more ad hoc manner. It is frequently driven by application details and physical considerations that muddy the design process in ways all too reminiscent of the pre-relational database era. In this paper, we argue for a return to sanity ‚Äì for a logical, data-Ô¨Årst, conceptually grounded approach to document database design. We explain how such an approach can work, yielding a clean, query-friendly document database design. We also highlight a collection of document (JSON) anti-patterns to avoid. The process and the anti-patterns both stem from the authors‚Äô experiences in current and past lives when dealing with a wide variety of JSON document data from commercial applications, government applications, and university research applications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Principled%2C%20Practical%20Document%20Database%20Design",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4804-carey.pdf",
    "session": "Industry 3: Document, Graph, and Vector Databases",
    "authors": [
      {
        "Name": "Michael Carey",
        "Affiliation": "UC Irvine"
      },
      {
        "Name": "Wail Alkowaileet",
        "Affiliation": "Saudi National Center for AI"
      },
      {
        "Name": "Nick DiGeronimo",
        "Affiliation": "UC Irvine"
      },
      {
        "Name": "Peeyush Gupta",
        "Affiliation": "Couchbase"
      },
      {
        "Name": "Sachin Smotra",
        "Affiliation": "Dataworkz"
      },
      {
        "Name": "Till Westmann",
        "Affiliation": "Couchbase"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1b600105-9d11-4222-8e17-6df8b101df03",
    "title": "GQL and SQL/PGQ: Theoretical Models and Expressive Power",
    "abstract": "",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GQL%20and%20SQL%2FPGQ%3A%20Theoretical%20Models%20and%20Expressive%20Power",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1798-libkin.pdf",
    "session": "Research 13: Graph Data Management II",
    "authors": [
      {
        "Name": "Amelie Gheerbrant",
        "Affiliation": "Universit√© Paris Cit√©"
      },
      {
        "Name": "Leonid Libkin",
        "Affiliation": "University of Edinburgh & RelationalAI"
      },
      {
        "Name": "Liat Peterfreund",
        "Affiliation": "Hebrew University of Jerusalem"
      },
      {
        "Name": "Alexandra Rogova",
        "Affiliation": "IRIF, Universit√© Paris Cit√©"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Empty or missing abstract"
    ]
  },
  {
    "id": "45c5f257-19b7-424b-9521-b8ea016b5165",
    "title": "PAR2QO: Parametric Penalty-Aware Robust Query Optimization",
    "abstract": "Parametric Query Optimization (PQO) is an important problem in database systems, yet existing approaches suffer from high training costs, sensitivity to estimation errors, and vulnerability to severe performance regressions. This paper introduces  PAR 2 QO ( PAR ametric  P enalty- A ware  R obust  Q uery  O ptimization), a system that integrates robust query optimization into PQO.  PAR 2 QO strategically obtains plans from a well-balanced set of probe locations informed by the workload, and caches them as plan-penalty profiles. At runtime,  PAR 2 QO  selects the plan with the lowest expected penalty, explicitly accounting for selectivity uncertainties. Extensive experiments show that  PAR 2 QO  delivers significant speedups over existing methods while ensuring robustness against performance degradation. Additionally, we introduce  CARVER , a workload generator aimed at covering possible cardinalities of subqueries. Not only does  CARVER  provide a more comprehensive way to evaluate PQO methods, but when used for training learned methods, it can also enhance their generalizability and stability.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PAR2QO%3A%20Parametric%20Penalty-Aware%20Robust%20Query%20Optimization",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4532-xiu.pdf",
    "session": "Research 16: Query Processing and Optimization II",
    "authors": [
      {
        "Name": "Haibo Xiu",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Yang Li",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Qianyu Yang",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Pankaj Agarwal",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Jun Yang",
        "Affiliation": "Duke University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1230a771-f188-4984-b26a-3fb1b8fc0436",
    "title": "Synthetic Tabular Data: Methods, Attacks and Defenses",
    "abstract": "Synthetic data is often positioned as a solution to replace sensitive fixed-size data sets with a source of unlimited matching data, freed from privacy concerns. There has been much progress in synthetic data generation over the last decade, leveraging corresponding advances in machine learning and data analytics. In this tutorial, we survey the key developments and the main concepts in tabular synthetic data generation, including paradigms based on probabilistic graphical models and on deep learning. We provide background and motivation, before giving a technical deep-dive into the methodologies. We also address the limitations of synthetic data, by studying attacks that seek to retrieve information about the original sensitive data. Finally, we present extensions and open problems in this area. KEYWORDS synthetic data, differential privacy, marginal distributions, membership inference.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Synthetic%20Tabular%20Data%3A%20methods%2C%20attacks%20and%20defenses",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5448-cormode.pdf",
    "session": "Tutorial 11: Synthetic Tabular Data: Methods, Attacks and Defenses",
    "authors": [
      {
        "Name": "Graham Cormode",
        "Affiliation": "University of Warwick"
      },
      {
        "Name": "Shripad Gade",
        "Affiliation": "Meta"
      },
      {
        "Name": "Samuel Maddock",
        "Affiliation": "Meta"
      },
      {
        "Name": "Enayat Ullah",
        "Affiliation": "Meta"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "50bbdf4a-28e4-471b-bbce-7d754e8cc977",
    "title": "FDepHunter: Harnessing Negative Examples to Expose Fakes and Reveal Ghosts",
    "abstract": "Functional dependency (FD) discovery is fundamental in data profiling. Inevitably, existing approaches can return fake FDs that hold only coincidentally. Moreover, these approaches fall short of identifying ghost FDs that would be observable in a clean dataset, but that remain undetected because of outliers in the data. We introduce an interactive method for dependency discovery that augments an Armstrong relation with additional tuples. We rely on artificially generated  negative examples  that emulate real-world tuples to help expose fake FDs. In addition, we rely on domain experts to confirm that  positive examples  indeed reflect the characteristics of the original dataset. Our tool prototype  FDepHunter  thus provides a novel human-in-the-loop workflow where the set of discovered FDs can be iteratively refined.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/FDepHunter%3A%20Harnessing%20Negative%20Examples%20to%20Expose%20Fakes%20and%20Reveal%20Ghosts",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5227-koupil.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Pavel Koupil",
        "Affiliation": "Charles University"
      },
      {
        "Name": "J√°chym B√°rt√≠k",
        "Affiliation": "Charles University"
      },
      {
        "Name": "Stefan Klessinger",
        "Affiliation": "University of Passau"
      },
      {
        "Name": "Andr√© Conrad",
        "Affiliation": "University of Hagen"
      },
      {
        "Name": "Stefanie Scherzinger",
        "Affiliation": "University of Passau"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "96720ede-769e-4fae-a27f-a06e57a7de4f",
    "title": "Deduplicated Sampling On-Demand",
    "abstract": "Data practitioners often sample their datasets to produce representative subsets for their downstream tasks. When entities in a dataset can be partitioned into multiple groups, str atif i ed sampling is commonly used to produce subsets that match a target group distribution, e.g., to select a balanced subset for training a machine learning model. However, real-world data frequently contains duplicates ‚Äî multiple representations of the same real-world entity ‚Äî that can bias sampling, necessitating deduplication. \nWe define  deduplicated sampling  as the task of producing a clean sample of a dirty dataset according to a target group distribution. The na√Øve approach to deduplicated sampling would first deduplicate the entire dataset upfront, then perform sampling  ex post . However, that approach might be prohibitively expensive for large datasets and time/resource constraints.  Deduplicated sampling ondemand  with RadlER is a novel approach to produce a clean sample by focusing the cleaning effort only on entities required to appear in that sample. Our experimental evaluation, performed on multiple datasets from different domains, demonstrates that RadlER consistently outperforms baseline approaches, providing data scientists with an efficient solution to quickly produce a clean sample of a dirty dataset according to a target group distribution.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Deduplicated%20Sampling%20On-Demand",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2482-zecchini.pdf",
    "session": "Research 36: Data Cleaning and Preparation II",
    "authors": [
      {
        "Name": "Luca Zecchini",
        "Affiliation": "BIFOLD & TU Berlin"
      },
      {
        "Name": "Vasilis Efthymiou",
        "Affiliation": "Harokopio University of Athens"
      },
      {
        "Name": "Felix Naumann",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Giovanni Simonini",
        "Affiliation": "University of Modena and Reggio Emilia"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7e2ed589-5656-4932-9117-ae537872b993",
    "title": "Chameleon: a Heterogeneous and Disaggregated Accelerator System for Retrieval-Augmented Language Models",
    "abstract": "A Retrieval-Augmented Language Model (RALM) combines a large language model (LLM) with a vector database to retrieve context- specific knowledge during text generation. This strategy facilitates impressive generation quality even with smaller models, thus re- ducing computational demands by orders of magnitude. To serve RALMs efficiently and flexibly, we propose Chameleon, a heteroge- neous accelerator system integrating both LLM and vector search accelerators in a disaggregated architecture. The heterogeneity en- sures efficient serving for both inference and retrieval, while the disaggregation allows independent scaling of LLM and vector search accelerators to fulfill diverse RALM requirements. Our Chameleon prototype implements vector search accelerators on FPGAs and assigns LLM inference to GPUs, with CPUs as cluster coordina- tors. Evaluated on various RALMs, Chameleon exhibits up to 2.16√ó reduction in latency and 3.18√ó speedup in throughput compared to the hybrid CPU-GPU architecture. The promising results pave the way for adopting heterogeneous accelerators for not only LLM inference but also vector search in future RALM systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Chameleon%3A%20a%20Heterogeneous%20and%20Disaggregated%20Accelerator%20System%20for%20Retrieval-Augmented%20Language%20Models",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p42-jiang.pdf",
    "session": "Research 46: Vector Data Management III",
    "authors": [
      {
        "Name": "Wenqi Jiang",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Marco Zeller",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Roger Waleffe",
        "Affiliation": "University of Wisconsin-Madison"
      },
      {
        "Name": "Torsten Hoefler",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Gustavo Alonso",
        "Affiliation": "ETHZ"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c24d0794-1b62-42cc-bd57-76cfcfc79962",
    "title": "RapidStore: An Efficient Dynamic Graph Storage System for Concurrent Queries",
    "abstract": "Dynamic graph storage systems are essential for real-time applications such as social networks and recommendation, where the graph continuously evolves. However, they face significant challenges in efficiently handling concurrent read and write operations. We find that existing methods suffer from write queries interfering with read efficiency, substantial time and space overhead due to per-edge versioning, and an inability to balance performance, such as slow searches. To address these issues, we propose RapidStore, a holistic approach for efficient in-memory dynamic graph storage designed for read-intensive workloads. Our key idea is to exploit the characteristics of graph queries through a decoupled system design that separates the management of read and write queries and decouples version data from graph data. Besides, we design an efficient dynamic graph store to cooperate with the graph concurrency control mechanism. Experiments show that RapidStore enables fast and scalable concurrent graph queries, effectively balancing the performance of inserts, searches, and scans, and significantly improving efficiency in dynamic graph storage systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RapidStore%3A%20An%20Efficient%20Dynamic%20Graph%20Storage%20System%20for%20Concurrent%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3587-sun.pdf",
    "session": "Research 49: Graph Data Management VI",
    "authors": [
      {
        "Name": "Chiyu Hao",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Jixian Su",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Shixuan Sun",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Hao Zhang",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Sen Gao",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Jianwen Zhao",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Chenyi Zhang",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Jieru Zhao",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Chen Chen",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Minyi Guo",
        "Affiliation": "Shanghai Jiao Tong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "288913f8-be6c-447e-b00a-80dd867dfa8e",
    "title": "Eventual Durability",
    "abstract": "For latency-critical transactional applications, durability is often what limits performance. That is, executing transactions is fast, but guaranteeing that they are durable is slow. As a result, most of each transaction‚Äôs latency is attributable to durability. To address this problem, some database systems allow applications to sacrifice durability guarantees in exchange for lower transaction latencies. These ad hoc techniques are effective, but they can make it difficult for applications to understand and manage the risks associated with failures.\nIn this paper, our goal is to offer a more principled foundation for these kinds of performance/durability tradeoffs. The major obstacle to doing this is the transaction model itself, because it couples transaction durability with transaction commit. That is, the model defines a single point at which a transaction becomes visible and durable. This forces all transaction guarantees to wait for the slowest one, which is often durability.\nThe primary contribution of this work is a new eventually durable transaction model, which decouples commit from durability. Transactions commit first, and become durable later. We argue for making this model the basis of the contract between transactional data systems and applications. We describe what it means to correctly implement eventually durable transactions, and consider how they can be exposed to applications. We also describe a prototype implementation of eventual durability in PostgreSQL, and show that it enables applications to reduce transaction latencies while managing the durability risks.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Eventual%20Durability",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4733-salem.pdf",
    "session": "Research 52: Transaction Management",
    "authors": [
      {
        "Name": "Tejasvi Kashi",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Kenneth Salem",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Jaemyung Kim",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Khuzaima Daudjee",
        "Affiliation": "University of Waterloo"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "488643d8-30bd-44c7-935b-671547f917d8",
    "title": "Tabular: Efficiently Building Efficient Indexes",
    "abstract": "Concurrent indexes are hard to build by requiring complex, careful yet error-prone processes of design and implementation. As prior work has observed, modeling indexes as transactional tables can largely ease programming. The developer only needs to write singlethreaded logic without worrying about concurrency or persistence, which are transparently supported by ACID table operations. However, this was deemed infeasible due to high overheads caused by the underlying OLTP engine. \nIn this paper, we argue that by adapting recent OLTP techniques which have been shown to deliver unprecedented performance, this idea is now feasible. We propose Tabular, a new library for building efficient indexes by modeling indexes as ACID tables which provide concurrency and persistence transparently. We elaborate the design of Tabular and its use cases. Our evaluation shows that compared to hand-crafted ones, indexes built using Tabular provide competitive performance with improved programming efficiency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Tabular%3A%20Efficiently%20Building%20Efficient%20Indexes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1991-yan.pdf",
    "session": "Research 60: Database Engine Performance and Manageability II",
    "authors": [
      {
        "Name": "Ziyi Yan",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Mohamed Farouk Drira",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Tianxun Hu",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Tianzheng Wang",
        "Affiliation": "Simon Fraser University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "40fc79f6-25e0-456f-bf3c-34e9fea28047",
    "title": "Alchemy: A Query Optimization Framework for Oblivious SQL",
    "abstract": "Data sharing opportunities are everywhere, but privacy concerns and regulatory constraints often prevent organizations from fully realizing their value. A private data federation tackles this challenge by enabling secure querying across multiple privately held data stores where only the final results are revealed to anyone. We investigate optimizing relational queries evaluated under secure multiparty computation, which provides strong privacy guarantees but at a significant performance cost. \nWe present Alchemy, a query optimization framework that generalizes conventional optimization techniques to secure query processing over circuits, the most popular paradigm for cryptographic computation protocols. We build atop VaultDB, our open-source framework for oblivious query processing. Alchemy leverages schema information and the query‚Äôs structure to minimize circuit complexity while maintaining strong security guarantees. Our optimization framework builds incrementally through four synergistic phases: (1) rewrite rules to minimize circuits; (2) cardinality bounding with schema metadata; (3) bushy plan generation; and (4) physical planning with our fine-grained cost model for operator selection and sort reuse. While our work focuses on MPC, our optimization techniques generalize naturally to other secure computation settings. We validated our approach on TPC-H, demonstrating speedups of up to 2 OOM.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Alchemy%3A%20A%20Query%20Optimization%20Framework%20for%20Oblivious%20SQL",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3021-sohn.pdf",
    "session": "Research 54: Database Engines II",
    "authors": [
      {
        "Name": "Donghyun Sohn",
        "Affiliation": "Northwestern University"
      },
      {
        "Name": "Kelly Jiang",
        "Affiliation": "Northwestern University"
      },
      {
        "Name": "Nicolas Hammer",
        "Affiliation": "Northwestern University"
      },
      {
        "Name": "Jennie Rogers",
        "Affiliation": "Northwestern University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7ca4ffd6-6d08-4b5e-91ae-c03dd3cf14e5",
    "title": "ACE: A Cardinality Estimator for Set-Valued Queries",
    "abstract": "Cardinality estimation is a fundamental functionality in database systems. Most existing cardinality estimators focus on handling predicates over numeric or categorical data. They have largely omitted an important data type, set-valued data, which frequently occur in contemporary applications such as information retrieval and recommender systems. The few existing estimators for such data either favor high-frequency elements or rely on a  partial independence assumption , which limits their practical applicability. \nWe propose ACE, an  A ttention-based  C ardinality  E stimator for estimating the cardinality of queries over set-valued data. We first design a distillation-based data encoder to condense the dataset into a compact matrix. We then design an attention-based query analyzer to capture correlations among query elements. To handle variable-sized queries, a pooling module is introduced, followed by a regression model (MLP) to generate final cardinality estimates. We evaluate ACE on three datasets with varying query element distributions, demonstrating that ACE outperforms the state-ofthe-art competitors in terms of both accuracy and efficiency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ACE%3A%20A%20Cardinality%20Estimator%20for%20Set-Valued%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2112-sheng.pdf",
    "session": "Research 9: Query Processing and Optimization I",
    "authors": [
      {
        "Name": "Yufan Sheng",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Xin Cao",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Kaiqi Zhao",
        "Affiliation": "University of Auckland"
      },
      {
        "Name": "Yixiang Fang",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Jianzhong Qi",
        "Affiliation": "The University of Melbourne"
      },
      {
        "Name": "Wenjie Zhang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0172004a-4523-43c9-8dd6-6ede1d8b4fee",
    "title": "CausalMesh: A Causal Cache for Stateful Serverless Computing",
    "abstract": "Stateful serverless workflows consist of multiple serverless functions that access state on a remote database. Developers sometimes add a cache layer between the serverless runtime and the database to improve I/O latency. However, in a serverless environment, functions in the same workflow may be scheduled to different nodes with different caches, which can cause non-intuitive anomalies. This paper presents CausalMesh, a novel approach to causally consistent caching in serverless computing. CausalMesh is the first cache system that supports coordination-free and abort-free read/write operations and read transactions when clients roam among multiple servers. CausalMesh also supports read-write transactional causal consistency in the presence of client roaming, but at the cost of abort-freedom. Our evaluation shows that CausalMesh has lower latency and higher throughput than existing proposals.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/CausalMesh%3A%20A%20Causal%20Cache%20for%20Stateful%20Serverless%20Computing",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4599-zhang.pdf",
    "session": "Research 1: Cloud Data Management",
    "authors": [
      {
        "Name": "Haoran Zhang",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Shuai Mu",
        "Affiliation": "Stony Brook University"
      },
      {
        "Name": "Sebastian Angel",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Vincent Liu",
        "Affiliation": "University of Pennsylvania"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "61dece39-ea2a-4245-8103-68cd395c0168",
    "title": "Time-Series Clustering: A Comprehensive Study of Data Mining, Machine Learning, and Deep Learning Methods",
    "abstract": "Time-series clustering is a key task in time series analysis, enabling unsupervised data exploration and often serving as a subroutine for other tasks. Despite decades of active cross-disciplinary research, benchmarking of time-series clustering methods has received limited attention. Existing studies have (i) excluded popular methods and entire method classes; (ii) used a narrow range of distance measures; (iii) evaluated only a few datasets; (iv) lacked statistical validation; (v) had poor reproducibility; or (vi) relied on questionable evaluation setups. The rise of deep learning‚Äîespecially foundation models claiming broad generalization‚Äîfurther emphasizes the need for comprehensive evaluation, as their role in time-series clustering remains largely untested. To address these gaps, we evaluate 84 time-series clustering methods across 10 method classes from data mining, machine learning, and deep learning. Our analysis spans 128 time-series datasets and uses rigorous statistical methods. Within a fair comparison framework, we (i) identify the top-performing method in each class; (ii) highlight previously overlooked, high-performing classes; (iii) challenge assumptions about elastic distance measures; (iv) refute the claimed superiority of deep learning methods, including foundation models; (v) expose reproducibility issues; (vi) analyze performance variation across dataset properties; and (vii) assess scalability. Our findings reveal an illusion of progress: no method significantly outperforms the decade-old ùëò -Shape method. Still, we highlight a deep learning-based approach with notable promise. Our results provide a strong benchmark for advancing time-series clustering, and we have open-sourced our work to support future research.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Time-Series%20Clustering%3A%20A%20Comprehensive%20Study%20of%20Data%20Mining%2C%20Machine%20Learning%2C%20and%20Deep%20Learning%20Methods",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4380-paparrizos.pdf",
    "session": "Research 25: Time Series Data III",
    "authors": [
      {
        "Name": "John Paparrizos",
        "Affiliation": "Ohio State University"
      },
      {
        "Name": "Sai Prasanna Teja Reddy Bogireddy",
        "Affiliation": "Exelon"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "16dbb3b2-6c94-468d-838d-715e5096eede",
    "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction",
    "abstract": "Temporal link prediction in dynamic graphs is a critical task with applications in diverse domains such as social networks, recommendation systems, and e-commerce platforms. While existing Temporal Graph Neural Networks (T-GNNs) have achieved notable success by leveraging complex architectures to model temporal and structural dependencies, they often suffer from scalability and efficiency challenges due to high computational overhead. In this paper, we propose EAGLE, a lightweight framework that integrates shortterm temporal recency and long-term global structural patterns. EAGLE consists of a time-aware module that aggregates information from a node‚Äôs most recent neighbors to reflect its immediate preferences, and a structure-aware module that leverages temporal personalized PageRank to capture the influence of globally important nodes. To balance these attributes, EAGLE employs an adaptive weighting mechanism to dynamically adjust their contributions based on data characteristics. Also, EAGLE eliminates the need for complex multi-hop message passing or memory-intensive mechanisms, enabling significant improvements in efficiency. Extensive experiments on seven real-world temporal graphs demonstrate that EAGLE consistently achieves superior performance against stateof-the-art T-GNNs in both effectiveness and efficiency, delivering more than a 50√ó speedup over effective transformer-based T-GNNs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/When%20Speed%20meets%20Accuracy%3A%20an%20Efficient%20and%20Effective%20%20Graph%20Model%20for%20Temporal%20Link%20Prediction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3396-li.pdf",
    "session": "Research 39: Analytics over Different Data Types II",
    "authors": [
      {
        "Name": "Haoyang LI",
        "Affiliation": "The Hong Kong Polytechnic University"
      },
      {
        "Name": "Yuming Xu",
        "Affiliation": "PolyU & SCUT"
      },
      {
        "Name": "Yiming LI",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Hanmo LIU",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Darian LI",
        "Affiliation": "PolyU & GDUT"
      },
      {
        "Name": "Chen Jason ZHANG",
        "Affiliation": "PolyU"
      },
      {
        "Name": "Lei CHEN",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Qing LI",
        "Affiliation": "PolyU"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cf0627e3-03da-4965-8e71-ac5164230139",
    "title": "Simulating a Transactional Server for Multi-Model Systems",
    "abstract": "Multi-model systems integrate heterogeneous models, making consistency management a critical challenge. We present M2TS, a transactional server simulator for multi-model environments, enabling users to analyze the impact of consistency-preserving transactions on system performance. Unlike traditional transactional models that focus on ACID consistency, M2TS ensures multi-model consistency via bookkeepers, which propagate updates across models. The simulator supports various concurrency and consistency settings, allowing users to explore trade-offs in real-time. Through this demonstration, we provide insights into managing transactions in complex, interconnected environments.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Simulating%20a%20Transactional%20Server%20for%20Multi-Model%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5395-acosta.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Zenon Zacouris",
        "Affiliation": "Technische Universit√§t M√ºnchen"
      },
      {
        "Name": "Maribel Acosta",
        "Affiliation": "Technische Universit√§t M√ºnchen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7b7fdbeb-f10b-4b77-8198-bb677d1d00a9",
    "title": "From Logs to Causal Inference: Diagnosing Large Systems",
    "abstract": "Causal inference can quantify cause-effect relationships in domains as varied as medicine, economics and public policy. Production computer systems exhibit a similar level of complexity and a recur-computer systems exhibit a similar level of complexity and a recurring need to diagnose problems quickly. However, systems are only observed imperfectly, often via long, messy, semi-structured logs. \nIn this work, we want to accelerate large systems debugging by applying causal inference over logs, enabling engineers to di-by applying causal inference over logs, enabling engineers to diagnose problems and assess interventions in a principled manner. Our framework achieves this through two human-in-the-loop mod-Our framework achieves this through two human-in-the-loop modules: (1) The  Candidate Cause Ranker , through which one can determine the causes of a variable without running a full causal dis-determine the causes of a variable without running a full causal discovery algorithm; and (2) the  Interactive Causal Graph Refiner , which helps engineers compute an unbiased estimation of their effect of interest without extensive manual causal graph verifica-effect of interest without extensive manual causal graph verification. Both modules are powered by the insight that only part of the causal graph of the system is needed to correctly quantify a given effect of interest. We also provide a data preparation pipeline, the Log Converter , which transforms raw, messy, real-world logs into an appropriate tabular input for causal inference, using methods drawn from data transformation, cleaning, and extraction. \nWe evaluate  LOGos , a prototype implementation, on both real-We evaluate  LOGos , a prototype implementation, on both realworld and synthetic logs and find that: (1) The  Candidate Cause Ranker  achieved an average precision 1.08√ó ‚Äì 18√ó  higher than the baselines, in interactive time; (2) The  Interactive Causal Graph Refiner  required a number of causal judgments 1.61√ó ‚àí‚àí 16.83√ó lower than the baselines; and (3) The latency of  Log Converter scaled linearly with three measures of the complexity of a log: length, distinct templates, and fraction of tokens that are variables.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/From%20Logs%20to%20Causal%20Inference%3A%20Diagnosing%20Large%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p158-markakis.pdf",
    "session": "Research 56: Analytics over Different Data Types III",
    "authors": [
      {
        "Name": "Markos Markakis",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Brit Youngmann",
        "Affiliation": "Technion"
      },
      {
        "Name": "Trinity Gao",
        "Affiliation": "MIT"
      },
      {
        "Name": "Ziyu Zhang",
        "Affiliation": "MIT"
      },
      {
        "Name": "Rana Shahout",
        "Affiliation": "Harvard"
      },
      {
        "Name": "Peter Baile Chen",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Chunwei Liu",
        "Affiliation": "MIT"
      },
      {
        "Name": "Ibrahim Sabek",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Michael Cafarella",
        "Affiliation": "MIT CSAIL"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fab17b4d-0da9-4dae-88db-7a9c6c2a99c1",
    "title": "ArrayMorph: Optimizing Hyperslab Queries on the Cloud for Machine Learning Pipelines",
    "abstract": "Cloud storage services such as Amazon S3, Azure Blob Storage, and Google Cloud Storage are widely used to store raw data for machine learning applications. When the data is later processed, the analysis predominantly focuses on regions of interest (such as a small bounding box in a larger image) and discards uninteresting regions. Machine learning applications can signiÔ¨Åcantly accelerate their I/O if they push this data Ô¨Åltering step to the cloud. Prior work has proposed diÔ¨Äerent methods to partially read array (tensor) objects, such as chunking, reading a contiguous byte range, and evaluating a lambda function. No method is optimal; estimating the total time and cost of a data retrieval requires an understanding of the data serialization order, the chunk size and platform-speciÔ¨Åc properties. This paper introduces ArrayMorph, a cloud-based array data storage system that automatically determines which is the best method to use to retrieve regions of interest from data on the cloud. ArrayMorph formulates data accesses as hyperslab queries, and optimizes them using a multi-phase cost-based approach. ArrayMorph seamlessly integrates with Python/PyTorch-based ML applications, and is experimentally shown to transfer up to 9.8X less data than existing systems. This makes ML applications run up to 1.7X faster and 9X cheaper than prior solutions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ArrayMorph%3A%20Optimizing%20Hyperslab%20Queries%20on%20the%20Cloud%20for%20Machine%20Learning%20Pipelines",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3189-jiang.pdf",
    "session": "Research 59: Distributed and Streaming Data Processing",
    "authors": [
      {
        "Name": "Ruochen Jiang",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "Spyros Blanas",
        "Affiliation": "The Ohio State University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ead50e1e-3f91-4710-8212-0a1a14d0bbb9",
    "title": "QUEST: Query Optimization in Unstructured Document Analysis",
    "abstract": "Most recently, researchers have started building large language models (LLMs) powered data systems that allow users to analyze unstructured text documents like working with a database because LLMs are very effective in extracting attributes from documents. In such systems, LLM-based extraction operations constitute the performance bottleneck of query execution due to the high monetary cost and slow LLM inference. Existing systems typically borrow the query optimization principles popular in relational databases to produce query execution plans, which unfortunately are ineffective in minimizing LLM cost. To fill this gap, we propose  QUEST , which features a bunch of novel optimization strategies for unstructured document analysis. First, we introduce an index-based strategy to minimize the cost of each extraction operation. With this index,  QUEST  quickly retrieves the text segments relevant to the target attributes and only feeds them to LLMs. Furthermore, we design an evidence-augmented retrieval strategy to reduce the possibility of missing relevant segments. Moreover, we develop an instance-optimized query execution strategy: because the attribute extraction cost could vary significantly document by document, QUEST  produces different plans for different documents. For each document,  QUEST  produces a plan to minimize the frequency of attribute extraction. The innovations include LLM cost-aware operator ordering strategies and an optimized join execution approach that transforms joins into filters. Extensive experiments on 3 realworld datasets demonstrate the superiority of  QUEST , achieving 30%-6 √ó  cost savings while improving the F1 score by 10% -27% compared with state-of-the-art baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/QUEST%3A%20Query%20Optimization%20in%20Unstructured%20Document%20Analysis",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4560-chai.pdf",
    "session": "Research 36: Data Cleaning and Preparation II",
    "authors": [
      {
        "Name": "Zhaoze Sun",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Chengliang Chai",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Qiyan Deng",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Kaisen Jin",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Xinyu Guo",
        "Affiliation": "University of Arizona"
      },
      {
        "Name": "Han Han",
        "Affiliation": "University of Arizona"
      },
      {
        "Name": "Ye Yuan",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guoren Wang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Lei Cao",
        "Affiliation": "University of Arizona/MIT"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "696b2157-ce9e-4f7b-b316-1f0f593e79b8",
    "title": "PARQO: Penalty-Aware Robust Plan Selection in Query Optimization",
    "abstract": "The effectiveness of a query optimizer relies on the accuracy of selectivity estimates. The execution plan generated by the optimizer can be extremely poor in reality due to uncertainty in these estimates. This paper presents PARQO (Penalty-Aware Robust Plan Selection in Query Optimization), a novel system where users can define powerful robustness metrics that assess the expected penalty of a plan with respect to true optimal plans under uncertain selectivity estimates. PARQO uses workload-informed profiling to build error models, and employs principled sensitivity analysis techniques to identify human-interpretable selectivity dimensions with the largest impact on penalty. Experiments on three benchmarks demonstrate that PARQO finds robust, performant plans, and enables efficient and effective parametric optimization.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/PARQO%3A%20Penalty-Aware%20Robust%20Plan%20Selection%20in%20Query%20Optimization",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4627-xiu.pdf",
    "session": "Research 16: Query Processing and Optimization II",
    "authors": [
      {
        "Name": "Haibo Xiu",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Pankaj K Agarwal",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Jun Yang",
        "Affiliation": "Duke University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5a45ceb0-61c3-4f1e-9f4f-9bd86988bf7e",
    "title": "Vortex: Overcoming Memory Capacity Limitations in GPU-Accelerated Large-Scale Data Analytics",
    "abstract": "Despite the high computational throughput of GPUs, limited memory capacity and bandwidth-limited CPU-GPU communication via PCIe links remain significant bottlenecks for accelerating largescale data analytics workloads. This paper introduces  Vortex , a GPU-accelerated framework designed for data analytics workloads that exceed GPU memory capacity. A key aspect of our framework is an optimized IO primitive that leverages all available PCIe links in multi-GPU systems for the IO demand of a single target GPU. It routes data through other GPUs to such target GPU that handles IO-intensive analytics tasks. This approach is advantageous when other GPUs are occupied with compute-bound workloads, such as popular AI applications that typically underutilize IO resources. We also introduce a novel programming model that separates GPU kernel development from IO scheduling, reducing programmer burden and enabling GPU code reuse. Additionally, we present the design of certain important query operators and discuss a late materialization technique based on GPU‚Äôs zero-copy memory access. Without caching any data in GPU memory,  Vortex  improves the performance of the state-of-the-art GPU baseline, Proteus, by 5.7 √ó on average and enhances price performance by 2.5 √ó compared to a CPU-based DuckDB baseline.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Vortex%3A%20Overcoming%20Memory%20Capacity%20Limitations%20in%20GPU-Accelerated%20Large-Scale%20Data%20Analytics",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1250-yuan.pdf",
    "session": "Research 38: Data Management on Novel Hardware",
    "authors": [
      {
        "Name": "Yichao Yuan",
        "Affiliation": "University of Michigan"
      },
      {
        "Name": "Advait Iyer",
        "Affiliation": "University of Michigan, Ann Arbor"
      },
      {
        "Name": "Lin Ma",
        "Affiliation": "University of Michigan"
      },
      {
        "Name": "Nishil Talati",
        "Affiliation": "University of Michigan"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b05cd137-c61a-4dda-9495-3ffb9bccfe48",
    "title": "BLAEQ: A Multigrid Index for Spatial Query on Geometry Data",
    "abstract": "The efficiency of spatial queries is pivotal for the analysis of geometry data in the fields such as computational simulation, point cloud processing and digital engineering. Utilizing the computational capabilities of modern hardware, such as GPUs, offers a promising avenue for accelerating spatial query processing. However, conventional tree-based indexing methods are not optimized for maximal exploitation of GPU resources. \nTo address this problem, we introduce BLAEQ, a multigrid index designed to maximize the potential of GPUs. BLAEQ adopts a multigrid strategy, which represents an index tree with vectors as layers and matrices as connectors. Although BLAEQ shares conceptual similarities with traditional tree-based indexes, its innovative multigrid architecture facilitates effective parallelization on GPUs during the query phase. To optimize GPU utilization, BLAEQ is entirely constructed using BLAS (Basic Linear Algebra Subprograms), leveraging the efficiency of hardware-tuned BLAS libraries like CuBLAS. This design confers BLAEQ with enhanced performance over existing spatial query methods. \nOur study assesses BLAEQ‚Äôs performance against state-of-theart spatial query techniques using a range of both real-world and synthetic datasets. The experimental outcomes demonstrate that BLAEQ outperforms the benchmark approaches in terms of query efficiency on geometry data.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/BLAEQ%3A%20A%20Multigrid%20Index%20for%20Spatial%20Query%20on%20Geometry%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2533-wang.pdf",
    "session": "Research 24: Database Engines I",
    "authors": [
      {
        "Name": "Song Wang",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Chen Wang",
        "Affiliation": "\" Tsinghua University, China\""
      },
      {
        "Name": "Jianchun Wang",
        "Affiliation": "CHINA SHIP SCIENTIFIC RESEARCH CENTER"
      },
      {
        "Name": "Shengguo Li",
        "Affiliation": "National University of Defense Technology"
      },
      {
        "Name": "Rui Li",
        "Affiliation": "EIRI, Tsinghua University"
      },
      {
        "Name": "Zhiyong Peng",
        "Affiliation": "\" Wuhan University, China\""
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c1df2263-d536-44c6-9fbf-1484cbfd03c8",
    "title": "Heta: Distributed Training of Heterogeneous Graph Neural Networks",
    "abstract": "Heterogeneous Graphs (HetGs) that capture relationships among different types of nodes are ubiquitous in real-world applications such as academic networks and e-commerce. Although Heterogeneous Graph Neural Networks (HGNNs) have demonstrated superior performance in learning from these complex structures, distributed training of HGNNs on large-scale graphs with billions of edges faces substantial communication overhead. This challenge is exacerbated by heterogeneous characteristics such as varying feature dimensions across node types and featureless nodes requiring learnable parameters. Existing systems and communication reduction techniques designed for homogeneous graphs become suboptimal or even inapplicable for HetGs and HGNNs by overlooking both these heterogeneous characteristics and the inherent computational structure of HGNNs. We present  Heta , a framework designed to address the communication bottleneck in distributed HGNN training.  Heta  leverages the key insight that HGNN aggregation is order-invariant and decomposable into relation-specific computations. Built on this insight, we introduce three key innovations: (1) a Relation-Aggregation-First (RAF) paradigm that conducts relation-specific aggregations within partitions and exchanges only partial aggregations across machines, proven to reduce communication complexity; (2) a meta-partitioning strategy that divides a HetG based on its graph schema and HGNN computation dependency while minimizing cross-partition communication and maintaining computation and storage balance; and (3) a heterogeneity-aware GPU cache system that accounts for varying miss-penalty ratios across node types. Through extensive evaluation of billion-edge heterogeneous graphs, we demonstrate that Heta  achieves up to 5.3 √ó  and 4.4 √ó  speedup over state-of-the-art systems DGL and GraphLearn while maintaining model accuracy.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Heta%3A%20Distributed%20Training%20of%20Heterogeneous%20Graph%20Neural%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2790-zhong.pdf",
    "session": "Research 6: Graph Data Management I",
    "authors": [
      {
        "Name": "Yuchen Zhong",
        "Affiliation": "University of Hong Kong"
      },
      {
        "Name": "Junwei Su",
        "Affiliation": "University of Hong Kong"
      },
      {
        "Name": "Chuan Wu",
        "Affiliation": "University of Hong Kong"
      },
      {
        "Name": "Minjie Wang",
        "Affiliation": "Amazon Web Services"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b00a6590-a065-4534-9ff4-ff49235ecd7e",
    "title": "Play2Win: A Windowing Playground for Continuous Queries",
    "abstract": "Continuous Queries (CQs) are designed to operate over in!nite data streams; the paradigm gained prominence with the rise of Stream Processing (SPs). Central to CQs are  window operators  as they enforce bounded computation by partitioning streams into !nite subsets. Although several window operators exist ‚Äîe.g., slideby-tuple, session-window, and frames‚Äîcommercial systems largely adopt a few due to implementation complexity, theoretical opacity, and input-dependent non-determinism. This demonstration shows Play2Win, an interactive playground that empowers users to explore and compare various windowing strategies under a uni!ed system semantics. Our platform offers three key contributions: (I) a real-time environment for experimenting with different window operators; (II) a graph-based representation of the window state that eases direct comparison; and (III) a compositional framework for rapid prototyping of novel windowing mechanisms. The demonstration explore multiple datasets across different scenarios, fostering a deeper understanding of window operators for querying streams.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Play2Win%3A%20A%20Windowing%20Playground%20for%20Continuous%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5407-ferri.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Mauro Fam√†",
        "Affiliation": "INSA Lyon"
      },
      {
        "Name": "Alessandro Ferri",
        "Affiliation": "TU Darmstadt"
      },
      {
        "Name": "Samuele Langhi",
        "Affiliation": "Lyon 1 University"
      },
      {
        "Name": "Riccardo Tommasini",
        "Affiliation": "INSA Lyon - LIRIS"
      },
      {
        "Name": "Angela Bonifati",
        "Affiliation": "Lyon 1 University, IUF"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "97c65f9e-78f6-4251-89f8-92ce34616db8",
    "title": "SDEcho: Efficient Explanation of Aggregated Sequence Difference",
    "abstract": "Understanding the reasons behind differences between aggregated sequences derived from SQL queries is crucial for data scientists. However, existing methods often suffer from being labor-intensive, lacking scalability, providing only approximate solutions, and inad-lacking scalability, providing only approximate solutions, and inadequately supporting sequence difference explanations. In response, we introduce SDEcho, a novel framework designed to automate the explanation searching for sequence differences in high-dimensional and high-volume datasets. SDEcho utilizes advanced pruning tech-and high-volume datasets. SDEcho utilizes advanced pruning techniques, considering pattern, order, and dimension perspectives, as well as their interactions, to prune the entire explanation space while maintaining explanations accurate and concise. This hybrid pruning approach significantly accelerates the explanation search-pruning approach significantly accelerates the explanation searching process, making SDEcho a valuable tool for data analysis tasks. Extensive experiments on synthetic and real-world datasets, a long with a case study, demonstrate that SDEcho outperforms exis t ing methods in terms of both effectiveness and efficiency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SDEcho%3A%20Efficient%20Explanation%20of%20Aggregated%20Sequence%20Difference",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p784-he.pdf",
    "session": "Research 14: User Interfaces for Data Exploration and Explainable AI",
    "authors": [
      {
        "Name": "Fei Ye",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "zikang liu",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Xi Zhang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Yinan Jing",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Zhenying He",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Yuxin Che",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Haoran Xiong",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Kai Zhang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "X. Sean Wang",
        "Affiliation": "Fudan University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "966d40ef-04b6-4580-a2f1-6ca8c5f6f7f6",
    "title": "SimRN: Trajectory Similarity Learning in Road Networks based on Distributed Deep Reinforcement Learning",
    "abstract": "Trajectory similarity computation in road networks is crucial for data analytics. However, both non-learning-based and learningbased methods face challenges. First, they suffer from low accuracy due to manual parameter selection for model training and the omission of key spatio-temporal features in road networks. Second, they have low efficiency, stemming from the high time complexity of similarity computation and the time-consuming training process. Third, learning-based methods struggle with poor model generality due to the small size of available training samples. \nTo address these challenges, we propose an effective and efficient trajectory similarity learning framework for road networks, called  SimRN . To our knowledge,  SimRN  is the first deep reinforcement learning (DRL) approach for trajectory similarity computation. Specifically,  SimRN  consists of three key modules: the spatio-temporal prompt information extraction (STP) module, the trajectory representation based on DRL (TrajRL) module, and the graph contrastive learning (GCL) module. The STP module captures spatio-temporal features from road networks to improve the training of the trajectory representation. The TrajRL module automatically selects optimal parameters and enables parallel training, improving both trajectory representation and the efficiency of similarity computations. The GCL module employs a self-supervised contrastive learning paradigm to generate sufficient samples while preserving spatial constraints and temporal dependencies of trajectories. Extensive experiments on two real-world datasets, compared with three state-of-the-art methods, show that  SimRN : (i) improves accuracy by 20%‚Äì40%, (ii) achieves speedups of 2‚Äì4x, and (iii) demonstrates strong generality, enabling effective similarity learning with very small sample sizes.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SimRN%3A%20Trajectory%20Similarity%20Learning%20in%20Road%20Networks%20based%20on%20Distributed%20Deep%20Reinforcement%20Learning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2057-chen.pdf",
    "session": "Research 20: Road Networks, Social Networks",
    "authors": [
      {
        "Name": "Danlei Hu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yilin Li",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lu Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Ziquan Fang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yushuai Li",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Tianyi Li",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "11440a99-2c56-4b52-aba3-40e6d41bd789",
    "title": "Open Science: A New Paradigm for the Research Lifecycle",
    "abstract": "Open Science is a fundamentally new philosophy about the goals, methods, and business models of scientific research and scholarly communication. The motivation for research remains the same, as Open Science continues to honor scientific curiosity and societal or industrial challenges as its primary drivers. Open Science methodologies, however, continue to spotlight individual researchers for their achievements, but also promote common good. They are motivated by the following needs: increase researchers‚Äô collaboration, accountability, and transparency; democratize researchers‚Äô and the general public‚Äôs access to knowledge around the world; reconceive of what constitutes a publication to include data, software, and other artifacts beyond traditional papers; elevate the importance of research results‚Äô reproducibility; and increase society‚Äôs trust to the scientific community. Open Science affects every single stage of the research lifecycle significantly. Naturally, the scientific community and the extended ecosystem around it (university administrators, research funders, journal publishers, conference organizers, ...) need ample time to move towards the new mentality. In preparation for this change, this panel exposes the key dimensions of Open Science to the database community, highlights the various choices offered in each dimension, and debates their advantages and disadvantages and the way forward.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Open%20Science%3A%20A%20New%20Paradigm%20for%20the%20Research%20Lifecycle",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5512-yannis.pdf",
    "session": "Panel 4: Open Science: A New Paradigm for the Research Lifecycle",
    "authors": [
      {
        "Name": "Yannis Ioannidis",
        "Affiliation": "University of Athens"
      },
      {
        "Name": "Yanlei Diao",
        "Affiliation": "Ecole Polytechnique"
      },
      {
        "Name": "Dame Wendy Hall",
        "Affiliation": "University of Southampton"
      },
      {
        "Name": "Wolfgang Lehner",
        "Affiliation": "TU Dresden"
      },
      {
        "Name": "Natalia Manola",
        "Affiliation": "OpenAIRE AMKE"
      },
      {
        "Name": "Julia Stoyanovich",
        "Affiliation": "NYU"
      },
      {
        "Name": "Matei Zaharia",
        "Affiliation": "UC Berkeley"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "46e34d3a-affe-45fa-af11-457b3390fd2e",
    "title": "Less is More: Efficient Time Series Dataset Condensation via Two-fold Modal Matching",
    "abstract": "The expanding instrumentation of processes throughout society with sensors yields a proliferation of time series data that may in turn enable important applications, e.g., related to trans portation infrastructures or power grids. Machine-learning based  methods are increasingly being used to extract value from such data. We provide means of reducing the resulting considerable computational and data storage costs. We achieve this by providing means of condensing large time series datasets such that models trained on the condensed data achieve performance comparable to those trained on the original, large data. Specifically, we propose a   time series   d ataset   c ondensation framework, TimeDC, that employs two-fold modal matching, encompassing frequency matching and training trajectory matching. Thus, TimeDC performs time series feature extraction and decomposition-driven frequency matching to preserve complex temporal dependencies in the reduced time series. Further, TimeDC employs curriculum training trajectory matching to ensure effective and generalized time series dataset condensation. To avoid memory overflow and to reduce the cost of dataset condensation, the framework includes an expert buffer storing pre-computed expert trajectories. Extensive experiments on real data offer insight into the effectiveness and efficiency of the proposed solutions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Less%20is%20More%3A%20Efficient%20Time%20Series%20Dataset%20Condensation%20via%20Two-fold%20Modal%20Matching",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p226-miao.pdf",
    "session": "Research 19: Time Series Data II",
    "authors": [
      {
        "Name": "Hao Miao",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Ziqiao Liu",
        "Affiliation": "UESTC"
      },
      {
        "Name": "Yan Zhao",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Chenjuan Guo",
        "Affiliation": "ECNU"
      },
      {
        "Name": "Bin Yang",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Kai Zheng",
        "Affiliation": "University of Electronic Science and Technology of China"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "022d4e0c-831b-43cf-b019-0f40b50e3fef",
    "title": "Topology-preserving Graph Coarsening: An Elementary Collapse-based Approach",
    "abstract": "Graph coarsening techniques aim at simplifying the graph structure while preserving key properties in the resulting coarsened graph, have been widely used in graph partitioning and graph neural networks (GNNs). Existing graph coarsening techniques mainly focus on preserving cuts or graph spectrums. In this paper, we propose a new method that focuses on preserving graph topological features. In particular, we develop a novel graph coarsening approach, called Graph Elementary Collapse (GEC), by extending the concept of elementary collapse in algebraic topology to graph analysis. With this novel method, we can ensure a kind of equivalence relationship called homotopy equivalence of the graph during the coarsening process, thereby preserving numerous topological properties, including connectivity, rings, and voids. To enhance the scalability, we also propose several carefully-designed optimization techniques to reduce the time and memory consumption of our approach. Extensive experiments on several real-world datasets demonstrate the effectiveness and efficiency of our proposed method across various GNN prediction tasks.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Topology-preserving%20Graph%20Coarsening%3A%20An%20Elementary%20Collapse-based%20Approach",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4760-li.pdf",
    "session": "Research 41: Graph Data Management V",
    "authors": [
      {
        "Name": "Yuchen Meng",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Ronghua Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Longlong Lin",
        "Affiliation": "Southwest University"
      },
      {
        "Name": "Xunkai Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guoren Wang",
        "Affiliation": "Beijing Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8111b13d-ea8f-4f6a-861a-81256a42282b",
    "title": "Faster Convergence in Mini-batch Graph Neural Networks Training with Pseudo Full Neighborhood Compensation",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in various graph-related tasks. However, training GNNs on large-scale graphs is hindered by the neighbor explosion problem, rendering full-batch training computationally infeasible. Mini-batch training with neighbor sampling is a widely adopted solution, but it introduces gradient estimation errors that slow convergence and reduce model accuracy. In this work, we identify two primary sources of these errors: (1) missing gradient contributions from unsampled target nodes, and (2) inaccuracies in messages computed from sampled nodes. While existing methods largely focus on mitigating the second source, they often overlook the first, resulting in incomplete gradient estimation. To address this gap, we propose the Pseudo Full Neighborhood Compensation  (PFNC) framework, which leverages historical information to simultaneously compensate for both errors. PFNC is designed to integrate seamlessly with any neighbor sampling technique and significantly lowers memory demands by maintaining only a partial cache of historical embeddings and gradients. Theoretical analysis demonstrates that PFNC provides a closer approximation to the ideal gradient, enhancing convergence. Extensive experiments across multiple benchmark datasets confirm that PFNC accelerates convergence and improves generalization across diverse neighbor sampling strategies.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Faster%20Convergence%20in%20Mini-batch%20Graph%20Neural%20Networks%20Training%20with%20Pseudo%20Full%20Neighborhood%20Compensation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4309-zhou.pdf",
    "session": "Research 20: Road Networks, Social Networks",
    "authors": [
      {
        "Name": "Qiqi ZHOU",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Yanyan Shen",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a1c12b89-c58a-4ae3-807e-25e28e2de07a",
    "title": "FDBKeeper: Enabling Scalable Coordination Services for Metadata Management using Distributed Key-Value Databases",
    "abstract": "High-reliability distributed coordination services have become an indispensable part of modern large-scale distributed systems. Popular coordination services (e.g., ZooKeeper) adopt a single-writer design to provide a centralized service for managing system metadata, including various con√øguration information and data catalogs, and to provide distributed synchronization functions. With the continuous increase in metadata size and the scale of distributed systems, these coordination services gradually become performance bottlenecks due to their limitations in capacity, read and write performance, and scalability. \nTo bridge the gaps, we propose FDBKeeper, a novel solution that enables scalable coordination services on distributed ACID keyvalue database systems. Our motivation is that transactional keyvalue stores (i.e., FoundationDB) meet the demands of performance and scalability required by large-scale distributed systems over coordination service. To leverage these advantages, coordination services can be implemented as an upper layer on top of distributed ACID key-value databases. Our experimental results demonstrate that FDBKeeper signi√øcantly outperforms ZooKeeper across key metrics. Additionally, FDBKeeper reduces hardware resource costs on average by 33% in the production environment, resulting in substantial monetary cost savings. We have successfully replaced ZooKeeper with FDBKeeper in the production-grade ClickHouse cluster deployment.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/FDBKeeper%3A%20Enabling%20Scalable%20Coordination%20Services%20for%20Metadata%20Management%20using%20Distributed%20Key-Value%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5004-cai.pdf",
    "session": "Industry 5: Transactions and Concurrency Control",
    "authors": [
      {
        "Name": "Jun-Peng Zhu",
        "Affiliation": "East China Normal University & PingCAP"
      },
      {
        "Name": "Lingfeng Zhang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Peng Cai",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Xuan Zhou",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Peisen Zhao",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Xue Wang",
        "Affiliation": "Moqi Inc"
      },
      {
        "Name": "Linpeng Tang",
        "Affiliation": "Moqi Inc"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c77b84c5-3c8a-4ff1-b890-7ce38046cb13",
    "title": "BiST: A Lightweight and Efficient Bi-directional Model for Spatiotemporal Prediction",
    "abstract": "While existing spatiotemporal prediction models have shown promising performance, they often rely on the assumption of input-label spatiotemporal consistency, and their high complexity raises concerns about scalability. To enhance both efficiency and performance, we integrate label information into the learning process and propose a spatiotemporal dynamic theory that outlines a bi-directional learning paradigm. Building on this paradigm, we design BiST, a lightweight yet effective  Bi -directional  S patio- T emporal prediction model. BiST incorporates two key processes: a forward spatiotemporal learning process and a backward correction process. The forward process utilizes MLP layers exclusively to model input correlations and generate base prediction. In the backward process, we implement a spatiotemporal decoupling module, which can learn the residual modeling deviation between input and label representations from a decoupled perspective. After smoothing the residual with a diffusion module, we can obtain the correction term to correct the base predictions. This innovative design enables BiST to achieve competitive performance while remaining lightweight. We evaluate BiST against 26 baselines across 13 datasets, including a large-scale dataset with ten thousand nodes and a longrange dataset spanning 20 years. An impressive experimental result demonstrates that BiST achieves a 8.13% improvement in performance compared to state-of-the-art models while consuming only 1.86% of the training time and 7.36% of the memory usage.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/BiST%3A%20A%20Lightweight%20and%20Efficient%20Bi-directional%20Model%20for%20Spatiotemporal%20Prediction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1663-wang.pdf",
    "session": "Research 4: Analytics over Different Data Types I",
    "authors": [
      {
        "Name": "Jiaming Ma",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Binwu Wang",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Pengkun Wang",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Zhengyang Zhou",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Xu Wang",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Yang Wang",
        "Affiliation": "University of Science and Technology of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4c2086a0-7cbd-4184-93c6-b125b2ef25ab",
    "title": "MOMENTI: Scalable Motif Mining in Multidimensional Time Series",
    "abstract": "Time series play a fundamental role in many domains, capturing a plethora of information about the underlying data-generating processes. When a process generates multiple  synchronized  signals we are faced with  multidimensional  time series. In this context a fundamental problem is that of  motif mining , where we seek patterns repeating twice with minor variations, spanning some of the dimensions. State of the art exact solutions for this problem run in time quadratic in the length of the input time series. \nWe provide a scalable method to find the top- ùëò motifs in multidimensional time series with probabilistic guarantees on the quality of the results. Our algorithm runs in subquadratic time in the length of the input, and returns the exact solution with probability at least 1 ‚àí ùõø , where  ùõø is a user-defined parameter. The algorithm is designed to be  adaptive  to the input distribution, self-tuning its parameters while respecting user-defined limits on the memory to use. \nOur theoretical analysis is complemented by an extensive experimental evaluation, showing that our algorithm is orders of magnitude faster than the state of the art.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/MOMENTI%3A%20Scalable%20Motif%20Mining%20in%20Multidimensional%20Time%20Series",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3463-ceccarello.pdf",
    "session": "Research 25: Time Series Data III",
    "authors": [
      {
        "Name": "Matteo Ceccarello",
        "Affiliation": "University of Padova"
      },
      {
        "Name": "Francesco Pio Monaco",
        "Affiliation": "University of Padova"
      },
      {
        "Name": "Francesco Silvestri",
        "Affiliation": "University of Padova"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dd7699ba-d0d6-44bc-a9e1-21c873ca9e9c",
    "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models",
    "abstract": "Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of realworld conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ContextCache%3A%20Context-Aware%20Semantic%20Cache%20for%20Multi-Turn%20Queries%20in%20Large%20Language%20Models",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5391-yan.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "jianxin yan",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Wangze Ni",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiaotong University"
      },
      {
        "Name": "Peng Cheng",
        "Affiliation": "Tongji University"
      },
      {
        "Name": "Zhan Qin",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Kui Ren",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ad203bdc-f5f5-4482-b53f-d9c50c65627f",
    "title": "PRICE: A Pretrained Model for Cross-Database Cardinality Estimation",
    "abstract": "Cardinality estimation (CardEst) is essential for optimizing query execution plans. Recent ML-based CardEst methods achieve high accuracy but face deployment challenges due to high preparation costs and lack of transferability across databases. In this paper, we propose  PRICE , a   PR etrained mul t I -table   C ard E st model, which addresses these limitations.  PRICE  takes low-level but transferable features w.r.t. data distributions and query information and ele-features w.r.t. data distributions and query information and elegantly applies self-attention models to learn meta-knowledge to compute cardinality in any database. It is generally and adaptively applicable to any unseen new database to attain high estimation accuracy, while its preparation cost is as little as the basic one-accuracy, while its preparation cost is as little as the basic onedimensional histogram-based CardEst methods. Moreover,  PRICE can be finetuned to further enhance its performance on any specific database. We pretrained  PRICE  using 30 diverse datasets, com-database. We pretrained  PRICE  using 30 diverse datasets, completing the process in about 5 hours with a resulting model size of only about 40MB. Evaluations show that  PRICE  consistently outperforms existing methods, achieving the highest estimation accuracy on several unseen databases and generating faster exe-accuracy on several unseen databases and generating faster execution plans with lower overhead. After finetuning with a small volume of database-specific queries,  PRICE  could even find plans that were very close to the optimal ones. Meanwhile,  PRICE  is generally applicable to different settings such as data updates, data scaling, and query workload shifts.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PRICE%3A%20A%20Pretrained%20Model%20for%20Cross-Database%20Cardinality%20Estimation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p637-zhu.pdf",
    "session": "Research 61: Learned Query Optimization",
    "authors": [
      {
        "Name": "Tianjing Zeng",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Junwei Lan",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Jiahong Ma",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Wenqing Wei",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Rong Zhu",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Yingli Zhou",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Pengfei Li",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Bolin Ding",
        "Affiliation": "\"Data Analytics and Intelligence Lab, Alibaba Group\""
      },
      {
        "Name": "Defu Lian",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Zhewei Wei",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Jingren Zhou",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "00333d05-e1f9-4a75-b0c4-905e94ec83af",
    "title": "Efficient and Effective Algorithms for A Family of Influence Maximization Problems with A Matroid Constraint",
    "abstract": "Influence maximization ( IM ) is a classic problem that aims to iden-Influence maximization ( IM ) is a classic problem that aims to identify a small group of critical individuals, known as seeds, who can influence the largest number of users in a social network through word-of-mouth. This problem finds important applications includ-word-of-mouth. This problem finds important applications including viral marketing, infection detection, and misinformation con-ing viral marketing, infection detection, and misinformation containment. The conventional  IM  problem is typically studied with the oversimplified goal of selecting a  single  seed set. Many real-the oversimplified goal of selecting a  single  seed set. Many realworld scenarios call for  multiple  sets of seeds, particularly on social media platforms where various viral marketing campaigns need different sets of seeds to propagate effectively. To this end, previous works have formulated various  IM  variants, central to which is the requirement of multiple seed sets, naturally modeled as a matroid constraint. However, the current best-known solutions for these variants either offer a weak  (1/2 ‚àí ùúñ)-approximation, or offer a (1 ‚àí 1/ùëí ‚àí ùúñ)-approximation algorithm that is very expensive. We propose an efficient seed selection method called  AMP , an algo-propose an efficient seed selection method called  AMP , an algorithm with a  (1 ‚àí 1/ùëí ‚àí ùúñ)-approximation guarantee for this family of  IM  variants. To further improve efficiency, we also devise a fast implementation, called  RAMP . We extensively evaluate the perfor-implementation, called  RAMP . We extensively evaluate the performance of our proposal against 6 competitors across 4  IM  variants and on 7 real-world networks, demonstrating that our proposal outperforms all competitors in terms of result quality, running time, and memory usage. We have also deployed  RAMP  in a real industry strength application involving online gaming, where we show that our deployed solution significantly improves upon the baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20and%20Effective%20Algorithms%20for%20A%20Family%20of%20Influence%20Maximization%20Problems%20with%20A%20Matroid%20Constraint",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p117-huang.pdf",
    "session": "Research 28: Social Networks",
    "authors": [
      {
        "Name": "Yiqian Huang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Shiqi Zhang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Laks Lakshmanan",
        "Affiliation": "The University of British Columbia"
      },
      {
        "Name": "Wenqing Lin",
        "Affiliation": "Tencent"
      },
      {
        "Name": "Xiaokui Xiao",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Bo Tang",
        "Affiliation": "Southern University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a2e54f2d-82ba-45c1-8a72-7062e4ebff14",
    "title": "CoLA: Model Collaboration for Log-based Anomaly Detection",
    "abstract": "Log-based anomaly detection plays a crucial role in ensuring the reliability of systems. While deep learning-based small detection models (SDMs) are efficient, the large language models (LLMs) are accurate and capable of providing explanations. Intuitively, a compelling question arises: Can we seamlessly combine the advantages of both approaches? In this work, we delve into this underexplored research direction and propose CoLA, a novel   co llaborative   l og a nomaly detection framework. During collaborative inference, an SDM serves as a filter to select potentially anomalous instances, while a downstream LLM acts as an expert to detect anomalies, offer explanations, and refine the SDM. Extensive experiments on three large real-world datasets demonstrate that CoLA significantly outperforms state-of-the-art methods in terms of effectiveness, efficiency, and explainability, while also greatly reducing labor costs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/CoLA%3A%20Model%20Collaboration%20for%20Log-based%20Anomaly%20Detection",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3979-tang.pdf",
    "session": "Research 10: Data Mining and Analytics",
    "authors": [
      {
        "Name": "Xuhang Zhu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Xiu Tang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Sai Wu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Jichen Li",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Haobo Wang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Chang Yao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Quanqing Xu",
        "Affiliation": "OceanBase, Ant Group"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ef0c3b2d-4656-41e1-8361-29e15aba7406",
    "title": "DemandClean: A Multi-Objective Learning Framework for Balancing Model Tolerance to Data Authenticity and Diversity",
    "abstract": "Real-world datasets often suÔ¨Äer from multiple quality issues, hindering downstream model performance and increasing cleaning costs. To address this, we propose  DemandClean , a reinforcement learning-based adaptive data cleaning framework that dynamically balances cleaning eÔ¨Äectiveness and operational costs.  DemandClean explicitly considers data authenticity (alignment with real-world facts), diversity (richness of feature values), and downstream models‚Äô noise tolerance. We categorize data errors as missing (reducing authenticity and diversity), semantic (aÔ¨Äecting only authenticity), and syntactic (aÔ¨Äecting authenticity but potentially increasing diversity). Based on these errors,  DemandClean  intelligently selects among Repair, Delete, or No actions, guided by error rates and model robustness. For interpretability, the framework visually distinguishes authenticity, diversity, and tolerance. Extensive experiments conÔ¨Årm that  DemandClean  achieves near-optimal accuracy at substantially reduced preprocessing costs. SpeciÔ¨Åcally, it reduces repair actions by 80.0% and deletions by 80.7% compared to ‚ÄúRepair All‚Äù strategies, while maintaining or even exceeding their predictive performance, thus oÔ¨Äering an interpretable, cost-eÔ¨Äective, and scalable solution for practical applications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DemandClean%3A%20A%20Multi-Objective%20Learning%20Framework%20for%20Balancing%20Model%20Tolerance%20to%20Data%20Authenticity%20and%20Diversity",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5339-wang.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Zekai Qian",
        "Affiliation": "Harbin Institute of technology"
      },
      {
        "Name": "Xiaoou Ding",
        "Affiliation": "Harbin Institute of technology"
      },
      {
        "Name": "Chen Wang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Hongzhi Wang",
        "Affiliation": "Harbin Institute of technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3a711ff4-8dcd-4589-8e92-27a82d0b97bc",
    "title": "Datamap-Driven Tabular Coreset Selection for Classifier Training",
    "abstract": "In the era of data-driven decision-making, efficient machine learn-In the era of data-driven decision-making, efficient machine learning model training is crucial. We present a novel algorithm for con-ing model training is crucial. We present a novel algorithm for constructing  tabular data  coresets using datamaps created for Gradient Boosting Decision Trees models. The resulting coresets, computed within minutes, consistently outperform other baselines and match or exceed the performance of models trained on the entire dataset. Additionally, a training enhancement method leveraging datamap insights during the inference phase improves performance with mathematical guarantees, given a defined property holds. An ex-mathematical guarantees, given a defined property holds. An explainability layer and tools for coreset size optimization further enhance the efficiency of training tabular machine learning models.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Datamap-Driven%20Tabular%20Coreset%20Selection%20for%20Classifier%20Training",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p876-razmadze.pdf",
    "session": "Research 58: User Interfaces for Data Exploration and Recommender Systems",
    "authors": [
      {
        "Name": "Aviv Hadar",
        "Affiliation": "Tel Aviv University"
      },
      {
        "Name": "Tova Milo",
        "Affiliation": "Tel Aviv University"
      },
      {
        "Name": "Kathy Razmadze",
        "Affiliation": "Tel Aviv University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "13b1eeb3-15ff-4456-81ed-2c76798aada3",
    "title": "Locality-Aware Cache Replacement Policy for Graph Traversals",
    "abstract": "Many graph processing applications consist of read-only workloads that need to perform low-latency traversals over large graphs. These traversals are inherently expensive, and storage and processing systems need to be optimized for them. The performance of secondary storage-based systems can be improved by caching locality-driven data in memory. Exploring the data reuse of graph objects in applications is important to decrease the page faults in the cache. However, graph applications can suffer from poor access locality, making caching of graph data challenging. Locality can be imposed through graph ordering algorithms that can be exploited by cache replacement algorithms. We propose a graph locality-aware cache replacement policy called LAC that exploits the serialization layout obtained by graph ordering techniques. We show that the spatial locality that is captured on disk pages offers temporal locality for subsequent accesses of cache pages, and this information can be used to make improved cache replacement decisions. We evaluate LAC against the popular GCLOCK algorithm for input graphs with different structural properties while running various query types. Our evaluation shows that LAC can outperform GCLOCK through page fault improvements by reducing latency up to 1.42 √ó  in simulation studies and up to 1.23 √ó  with integration into the Neo4j system.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Locality-Aware%20Cache%20Replacement%20Policy%20for%20Graph%20Traversals",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2859-korkmaz.pdf",
    "session": "Research 57: Graph Data Management VII",
    "authors": [
      {
        "Name": "Zeynep Korkmaz",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Tamer √ñzsu",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Khuzaima Daudjee",
        "Affiliation": "University of Waterloo"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "686d98a2-3f08-48ce-bc48-13821e14efaa",
    "title": "Fully Automated Correlated Time Series Forecasting in Minutes",
    "abstract": "Societal and industrial infrastructures and systems increasingly leverage sensors that emit correlated time series. Forecasting of future values of such time series based on recorded historical val-future values of such time series based on recorded historical values has important benefits. Automatically designed models achieve higher accuracy than manually designed models. Given a fore-higher accuracy than manually designed models. Given a forecasting task, which includes a dataset and a forecasting horizon, automated design methods automatically search for an optimal forecasting model for the task in a manually designed search space, and then train the identified model using the dataset to enable the forecasting. Existing automated methods face three challenges. First, the search space is constructed by human experts, rending the methods only semi-automated and yielding search spaces prone to subjective biases. Second, it is time consuming to search for an optimal model. Third, training the identified model for a new task is also costly. These challenges limit the practicability of automated methods in real-world settings. To contend with the challenges, we propose a fully automated and highly efficient correlated time series forecasting framework where the search and training can be done in minutes. The framework includes a data-driven, iterative strategy to automatically prune a large search space to obtain a high-quality search space for a new forecasting task. It includes a zero-shot search strategy to efficiently identify the optimal model in the customized search space. And it includes a fast parameter adap-the customized search space. And it includes a fast parameter adaptation strategy to accelerate the training of the identified model. Experiments on seven benchmark datasets offer evidence that the framework is capable of state-of-the-art accuracy and is much more efficient than existing methods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Fully%20Automated%20Correlated%20Time%20Series%20Forecasting%20in%20Minutes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p144-wu.pdf",
    "session": "Research 53: Applied ML and AI for Data Management IV",
    "authors": [
      {
        "Name": "Xinle Wu",
        "Affiliation": "Aalborg Universigy"
      },
      {
        "Name": "Xingjian Wu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Dalin Zhang",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Miao Zhang",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Chenjuan Guo",
        "Affiliation": "ECNU"
      },
      {
        "Name": "Bin Yang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e3d4eb45-a5a6-4c7d-8c72-f3ba045c76bc",
    "title": "Outback: Fast and Communication-efficient Index for Key-Value Store on Disaggregated Memory",
    "abstract": "Disaggregated memory systems achieve resource utilization ef-Disaggregated memory systems achieve resource utilization efficiency and system scalability by distributing computation and memory resources into distinct pools of nodes. RDMA is an attrac-memory resources into distinct pools of nodes. RDMA is an attractive solution to support high-throughput communication between different disaggregated resource pools. However, existing RDMA solutions face a dilemma: one-sided RDMA completely bypasses computation at memory nodes, but its communication takes mul-computation at memory nodes, but its communication takes multiple round trips; two-sided RDMA achieves one-round-trip com-tiple round trips; two-sided RDMA achieves one-round-trip communication but requires non-trivial computation for index lookups at memory nodes, which violates the principle of disaggregated memory. This work presents Outback, a novel indexing solution for key-value stores with a one-round-trip RDMA-based network that does not incur computation-heavy tasks at memory nodes. Outback is the first to utilize dynamic minimal perfect hashing and separates its index into two components: one memory-efficient and compute-its index into two components: one memory-efficient and computeheavy component at compute nodes and the other memory-heavy and compute-efficient component at memory nodes. We implement a prototype of Outback and evaluate its performance in a public cloud. The experimental results show that Outback achieves higher throughput than both the state-of-the-art one-sided RDMA and two-sided RDMA-based in-memory KVS by 1.06-5.03√ó , due to the unique strength of applying a separated perfect hashing index.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Outback%3A%20Fast%20and%20Communication-efficient%20Index%20for%20Key-Value%20Store%20on%20Disaggregated%20Memory",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p335-liu.pdf",
    "session": "Research 15: Views, Indexing, and Search I",
    "authors": [
      {
        "Name": "Yi Liu",
        "Affiliation": "University of California, Santa Cruz"
      },
      {
        "Name": "Minghao Xie",
        "Affiliation": "UC Santa Cruz"
      },
      {
        "Name": "Shouqian Shi",
        "Affiliation": "University of California, Santa Cruz"
      },
      {
        "Name": "Yuanchao Xu",
        "Affiliation": "University of California, Santa Cruz"
      },
      {
        "Name": "Heiner Litz",
        "Affiliation": "UC Santa Cruz"
      },
      {
        "Name": "Chen Qian",
        "Affiliation": "University of California, Santa Cruz"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0dcce19f-b4f4-4003-857d-839cb8c33779",
    "title": "Efficient Graph Embedding Generation and Update for Large-Scale Temporal Graph",
    "abstract": "Graph embedding aims at mapping each node to a low-dimensional vector, beneÔ¨Åcial for various applications like pattern matching, retrieval augmented generation and recommendation. In this paper, we study the large-scale temporal graph embedding problem. DiÔ¨Äerent from simple graphs, each edge has a timestamp in temporal graphs, which requires the embeddings to encode the temporal biases. Factorizing similarity matrix is a common approach for generating simple graph embeddings where similarity can be well characterized by some conventional metrics like personalized PageRank. However, how to construct a similarity that can encode interactions with temporal biases is a critical problem for large scale temporal graphs. To address this, we introduce the concept of temporal-based bipartite graph (TBG) and develop the temporal preferential attachment similarity (TPASim) that reÔ¨Çects concurrent node activity over time. Directly factorizing the TPASim matrix, which contains nearly  n2   non-zeros, is not feasible for large graphs with  n nodes. Instead, we present  LTGE , which constructs and factorizes a temporal matrix with at most  2m non-zeros, where  m is the number of edges. Our theoretical analysis shows that  LTGE achieves the same embeddings as factorizing the TPASim matrix but signiÔ¨Åcantly reduces complexity by a factor of n2/m. On the other hand, when graphs evolve over time, to avoid recomputing, we further propose  LTGEInc  that utilizes a novel incremental singular value decomposition (SVD) algorithm with provable guarantee for updating the embeddings. Extensive experiments on several datasets with up to 17 million nodes and 1.3 billion edges demonstrate that  LTGE  outperforms the state of the art signiÔ¨Åcantly and is orders of magnitude faster than the baselines specially designed for temporal graphs. For embeddings update,  LTGEInc  retains the performance with small computational overhead.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Graph%20Embedding%20Generation%20and%20Update%20for%20Large-Scale%20Temporal%20Graph",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p929-tang.pdf",
    "session": "Research 34: Graph Data Management IV",
    "authors": [
      {
        "Name": "Yifan Song",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Xiaolong Chen",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Wenqing Lin",
        "Affiliation": "Tencent"
      },
      {
        "Name": "Jia Li",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Chen Zhang",
        "Affiliation": "Zhejiang CreateLink Technology"
      },
      {
        "Name": "Yan Zhou",
        "Affiliation": "Zhejiang CreateLink Technology"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Jing Tang",
        "Affiliation": "The Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3e636127-f16b-419b-8179-e0e072fb7297",
    "title": "Searching and Detecting Structurally Similar Communities in Large Heterogeneous Information Networks",
    "abstract": "Heterogeneous information networks (HINs) are prevalent in vari- ous domains, including bibliographic information networks, social media, and knowledge graphs. As a fundamental topic in HIN min- ing, community mining has found various real applications, such as recommendation, biological data analysis, and event organization. Most existing works often rely on meta-paths, relational constraints, spectral partitioning, label propagation, and network representa- tion to define the communities. However, almost all these works do not explicitly consider the structural similarity between vertices, which plays a vital role in modeling communities and also ignore the specific roles of vertices. In this paper, we propose a novel com- munity model, called structurally similar community (SSC), which models the HIN communities by explicitly considering the struc- tural similarity between vertices. In particular, SSC can not only support various structural similarity measures, but also identify different roles of the vertices in the community, such as cores, non- cores, hubs, and outliers. Based on the SSC, we develop fast online and index-based algorithms that support both efficient searching and detecting SSCs in large HINs, where the former one searches an SSC containing a specific query vertex while the latter one detects all the SSCs from the HIN. Extensive experiments on real-world datasets demonstrate the effectiveness of SSC model in revealing meaningful communities and the high efficiency of our proposed algorithms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Searching%20and%20Detecting%20Structurally%20Similar%20Communities%20in%20Large%20Heterogeneous%20Information%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1425-wang.pdf",
    "session": "Research 27: Graph Data Management III",
    "authors": [
      {
        "Name": "Shu Wang",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Yixiang Fang",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Wensheng Luo",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e4ba760a-f387-4a30-b64a-b04be394ca08",
    "title": "Filtered Vector Search: State-of-the-art and Research Challenges",
    "abstract": "This tutorial provides a comprehensive overview of filtered vector search (fvs). Fvs queries combine vector search with relational operators. The tutorial explores the challenges of integrating vector search into database engines and emphasizes the need for new optimization techniques. It explains the three primary filtered search methods for fvs queries over generic tree-based and graph-based indices and examines the factors influencing the selection of the most efficient method. A key objective is to highlight the importance of achieving stable recall, ideally in a declarative manner, ensuring consistent recall across queries. The tutorial then discusses recent filter-optimized vector indices and concludes by identifying open research challenges in the field of fvs, aiming to inspire further research and development.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Filtered%20Vector%20Search%3A%20State-of-the-art%20and%20Research%20Challenges",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5488-caminal.pdf",
    "session": "Tutorial 2: Filtered Vector Search: State-of-the-art and Research Challenges",
    "authors": [
      {
        "Name": "Helena Caminal",
        "Affiliation": "Google"
      },
      {
        "Name": "Yannis Chronis",
        "Affiliation": "ETH Z√ºrich"
      },
      {
        "Name": "Yannis Papakonstantinou",
        "Affiliation": "Google"
      },
      {
        "Name": "Fatma √ñzcan",
        "Affiliation": "Google"
      },
      {
        "Name": "Anastasia Ailamaki",
        "Affiliation": "EPFL"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "aa879e90-6c66-427e-8d77-d795c576160c",
    "title": "GraphCSR: A Degree-Equalized CSR Format for Large-scale Graph Processing",
    "abstract": "Graph processing underpins a vast array of data-centric applications, serving as a crucial component in fields such as social network analysis, recommendation systems, bio-informatics, and search engines. As graph data grows in scale and complexity, highperformance graph processing is increasingly essential. Many graph processing tasks depend on efficient data structures to manage the sparsity typical of real-world graphs, where most vertices have limited connectivity. This sparsity poses challenges for memory and computational efficiency in large-scale graph processing, and conventional sparse formats like Compressed Sparse Row (CSR) often struggle with memory and computation inefficiencies when handling massive graphs. To address these challenges, we introduce GraphCSR, a degree-equalized CSR format specifically tailored to enhance the spatio-temporal efficiency of distributed graph processing across various tasks. GraphCSR aggregates low-degree vertices into synthetic high-degree ones and applies group-wise compression to reduce storage overhead by recording only the starting index for each aggregated group. This reduces memory usage and supports batch-memory access to improve performance. Our extensive evaluations in various graph processing algorithms and datasets demonstrate that GraphCSR not only reduces the memory footprint required for large-scale graphs, but also improves performance across multiple types of graph processing tasks, outperforming popular sparse storage formats. Furthermore, when deployed on a production-scale supercomputer with 79,024 nodes, GraphCSR achieved a graph processing throughput that exceeded the top-ranked system on the Graph500 benchmark.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GraphCSR%3A%20A%20Degree-Equalized%20CSR%20Format%20for%20Large-scale%20Graph%20Processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4255-gan.pdf",
    "session": "Research 6: Graph Data Management I",
    "authors": [
      {
        "Name": "Xinbiao Gan",
        "Affiliation": "NUDT"
      },
      {
        "Name": "Tiejun Li",
        "Affiliation": "NUDT"
      },
      {
        "Name": "Chunye Gong",
        "Affiliation": "NUDT"
      },
      {
        "Name": "Dongsheng Li",
        "Affiliation": "NUDT"
      },
      {
        "Name": "dezun Dong",
        "Affiliation": "NUDT"
      },
      {
        "Name": "Jie Liu",
        "Affiliation": "NUDT"
      },
      {
        "Name": "KAI LU",
        "Affiliation": "NUDT"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eca00a76-52e9-49c8-909e-080144243444",
    "title": "PolyBase: Adapting to Data Affinity Changes in Geo-Replicated Database via Row-Level Consensus-Group Affiliation Re-Assignment",
    "abstract": "Transaction performance in geo-replicated databases heavily relies on the request location: when not issued by the primary region, transactions are forced to involve costly wide-area communication. While existing systems distribute primary roles across regions, such assignment typically occurs at the shard level, making it diffcult to align with geographically dispersed access to individual records. \nThis paper introduces PolyBase, a pioneering architecture to address such misalignment, leveraging the widely adopted Paxos-address such misalignment, leveraging the widely adopted Paxosbased log replication mechanisms. It enables flexible  row-level con-based log replication mechanisms. It enables flexible  row-level consensus group aÔøøliation , which runs on an  unchanged Paxos protocol , but  dynamically re-assigns  database rows between Paxos log repli-but  dynamically re-assigns  database rows between Paxos log replication groups, whose leaders become the primary region, enjoy-cation groups, whose leaders become the primary region, enjoying faster writes and up-to-date versions for reads. With carefully designed data structures and protocols, PolyBase significantly re-designed data structures and protocols, PolyBase significantly reduces wide-area RTTs without compromising transaction or log replication consistency or reliability guarantees. We implemented PolyBase with optimized re-assignment policies and integrated it into two popular databases (RocksDB and MySQL). Our evaluation on AWS, using a production e-commerce workload and microbench-on AWS, using a production e-commerce workload and microbenchmarks conÔøørms that PolyBase offers significantly higher transaction throughput and lower average/tail latency compared to baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PolyBase%3A%20Adapting%20to%20Data%20Affinity%20Changes%20in%20Geo-Replicated%20Database%20via%20Row-Level%20Paxos-Group%20Affiliation%20Re-Assignment",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p702-ruan.pdf",
    "session": "Research 48: Distributed Transactions II",
    "authors": [
      {
        "Name": "Chaoyi Ruan",
        "Affiliation": "USTC, Alibaba Group"
      },
      {
        "Name": "Yingqiang Zhang",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Juncheng Zhang",
        "Affiliation": "USTC, Alibaba Group"
      },
      {
        "Name": "Cheng Li",
        "Affiliation": "USTC"
      },
      {
        "Name": "Xiaosong Ma",
        "Affiliation": "MBZUAI"
      },
      {
        "Name": "Hao Chen",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Jie Zhou",
        "Affiliation": "Alibaba"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "xinjun Yang",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fc8964c9-6808-4fbc-837d-9772bcc28798",
    "title": "Access Control for Information-Theoretically Secure Data",
    "abstract": "This paper presents a novel key-based access control technique for secure outsourcing key-value stores where values correspond to documents that are indexed and accessed using keys. The proposed approach adopts Shamir‚Äôs secret-sharing that offers unconditional or information-theoretic security. It supports keyword-based document retrieval while preventing leakage of the data, access rights of users, or the size ( i . e ., volume of the output that satisÔ¨Åes a query). The proposed approach allows servers to detect (and abort) malicious clients from gaining unauthorized access to data, and prevents malicious servers from altering data undetected while ensuring efÔ¨Åcient access ‚Äì it takes 231.5ms over 5,000 keywords across 500,000 Ô¨Åles.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Access%20Control%20for%20Information-Theoretically%20Secure%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3243-sharma.pdf",
    "session": "Research 18: Data Privacy and Security I",
    "authors": [
      {
        "Name": "Yin Li",
        "Affiliation": "Dongguan University of Technology"
      },
      {
        "Name": "Sharad Mehrotra",
        "Affiliation": "University Of California Irvine"
      },
      {
        "Name": "Shantanu Sharma",
        "Affiliation": "New Jersey Institute of Technology"
      },
      {
        "Name": "Komal Kumari",
        "Affiliation": "New Jersey Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "342e6057-820b-4f08-ba60-fd43bba78aa6",
    "title": "Simpler is More: Efficient Top-K Nearest Neighbors Search on Large Road Networks",
    "abstract": "Top-ùëò Nearest Neighbors (ùëòNN) problem on road network has numerous applications on location-based services. As direct search using the Dijkstra‚Äôs algorithm results in a large search space, a plethora of complex-index-based approaches have been proposed to speedup the query processing. However, even with the current state-of-the-art approach, long query processing delays persist, along with significant space overhead and prohibitively long indexing time. In this paper, we depart from the complex index designs prevalent in existing literature and propose a simple index named KNN-Index. With KNN-Index, we can answer a ùëòNN query optimally and progressively with small and size-bounded index. To improve the index construction performance, we propose a bidirectional construction algorithm which can effectively share the common computation during the construction. Theoretical analysis and experimental results on real road networks demonstrate the superiority of KNN-Index over the state-of-the-art approach in query processing performance, index size, and index construction efficiency.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Simpler%20is%20More%3A%20Efficient%20Top-K%20Nearest%20Neighbors%20Search%20on%20Large%20Road%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4683-yuan.pdf",
    "session": "Research 20: Road Networks, Social Networks",
    "authors": [
      {
        "Name": "Yiqi WANG",
        "Affiliation": "UNSW"
      },
      {
        "Name": "Long Yuan",
        "Affiliation": "Nanjing University of Science and Technology"
      },
      {
        "Name": "Wenjie Zhang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Zi Chen",
        "Affiliation": "Nanjing University of Aeronautics and Astronautics"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiaotong University"
      },
      {
        "Name": "Qing Liu",
        "Affiliation": "Data61, CSIRO"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "72b43a75-8231-4f0e-b90a-a00af7d96f2c",
    "title": "A Branch-&-Bound Algorithm for Fractional Hypertree Decomposition",
    "abstract": "Conjunctive queries (CQs) have been widely used in database systems in which acyclic CQs can be computed efficiently, whereas cyclic CQs may not. Here, a CQ is acyclic if its hypergraph representation H is acyclic. In order to find a class of CQs that are ‚Äúmildly cyclic‚Äù, hypertree decompositions (HDs) have been studied. The quality of such HDs is by the so-called hypertree width. The class of acyclic queries is the queries whose hypertree width is 1, and a mildly cyclic CQ can be processed efficiently if its hypertree width is bounded. There are several HDs, such as tree decomposition (TD), generalized hypertree decomposition (GHD), fractional hypertree decomposition (FHD), as well as hypertree decomposition (HD). The minimum hypertree width by FHD is the smallest among all, and it is NP-complete to check if the minimum hypertree width by FHD exists for a given hypertree width at most ùëò. In the literature, there is no dynamic programming (DP) algorithm or branch-&-bound algorithm reported to compute FHD. In this paper, we show that there is a DP algorithm for FHD, and we give a branch-&-bound algorithm based on our DP algorithm to compute FHD with upper/lower bounds. We confirm the effectiveness and efficiency of our algorithm by testing all 3,648 hypergraphs given in a benchmark for HDs, and we also confirm our approach in query evaluation in real database systems.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/A%20Branch-%26-Bound%20Algorithm%20for%20Fractional%20Hypertree%20Decomposition",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4655-he.pdf",
    "session": "Research 35: Database Engines for Graphs",
    "authors": [
      {
        "Name": "Zongyan He",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Jeffrey Xu Yu",
        "Affiliation": "Chinese University of Hong Kong"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "631980a7-e696-4044-bd5e-94c1c2ac072a",
    "title": "Federated Incomplete Tabular Data Prediction with Missing Complementarity",
    "abstract": "Tabular data is abundant and crucial across both industry and academia. Federated learning (FL) offers a promising solution for the analysis of tabular data distributed across multiple organizations, without the need to share the privacy information of each client. Existing federated tabular data prediction methods optimize performance and privacy leakage under the completeness assumption of tabular data. They are not applicable in real-world scenarios that are struggling with missing values in tabular data. In this paper, we propose a novel federated prediction framework for incomplete tabular data, named  DARN , which leverages the  missing complementarity  to directly optimize prediction performance without relying on the imputed values. It is especially beneficial when clients exhibit heterogeneity in missing data distributions, and the pairwise observed data are complementary. Specifically, each client trains a  missing distribution learning model  to capture the distribution of locally incomplete data. To assist in this, we present a missing-aware transformer block  with a novel missing-aware attention mechanism to represent incomplete tabular data directly. The server calculates the personalized weights of the prediction models by combining  missing complementary score  and  observed sample size score , thereby maximizing the utility of the available data. Extensive experiments on four publicly available real-world datasets demonstrate that  DARN  outperforms state-of-the-art methods with 25.80% improvement in both classification and regression tasks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Federated%20Incomplete%20Tabular%20Data%20Prediction%20with%20Missing%20Complementarity",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3531-miao.pdf",
    "session": "Research 51: Information Integration and Data Quality III",
    "authors": [
      {
        "Name": "Yan Zhang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Shuwei Liang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Xiaoye Miao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yangyang Wu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Jianwei Yin",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6904e9d7-6f70-4394-9781-187b59fb64bf",
    "title": "Maximum k-Plex Finding: Choices of Pruning Techniques Matter!",
    "abstract": "A k-plex is a dense subgraph structure where every vertex can be disconnected with at most k vertices. Finding a maximum  k-plex (MkP) in a big graph is a key primitive in many real applicati ons such as community detection and biological network analysis. A lot of MkP algorithms have been actively proposed in recent years in top AI and DB conferences, featuring a broad range of sophisticated pruning techniques. In this paper, we study the various pruning techniques from nine recent MkP algorithms including kPlexT, Maple, Seesaw, DiseMKP, kPlexS, KpLeX, Maplex, BnB and BS by unifying them in a common framework called U-MkP. We summarize their proposed techniques into three categories, those for (1) branching, (2) upper bounding, and (3) reduction during subgraph exploration. We Ô¨Ånd that different pruning techniques can have drastically different performance impacts, but there exists a conÔ¨Åguration of the techniques dependent on  k that leads to the best performance in vast majority of the time. Interestingly, extensive experiments with our uniÔ¨Åed framework reveal that some techniques are not effective as claimed in the original works, and we also discover an unmentioned technique that is actually the major performance booster when  k  >  5 . We also study problem variants such as Ô¨Ånding all the MkPs and Ô¨Ånding the densest MkP (i.e., with the most edges) to cover community diversity, and effective algorithm parallelization. Our source code is released at https://github.com/akhlaqueak/MKP-Study.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Maximum%20k-Plex%20Finding%3A%20Choices%20of%20Pruning%20Techniques%20Matter!",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2928-ahmad.pdf",
    "session": "Research 49: Graph Data Management VI",
    "authors": [
      {
        "Name": "Akhlaque Ahmad",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Da Yan",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Xiao Chen",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Lyuheng Yuan",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Qin Zhang",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Saugat Adhikari",
        "Affiliation": "Indiana University Bloomington"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2fe1471a-e839-4e69-a9fd-c3929b41b6f0",
    "title": "EasyAD: A Demonstration of Automated Solutions for Time-Series Anomaly Detection",
    "abstract": "Despite the recent focus on time-series anomaly detection, the eÔ¨Äectiveness of the proposed anomaly detectors is restricted to speciÔ¨Åc domains. A model that performs well on one dataset may not perform well on another. Therefore, how to develop automated solutions for anomaly detection for a particular dataset has emerged as a pressing issue. However, there is a noticeable gap in the literature regarding providing a comprehensive review of the ongoing eÔ¨Äorts toward automated solutions for selecting or generating scores in an automated manner. Conducting a meta-analysis of proposed methods is challenging due to: (i) their evaluation across limited datasets; (ii) diÔ¨Äerent assumptions on application scenarios; and (iii) the absence of evaluations for out-of-distribution performance. Motivated by the limitations above, we introduce the EasyAD, a modular web engine designed to facilitate the exploration of the Ô¨Årst comprehensive benchmark for automated time-series anomaly detection. The EasyAD engine enables rigorous statistical analysis of 20 automated methods and 70 of their variants across the TSB-AD benchmark, a recently curated, heterogeneous dataset spanning nine application domains. The engine supports a two-dimensional evaluation framework, incorporating both accuracy and runtime performance. Our engine allows users to assess the performance of various methods per dataset and per instance, which oÔ¨Äers Ô¨Åne-grained analysis per time series. Furthermore, the engine accommodates the processing of user-uploaded data, enabling users to experiment with diÔ¨Äerent model selection strategies on their own datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/EasyAD%3A%20A%20Demonstration%20of%20Automated%20Solutions%20for%20Time-Series%20Anomaly%20Detection",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5431-liu.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Qinghua Liu",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "Seunghak Lee",
        "Affiliation": "Meta"
      },
      {
        "Name": "John Paparrizos",
        "Affiliation": "The Ohio State University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d44a9f7d-440a-4739-a47f-395d8c52e4d5",
    "title": "Beyond Shortest Paths: Node Fairness in Route Recommendation",
    "abstract": "Traditionally, route recommendation systems focused on minimizing distance (or time) to travel between two points. However, recent attention has shifted to other factors beyond mere length. This paper addresses the challenge of ensuring a fair distribution of visits among network nodes when handling a high volume of point-topoint path queries. In doing so, we adopt a  Rawlsian  notion of individual-level fairness exploiting the power of randomization. Specifically, we aim to create a probabilistic distribution over paths that maximizes the minimum probability of any eligible node being included in the recommended path. \nA key idea of our work is the notion of  forward paths , i.e., paths where travelling along any edge decreases the distance to the destination. In unweighted graphs forward paths and shortest paths coincide, but in weighted graphs forward paths provide a richer set of alternative routes, involving many more nodes while remaining close in length to the shortest path. Thus, they offer diversity and a wider basis for fairness, while maintaining near-optimal path lengths. We devise an algorithm that extracts a directed acyclic graph (DAG) containing all the forward paths in the input graph, with the same computational runtime as solving a single shortestpath query. This avoids enumerating all possible forward paths, which can be exponential in the number of nodes. We then design a flow problem on this DAG to derive the probabilistic distribution over forward paths with the desired fairness property, solvable in polynomial time through a sequence of small linear programs. \nOur experiments on real-world datasets validate our theoretical results, demonstrating that our technique provides individual node satisfaction while maintaining near-optimal path lengths. Moreover, our experiments show that our method can handle networks with millions of nodes and edges on a commodity laptop, and scales better than the baselines when there is a large volume of path queries for the same source and destination pair.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Beyond%20Shortest%20Paths%3A%20Node%20Fairness%20in%20Route%20Recommendation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3230-ferrara.pdf",
    "session": "Research 41: Graph Data Management V",
    "authors": [
      {
        "Name": "Antonio Ferrara",
        "Affiliation": "CENTAI"
      },
      {
        "Name": "David Garcia-Soriano",
        "Affiliation": "Universitat Politecnica de Catalunya"
      },
      {
        "Name": "Francesco Bonchi",
        "Affiliation": "CENTAI"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6a344437-32be-409e-a40b-0ac797e52f0d",
    "title": "STsCache: An Efficient Semantic Caching Scheme for Time-series Data Workloads Based on Hybrid Storage",
    "abstract": "Due to the increasing demand for extreme-scale time-series data workloads in data centers, it is required to build a high-performance semantic caching system that leverages the semantics and results of historical queries to answer time-series queries. Existing caching solutions either ignore the semantics of queries, offering suboptimal performance, or focus only on specific scenarios, providing smallcapacity, limited functionality. \nIn this paper, we summarize the query patterns of time-series data workload and propose the definition of semantic time-series caching for the first time. Accordingly, we present a semantic timeseries caching system, STsCache, based on a hybrid storage model with memory and NVMe SSD. We propose a series of optimized strategies, such as slab-based semantic data management, semantic index, semantic value-driven batch eviction, time-aware deduplication insertion, and lazy compaction. We implemented and evaluated STsCache via benchmarks and production environments. STsCache can increase throughput of popular time-series databases (InfluxDB, TimescaleDB) by 4.8-10.8 √ó  and reduce latency by 79.9%-93.5%. Compared with the latest time-series caching schemes (TSCache, BSCache), STsCache can increase throughput by 1.5-4.5 √ó , reduce latency by 59.4%-81.9%, and increase hit ratios by 22.5%-82.4%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/STsCache%3A%20An%20Efficient%20Semantic%20Caching%20Scheme%20for%20Time-series%20Data%20Workloads%20Based%20on%20Hybrid%20Storage",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2964-li.pdf",
    "session": "Research 19: Time Series Data II",
    "authors": [
      {
        "Name": "Tao Kong",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Hui Li",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Yuxuan Zhao",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Liping Li",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Xiyue Gao",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Qilong Wu",
        "Affiliation": "Xidian University"
      },
      {
        "Name": "Jiangtao Cui",
        "Affiliation": "Xidian University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a27e5bd7-4838-496c-9eb6-3706c2fa7931",
    "title": "Neighborhood-Preserving Graph Sparsification",
    "abstract": "We introduce a new graph sparsification method that targets the neighborhood information available for each node. Our approach is motivated by the fact that neighborhood information is used by several mining and learning tasks on graphs as well as reachability queries. The result of our sparsification technique is a sparsified graph that can be used instead of the original graph in the above tasks while still ensuring fairly good approximations for the results. Moreover, our sparsification method allows users to control the size of the resulting sparsified graph by adjusting the amount of information loss tolerated by the targeted applications. Our extensive experiments conducted on various real and synthetic graphs show that our sparsification considerably reduces the size of the graphs by achieving 40% sparsification rate on average on several input graphs. Furthermore, in the experimental study we show the utility and efficiency of our sparsification algorithm for notable data-driven tasks, such as node classification, graph classification and shortest path approximations.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Neighborhood-Preserving%20Graph%20Sparsification",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4853-seba.pdf",
    "session": "Research 41: Graph Data Management V",
    "authors": [
      {
        "Name": "Abd Errahmane KIOUCHE",
        "Affiliation": "LIRIS"
      },
      {
        "Name": "Julien Baste",
        "Affiliation": "Universit√© de Lille"
      },
      {
        "Name": "Mohammed Haddad",
        "Affiliation": "Universit√© Lyon 1"
      },
      {
        "Name": "Hamida SEBA",
        "Affiliation": "University Lyon 1"
      },
      {
        "Name": "Angela Bonifati",
        "Affiliation": "Univ. of Lyon"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "05523dd3-6509-4905-801b-2929493dafb7",
    "title": "NeuroFlinkCEP: Neurosymbolic Complex Event Recognition Optimized across IoT Platforms",
    "abstract": "We demonstrate NeuroFlinkCEP, the first framework that integrates neural and symbolic Complex Event Recognition (CER) over a state-of-the-art Big Data platform, also optimizing neurosymbolic CER upon operating over IoT settings. NeuroFlinkCEP receives expressed patterns as extended regular expressions and automatically transforms them to FlinkCEP jobs per device. To enable detection of simple events involved in CER patterns, NeuroFlinkCEP can integrate any neural model in FlinkCEP jobs. To optimally assign operator execution in-network, we incorporate and extend a stateof-the-art IoT optimizer.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/NeuroFlinkCEP%3A%20Neurosymbolic%20Complex%20Event%20Recognition%20Optimized%20across%20IoT%20Platforms",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5355-giatrakos.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Ourania Ntouni",
        "Affiliation": "Technical University of Crete"
      },
      {
        "Name": "Dimitrios Banelas",
        "Affiliation": "Technical University of Crete"
      },
      {
        "Name": "Nikos Giatrakos",
        "Affiliation": "Technical University of Crete"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dcf1224a-5319-4ed1-8860-5ffc9373187c",
    "title": "New Trends in Data Forgetting for Sustainable Data Management",
    "abstract": "Our ability to collect data is rapidly surpassing our ability to store it. As a result, organizations are faced with difficult decisions about what data to retain, and in what form, in order to meet their business goals while complying with storage restrictions. This is typically known as data reduction. This tutorial aims at introducing researchers and practitioners to the topic, and provides a holistic overview of the recent advancement in the field. It covers fundamental principles of data summarization, with a particular emphasis on submodular algorithms, alongside a detailed discussion on the limited existing data forgetting routines. It further underscores the limitations of the data summarization paradigm by introducing the concept of ‚Äúdata rotting‚Äù and illustrates the necessity of adopting the new stack data reduction techniques: data forgetting routines. Last, but not least, it discusses the challenges and open research questions in this newly born field.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/New%20Trends%20in%20Data%20Forgetting%20for%20Sustainable%20Data%20Management",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5472-velegrakis.pdf",
    "session": "Tutorial 12: New Trends in Data Forgetting for Sustainable Data Management",
    "authors": [
      {
        "Name": "Ramon Rico",
        "Affiliation": "Utrecht University"
      },
      {
        "Name": "Arno Siebes",
        "Affiliation": "Utrecht University"
      },
      {
        "Name": "Yannis Velegrakis",
        "Affiliation": "Utrecht University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fb7baed2-2fa6-4514-87d3-9a0475f9c7b0",
    "title": "Explaining Black-Box Clustering Pipelines With Cluster-Explorer",
    "abstract": "Explaining the results of clustering pipelines by unraveling the characteristics of each cluster is a challenging task, often addressed manually through visualizations and queries. Existing solutions from the domain of Explainable Artificial Intelligence (XAI) are largely ineffective for cluster explanations, and interpretable-by- design clustering algorithms may be unsuitable when the clustering algorithm does not fit the data properties.\nTo bridge this gap, we introduce Cluster-Explorer, a novel explainability tool for black-box clustering pipelines. Our approach formulates the explanation of clusters as the identification of con- cise conjunctions of predicates that maximize the coverage of the cluster‚Äôs data points while minimizing separation from other clusters. We achieve this by reducing the problem to generalized frequent-itemsets mining (gFIM), where items correspond to ex- planation predicates, and itemset frequency indicates coverage. To enhance efficiency, we leverage inherent problem properties and implement attribute selection to further reduce computational costs. Experimental evaluations on a benchmark collection of 98 cluster- ing results demonstrate the superiority of Cluster-Explorer in both explanation quality and execution times compared to XAI baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Explaining%20Black-Box%20Clustering%20Pipelines%20With%20Cluster-Explorer",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1495-somech.pdf",
    "session": "Research 14: User Interfaces for Data Exploration and Explainable AI",
    "authors": [
      {
        "Name": "Sariel Ofek",
        "Affiliation": "Bar-Ilan University"
      },
      {
        "Name": "Amit Somech",
        "Affiliation": "Bar-Ilan University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1843f652-9370-4a42-90d5-920f9e30e5b8",
    "title": "Subgraph Matching: A New Decomposition Based Approach",
    "abstract": "We study the subgraph matching problem, which is to Ô¨Ånd all subgraph isomorphisms of a given pattern graph  ƒ¶  in a data graph ƒÉ . Traditional approaches typically use a backtracking search approach or worst-case optimal join, both of which directly operate on  ƒ¶ . In this paper, we revisit the tree decomposition based approach. For a complex pattern graph  ƒ¶ , we Ô¨Ånd its optimal tree decomposition  ƒê  based on the fractional hypertree width, where a node in  ƒê  represents a subgraph of  ƒ¶ , which is also called a bag, and a node in  ƒ¶  may appear in multiple bags in  ƒê . The tree decomposition based approach initially computes and materializes the matches of subgraphs speciÔ¨Åed by the bags, then treats these matches as new relations and employs an acyclic join to compute the matches of  ƒ¶  itself. However, previous approaches fail to integrate the tree decomposition with eÔ¨Äective join attribute orders, and conversely, previous join attribute ordering approaches do not consider the need to share computations in multiple bags. Additionally, the materialization strategies in previous tree decomposition based approaches can lead to high computation costs. In this paper, we propose a new subgraph matching algorithm  ASDMatch ( A daptive   S hared   D ecomposition-based matching). We propose a new dynamic programming approach that Ô¨Ånds optimal attribute orders for each bag based on a cost model that incorporates the computation sharing. Furthermore, we introduce a new adaptive materialization strategy to reduce the computation cost. We conÔ¨Årmed that our  ASDMatch  outperforms state-of-the-art algorithms and can process many challenging queries that previous algorithms can not Ô¨Ånish within the time limit.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Subgraph%20Matching%3A%20A%20New%20Decomposition%20Based%20Approach",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4282-li.pdf",
    "session": "Research 28: Social Networks",
    "authors": [
      {
        "Name": "Qiyan Li",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Jeffrey Yu",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Zongyan He",
        "Affiliation": "The Chinese University of Hong Kong"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d513cc58-1d91-4d2c-bcee-511eec439c47",
    "title": "Still More Shades of Null: An Evaluation Suite for Responsible Missing Value Imputation",
    "abstract": "Data missingness is a practical challenge of sustained interest to the scientific community. In this paper, we present  Shades-of-Null , an evaluation suite for responsible missing value imputation. Our work is novel in two ways (i) we model realistic and socially-salient missingness scenarios that go beyond Rubin‚Äôs classic Missing Completely at Random ( MCAR ), Missing At Random ( MAR ) and Missing Not At Random ( MNAR ) settings, to include multi-mechanism missingness (when different missingness patterns co-exist in the data) and missingness shift (when the missingness mechanism changes between training and test) (ii) we evaluate imputers holistically, based on imputation quality and imputation fairness, as well as on the predictive performance, fairness and stability of the models that are trained and tested on the data post-imputation. \nWe use  Shades-of-Null  to conduct a large-scale empirical study involving 29,736 experimental pipelines, and find that while there is no single best-performing imputation approach for all missingness types, interesting trade-offs arise between predictive performance, fairness and stability, based on the combination of missingness scenario, imputer choice, and the architecture of the predictive model. We make  Shades-of-Null  publicly available, to enable researchers to rigorously evaluate missing value imputation methods on a wide range of metrics in plausible and socially meaningful scenarios.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Still%20More%20Shades%20of%20Null%3A%20An%20Evaluation%20Suite%20for%20Responsible%20Missing%20Value%20Imputation%20%5BExperiment%2C%20Analysis%20and%20Benchmark%5D",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2899-stoyanovich.pdf",
    "session": "Research 5: Data Cleaning and Preparation I",
    "authors": [
      {
        "Name": "Falaah Arif Khan",
        "Affiliation": "New York University"
      },
      {
        "Name": "Denys Herasymuk",
        "Affiliation": "Ukrainian Catholic University"
      },
      {
        "Name": "Nazar Protsiv",
        "Affiliation": "Ukrainian Catholic University"
      },
      {
        "Name": "Julia Stoyanovich",
        "Affiliation": "New York University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f22ad218-3b57-4f6a-b35c-415d8089f54b",
    "title": "The FastLanes File Format",
    "abstract": "This paper introduces a new open-source big data Ô¨Åle format, called FastLanes. It is designed for modern data-parallel execution (SIMD or GPU), and evolves the features of previous data formats such as Parquet, which are the foundation of data lakes, and which increasingly are used in AI pipelines. It does so by avoiding generic compression methods (e.g. Snappy) in favor of lightweight encodings, that are fully data-parallel. To enhance compression ratio, it cascades encodings using a Ô¨Çexible  expression encoding  mechanism. This mechanism also enables multi-column compression (MCC), enhancing compression by exploiting correlations between columns, a long-time weakness of columnar storage. We contribute a 2-phase algorithm to Ô¨Ånd encodings expressions during compression. \nFastLanes also innovates in its API, providing Ô¨Çexible support for  partial  decompression, facilitating engines to execute queries on compressed data. FastLanes is designed for Ô¨Åne-grained access, at the level of small batches rather than rowgroups; in order to limit the decompression memory footprint to Ô¨Åt CPU and GPU caches. \nWe contribute an open-source implementation of FastLanes in portable (auto-vectorizing) C++. Our evaluation on a corpus of real-world data shows that FastLanes improves compression ratio over Parquet, while strongly accelerating decompression, making it a win-win over the state-of-the-art.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20FastLanes%20File%20Format",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4629-afroozeh.pdf",
    "session": "Research 8: Database Engines for OLAP",
    "authors": [
      {
        "Name": "Azim Afroozeh",
        "Affiliation": "CWI"
      },
      {
        "Name": "Peter Boncz",
        "Affiliation": "CWI"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2d85a46c-69cd-4ba2-916c-7254d96f3317",
    "title": "AnalyticDB-PG: A Cloud-native High-performance Data Warehouse in Alibaba Cloud",
    "abstract": "In the era of big data, the landscape of data management and analytics has signiÔ¨Åcantly transformed, presenting diverse challenges for cloud platforms. Modern data warehouses face increasing challenges in handling hybrid transactional and analytical processing (HTAP) workloads eÔ¨Éciently in cloud environments. Traditional shared-nothing architectures provide high-performance query execution but suÔ¨Äer from high storage costs and limited elasticity, while shared-storage approaches improve scalability but often struggle with query eÔ¨Éciency due to increased data movement and indexing overhead. Furthermore, existing execution engines lack optimized support for vectorized processing and real-time analytics, limiting their ability to handle large-scale workloads eÔ¨Éciently. \nTo address these limitations, we introduce AnalyticDB-PG (ADBPG), a cloud-native, high-performance data warehouse designed for modern analytical workloads. It integrates a uniÔ¨Åed architecture supporting both Shared-Nothing and Shared-Storage modes, allowing Ô¨Çexible deployment and seamless elasticity. In ADB-PG, we introduce Beam, a hybrid storage engine that eÔ¨Éciently balances row-based and columnar storage for real-time analytics, and Laser, an optimized execution engine leveraging vectorized execution and Just-In-Time compilation to accelerate query processing. The system further incorporates advanced indexing mechanisms, adaptive runtime Ô¨Åltering, and dictionary encoding to enhance performance. Extensive evaluations on TPC-H and TPC-DS benchmarks demonstrate that ADB-PG achieves signiÔ¨Åcant performance improvements while reducing storage and operational costs, making it a compelling solution for modern cloud-based data analytics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/AnalyticDB-PG%3A%20A%20Cloud-native%20High-performance%20Data%20Warehouse%20in%20Alibaba%20Cloud",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5139-zhou.pdf",
    "session": "Industry 2: Data Platforms for Analytics",
    "authors": [
      {
        "Name": "Fangyuan Zhang",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Caihua Yin",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Hua Fan",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Fenghua Fang",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Yineng Chen",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Xuqi Wang",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Mengqi Wu",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Bing Chen",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Tianbo Jin",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Sibo Wang",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Wenchao Zhou",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Cloud"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4e2603e2-aa30-4492-bc2f-a83dcf6ed18e",
    "title": "Is Long Context All You Need? Leveraging LLM's Extended Context for NL2SQL",
    "abstract": "",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Is%20Long%20Context%20All%20You%20Need%EF%BC%9F%20Leveraging%20LLM",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2735-ozcan.pdf",
    "session": "Research 7: Natural Language Interfaces to Data",
    "authors": [
      {
        "Name": "Yeounoh Chung",
        "Affiliation": "Google"
      },
      {
        "Name": "Gaurav Tarlok Kakkar",
        "Affiliation": "Georgia Institute of Technology"
      },
      {
        "Name": "Yu Gan",
        "Affiliation": "Google"
      },
      {
        "Name": "Brenton Milne",
        "Affiliation": "Google"
      },
      {
        "Name": "Fatma Ozcan",
        "Affiliation": "Google"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Empty or missing abstract"
    ]
  },
  {
    "id": "ca83445c-b705-4e6b-8c7e-827dba5dacb1",
    "title": "Semantic Conformance Testing of Relational DBMS",
    "abstract": "Relational DBMS implementations are expected to adhere to SQL standards. However, there are currently no tools available that can automatically verify this conformance. The main reasons are two-automatically verify this conformance. The main reasons are twofold. First, the SQL standard specification, documented in natural language, tends to be ambiguous and is not directly executable. Second, it is difficult to generate test queries that thoroughly cover all aspects, e.g., keywords and parameters, defined in the SQL spe-all aspects, e.g., keywords and parameters, defined in the SQL specification. In this work, we introduce the first method for semantic conformance testing of RDBMSs. Our contributions are threefold. Firstly, we formally define the denotational semantics of SQL and implement them in Prolog, creating an executable reference RDBMS for differential testing against existing RDBMSs. Secondly, we pro-for differential testing against existing RDBMSs. Secondly, we propose three coverage criteria based on these formal semantics, along with a coverage-guided query generation algorithm that effect-with a coverage-guided query generation algorithm that effectively generates queries achieving high semantic coverage. Lastly, we apply our approach to six widely-used and thoroughly tested RDBMSs, e.g., MySQL, PostgreSQL and OceanBase, uncovering 19 bugs and 13 inconsistencies, all of which are confirmed by RDBMS developers.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Semantic%20Conformance%20Testing%20of%20Relational%20DBMS",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p850-liu.pdf",
    "session": "Research 47: Provenance and Workflows",
    "authors": [
      {
        "Name": "Shuang Liu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Chenglin Tian",
        "Affiliation": "Beijing University of Posts and Telecommunications"
      },
      {
        "Name": "Jun Sun",
        "Affiliation": "SMU"
      },
      {
        "Name": "Ruifeng Wang",
        "Affiliation": "Tianjin University"
      },
      {
        "Name": "WEI LU",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yongxin Zhao",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Yinxing Xue",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Junjie Wang",
        "Affiliation": "Tianjin University"
      },
      {
        "Name": "Xiaoyong Du",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bc18f356-35d9-4bee-a306-f32974870147",
    "title": "Falcon: Advancing Asynchronous BFT Consensus for Lower Latency and Enhanced Throughput",
    "abstract": "Asynchronous  Byzantine Fault Tolerant  (BFT) consensus protocols have garnered significant attention with the rise of blockchain technology. A typical asynchronous protocol is designed by executing sequential instances of the  Asynchronous Common Sub-seQuence (ACSQ). The ACSQ protocol consists of two primary components: the  Asynchronous Common Subset  (ACS) protocol and a block sorting mechanism, with the ACS protocol comprising two stages: broadcast and agreement. However, current protocols encounter three critical issues: high latency arising from the execution of the agreement stage, latency instability due to the integral-sorting mechanism, and reduced throughput caused by block discarding. \nTo address these issues, we propose Falcon, an asynchronous BFT protocol that achieves low latency and enhanced throughput. Falcon introduces a novel broadcast protocol,  Graded Broadcast  (GBC), which enables a block to be included in the  ACS  set directly, bypassing the agreement stage and thereby reducing latency. To ensure safety, Falcon incorporates a new binary agreement protocol called Asymmetrical Asynchronous Binary Agreement  (AABA), designed to complement GBC. Additionally, Falcon employs a partial-sorting mechanism, allowing continuous rather than simultaneous block committing, enhancing latency stability. Finally, we incorporate an agreement trigger that, before its activation, enables nodes to wait for more blocks to be delivered and committed, thereby boosting throughput. We conduct a series of experiments to evaluate Falcon, demonstrating its superior performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Falcon%3A%20Advancing%20Asynchronous%20BFT%20Consensus%20for%20Lower%20Latency%20and%20Enhanced%20Throughput",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2136-dai.pdf",
    "session": "Research 50: Access Control and Privacy, Blockchain",
    "authors": [
      {
        "Name": "Xiaohai Dai",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Chaozheng Ding",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Wei Li",
        "Affiliation": "The University of Sydney"
      },
      {
        "Name": "Jiang Xiao",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Bolin Zhang",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Chen Yu",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Albert Zomaya",
        "Affiliation": "The University of Sydney"
      },
      {
        "Name": "Hai Jin",
        "Affiliation": "Huazhong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0867f3ce-9f76-4066-8303-3db371476673",
    "title": "Incremental Detection of Denial Constraint Violations",
    "abstract": "Denial constraints (DCs) are well-known to express business rules on data. They subsume other integrity constraints ( IC s), such as key constraints or functional dependencies. One can use traditional DBMS or specialized algorithms to validate such dependencies on a dataset. However, no known approach exists to detect DC violations incrementally . Data typically changes over time, and recomputing the entire violation set after every update is wasteful. Alerting data practitioners of data quality issues immediately, enables them to take measures earlier and can help prevent follow-up issues. \nWe present Weever, the first incremental approach to detect all violations of a given set of DCs. It uses a novel index structure to process inequality predicates and a new method to plan the execution order of predicates depending on their selectivity, reducing redundant computations when handling multiple DCs. Our evaluation shows that Weever outperforms a DBMS-based baseline by up to two orders of magnitude. And in the same time that a state-of-the-art static approach takes to analyze an entire dataset, Weever processes up to 200 000 insertions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Incremental%20Detection%20of%20Denial%20Constraint%20Violations",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1000-kaminsky.pdf",
    "session": "Research 36: Data Cleaning and Preparation II",
    "authors": [
      {
        "Name": "Youri Kaminsky",
        "Affiliation": "Hasso Plattner Institute"
      },
      {
        "Name": "Eduardo Pena",
        "Affiliation": "UTFPR"
      },
      {
        "Name": "Felix Naumann",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "61e933c8-d1c0-469c-9903-61a4c022c1d1",
    "title": "VSAG: An Optimized Search Framework for Graph-based Approximate Nearest Neighbor Search",
    "abstract": "Approximate nearest neighbor search (ANNS) is a fundamental problem in vector databases and AI infrastructures. Recent graphbased ANNS algorithms have achieved high search accuracy with practical efficiency. Despite the advancements, these algorithms still face performance bottlenecks in production, due to the random memory access patterns of graph-based search and the high computational overheads of vector distance. In addition, the performance of a graph-based ANNS algorithm is highly sensitive to parameters, while selecting the optimal parameters is cost-prohibitive, e.g., manual tuning requires repeatedly re-building the index. This paper introduces  VSAG , an open-source framework that aims to enhance the in production performance of graph-based ANNS algorithms. VSAG  has been deployed at scale in the services of Ant Group, and it incorporates three key optimizations:  (i) efficient memory access : it reduces L3 cache misses with pre-fetching and cache-friendly vector organization;  (ii) automated parameter tuning : it automatically selects performance-optimal parameters without requiring index rebuilding;  (iii) efficient distance computation : it leverages modern hardware, scalar quantization, and smartly switches to low-precision representation to dramatically reduce the distance computation costs. We evaluate  VSAG  on real-world datasets. The experimental results show that  VSAG  achieves the state-of-the-art performance and provides up to 4 √ó  speedup over HNSWlib (an industry-standard library) while ensuring the same accuracy.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/VSAG%3A%20An%20Optimized%20Search%20Framework%20for%20Graph-based%20Approximate%20Nearest%20Neighbor%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5017-cheng.pdf",
    "session": "Industry 3: Document, Graph, and Vector Databases",
    "authors": [
      {
        "Name": "Xiaoyao Zhong",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Haotian Li",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Jiabao Jin",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Mingyu Yang",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Deming Chu",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Xiangyu Wang",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Zhitao Shen",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Wei Jia",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "George Gu",
        "Affiliation": "Intel"
      },
      {
        "Name": "Yi Xie",
        "Affiliation": "Intel"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiaotong University"
      },
      {
        "Name": "Heng Tao Shen",
        "Affiliation": "Tongji University"
      },
      {
        "Name": "Jingkuan Song",
        "Affiliation": "Tongji University"
      },
      {
        "Name": "Peng Cheng",
        "Affiliation": "Tongji University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4b713b5f-5ccc-4c89-b421-a232c31b3db9",
    "title": "Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods",
    "abstract": "Graph-theoretic algorithms and graph machine learning models are essential tools for addressing many real-life problems, such as social network analysis and bioinformatics. To support largescale graph analytics, graph-parallel systems have been actively developed for over one decade, such as Google‚Äôs Pregel and Spark‚Äôs GraphX, which (i) promote a think-like-a-vertex computing model and target (ii) iterative algorithms and (iii) those problems that output a value for each vertex. However, this model is too restricted for supporting the rich set of heterogeneous operations for graph analytics and machine learning that many real applications demand. \nIn recent years, two new trends emerge in graph-parallel systems research: (1) a novel think-like-a-task computing model that can efficiently support the various computationally expensive problems of subgraph search; and (2) scalable systems for learning graph neural networks. These systems effectively complement the diversity needs of graph-parallel tools that can flexibly work together in a comprehensive graph processing pipeline for real applications, with the capability of capturing structural features. This tutorial will provide an effective categorization of the recent systems in these two directions based on their computing models and adopted techniques, and will review the key design ideas of these systems. Slides are available at https://github.com/akhlaqueak/VLDB-2025-Tutorial.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Systems%20for%20Scalable%20Graph%20Analytics%20and%20Machine%20Learning%3A%20Trends%20and%20Methods",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5460-yan.pdf",
    "session": "Tutorial 6: Systems for Scalable Graph Analytics and Machine Learning: Trends and Methods",
    "authors": [
      {
        "Name": "Da Yan",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Lyuheng Yuan",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Akhlaque Ahmad",
        "Affiliation": "Indiana University Bloomington"
      },
      {
        "Name": "Saugat Adhikari",
        "Affiliation": "Indiana University Bloomington"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f57eb9f8-a5ab-4d73-824b-b33dc8407ac1",
    "title": "Efficient Computation of Hyper-triangles on Hypergraphs",
    "abstract": "Hypergraphs, which use hyperedges to capture groupwise inter-Hypergraphs, which use hyperedges to capture groupwise interactions among different entities, have gained increasing attention recently for their versatility in effectively modeling real-world net-recently for their versatility in effectively modeling real-world networks. In this paper, we study the problem of computing hyper-works. In this paper, we study the problem of computing hypertriangles (formed by three fully-connected hyperedges), which is a basic structural unit in hypergraphs. Although existing approaches can be adopted to compute hyper-triangles by exhaustively exam-can be adopted to compute hyper-triangles by exhaustively examining hyperedge combinations, they overlook the structural char-ining hyperedge combinations, they overlook the structural characteristics distinguishing different hyper-triangle patterns. Conse-acteristics distinguishing different hyper-triangle patterns. Consequently, these approaches lack specificity in computing particular hyper-triangle patterns and exhibit low efficiency. In this paper, we unveil a new formation pathway for hyper-triangles, transi-we unveil a new formation pathway for hyper-triangles, transitioning from hyperedges to hyperwedges before assembling into hyper-triangles, and classify hyper-triangle patterns based on hy-hyper-triangles, and classify hyper-triangle patterns based on hyperwedges. Leveraging this insight, we introduce a two-step frame-perwedges. Leveraging this insight, we introduce a two-step framework to reduce the redundant checking of hyperedge combinations. Under this framework, we propose efficient algorithms for comput-Under this framework, we propose efficient algorithms for computing a specific pattern of hyper-triangles. Approximate algorithms are also devised to support estimated counting scenarios. Further-are also devised to support estimated counting scenarios. Furthermore, we introduce a fine-grained hypergraph clustering coefficient measurement that can reflect diverse properties of hypergraphs based on different hyper-triangle patterns. Extensive experimen-based on different hyper-triangle patterns. Extensive experimental evaluations conducted on 11 real-world datasets validate the effectiveness and efficiency of our proposed techniques.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Computation%20of%20Hyper-triangles%20on%20Hypergraphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p729-wang.pdf",
    "session": "Research 49: Graph Data Management VI",
    "authors": [
      {
        "Name": "Haozhe Yin",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Kai Wang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Wenjie Zhang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Ying Zhang",
        "Affiliation": "University of Technology Sydney"
      },
      {
        "Name": "Ruijia Wu",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiaotong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b1b46171-bcf4-45aa-b3bb-4db3fcba9aef",
    "title": "From Genesis to Maturity: Managing Knowledge Graph Ecosystems Through Life Cycles",
    "abstract": "Knowledge graphs (KGs) play a crucial role in the integration and organization of heterogeneous data and knowledge, enabling ad-organization of heterogeneous data and knowledge, enabling advanced data analytics and decision-making across various indus-vanced data analytics and decision-making across various industries. This vision paper addresses critical challenges in managing KGs, emphasizing their relevance in integrating information from disparate sources. We propose the concept of knowledge graph ecosystems and life cycles to systematically manage tasks, e.g., data integration, standardization, continuous updates, efficient querying, and provenance tracking. By adopting our approach, organizations can enhance the accuracy, consistency, and reliability of KGs, thus improving knowledge management, enabling the extraction of valu-improving knowledge management, enabling the extraction of valuable insights, and ensuring transparency and accountability.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/From%20Genesis%20to%20Maturity%3A%20Managing%20Knowledge%20Graph%20Ecosystems%20Through%20Life%20Cycles",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1390-geisler.pdf",
    "session": "Research 63: Knowledge Graph Management",
    "authors": [
      {
        "Name": "Sandra Geisler",
        "Affiliation": "RWTH Aachen University"
      },
      {
        "Name": "Cinzia Cappiello",
        "Affiliation": "Politecnico di Milano"
      },
      {
        "Name": "Irene Celino",
        "Affiliation": "CEFRIEL"
      },
      {
        "Name": "David Chaves-Fraga",
        "Affiliation": "Universidade de Santiago de Compostela"
      },
      {
        "Name": "Anastasia Dimou",
        "Affiliation": "KU Leuven"
      },
      {
        "Name": "Ana Iglesias-Molina",
        "Affiliation": "Universidad Polit√©cnica de Madrid"
      },
      {
        "Name": "Maurizio Lenzerini",
        "Affiliation": "Sapienza University of Rome"
      },
      {
        "Name": "Anisa Rula",
        "Affiliation": "University of Brescia"
      },
      {
        "Name": "Dylan Van Assche",
        "Affiliation": "IDLab, Ghent University - imec"
      },
      {
        "Name": "Sascha Welten",
        "Affiliation": "RWTH Aachen University"
      },
      {
        "Name": "Maria-Esther Vidal",
        "Affiliation": "TIB Hannover, L3S Research Center, Leibniz University Hannover"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e9fae921-b4f4-4f61-8b18-7f12337e6737",
    "title": "BACH: Bridging Adjacency List and CSR Format using LSM-Trees for HGTAP Workloads",
    "abstract": "Modern data-intensive applications require databases that support fast analytical processing on massive dynamic graphs in real time, while simultaneously providing transactional guarantees for modi- fying graph-based objects (i.e., edges, vertices and their properties). Achieving efficient Hybrid Graph Transactional/Analytical Pro- cessing (HGTAP) in a database poses significant challenges due to the simultaneous requirements of high operation throughput, high data freshness, and high performance isolation when processing concurrent read/write queries on intricate graph topology. Existing disk-based graph databases fail to meet these requirements at the same time due to their inclined data layout, such as the transac- tional storage based on adjacency list and the analytical storage based on CSR (compressed sparse row) format.\nTo address these challenges, we present BACH (Bridging Adja- cency List and CSR Format using LSM (Log-Structured Merge)- Trees for HGTAP Workloads) to fill the gaps in HGTAP databases. BACH expands the design space of traditional LSM-Trees to ac- commodate different graph data layouts in different levels. The compaction process is further extended to seamlessly transform the graph layout from the TP-friendly adjacency list to the AP- friendly CSR format through the data propagation to deeper levels in the LSM-Tree. A novel compaction policy, namely elastic merge, is carefully devised to adapt to diverse workloads and the skew vertex degree distribution on graph data. These techniques lead to a Graph-aware Real-time (GR)-LSM-Tree, which can provide consis- tently efficient data access for diverse workloads throughout the entire lifespan of graph objects. Then, a lightweight multi-version scheme is devised for the GR-LSM-Tree to accelerate the concur- rent read/write processing with the snap-shot isolation guarantee. Comprehensive experiments demonstrate that BACH significantly outperforms other disk-based graph database solutions in HGTAP workloads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/BACH%3A%20Bridging%20Adjacency%20List%20and%20CSR%20Format%20using%20LSM-Trees%20for%20HGTAP%20Workloads",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1509-miao.pdf",
    "session": "Research 13: Graph Data Management II",
    "authors": [
      {
        "Name": "Jianfeng Huang",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Yihao Cao",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Shubing Ren",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Baohua Wu",
        "Affiliation": "Central South University"
      },
      {
        "Name": "Dongjing Miao",
        "Affiliation": "Harbin Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "63bcd183-d971-4821-aaa1-9721a6d90efa",
    "title": "SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning",
    "abstract": "This paper introduces SagaLLM, a structured multi-agent architecture designed to address four foundational limitations of current LLM-based planning systems: unreliable self-validation, context loss, lack of transactional safeguards, and insufficient inter-agent coordination. While recent frameworks leverage LLMs for task decomposition and multi-agent communication, they often fail to ensure consistency, rollback, or constraint satisfaction across distributed workflows. SagaLLM bridges this gap by integrating the Saga transactional pattern with persistent memory, automated compensation, and independent validation agents. It leverages LLMs‚Äô generative reasoning to automate key tasks traditionally requiring hand-coded coordination logic, including state tracking, dependency analysis, log schema generation, and recovery orchestration. Although SagaLLM relaxes strict ACID guarantees, it ensures workflow-wide consistency and recovery through modular checkpointing and compensable execution. Empirical evaluations across planning domains demonstrate that standalone LLMs frequently violate interdependent constraints or fail to recover from disruptions. In contrast, SagaLLM achieves significant improvements in consistency, validation accuracy, and adaptive coordination under uncertainty‚Äîestablishing a robust foundation for real-world, scalable LLM-based multi-agent systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SagaLLM%3A%20Context%20Management%2C%20Validation%2C%20and%20Transaction%20Guarantees%20for%20Multi-Agent%20LLM%20Planning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4874-chang.pdf",
    "session": "Industry 4: Machine Learning, AI, and Databases",
    "authors": [
      {
        "Name": "Longling Geng",
        "Affiliation": "Stanford University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5523c918-dd20-42d3-ad47-bfc57cb3e7d2",
    "title": "Efficient Discovery of Relaxed Functional Dependencies",
    "abstract": "This paper studies the discovery of relaxed functional dependencies ( RFDs ). We consider  RFDs  that relax restrictions in both value equality and constraint satisfaction: treating values as equal if their distance is less than a given similarity threshold, and considering  RFDs  with violations below a given error threshold as valid. As a highly non-trivial extension of the row-based approach to functional dependency ( FD ) discovery, we present the first algorithm capable of discovering all valid and minimal  RFDs . We extend the structure called ‚Äú difference-set ‚Äù for  predicates  that are combinations of attributes and similarity thresholds. We present an efficient method for difference-set construction, incorporating optimizations for both time and space complexity. When inferring  RFDs  from difference-sets, we enumerate  RFDs  based on the subsumption relationship of their right-hand-side predicates to share computations. An extensive experimental evaluation verifies that the proposed discovery algorithm is faster than baseline methods up to orders of magnitude and effective in finding hidden  FDs  from dirty data.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Discovery%20of%20Relaxed%20Functional%20Dependencies",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2044-tan.pdf",
    "session": "Research 39: Analytics over Different Data Types II",
    "authors": [
      {
        "Name": "Mengran Li",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Zijing Tan",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Honghui Yang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Shuai Ma",
        "Affiliation": "Beihang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ef007914-c39d-417c-b34b-7b3ab8577031",
    "title": "Stochastic SketchRefine: Scaling In-Database Decision-Making under Uncertainty to Millions of Tuples",
    "abstract": "Decision making under uncertainty often requires choosing  packages , or bags of tuples, that collectively optimize expected outcomes while limiting risks. Processing  Stochastic Package Queries  (SPQs) involves solving very large optimization problems on uncertain data. Monte Carlo methods create numerous  scenarios , or sample realizations of the stochastic attributes of  all  the tuples, and generate packages with optimal objective values across these scenarios. The number of scenarios needed for accurate approximation‚Äîand hence the size of the optimization problem when using prior methods‚Äî increases with variance in the data, and the search space of the optimization problem increases exponentially with the number of tuples in the relation. Existing solvers take hours to process SPQs on large relations containing stochastic attributes with high variance. Besides enriching the SPaQL language to capture a broader class of risk specifications, we make two fundamental contributions toward scalable SPQ processing. First, we propose  risk-constraint linearization  (RCL), which converts SPQs into Integer Linear Programs (ILPs) whose size is independent of the number of scenarios used. Solving these ILPs gives us feasible and near-optimal packages. Second, we propose Stochastic SketchRefine, a divide and conquer framework that breaks down a large stochastic optimization problem into subproblems involving smaller subsets of tuples. Our experiments show that, together, RCL and Stochastic SketchRefine produce high-quality packages in orders of magnitude lower runtime than the state of the art.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Stochastic%20SketchRefine%3A%20Scaling%20In-Database%20Decision-Making%20under%20Uncertainty%20to%20Millions%20of%20Tuples",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3106-haque.pdf",
    "session": "Research 21: Specialized and Domain-Specific Data Management",
    "authors": [
      {
        "Name": "Riddho Ridwanul Haque",
        "Affiliation": "University of Massachusetts Amherst"
      },
      {
        "Name": "Anh Mai",
        "Affiliation": "NYU Abu Dhabi"
      },
      {
        "Name": "Matteo Brucato",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Azza Abouzied",
        "Affiliation": "NYU Abu Dhabi"
      },
      {
        "Name": "Peter Haas",
        "Affiliation": "University of Massachusetts Amherst"
      },
      {
        "Name": "Alexandra Meliou",
        "Affiliation": "University of Massachusetts Amherst"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e6fa3bc8-d75f-4741-90fe-6886bdcaa254",
    "title": "DocETL: Agentic Query Rewriting and Evaluation for Complex Document Processing",
    "abstract": "Analyzing unstructured data has been a persistent challenge in data processing. Recent proposals o!er declarative frameworks for LLMpowered processing of unstructured data, but they typically execute user-specified operations as-is in a single LLM call‚Äîfocusing on cost rather than accuracy. This is problematic for complex tasks, where even well-prompted LLMs can miss relevant information. For instance, reliably extracting  all  instances of a specific clause from legal documents often requires decomposing the task, the data, or both. \nWe present DocETL, a system that optimizes complex document processing pipelines, while accounting for LLM shortcomings. DocETL offers a declarative interface for users to define such pipelines and uses an agent-based approach to automatically optimize them, leveraging novel agent-based rewrites (that we call  rewrite directives ), as well as an optimization and evaluation framework. We introduce  (i)  logical rewriting of pipelines, tailored for LLM-based tasks,  (ii)  an agent-guided plan evaluation mechanism, and  (iii) an optimization algorithm that efficiently finds promising plans, considering the latencies of LLM execution. Across four real-world document processing tasks, DocETL improves accuracy by 21‚Äì80% over strong baselines. DocETL is open-source at  docetl.org  and, as of March 2025, has over 1.7k GitHub stars across diverse domains.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DocETL%3A%20Agentic%20Query%20Rewriting%20and%20Evaluation%20for%20Complex%20Document%20Processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3035-shankar.pdf",
    "session": "Research 17: Applied ML and AI for Data Management II",
    "authors": [
      {
        "Name": "Shreya Shankar",
        "Affiliation": "University of California Berkeley"
      },
      {
        "Name": "Tristan Chambers",
        "Affiliation": "University of California Berkeley"
      },
      {
        "Name": "Tarak Shah",
        "Affiliation": "University of California Berkeley"
      },
      {
        "Name": "Aditya Parameswaran",
        "Affiliation": "University of California Berkeley"
      },
      {
        "Name": "Eugene Wu",
        "Affiliation": "Columbia University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9b885d0c-a4b1-49eb-aecc-5371b9800f3e",
    "title": "Mining the Minoria: Unknown, Under-represented, and Under-performing Minority Groups",
    "abstract": "Due to a variety of reasons, such as privacy, data in the wild often misses the grouping information required for identifying minori- ties. On the other hand, it is known that machine learning models are only as good as the data they are trained on and, hence, may underperform for the under-represented minority groups. The miss- ing grouping information presents a dilemma for responsible data scientists who find themselves in an unknown-unknown situation, where not only do they not have access to the grouping attributes but do not also know what groups to consider.\nThis paper is an attempt to address this dilemma. Specifically, we propose a minority mining problem, where we find vectors in the attribute space that reveal potential groups that are under-represented and under-performing. Technically speaking, we propose a geometric transformation of data into a dual space and use notions such as the arrangement of hyperplanes to design an efficient algorithm for the problem in lower dimensions. Generalizing our solution to the higher dimensions is cursed by dimensionality. Therefore, we propose a solution based on smart exploration of the search space for such cases. We conduct comprehensive experiments using real-world and synthetic datasets alongside the theoretical analysis. Our experiment results demonstrate the effectiveness of our pro- posed solutions in mining the unknown, under-represented, and under-performing minorities.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Mining%20the%20Minoria%3A%20Unknown%2C%20Under-represented%2C%20and%20Under-performing%20Minority%20Groups",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1453-dehghankar.pdf",
    "session": "Research 21: Specialized and Domain-Specific Data Management",
    "authors": [
      {
        "Name": "Mohsen Dehghankar",
        "Affiliation": "University of Illinois at Chicago"
      },
      {
        "Name": "Abolfazl Asudeh",
        "Affiliation": "University of Illinois Chicago"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "82d65ba9-3be0-4d15-b0db-0584b96f0679",
    "title": "Unify: A System For Unstructured Data Analytics",
    "abstract": "Unstructured data comprises over 80% of today‚Äôs information, yet no specialized system effectively supports its semantic analytics. Traditional SQL-based approaches rely on predefined schemas, making them unsuitable. While large language models (LLMs) enable semantic analysis of unstructured data, manually orchestrating execution plans remains inefficient. This raises a critical question: how can we automate unstructured data analytics?  In this demonstration, we present  Unify , a system that automates unstructured data analytics for natural language queries.  Unify  defines a set of core operators for unstructured data processing, with both preprogrammed and LLM-based implementations. It guides LLMs to decompose queries into logical steps and map them to appropriate operators for accurate execution. Our demonstration showcases Unify  by real-world scenarios, highlighting its ability to bridge the gap between unstructured data and actionable analytics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Unify%3A%20A%20System%20For%20Unstructured%20Data%20Analytics",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5287-wang.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Jiayi Wang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Yuan Li",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Jianming Wu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Shihui Xu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a9397830-b495-4edf-9f8b-aa26069679ad",
    "title": "Themis: A GPU-accelerated Relational Query Execution Engine",
    "abstract": "GPU-accelerated relational query execution engines have paral-GPU-accelerated relational query execution engines have parallelized the execution of a pipeline, a sequence of operators. For the parallelization, the engines evenly partition the tuples in a table that will be scanned by the pipeline‚Äôs first operator (a scan), and each thread executes the pipeline for the tuples in a partition. However, this approach leads to load imbalances since an operator returns a varying number of output tuples per input tuple, particularly under non-uniform data distributions such as skewed join key val-under non-uniform data distributions such as skewed join key values. The load imbalances are classified into  intra- and inter-warp load imbalances  (intra-WLIs and inter-WLIs) since 1) threads are grouped into warps and 2) every thread in a warp evaluates the same operator for an input tuple concurrently following a  single-same operator for an input tuple concurrently following a  singleinstruction-multiple-thread  manner. In contrast, threads in different warps can evaluate different operators concurrently. Although load balancing techniques have been proposed, however, they fail to solve the load imbalances on various workloads. In this paper, we propose a query execution engine, Themis, named after the deity of fairness, which symbolizes balanced workloads within our context. Themis minimizes intra-WLIs and inter-WLIs across various work-Themis minimizes intra-WLIs and inter-WLIs across various workloads. First, Themis minimizes intra-WLIs by redistributing tuples between the threads in a warp and making the threads evaluate an operator only when all of them hold inputs. Second, Themis mitigates the inter-WLIs by redistributing the tuples of warps with heavy workloads to idle warps. To check whether a warp‚Äôs work-heavy workloads to idle warps. To check whether a warp‚Äôs workload is heavy, we propose a method to approximate the sizes of warps‚Äô workloads. Based on these approximations, Themis adap-warps‚Äô workloads. Based on these approximations, Themis adaptively adjusts the threshold for determining a warp‚Äôs workload as heavy. In a recent benchmark JCC-H, which introduces skewed join key distributions to TPC-H, Themis significantly alleviates the inter-WLIs and intra-WLIs, outperforming the runner-up by up to 379x.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Themis%3A%20A%20GPU-accelerated%20Relational%20Query%20Execution%20Engine",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p426-han.pdf",
    "session": "Research 64: Database Engines and Architectures",
    "authors": [
      {
        "Name": "Kijae Hong",
        "Affiliation": "POSTECH"
      },
      {
        "Name": "Kyoungmin Kim",
        "Affiliation": "EPFL"
      },
      {
        "Name": "Young-Koo Lee",
        "Affiliation": "Kyung Hee University"
      },
      {
        "Name": "Yang-Sae Moon",
        "Affiliation": "\" Kangwon National University, Korea\""
      },
      {
        "Name": "Sourav S Bhowmick",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Wook-Shin Han",
        "Affiliation": "POSTECH"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "892828ac-6629-4169-95b0-676ff580747f",
    "title": "Enhancing Graph Edit Distance Computation: Stronger and Orientation-based ILP Formulations",
    "abstract": "The graph edit distance (GED) is among the most widely used graph similarity measures in practice. It asks for a minimum cost edit path between two given labeled graphs  ùê∫ and  ùêª , where the edit path is defined as a sequence of operations (e.g., node and edge insertions, deletions or substitutions) that successively transform the graph  ùê∫ into  ùêª . \nIn this work, we suggest a new ILP formulation (FORI) based on orienting the corresponding edge variables. Moreover, we suggest enhancing two state-of-the-art ILP formulations by incorporating additional inequalities. We theoretically compare the strength of the formulations with respect to their Linear Programming relaxations. The result is a hierarchy with (FORI) at the top. \nOur extensive evaluation on widely used benchmark sets shows that our improved formulations run significantly faster than the previous ones. These allow to solve to proven optimality all the reference instances from common databases, such as the IAM Graph Database, many of which were prohibitive with state-of-the-art methods. Moreover, we are able to compute the GED of a small pattern and a large graph such as CORA and PUBMED, having up to 19,717 nodes and 44,327 edges.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Enhancing%20Graph%20Edit%20Distance%20Computation%3A%20Stronger%20and%20Orientation-based%20ILP%20Formulations",
    "pdf_link": "",
    "session": "Research 41: Graph Data Management V",
    "authors": [
      {
        "Name": "Andrea D'Ascenzo",
        "Affiliation": "Luiss University"
      },
      {
        "Name": "Julian Meffert",
        "Affiliation": "University of Bonn"
      },
      {
        "Name": "Petra Mutzel",
        "Affiliation": "University of Bonn"
      },
      {
        "Name": "Fabrizio Rossi",
        "Affiliation": "University of L'Aquila"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Missing PDF link"
    ]
  },
  {
    "id": "5cfe117c-46c3-485f-8a47-9a72907c3f9e",
    "title": "The Case for DBMS Live Patching",
    "abstract": "Traditionally, when the code of a database management system (DBMS) needs to be updated, the system is restarted, and database clients suffer downtime, or the provider instantiates hot-standby instances and rolls over the workload. We investigate a third option, live patching of the DBMS binary. For certain code changes, live patching allows to modify the application code in memory, without restart. The memory state and all client connections can be maintained. Although live patching has been explored in the operating systems research community, it remains a blind spot in DBMS research. In this Experiment, Analysis & Benchmark article, we systematically explore this field from the DBMS perspective. We discuss what distinguishes database management systems from generic multi-threaded applications when it comes to live patching. We then propose domain-specific strategies for injecting quiescence points into the DBMS source code so that threads can safely migrate to the patched process version. We experimentally investigate the interplay between the query workload and different quiescence methods, monitoring both transaction throughput and tail latencies. We show that live patching can be a viable option for updating database management systems, since database providers can make informed decisions w.r.t. the latency overhead on the client side.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/The%20Case%20for%20DBMS%20Live%20Patching",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4557-fruth.pdf",
    "session": "Research 60: Database Engine Performance and Manageability II",
    "authors": [
      {
        "Name": "Michael Fruth",
        "Affiliation": "University of Passau"
      },
      {
        "Name": "Stefanie Scherzinger",
        "Affiliation": "University of Passau"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e2c54784-e6a9-406a-9545-3e205dea775e",
    "title": "Towards Practical Oblivious Map",
    "abstract": "Oblivious map (OMAP) is an important component in encrypted databases, utilized to prevent the server inferring sensitive infor-databases, utilized to prevent the server inferring sensitive information about client‚Äôs encrypted databases based on  access patterns . Despite its widespread usage and importance, existing OMAP so-Despite its widespread usage and importance, existing OMAP solutions face practical challenges, including the need for a large number of interaction rounds between the client and server, as well as substantial communication bandwidth. For example, the SOTA protocol OMIX++ in VLDB 2024 still requires  ùëÇ ( log ùëõ )  interaction rounds and  ùëÇ ( log 2   ùëõ )  communication bandwidth per access, where ùëõ denotes the total number of key-value pairs stored. In this work, we introduce more practical and efficient OMAP constructions. Consistent with all prior OMAPs, our constructions also adapt only the  tree-based Oblivious RAM (ORAM)  and  oblivious data structures (ODS) to achieve OMAP for enhanced practicality. In complexity, our approach needs ùëÇ ( log ùëõ / log log ùëõ )+ ùëÇ ( log  ùúÜ )  interaction rounds and  ùëÇ ( log 2   ùëõ / log log ùëõ ) + ùëÇ ( log  ùúÜ log ùëõ )  communication bandwidth per data access where  ùúÜ is the security parameter. This new com-per data access where  ùúÜ is the security parameter. This new complexity results from our two main contributions. First, unlike prior works relying solely on  search trees , we design a novel framework for OMAP that combines  hash table  with search trees. Second, we propose a more efficient tree-based ORAM named  DAORAM , which is of significant independent interest. This new ORAM accelerates our constructions as it supports obliviously accessing hash tables more efficiently. We implement both our proposed constructions and prior methods to experimentally demonstrate that our construc-and prior methods to experimentally demonstrate that our constructions substantially outperform prior methods in terms of efficiency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Practical%20Oblivious%20Map",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p688-liu.pdf",
    "session": "Research 50: Access Control and Privacy, Blockchain",
    "authors": [
      {
        "Name": "Xinle Cao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Weiqi Feng",
        "Affiliation": "University of Massachusetts Amherst"
      },
      {
        "Name": "Jian Liu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Jinjin Zhou",
        "Affiliation": "Ant group"
      },
      {
        "Name": "Wenjing Fang",
        "Affiliation": "Ant group"
      },
      {
        "Name": "Lei Wang",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Quanqing Xu",
        "Affiliation": "OceanBase, Ant Group"
      },
      {
        "Name": "Chuanhui Yang",
        "Affiliation": "OceanBase"
      },
      {
        "Name": "Kui Ren",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "678e0e44-f3db-4cc9-bd77-038899d6c59e",
    "title": "RankPQO: Learning-to-Rank for Parametric Query Optimization",
    "abstract": "Parametric Query Optimization (PQO) is crucial for efficiently han-Parametric Query Optimization (PQO) is crucial for efficiently handling parametrized queries (PQ) in many database applications. This paper addresses two key challenges in existing PQO techniques, focusing on plan set generation and best plan selection. Regarding plan set generation, existing methods rely on modifying sub-plan cardinalities, often resulting in inefficiency and sub-optimal perfor-cardinalities, often resulting in inefficiency and sub-optimal performance due to unclear extents of modifications needed. To overcome this issue, we propose a hybrid plan enumeration algorithm that adeptly adjusts both cardinality and join order. Regarding best plan selection, recent methods rely on machine learning models to choose plans with minimum predicted latency, but they struggle with accurate predictions when parameter bindings vary. Even mi-with accurate predictions when parameter bindings vary. Even minor variations in parameters can significantly impact cardinality, affecting plan optimality. To overcome this issue, we propose to utilize a learning-to-rank model, which uses relative rankings as a more reliable performance indicator. Our approach, integrated into PostgreSQL, undergoes extensive experiments on real datasets, showcasing significant improvements in both efficiency and ac-showcasing significant improvements in both efficiency and accuracy, as compared to baselines. Specifically, it accelerates the PostgreSQL optimizer by up to 2.57 √ó  and surpasses the best exist-PostgreSQL optimizer by up to 2.57 √ó  and surpasses the best existing baseline by up to 1.36 √ó .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RankPQO%3A%20Learning-to-Rank%20for%20Parametric%20Query%20Optimization",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p863-mo.pdf",
    "session": "Research 29: Learned Database Systems",
    "authors": [
      {
        "Name": "Songsong Mo",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Yue Zhao",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Zhifeng Bao",
        "Affiliation": "RMIT University"
      },
      {
        "Name": "Quanqing Xu",
        "Affiliation": "OceanBase, Ant Group"
      },
      {
        "Name": "Chuanhui Yang",
        "Affiliation": "OceanBase"
      },
      {
        "Name": "Gao Cong",
        "Affiliation": "Nanyang Technological Univesity"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "665ad9f5-ed15-4895-9e4f-6f6d60c646e1",
    "title": "T-Assess: An Efficient Data Quality Assessment System Tailored for Trajectory Data",
    "abstract": "With the widespread use of GPS-enabled devices and services, trajec-With the widespread use of GPS-enabled devices and services, trajectory data fuels services in a variety of fields, such as transportation and smart cities. However, trajectory data often contains errors stemming from inaccurate GPS measurements, low sampling rates, and transmission interruptions, yielding low-quality trajectory data with negative effects on downstream services. Therefore, a crucial yet tedious endeavor is to assess the quality of trajectory data, serv-yet tedious endeavor is to assess the quality of trajectory data, serving as a guide for subsequent data cleaning and analyses. Despite some studies addressing general-purpose data quality assessment, no studies exist that are tailored specifically for trajectory data. \nTo more effectively diagnose the quality of trajectory data, we propose  T-Assess , an automated trajectory data quality assessment system.  T-Assess  is built on three fundamental principles: i) exten-system.  T-Assess  is built on three fundamental principles: i) extensive coverage, ii) versatility, and iii) efficiency. To achieve compre-sive coverage, ii) versatility, and iii) efficiency. To achieve comprehensive coverage, we propose assessment criteria spanning validity, completeness, consistency, and fairness. To provide high versa-completeness, consistency, and fairness. To provide high versatility,  T-Assess  supports both offline and online evaluations for full-batch trajectory datasets as well as real-time trajectory streams. In addition, we incorporate an evaluation optimization strategy to achieve assessment efficiency. Extensive experiments on four real-life benchmark datasets offer insight into the effectiveness of  T-real-life benchmark datasets offer insight into the effectiveness of  TAssess  at quantifying trajectory data quality beyond the capabilities of state-of-the-art data quality systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/T-Assess%3A%20An%20Efficient%20Data%20Quality%20Assessment%20System%20Tailored%20for%20Trajectory%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p666-chen.pdf",
    "session": "Research 55: Spatial and Temporal Databases",
    "authors": [
      {
        "Name": "Junhao Zhu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Tao Wang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Danlei Hu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Ziquan Fang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lu Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Tianyi Li",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ece62dff-fff0-4563-be55-e8714d6ef2a6",
    "title": "Accelerating Tabular Inference: Training Data Generation with TENET",
    "abstract": "Tabular Natural Language Inference (TNLI) involves machine learning models that assess whether structured tabular data supports or contradicts a hypothesis formulated in natural language. TNLI models typically require large sets of training examples, which are costly to produce manually. In this demonstration, we present Tenet, a system for the automatic generation of training examples for TNLI applications. Existing TNLI training approaches either depend on costly human annotation or generate simplistic examples that lack data diversity and complex reasoning. In contrast, Tenet can start from a small set of manually annotated examples to automatically generate a large and diverse training dataset. Tenet is based on the idea that SQL queries are the right tool for obtaining rich and complex generated examples. To ensure data variety, evidence-queries extract cell values from tables based on diverse data patterns. Once the relevant data are identiÔ¨Åed, semantic queries deÔ¨Åne diÔ¨Äerent ways to interpret it using SQL clauses. These interpretations are then verbalized as text to create annotated examples for TNLI. This demonstration oÔ¨Äers an interactive experience where users will be able to select evidence from tabular data, inspect and reÔ¨Åne generated queries, and observe how Tenet transforms structured data into natural language hypotheses. By engaging with diÔ¨Äerent scenarios, users will see how Tenet enables the rapid creation of high-quality TNLI datasets, leading to inference models with performance comparable to those trained on manually crafted examples.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Accelerating%20Tabular%20Inference%3A%20Training%20Data%20Generation%20with%20TENET",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5303-veltri.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Enzo Veltri",
        "Affiliation": "Universit√† della Basilicata"
      },
      {
        "Name": "Donatello Santoro",
        "Affiliation": "Universit√† della Basilicata"
      },
      {
        "Name": "Jean-Flavien Bussotti",
        "Affiliation": "EURECOM"
      },
      {
        "Name": "Paolo Papotti",
        "Affiliation": "EURECOM"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "33f009fb-b945-4c8b-bf93-56b597ff8c5a",
    "title": "Concurrency Control as a Service",
    "abstract": "Existing disaggregated databases separate execution and storage layers, enabling independent and elastic scaling of resources. In most cases, this design makes transaction concurrency co ntrol (CC) a critical bottleneck, which demands significant computing resources for concurrent conflict management and struggles to scale due to the coordination overhead for concurrent conflict resolution. Coupling CC with execution or storage limits performance and elasticity, as CC‚Äôs resource needs do not align with the free scaling of the transaction execution layer or the storage-bound data layer. \nThis paper proposes Concurrency Control as a Service ( CCaaS ), which decouples CC from databases, building an execution-CCstorage three-layer decoupled database, allowing independent scaling and upgrades for improved elasticity, resource utilization, and development agility. However, adding a new layer increases latency due to the shift in communication from hardware to network. To address this, we propose a Sharded Multi-Write OCC (SM-OCC) algorithm with an asynchronous log push-down mechanism to minimize network communications overhead and transaction latency. Additionally, we implement a multi-write architecture with a deterministic conflict resolution method to reduce coordination overhead in the CC layer, thereby improving scalability.  CCaaS  is designed to be connected by a variety of execution and storage engines. Existing disaggregated databases can be revolutionized with  CCaaS to achieve high elasticity, scalability, and high performance. Results show that  CCaaS  achieves 1.02-3.11 √ó  higher throughput and 1.11-2.75 √ó  lower latency than SoTA disaggregated databases.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Concurrency%20Control%20as%20a%20Service",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2761-zhou.pdf",
    "session": "Research 26: Distributed Transactions I",
    "authors": [
      {
        "Name": "Weixing Zhou",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Yanfeng Zhang",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Xinji Zhou",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Zhiyou Wang",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Zeshun Peng",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Yang Ren",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Sihao Li",
        "Affiliation": "Huawei Tech. Co Ltd"
      },
      {
        "Name": "Huanchen Zhang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Ge Yu",
        "Affiliation": "Northeastern University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1407c892-2b80-41eb-831c-2dfdd43c0937",
    "title": "Holistic query Approximation via RL Modeling",
    "abstract": "In data exploration, executing queries over a large database can be time-consuming. Previous work has proposed approximate query processing as a way to speed up aggregate queries in this context, but do not address non-aggregate queries. Our paper introduces a novel holistic approach to handle both types of queries by finding an optimized subset of data, referred to as an  approximation set . The goal is to maximize query result quality while using a smaller set of data, thereby significantly reducing the query execution time. We formalize this problem as Holistic Approximate Query Processing and establish its NP-completeness. To tackle this, we propose an approximate solution using  Reinforcement Learning , termed  HARLM . While  HARLM  does not provide theoretical guarantees due to its reliance on Reinforcement Learning, it effectively overcomes challenges related to the large action space and the need for generalization beyond a known query workload. Experimental results on both non-aggregate and aggregate benchmarks show that  HARLM  significantly outperforms the baselines both in terms of accuracy (30% improvement) and efficiency (10-35X).",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Holistic%20query%20Approximation%20via%20RL%20Modeling",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1635-razmadze.pdf",
    "session": "Research 17: Applied ML and AI for Data Management II",
    "authors": [
      {
        "Name": "Susan Davidson",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Tova Milo",
        "Affiliation": "Tel Aviv University"
      },
      {
        "Name": "Kathy Razmadze",
        "Affiliation": "Tel Aviv University"
      },
      {
        "Name": "Gal Zeevi",
        "Affiliation": "Tel Aviv University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8d2fd7ee-bc22-4b31-9536-a215f6a532fa",
    "title": "Cache Coherence Over Disaggregated Memory",
    "abstract": "Disaggregating memory from compute oÔ¨Äers the opportunity to better utilize stranded memory in cloud data centers. It is important to cache data in the compute nodes and maintain cache coherence across multiple compute nodes. However, the limited computing power on disaggregated memory servers makes traditional cache coherence protocols suboptimal, particularly in the case of stranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache Coherence protocol that maintains cache coherence without imposing any computational burden on the remote memory side. It aligns the state machine of the shared-exclusive latch protocol with the MSI protocol, thereby ensuring both atomicity of data access and cache coherence with sequential consistency. SELCC embeds cacheownership metadata directly into the RDMA latch word, enabling eÔ¨Écient cache ownership management via RDMA atomic operations. SELCC can serve as an abstraction layer over disaggregated memory with APIs that resemble main-memory accesses. A concurrent B-tree and three transaction concurrency control algorithms are realized using SELCC‚Äôs abstraction layer. Experimental results show that SELCC signiÔ¨Åcantly outperforms RPC-based protocols for cache coherence under limited remote computing power. Applications on SELCC achieve comparable or superior performance over disaggregated memory compared to competitors.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cache%20Coherence%20Over%20Disaggregated%20Memory",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2978-wang.pdf",
    "session": "Research 1: Cloud Data Management",
    "authors": [
      {
        "Name": "Ruihong Wang",
        "Affiliation": "Purdue University"
      },
      {
        "Name": "Jianguo Wang",
        "Affiliation": "Purdue University"
      },
      {
        "Name": "Walid Aref",
        "Affiliation": "Purdue University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7217f61e-2ada-4798-ac6e-f9ca4482b83f",
    "title": "Dynamic Graph Databases with Out-of-order Updates",
    "abstract": "Several real-time applications rely on dynamic graphs to model and store data arriving from multiple streams. Providing both high ingestion rate and efficient analytics with transactional guarantees is challenging, even more so when updates may be received out-of-order at the database. In this work, we propose HAL, a novel in-memory dynamic graph database design, addressing these challenges. HAL outperforms comparable systems by a factor of up to 73√ó in terms of update processing throughput and up to 357√ó for analytics, while being the first to support out-of-order updates.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Dynamic%20Graph%20Databases%20with%20Out-of-order%20Updates",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4799-khan.pdf",
    "session": "Research 57: Graph Data Management VII",
    "authors": [
      {
        "Name": "MUHAMMAD GHUFRAN KHAN",
        "Affiliation": "INRIA"
      },
      {
        "Name": "Ioana Manolescu",
        "Affiliation": "Inria and Institut Polytechnique de Paris"
      },
      {
        "Name": "Angelos Christos Anadiotis",
        "Affiliation": "Oracle"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "55a26976-fd28-4e95-a8ab-81927dbcd5c8",
    "title": "CatDB: Data-catalog-guided, LLM-based Generation of Data-centric ML Pipelines",
    "abstract": "Data-centric machine learning (ML) pipelines extend traditional ML pipelines‚Äîof feature transformations, hyper-parameter tuning, and model training‚Äîby additional pre-processing steps for data cleaning, data augmentation, and feature engineering to create high-quality data with good coverage. Finding eÔ¨Äective data-centric ML pipelines is still a labor- and compute-intensive process though. While AutoML tools use eÔ¨Äective search strategies, they struggle to scale with large datasets. Large language models (LLMs) show promise for code generation but face challenges in generating datacentric ML pipelines due to private datasets not seen during training, complex pre-processing requirements, and the need for mitigating hallucinations. These demands exceed typical code generation as it requires actions tailored to the characteristics and requirements of a particular dataset. This paper introduces CatDB, a comprehensive, LLM-based system for generating eÔ¨Äective, error-free, and eÔ¨Écient data-centric ML pipelines. CatDB leverages data catalog information and reÔ¨Åned metadata to dynamically create datasetspeciÔ¨Åc rules (instructions) to guide the LLM. Moreover, CatDB includes a robust mechanism for automatic validation and error handling of the generated pipeline. Our experimental results show that CatDB reliably generates eÔ¨Äective ML pipelines across diverse datasets, achieving accuracy comparable to or better than existing LLM-based systems, standalone AutoML tools, and combined workÔ¨Çows of data cleaning and AutoML tools, while delivering up to orders of magnitude faster performance on large datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/CatDB%3A%20Data-catalog-guided%2C%20LLM-based%20Generation%20of%20Data-centric%20ML%20Pipelines",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2639-fathollahzadeh.pdf",
    "session": "Research 32: Data-centric Machine Learning",
    "authors": [
      {
        "Name": "Saeed Fathollahzadeh",
        "Affiliation": "Concordia University"
      },
      {
        "Name": "Essam Mansour",
        "Affiliation": "Concordia University"
      },
      {
        "Name": "Matthias Boehm",
        "Affiliation": "Technische Universit√§t Berlin"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "825a1517-fce9-4f2a-be76-ca3db8100584",
    "title": "RCRank: Multimodal Ranking of Root Causes of Slow Queries in Cloud Database Systems",
    "abstract": "With the continued migration of storage to cloud database systems, the impact of slow queries in such systems on services and user experience is increasing. Root-cause diagnosis plays an indispensable role in facilitating slow-query detection and revision. This paper proposes a method capable of both identifying possible root cause types for slow queries and ranking these according to their potential for accelerating slow queries. This enables prioritizing root causes with the highest impact, in turn improving slow-query revision effectiveness. To enable more accurate and detailed diagnoses, we propose the multimodal Ranking for the Root Causes of slow queries ( RCRank ) framework, which formulates root cause analysis as a multimodal machine learning problem and leverages multimodal information from query statements, execution plans, execution logs, and key performance indicators. To obtain expressive embeddings from its heterogeneous multimodal input,  RCRank integrates self-supervised pre-training that enhances cross-modal alignment and task relevance. Next, the framework integrates rootcause-adaptive cross Transformers that enable adaptive fusion of multimodal features with varying characteristics. Finally, the framework offers a unified model that features an impact-aware training objective for identifying and ranking root causes. We report on experiments on real and synthetic datasets, finding that  RCRank  is capable of consistently outperforming the state-of-the-art methods at root cause identification and ranking according to a range of metrics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RCRank%3A%20Multimodal%20Ranking%20of%20Root%20Causes%20of%20Slow%20Queries%20in%20Cloud%20Database%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1169-ouyang.pdf",
    "session": "Research 59: Distributed and Streaming Data Processing",
    "authors": [
      {
        "Name": "Biao Ouyang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Yingying Zhang",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Cheng Hanyin",
        "Affiliation": "HNU"
      },
      {
        "Name": "Yang Shu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Chenjuan Guo",
        "Affiliation": "ECNU"
      },
      {
        "Name": "Bin Yang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Qingsong Wen",
        "Affiliation": "Alibaba Group U.S."
      },
      {
        "Name": "Lunting Fan",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "58ba8729-66f1-415e-9a70-8fe8a5d5b4e6",
    "title": "Inference-friendly Graph Compression for Graph Neural Networks",
    "abstract": "Graph Neural Networks ( GNNs ) have demonstrated promising performance in graph analysis. Nevertheless, the inference process of GNNs  remains costly, hindering their applications for large graphs. This paper proposes  inference-friendly graph compression  ( IFGC ), a graph compression scheme to accelerate  GNNs  inference. Given a graph  ùê∫ and a  GNN  ùëÄ , an  IFGC  computes a small compressed graph  ùê∫ ùëê , to best preserve the inference results of  ùëÄ over  ùê∫ , such that the result can be directly inferred by accessing  ùê∫ ùëê with no or little decompression cost. (1) We characterize  IFGC  with a class of inference equivalence relation. The relation captures the node pairs in  ùê∫ that are not distinguishable for  GNN  inference. (2) We introduce three practical specifications of  IFGC  for representative  GNNs : structural preserving compression ( SPGC ), which computes ùê∫ ùëê that can be directly processed by  GNN  inference without decompression;  ( ùõº,ùëü ) -compression, that allows for a configurable trade-off between compression ratio and inference quality, and anchored compression that preserves inference results for specific nodes of interest. For each scheme, we introduce compression and inference algorithms with guarantees of efficiency and quality of the inferred results. We conduct extensive experiments on diverse sets of largescale graphs, which verifies the effectiveness and efficiency of our graph compression approaches.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Inference-friendly%20Graph%20Compression%20for%20Graph%20Neural%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3203-fan.pdf",
    "session": "Research 45: Graph Data Learning",
    "authors": [
      {
        "Name": "Yangxin Fan",
        "Affiliation": "Case Western Reserve University"
      },
      {
        "Name": "Haolai Che",
        "Affiliation": "Case Western Reserve University"
      },
      {
        "Name": "Yinghui Wu",
        "Affiliation": "Case Western Reserve University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "354dc972-d3e1-47ee-bc5c-113e0035681b",
    "title": "Sphinx: A Succinct Perfect Hash Index for x86",
    "abstract": "Many modern key-value stores rely on an in-memory index to map the location of each data entry in storage. The size of this index often becomes a memory bottleneck that makes it difficult to scale the system to large data sizes. To address this problem, the stateof-the-art approach is to structure this index as a succinct perfect hash table using only  ‚âà 4 bits per key. The downside is that the hash table encoding is computationally expensive to parse and may harm overall system performance. \nWe introduce Sphinx, a succinct perfect hash table reengineered for high performance on commodity CPUs. Sphinx is encoded in a manner that lends itself to efficient access using rank and select primitives, and it uses auxiliary metadata to decode common hash table slots instantaneously. Sphinx is also expandable and parallelizable. We compare Sphinx to the best alternatives and show that it leads to a 2x reduction in query latency, update latency, and memory footprint.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Sphinx%3A%20A%20Succinct%20Perfect%20Hash%20Index%20for%20x86",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4424-maghrebi.pdf",
    "session": "Research 15: Views, Indexing, and Search I",
    "authors": [
      {
        "Name": "Sajad Faghfoor Maghrebi",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Niv Dayan",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "733cf8ed-d8f0-4706-82c9-ae4a2680e0a1",
    "title": "Mining Platoon Patterns from Traffic Videos",
    "abstract": "Discovering co-movement patterns from urban-scale video data sources has emerged as an attractive topic. This task aims to identify groups of objects that travel together along a common route, which offers effective support for government agencies in enhancing smart city management. However, the previous work has made a strong assumption on the accuracy of recovered trajectories from videos and their co-movement pattern definition requires the group of objects to appear across consecutive cameras along the common route. In practice, this often leads to missing patterns if a vehicle is not correctly identified from a certain camera due to object occlusion or vehicle mis-matching. \nTo address this challenge, we propose a relaxed definition of co-movement patterns from video data, which removes the consecutiveness requirement in the common route and accommodates a certain number of missing captured cameras for objects within the group. Moreover, a novel enumeration framework called MaxGrowth is developed to efficiently retrieve the relaxed patterns. Unlike previous filter-and-refine frameworks comprising both candidate enumeration and subsequent candidate verification procedures, MaxGrowth incurs no verification cost for the candidate patterns. It treats the co-movement pattern as an equivalent sequence of clusters, enumerating candidates with increasing sequence length while avoiding the generation of any false positives. Additionally, we also propose two effective pruning rules to efficiently filter the non-maximal patterns. Extensive experiments are conducted to validate the efficiency of MaxGrowth and the quality of its generated co-movement patterns. Our MaxGrowth runs up to two orders of magnitude faster than the baseline algorithm. It also demonstrates high accuracy in real video dataset when the trajectory recovery algorithm is not perfect.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Mining%20Platoon%20Patterns%20from%20Traffic%20Videos",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1839-bei.pdf",
    "session": "Research 39: Analytics over Different Data Types II",
    "authors": [
      {
        "Name": "Yijun Bei",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Teng Ma",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Dongxiang Zhang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Sai Wu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Kian-Lee Tan",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e55f9d40-a365-46f2-9803-3898b0fae6f6",
    "title": "Query running too slow? Rewrite it with Quorion!",
    "abstract": "We will demonstrate Quorion, a query rewriter with theoretical guarantees and better practical performance. Quorion adopts some of the recently developed query planning methods that provide optimality guarantees, including Yannakakis + , an optimized version of the Yannakakis algorithm, generalized hypertree decompositions (GHD), GYO reduction, and cost-based optimization. Quorion also provides a platform for users to explore different query plans for a given query through a web-based interface and compare their performance with classical query plans. Quorion currently supports DuckDB, MySQL, and PostgreSQL, and can be connected to any user-provided database easily through a JDBC connector. KEYWORDS conjunctive query, query rewrite, compilation, visualization",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Query%20running%20too%20slow%EF%BC%9F%20Rewrite%20it%20with%20Quorion!",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5243-wang.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Bingnan Chen",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Binyang Dai",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Qichen Wang",
        "Affiliation": "EPFL"
      },
      {
        "Name": "Ke Yi",
        "Affiliation": "Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0311631e-ce79-401e-a1ff-5a499e446f73",
    "title": "Freely Moving Between the OLTP and OLAP Worlds: Hermes ‚Äì an High-Performance OLAP Accelerator for MySQL",
    "abstract": "Users often want to run analytics on their OLTP databases, to avoid costly and cumbersome Extract-Transform-Load (ETL) processes. Typically, analytical queries run rather slow on OLTP DBMS, making Hybrid Transaction/Analytic Processing (HTAP) solutions popular. One possible solution is to add an accelerator (for analytics) to the already existing OLTP DBMS. \nTypically, analytical systems, especially for the cloud, focus on extremely large datasets (\"exa-scale\") and distributed query execution (across multiple machines). We argue that many customers do not have large enough datasets to justify expensive multi-node DBMSs. Compared to single-node systems, such multi-node systems typically come with a baseline drop in performance (but might scale), as they need to introduce data transfers across the network. \nFor this reason, we propose Hermes as cloud-native, but singlenode, accelerator for MySQL. Hermes speeds up analytical queries by, often 2-3, orders of magnitude and outperforms competing systems by up to 5 √ó  (including multi-node systems). We achieve this by keeping Hermes relatively lean and focusing on the core features required. In the paper we describe Hermes‚Äô architecture, data storage and integration with MySQL as well as Hermes‚Äô query engine. Importantly, Hermes provides the highest degree of data freshness. If data is not replicated yet, Hermes waits. The waiting times, however, are practically negligible (single digit vs. three digit milliseconds). \nWe evaluate Hermes on TPC-H as well as micro-benchmarks. Besides the aforementioned improvements, our replication mechanisms achieved high and stable throughput rates of up to 60k changes per second, leading to low waiting times. \nIn summary, Hermes is a lean accelerator for MySQL. Its singlenode design keeps costs for users low and performance high. Additionally, Hermes guaranteeing data freshness and compatibility with MySQL (both, Hermes and MySQL, return the same result).",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Freely%20Moving%20Between%20the%20OLTP%20and%20OLAP%20Worlds%3A%20Hermes%20%E2%80%93%20an%20High-Performance%20OLAP%20Accelerator%20for%20MySQL",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5113-gubner.pdf",
    "session": "Industry 2: Data Platforms for Analytics",
    "authors": [
      {
        "Name": "Tim Gubner",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Rune Humborstad",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Manyi Lu",
        "Affiliation": "Huawei"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1530d74d-e211-412b-a205-6195ac372e2e",
    "title": "FLEET: High-Performance Durable Replicated State Machines using Scattered and Coordinated Log Entries",
    "abstract": "Distributed coordination services are fundamental components of distributed systems, employing durable replicated state machines (RSMs) to ensure consistency across replicas and prevent data loss, even in the event of all nodes failing. These services typically rely on persistent logs for rapid recovery, as a universally agreed-upon log allows replicas to restore their state by sequentially replaying ordered log entries. However, the requirement for a totally ordered log inherently limits opportunities for parallelism.\nThis paper introduces Fleet, a high-performance durable RSM protocol that combines a hybrid scattered-entry log with an asyn- chronous ordered log. Our approach integrates synchronous persis- tence of scattered entries with asynchronous persistence of ordered entries, ensuring both rapid recovery and high levels of parallelism. Additionally, we propose a parallel applying optimization for the etcd database, named pre-apply. Experimental results demonstrate that Fleet significantly outperforms Raft and Scalog in terms of throughput and latency, achieving up to 10√ó the throughput under specific configurations and scaling effectively across multiple nodes. Additionally, with the pre-apply optimization, Fleet delivers a 10- fold increase in throughput compared to sequential applying on etcd. Although Fleet incurs a 5% overhead in recovery time during leader failure, this delay is tolerable given the rarity of such events.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/FLEET%3A%20High-Performance%20Durable%20Replicated%20State%20Machines%20using%20Scattered%20and%20Coordinated%20Log%20Entries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1522-fan.pdf",
    "session": "Research 26: Distributed Transactions I",
    "authors": [
      {
        "Name": "Hua Fan",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Hao Tan",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Wenchao Zhou",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f4dbaa95-f0bb-42c1-af2c-fdf15c17c9bb",
    "title": "PBench: Workload Synthesizer with Real Statistics for Cloud Analytics Benchmarking",
    "abstract": "Cloud service providers commonly use standard benchmarks like TPC-H and TPC-DS to evaluate and optimize cloud data analytics systems. However, these benchmarks rely on !xed query patterns and fail to capture real execution statistics of production cloud workloads. Although some cloud database vendors have recently released real workload traces, these traces alone do not qualify as benchmarks, as they typically lack essential components (i.e., queries and databases). To overcome this limitation, this paper studies a new problem of  workload synthesis with real statistics , which generates  synthetic workloads  that closely approximate real execution statistics, including key performance metrics and operator distributions. To address this problem, we propose PBench, a novel workload synthesizer that constructs synthetic workloads by (1) selecting and combining workload components from existing benchmarks and (2) augmenting new workload components. This paper studies the key challenges in PBench. First, we address the challenge of balancing performance metrics and operator distributions by introducing a multi-objective optimization-based component selection method. Second, to capture the temporal dynamics of real workloads, we design a timestamp assignment method that progressively re!nes workload timestamps. Third, to handle the disparity between the original workload and the candidate workload, we propose a component augmentation approach that leverages large language models (LLMs) to generate additional workload components while maintaining statistical !delity. Experimental results show that PBench reduces approximation error by up to 6 ¬≥ compared to state-of-the-art methods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PBench%3A%20Workload%20Synthesizer%20with%20Real%20Statistics%20for%20Cloud%20Analytics%20Benchmarking",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3883-fan.pdf",
    "session": "Research 60: Database Engine Performance and Manageability II",
    "authors": [
      {
        "Name": "Yan Zhou",
        "Affiliation": "Renmin University, China"
      },
      {
        "Name": "Chunwei Liu",
        "Affiliation": "MIT CSAIL"
      },
      {
        "Name": "Bhuvan Urgaonkar",
        "Affiliation": "Penn State and AWS"
      },
      {
        "Name": "Zhengle Wang",
        "Affiliation": "Renmin University, China"
      },
      {
        "Name": "Magnus Mueller",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Chao Zhang",
        "Affiliation": "Renmin University, China"
      },
      {
        "Name": "Songyue Zhang",
        "Affiliation": "Renmin University, China"
      },
      {
        "Name": "Pascal Pfeil",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Dominik Horn",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Zhengchun Liu",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Davide Pagano",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Tim Kraska",
        "Affiliation": "MIT and AWS"
      },
      {
        "Name": "Samuel Madden",
        "Affiliation": "MIT CSAIL"
      },
      {
        "Name": "Ju Fan",
        "Affiliation": "Renmin University, China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "22daaea6-a077-4e68-bcf2-434e816fcc0d",
    "title": "Extensible and Robust Evaluation of Similarity Queries",
    "abstract": "We study the similarity join problem from a systems perspective. A similarity join retrieves all similar record pairs from two collections based on a given distance function. Existing solutions are often optimized for a single distance function and domain. Such monolithic solutions are limited in both their extensibility to new distance functions and their robustness against changing data characteristics. \nTo address these challenges, we introduce Fast, a similarity join algorithm designed for extensible and robust query evaluation. It leverages a novel abstraction called reductions, which transform similarity join problems from complex domains into simpler ones. A reduction graph is constructed to systematically enumerate query plans. Since cost models for similarity queries are typically unavailable, Fast employs runtime partitioning and a sampling-based strategytoselectanear-optimalqueryplanwithperformanceguarantees. It can utilize prebuilt indexes or build them on-the-fly, incorporating caching techniques to accelerate index construction and probing. Extensive experiments across diverse datasets, domains, and distance functions show that Fast consistently performs close to the optimal plan. Finally, two case studies highlight its strength as a baseline and its utility for prototyping future similarity join algorithms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Extensible%20and%20Robust%20Evaluation%20of%20Similarity%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3868-schmitt.pdf",
    "session": "Research 22: Views, Indexing, and Search II",
    "authors": [
      {
        "Name": "Daniel Schmitt",
        "Affiliation": "University of Salzburg"
      },
      {
        "Name": "Thomas H√ºtter",
        "Affiliation": "Software Competence Center Hagenberg"
      },
      {
        "Name": "Nikolaus Augsten",
        "Affiliation": "University of Salzburg"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e28bf426-3fa0-4995-9ec0-c9accf183c64",
    "title": "Goku: A Schemaless Time Series Database for Large Scale Monitoring at Pinterest",
    "abstract": "Engineers rely heavily on observability tools to monitor their busi-Engineers rely heavily on observability tools to monitor their business and system metrics and set up alerting on it. A reliable and efficient monitoring system is very important for development ve-efficient monitoring system is very important for development velocity. In this paper, we introduce Goku, a time series database (TSDB) we built from the ground up at Pinterest. Over the years, we have studied user patterns and common requests to constantly evolve Goku to store and serve the use cases at Pinterest with high efficiency and reduced costs. At its core, Goku uses tiered storage to store new and frequently queried metrics data in memory while leveraging solid state drive (SSD) and hard disks (HDD) for older data. Goku aggregates metrics data at write time while also rolling up datapoints with lower time granularity to low latency to certain use cases. Goku also supports modifying configurations on metrics data like time to live (TTL), rollup granularity, backfilling capability, etc. Using multiple replicas and AWS S3 as backup, Goku is highly available and fault tolerant.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Goku%3A%20A%20Schemaless%20Time%20Series%20Database%20for%20Large%20Scale%20Monitoring%20at%20Pinterest",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p503-sanghavi.pdf",
    "session": "Research 40: Time Series and Vector Data",
    "authors": [
      {
        "Name": "Monil Mukesh Sanghavi",
        "Affiliation": "Pinterest Inc."
      },
      {
        "Name": "Ming-May Hu",
        "Affiliation": "Pinterest"
      },
      {
        "Name": "Zhenxiao Luo",
        "Affiliation": "Pinterest"
      },
      {
        "Name": "Xiao Li",
        "Affiliation": "Pinterest"
      },
      {
        "Name": "Kapil Bajaj",
        "Affiliation": "Pinterest"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a63252b2-8ada-4778-bdf2-d5d5c84fe1d1",
    "title": "Graph Neural Network Training Systems: A Performance Comparison of Full-Graph and Mini-Batch",
    "abstract": "Graph Neural Networks (GNNs) have gained signiÔøøcant attention in recent years due to their ability to learn representations of graphstructured data. Two common methods for training GNNs are minibatch training and full-graph training. Since these two methods require diÔøøerent training pipelines and systems optimizations, two separate classes of GNN training systems emerged, each tailored for one method. Works that introduce systems belonging to a particular category predominantly compare them with other systems within the same category, oÔøøering limited or no comparison with systems from the other category. Some prior work also justiÔøøes its focus on one speciÔøøc training method by arguing that it achieves higher accuracy than the alternative. The literature, however, has incomplete and contradictory evidence in this regard. \nIn this paper, we provide a comprehensive empirical comparison of representative full-graph and mini-batch GNN training systems. We Ôøønd that the mini-batch training systems consistently converge faster than the full-graph training ones across multiple datasets, GNN models, and system conÔøøgurations. We also Ôøønd that minibatch training techniques converge to similar to or often higher accuracy values than full-graph training ones, showing that minibatch sampling is not necessarily detrimental to accuracy. Our work highlights the importance of comparing systems across diÔøøerent classes, using time-to-accuracy rather than epoch time for performance comparison, and selecting appropriate hyperparameters for each training method separately.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Graph%20Neural%20Network%20Training%20Systems%3A%20A%20Performance%20Comparison%20of%20Full-Graph%20and%20Mini-Batch.",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1196-bajaj.pdf",
    "session": "Research 37: Applied ML and AI for Graph, Temporal, and Trajectory Data",
    "authors": [
      {
        "Name": "Saurabh Bajaj",
        "Affiliation": "University of Massachusetts, Amherst"
      },
      {
        "Name": "Hui Guan",
        "Affiliation": "University of Massachusetts, Amherst"
      },
      {
        "Name": "Marco Serafini",
        "Affiliation": "University of Massachusetts Amherst"
      },
      {
        "Name": "Juelin Liu",
        "Affiliation": "University of Massachusetts Amherst"
      },
      {
        "Name": "Hojae Son",
        "Affiliation": "UMass Amherst"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8ffb7222-7e02-423b-8532-9de52e07b779",
    "title": "Saving Private Hash Join",
    "abstract": "Modern analytical database systems offer high-performance inmemory joins. However, if the build side of a join does not fit in RAM, performance degrades sharply due to switching to traditional external join algorithms such as sort-merge. In streaming query execution, this problem is worsened if multiple joins are evaluated simultaneously, as the database system must decide how to allocate memory to each join, which can greatly affect performance. \nWe revisit larger-than-memory join processing on modern hardware, aiming for robust performance that avoids a ‚Äúperformance cliff‚Äù when memory runs out, even in query plans with many joins. \nTo achieve this, we propose three techniques. First, an adaptive, external hash join algorithm that stores temporary data in a unified buffer pool that oversees temporary and persistent data. Second, an optimizer that creates expressions to compress columns at runtime, reducing the size of materialized temporary data. Third, a strategy for dynamically managing the memory of concurrent operators during query execution to reduce spilling. \nWe integrate these techniques into DuckDB and experimentally show that when processing memory-intensive join query plans, our implementation gracefully degrades performance as the space requirement exceeds the memory limit. This greatly increases the size of datasets that can be processed on economical hardware.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Saving%20Private%20Hash%20Join",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2748-kuiper.pdf",
    "session": "Research 54: Database Engines II",
    "authors": [
      {
        "Name": "Laurens Kuiper",
        "Affiliation": "CWI"
      },
      {
        "Name": "Paul Gross",
        "Affiliation": "Centrum Wiskunde & Informatica"
      },
      {
        "Name": "Peter Boncz",
        "Affiliation": "CWI"
      },
      {
        "Name": "Hannes M√ºhleisen",
        "Affiliation": "CWI"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "65358a9d-e753-4d0b-b61b-77a37a86066e",
    "title": "JUSTINE (JUST-INsert Engine): Demonstrating Self-organizing Data Schemas",
    "abstract": "Relational databases are great for data analysis and exploration, but require a carefully crafted schema, which causes high manual overhead. Moreover, entities not considered during schema design cannot be stored. In contrast, schemaless approaches allow users to store all kinds of data without the need for a schema, but require schema-checking on read to ensure that queries can read certain attributes. We therefore advocate for a new class of database systems that organize the data in a schema autonomously when it is inserted schemalessly by users. Such databases should thus be able to store data semantically meaningful but without requiring the user to design a schema, neither upfront during setup nor when an insert is executed. In this demo, we showcase JUSTINE, which is a first implementation of this new class of database systems that can automatically adjust a database schema based on input queries. Our showcase features both (1) an interactive mode where attendees can enter their own data as well as (2) the execution of a full workload where users can see how the database schema evolves during batch execution. The workload can be customized by changing different parameters.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/JUSTINE%20(JUST-INsert%20Engine)%3A%20Demonstrating%20Self-organizing%20Data%20Schemas",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5283-hattasch.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Benjamin H√§ttasch",
        "Affiliation": "DFKI, Technical University of Darmstadt"
      },
      {
        "Name": "Leon Kr√ºger",
        "Affiliation": "Technical University of Darmstadt"
      },
      {
        "Name": "Carsten Binnig",
        "Affiliation": "Technical University of Darmstadt, DFKI"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "584678e6-1ee7-4020-93e4-2e4dfcf69960",
    "title": "Optimal Sharding for Scalable Blockchains with Deconstructed SMR",
    "abstract": "Sharding enhances blockchain scalability by dividing nodes into multiple shards to handle transactions in parallel. However, a sizesecurity dilemma where every shard must be large enough to ensure its security constrains the efficacy of individual shards and the degree of sharding. Most existing solutions therefore rely on either weakening the adversary or making stronger network assumptions. \nThis paper presents Arete, an optimally scalable blockchain sharding protocol designed to resolve the dilemma based on an observation that if individual shards can tolerate a higher fraction of Byzantine faults, we can securely create smaller shards in a larger quantity. The key idea of Arete, therefore, is to improve the security resilience of shards by dividing the blockchain‚Äôs State Machine Replication (SMR) process. Like modern blockchains, Arete first decouples SMR in three steps: transaction dissemination, ordering, and execution. However, for Arete, a single ordering shard performs the ordering task while multiple processing shards perform the dissemination and execution of blocks. As processing shards do not run consensus, each of those tolerates up to half compromised nodes. Moreover, the SMR process in the ordering shard is extremely lightweight as it only operates on the block digests. Second, Arete considers safety and liveness against Byzantine failures separately to improve the safety threshold further while tolerating temporary liveness violations in a controlled manner. Apart from creating more optimal-size shards, such a deconstructed SMR scheme empowers us to devise a novel certify-order-execute architecture to fully parallelize transaction handling, thereby significantly improving the performance. We implement Arete and evaluate it on the AWS environment by running up to 500 nodes. Our results demonstrate that Arete outperforms representative sharding protocols in scalability, throughput, and cross-shard latency without compromising on intra-shard latency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Optimal%20Sharding%20for%20Scalable%20Blockchains%20with%20Deconstructed%20SMR",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2198-zhang.pdf",
    "session": "Research 50: Access Control and Privacy, Blockchain",
    "authors": [
      {
        "Name": "Jianting Zhang",
        "Affiliation": "Purdue University"
      },
      {
        "Name": "Zhongtang Luo",
        "Affiliation": "Purdue University"
      },
      {
        "Name": "Raghavendra Ramesh",
        "Affiliation": "SupraOracles"
      },
      {
        "Name": "Aniket Kate",
        "Affiliation": "Purdue University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3f35afcc-16f7-479b-83f4-6247daadb225",
    "title": "QOVIS: Understanding and Diagnosing Query Optimizer via a Visualization-assisted Approach",
    "abstract": "Understanding and diagnosing query optimizers is crucial to guarantee the correctness and efficiency of query processing in database systems. However, achieving this is non-trivial as there are three technical challenges: (i) hundreds and thousands of query plans are generated for each query during the query optimization procedure; (ii) the transformation logic among query plans is not easy to investigate even for expert database system developers; and (iii) navigating users to the root causes of the bugs/errors is inherently hard as the changes of the operators among query plans are missing in the query processing log. In this work, we propose QOVIS  to overcome these challenges, which identifies the query optimization bugs/issues and investigates their root causes via a visualization-assisted approach. Specifically,  QOVIS  consists of data preprocessing layer, transformation logic computation layer, and visual analysis layer. We conduct extensive experimental studies (e.g., user study, case study, and performance study) to evaluate the efficiency and effectiveness of  QOVIS . In particular, our user study (on 24 database developers and researchers) confirms that  QOVIS significantly reduces the time required to investigate the bugs/errors in the query optimizer. Moreover, the generality of  QOVIS  is verified by utilizing it to understand and diagnose the real-world reported bugs/errors in different query optimizers of three widelyused systems: Apache Spark, Apache Hive, and DuckDB.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/QOVIS%3A%20Understanding%20and%20Diagnosing%20Query%20Optimizer%20via%20a%20Visualization-assisted%20Approach%20(Revision)",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1677-tang.pdf",
    "session": "Research 16: Query Processing and Optimization II",
    "authors": [
      {
        "Name": "Zhengxin You",
        "Affiliation": "Southern University of Science and Technology"
      },
      {
        "Name": "Qiaomu Shen",
        "Affiliation": "Southern University of Science and Technology"
      },
      {
        "Name": "Man Lung Yiu",
        "Affiliation": "Hong Kong Polytechnic University"
      },
      {
        "Name": "Bo Tang",
        "Affiliation": "Southern University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "66d9b778-bc9c-49c4-a873-1f2e5e86388b",
    "title": "Turbocharging Vector Databases using Modern SSDs",
    "abstract": "Efficient and scalable vector search is critical for modern AI applications, particularly in retrieval-augmented generation (RAG) and large-scale semantic search. However, disk-based vector databases often suffer from significant I/O bottlenecks due to suboptimal cache hit ratios and inefficient use of modern SSD architectures. In this work, we introduce a suite of optimizations to enhance the performance of disk-resident Approximate Nearest Neighbor (ANN) indices, specifically focusing on hierarchical graph-based indexing such as HNSW. Our approach leverages three key strategies: (1) Parallel I/O leveraging io_uring to exploit SSD concurrency and reduce retrieval latency, (2) Spatially-aware insertion reordering to improve cache efficiency by dynamically adjusting insert execution order based on locality, and (3) Locality-preserving colocation to restructure index layouts and minimize costly random disk accesses. \nWe implement these techniques within  pgvector , a  PostgreSQL extension for vector search, and conduct extensive evaluations using real-world datasets. Our optimizations yield up to 11.1 √ó  improvement in query throughput, a 3.23 √ó  increase in cache hit ratio, and a 98.4% reduction in index build time. Moreover, our findings underscore the importance of SSD-aware indexing strategies for scalable vector retrieval. By integrating hardware-aware I/O optimizations with intelligent data placement techniques, this work paves the way for more efficient, high-performance disk-based vector search engines that could fully leverage modern SSD‚Äôs high parallelism.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Turbocharging%20Vector%20Databases%20using%20Modern%20SSDs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4710-do.pdf",
    "session": "Research 31: Vector Data Management II",
    "authors": [
      {
        "Name": "Joobo Shim",
        "Affiliation": "Seoul National University"
      },
      {
        "Name": "Jaewon Oh",
        "Affiliation": "Seoul National University"
      },
      {
        "Name": "Hongchan Roh",
        "Affiliation": "Dnotitia"
      },
      {
        "Name": "Jaeyoung Do",
        "Affiliation": "Seoul National University"
      },
      {
        "Name": "Sang-Won Lee",
        "Affiliation": "Seoul National University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fe82c764-2979-4b93-82a0-5c3e56895538",
    "title": "Scaling your Hybrid CPU-GPU DBMS to Multiple GPUs",
    "abstract": "GPU-accelerated databases have been gaining popularity in recent years due to their massive parallelism and high memory bandwidth. The limited GPU memory capacity, however, is still a major bottleneck for GPU databases. \nExisting approaches have attempted to address this limitation by using (1) hybrid CPU-GPU DBMS or (2) multi-GPU DBMS. We aim to improve prior solutions further by leveraging both hybrid CPU- GPU DBMS and multi-GPU DBMS at the same time. In particular, we explore the design space and optimize the data placement and query execution in hybrid CPU and multi-GPU DBMS. To improve data placement, we introduce the cache-aware replication policy which takes into account the cost of shuffle when replicating data and could coordinate both caching and replication decisions for the best performance. To improve query execution, we extend the existing hybrid CPU-GPU query execution strategy with distributed query processing techniques to support multiple GPUs. We build a system called Lancelot, a hybrid CPU and Multi-GPU data analytics engine with all the optimizations integrated. \nOur evaluation shows that the cache-aware replication outperforms other policies by up to 2.5√ó and Lancelot outperforms existing GPU DBMSes by at least 2√ó on Star Schema Benchmark and 12√ó on TPC-H Benchmark.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Scaling%20your%20Hybrid%20CPU-GPU%20DBMS%20to%20Multiple%20GPUs",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4709-yogatama.pdf",
    "session": "Research 38: Data Management on Novel Hardware",
    "authors": [
      {
        "Name": "Bobbi W Yogatama",
        "Affiliation": "University of Wisconsin-Madison"
      },
      {
        "Name": "Weiwei Gong",
        "Affiliation": "Oracle America"
      },
      {
        "Name": "Xiangyao Yu",
        "Affiliation": "University of Wisconsin-Madison"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6ec08588-bdcd-4bf9-921f-1ef36bb9c4ef",
    "title": "A Single Machine System for Querying Big Graphs with PRAM",
    "abstract": "This paper develops  Planar  ( Plug and play PRAM), a single-machine system for graph analytics by reusing existing PRAM algorithms, without the need for designing new parallel algorithms.  Planar supports both out-of-core and in-memory analytics. When a graph is too big to fit into the memory of a machine,  Planar  adapts PRAM to limited resources by extending a fixpoint model with multi-core parallelism, using disk as memory extension. For an in-memory task, it dedicates all available CPU cores to the task, and allows parallelly scalable PRAM algorithms to retain the property,  i.e.,  the more cores are available, the less runtime is taken. We develop a graph partitioning and work scheduling strategy to accommodate subgraph I/O, balance memory usage and reduce runtime, beyond traditional partitioners for multi-machine systems. Using real-life graphs, we empirically verify that  Planar  outperforms SOTA in-graphs, we empirically verify that Planar outperforms SOTA inmemory and out-of-core systems in efficiency and scalability.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Single%20Machine%20System%20for%20Querying%20Big%20Graphs%20with%20PRAM",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p756-liu.pdf",
    "session": "Research 57: Graph Data Management VII",
    "authors": [
      {
        "Name": "Yang Liu",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Wenfei Fan",
        "Affiliation": "Univ. of Edinburgh"
      },
      {
        "Name": "Shuhao Liu",
        "Affiliation": "Shenzhen Institute of Computing Sciences"
      },
      {
        "Name": "Xiaoke Zhu",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Jianxin Li",
        "Affiliation": "Beihang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2fea032b-412a-4191-8d29-381205338de7",
    "title": "Interactive Graph Search for Multiple Targets on DAGs",
    "abstract": "Interactive graph search (IGS) over DAGs aims to find a hidden target by asking interactive questions as few as possible. IGS is useful for many applications, e.g., facilitating supervised learning tasks by harnessing labeled data, image categorization, and product classification. However, most of the existing IGS methods only work for either  single target search on DAGs  or  multiple targets search on simple trees . To overcome the gap, it motivates us to study a challenging and yet not solved problem of multiple targets search over DAGs. We analyze the new problem in-depth and propose a key concept of uncertain candidates. Based on it, we design an effective gain function to determine the best vertex to be asked questions and shrink the search space of potential targets greatly. Leveraging our uncertain candidates and gain function, we develop a unified  k-EIS  framework to search both single target and multiple targets. We analyze all algorithm complexities and theoretically show that our solution can significantly improve existing DFS-treebased methods by asking ùëÇ(ùëõ) questions to ùëÇ(log2  ùëõ)  questions in worst cases. To further improve IGS for multiple targets, we propose an advanced solution by dividing the whole DAG into ùëò disjoint subgraphs with single targets and then tackling each subgraph one by one independently. Extensive experiments on real-world datasets validate that our proposed k-EIS framework can save lots of questions to search exact targets against four state-of-the-art IGS competitors.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Interactive%20Graph%20Search%20for%20Multiple%20Targets%20on%20DAGs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1091-wu.pdf",
    "session": "Research 65: Graph Data Management VIII",
    "authors": [
      {
        "Name": "Zheng WU",
        "Affiliation": "Hong Kong Baptist University"
      },
      {
        "Name": "Xuliang Zhu",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Yixiang Fang",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Jianliang Xu",
        "Affiliation": "Hong Kong Baptist University"
      },
      {
        "Name": "Xin Huang",
        "Affiliation": "Hong Kong Baptist University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "08d0dd98-cfd2-42a6-b7bd-ca47e7210747",
    "title": "Anarchy in the Database: A Survey and Evaluation of Database Management System Extensibility",
    "abstract": "Extensions allow applications to expand the capabilities of database management systems (DBMSs) with custom logic. However, the extensibility environment for some DBMSs is fraught with perils, causing developers to resort to unorthodox methods to achieve their goals. This paper studies and evaluates the design of DBMS extensibility. First, we provide a comprehensive taxonomy of the types of DBMS extensibility. We then examine the extensibility of six DBMSs: PostgreSQL, MySQL, MariaDB, SQLite, Redis, and DuckDB. We present an automated extension analysis toolkit that collects static and dynamic information on how an extension integrates into the DBMS. Our evaluation of over 400 PostgreSQL extensions shows that 16.8% of them are incompatible with at least one other extension and can cause system failures. These results also show the correlation between these failures and factors related to extension complexity and implementation.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Anarchy%20in%20the%20Database%3A%20A%20Survey%20and%20Evaluation%20of%20Database%20Management%20System%20Extensibility",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1962-kim.pdf",
    "session": "Research 58: User Interfaces for Data Exploration and Recommender Systems",
    "authors": [
      {
        "Name": "Abigale Kim",
        "Affiliation": "Carnegie Mellon University, University of Wisconsin, Madison"
      },
      {
        "Name": "Marco Slot",
        "Affiliation": "Crunchy Data"
      },
      {
        "Name": "David Andersen",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Andrew Pavlo",
        "Affiliation": "Carnegie Mellon University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "30f23ed9-2375-4333-ac5f-2190f4de4032",
    "title": "GraphSparseNet: a Novel Method for Large Scale Traffic Flow Prediction",
    "abstract": "Traffic flow forecasting is a critical spatio-temporal data mining task with wide-ranging applications in intelligent route planning and dynamic traffic management. Recent advancements in deep learning, particularly through Graph Neural Networks (GNNs), have significantly enhanced the accuracy of these forecasts by capturing complex spatio-temporal dynamics. However, the scalability of GNNs remains a challenge due to their exponential growth in model complexity with increasing nodes in the graph. Existing methods to address this issue, including sparsification, decomposition, and kernel-based approaches, either do not fully resolve the complexity issue or risk compromising predictive accuracy. This paper introduces GraphSparseNet (GSNet), a novel framework designed to improve both the scalability and accuracy of GNN-based traffic forecasting models. GraphSparseNet is comprised of two core modules: the Feature Extractor and the Relational Compressor. These modules operate with linear time and space complexity, thereby reducing the overall computational complexity of the model to a linear scale. Our extensive experiments on multiple real-world datasets demonstrate that GraphSparseNet not only significantly reduces training time by 3.51x compared to state-of-the-art linear models but also maintains high predictive performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GraphSparseNet%3A%20a%20Novel%20Method%20for%20Large%20Scale%20Trafffic%20Flow%20Prediction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2295-liu.pdf",
    "session": "Research 4: Analytics over Different Data Types I",
    "authors": [
      {
        "Name": "Weiyang Kong",
        "Affiliation": "Sun Yat-Sen university"
      },
      {
        "Name": "Kaiqi Wu",
        "Affiliation": "Sun Yat-Sen University"
      },
      {
        "Name": "Sen Zhang",
        "Affiliation": "Sun Yat-sen University"
      },
      {
        "Name": "Yubao Liu",
        "Affiliation": "Sun Yat-Sen University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7d230997-bcd9-4b89-a402-21038df4af28",
    "title": "SIMformer: Single-Layer Vanilla Transformer Can Learn Free-Space Trajectory Similarity",
    "abstract": "Free-space trajectory similarity calculation, e.g., DTW, Hausdorff, and Fr√©chet, often incur quadratic time complexity, thus learning-and Fr√©chet, often incur quadratic time complexity, thus learningbased methods have been proposed to accelerate the computation. The core idea is to train an encoder to transform trajectories into representation vectors and then compute vector similarity to ap-representation vectors and then compute vector similarity to approximate the ground truth. However, existing methods face dual challenges of effectiveness and efficiency: 1) they all utilize Eu-challenges of effectiveness and efficiency: 1) they all utilize Euclidean distance to compute representation similarity, which leads to the severe curse of dimensionality issue ‚Äì reducing the distin-to the severe curse of dimensionality issue ‚Äì reducing the distinguishability among representations and significantly affecting the accuracy of subsequent similarity search tasks; 2) most of them are trained in triplets manner and often necessitate additional informa-trained in triplets manner and often necessitate additional information which downgrades the efficiency; 3) previous studies, while emphasizing the scalability in terms of efficiency, overlooked the deterioration of effectiveness when the dataset size grows. To cope with these issues, we propose a simple, yet accurate, fast, scalable model that only uses a single-layer vanilla transformer encoder as the feature extractor and employs tailored representation sim-as the feature extractor and employs tailored representation similarity functions to approximate various ground truth similarity measures. Extensive experiments demonstrate our model signifi-measures. Extensive experiments demonstrate our model significantly mitigates the curse of dimensionality issue and outperforms the state-of-the-arts in effectiveness, efficiency, and scalability.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SIMformer%3A%20Single-Layer%20Vanilla%20Transformer%20Can%20Learn%20Free-Space%20Trajectory%20Similarity",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p390-jiang.pdf",
    "session": "Research 32: Data-centric Machine Learning",
    "authors": [
      {
        "Name": "Chuang Yang",
        "Affiliation": "The University of Tokyo"
      },
      {
        "Name": "Renhe Jiang",
        "Affiliation": "The University of Tokyo"
      },
      {
        "Name": "Xiaohang Xu",
        "Affiliation": "The University of Tokyo"
      },
      {
        "Name": "Chuan Xiao",
        "Affiliation": "Osaka University, Nagoya University"
      },
      {
        "Name": "Kaoru Sezaki",
        "Affiliation": "The University of Tokyo"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fff03f71-7438-4015-aa98-aae3e6502038",
    "title": "Vive la Diff√©rence: Practical Diff Testing of Stateful Applications",
    "abstract": "Software rollout is the process of replacing the version of an application that is currently running in production with a new version. Many subtle and catastrophic bugs occur during software rollout. There are many existing techniques to improve the odds of a rollout completing successfully, but these techniques don‚Äôt work well when the application has shared, persistent, mutable state. In this paper, we present a practical framework to test the rollout of stateful applications. Our framework uses diff testing to verify that the new version of an application behaves identically to the currently running version that will be replaced. The framework has three main components to safely and efficiently compare the behavior of the two versions. First, we implement database branching on top of Postgres. Second, we implement an efficient algorithm to diff two database branches. Third, we describe how to replay client requests to improve test coverage. Finally, we identify three common categories of rollout bugs and demonstrate how our framework can find these bugs with minimal performance overhead.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Vive%20la%20Diff%C3%A9rence%3A%20Practical%20Diff%20Testing%20of%20Stateful%20Applications",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2018-zhu.pdf",
    "session": "Research 47: Provenance and Workflows",
    "authors": [
      {
        "Name": "Kexin Zhu",
        "Affiliation": "Google"
      },
      {
        "Name": "Michael Whittaker",
        "Affiliation": "Google"
      },
      {
        "Name": "Srdjan Petrovic",
        "Affiliation": "Google Inc."
      },
      {
        "Name": "Robert Grandl",
        "Affiliation": "Google"
      },
      {
        "Name": "Sanjay Ghemawat",
        "Affiliation": "Google"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "df5d0017-bfd9-4f0a-82b0-2436dbc81146",
    "title": "The Key to Effective UDF Optimization: Before Inlining, First Perform Outlining",
    "abstract": "Although user-defined functions (UDFs) are a popular way to aug- ment SQL‚Äôs declarative approach with procedural code, the mis- match between programming paradigms creates a fundamental optimization challenge. UDF inlining automatically removes all UDF calls by replacing them with equivalent SQL subqueries. Al- though inlining leaves queries entirely in SQL (resulting in large performance gains), we observe that inlining the entire UDF often leads to sub-optimal performance. A better approach is to analyze the UDF, deconstruct it into smaller pieces, and inline only the pieces that help query optimization. To achieve this, we propose UDF outlining, a technique to intentionally hide pieces of a UDF from the optimizer, resulting in simpler UDFs and significantly faster query plans. Our implementation (PRISM) demonstrates that UDF outlining improves performance over conventional inlining (on average 1.29√ó speedup for DuckDB and 298.73√ó for SQL Server) through a combination of more effective unnesting, improved data skipping, and by avoiding unnecessary joins.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20Key%20to%20Effective%20UDF%20Optimization%3A%20Before%20Inlining%2C%20First%20Perform%20Outlining",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1-arch.pdf",
    "session": "Research 8: Database Engines for OLAP",
    "authors": [
      {
        "Name": "Samuel Arch",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Yuchen Liu",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Todd Mowry",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Jignesh Patel",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Andrew Pavlo",
        "Affiliation": "Carnegie Mellon University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3628a2af-b8bc-40b4-8128-0d59aaa90f28",
    "title": "Privacy for Free: Leveraging Local Differential Privacy Perturbed Data from Multiple Services",
    "abstract": "Local Differential Privacy (LDP) has emerged as a widely adopted privacy-preserving technique in modern data analytics, enabling users to share statistical insights while maintaining robust privacy guarantees. However, current LDP applications assume a single service gathering perturbed information from users. In reality, multiple services may be interested in collecting users‚Äô data, which poses privacy burdens to users as more such services emerge. To address this issue, this paper proposes a framework for collecting and aggregating data based on perturbed information from multiple services, regardless of their estimated statistics (e.g., mean or distribution) and perturbation mechanisms. \nThen for mean estimation, we introduce the Unbiased Averaging (UA) method and its optimized version, User-level Weighted Averaging (UWA). The former utilizes biased perturbed data, while the latter assigns weights to different perturbed results based on perturbation information, thereby achieving minimal variance. For distribution estimation, we propose the User-level Likelihood Estimation (ULE), which treats all perturbed results from a user as a whole for maximum likelihood estimation. Experimental results demonstrate that our framework and constituting methods significantly improve the accuracy of both mean and distribution estimation.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Privacy%20for%20Free%3A%20Leveraging%20Local%20Differential%20Privacy%20Perturbed%20Data%20from%20Multiple%20Services",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1743-du.pdf",
    "session": "Research 18: Data Privacy and Security I",
    "authors": [
      {
        "Name": "Rong Du",
        "Affiliation": "PolyU"
      },
      {
        "Name": "Qingqing Ye",
        "Affiliation": "Hong Kong Polytechnic University"
      },
      {
        "Name": "Yue Fu",
        "Affiliation": "The Hong Kong Polytechnic University"
      },
      {
        "Name": "Haibo Hu",
        "Affiliation": "Hong Kong Polytechnic University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bd79df7b-ab05-400b-96e2-3bbd7f6b70b1",
    "title": "Discovering Leitmotifs in Multidimensional Time Series",
    "abstract": "A leitmotif is a recurring theme in literature, movies or music that carries symbolic significance for the piece it is contained in. When this piece can be represented as a multi-dimensional time series (MDTS), such as acoustic or visual observations, finding a leitmotif is equivalent to the pattern discovery problem, which is an unsu-is equivalent to the pattern discovery problem, which is an unsupervised and complex problem in time series analytics. Compared to the univariate case, it carries additional complexity because pat-to the univariate case, it carries additional complexity because patterns typically do not occur in all dimensions but only in a few -terns typically do not occur in all dimensions but only in a few which are, however, unknown and must be detected by the method itself. In this paper, we present the novel, efficient and highly effect-itself. In this paper, we present the novel, efficient and highly effective leitmotif discovery algorithm LAMA for MDTS. LAMA rests on two core principals: (a) a leitmotif manifests solely given a yet unknown number of sub-dimensions - neither too few, nor too many, and (b) the set of sub-dimensions are not independent from the best pattern found therein, necessitating both problems to be approached in a joint manner. In contrast to most previous methods, LAMA tackles both problems jointly - instead of independently se-LAMA tackles both problems jointly - instead of independently selecting dimensions (or leitmotifs) and finding the best leitmotifs (or dimensions). Our experimental evaluation on a novel ground-truth annotated benchmark of 14 distinct real-life data sets shows that LAMA, when compared to four state-of-the-art baselines, shows superior performance in detecting meaningful patterns without increased computational complexity. KEYWORDS Multidimensional, Sub-Dimensional, Multivariate, Time Series, Mo-Multidimensional, Sub-Dimensional, Multivariate, Time Series, Motif, Motif Set, Leitmotif",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Discovering%20Leitmotifs%20in%20Multidimensional%20Time%20Series",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p377-schafer.pdf",
    "session": "Research 25: Time Series Data III",
    "authors": [
      {
        "Name": "Patrick Sch√§fer",
        "Affiliation": "Humboldt-Universit√§t zu Berlin"
      },
      {
        "Name": "Ulf Leser",
        "Affiliation": "Humboldt-Universit√§t zu Berlin"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0901e723-2c15-4daf-8431-9a21cbc2dc71",
    "title": "Keigo: Co-designing Log-Structured Merge Key-Value Stores with a Non-Volatile, Concurrency-aware Storage Hierarchy",
    "abstract": "We present Keigo, a concurrency- and workload-aware storage middleware that enhances the performance of log-structured merge key-value stores (LSM KVS) when they are deployed on a hierarchy of storage devices. The key observation behind Keigo is that there is no  one-size-fits-all  placement of data across the storage hierarchy that optimizes for all workloads. Hence, to leverage the benefits of combining different storage devices, Keigo places files across different devices based on their parallelism, I/O bandwidth, and capacity. We introduce three techniques ‚Äì  concurrency-aware data placement ,  persistent read-only caching , and  context-based I/O differentiation . Keigo is portable across different LSMs, is adaptable to dynamic workloads, and does not require extensive profiling. Our system enables established production KVS such as RocksDB, LevelDB, and Speedb to benefit from heterogeneous storage setups. \nWe evaluate Keigo using synthetic and realistic workloads, showing that it improves the throughput of production-grade LSMs up to 4 √ó  for write- and 18 √ó  for read-heavy workloads when compared to general-purpose storage systems and specialized LSM KVS.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Keigo%3A%20Co-designing%20Log-Structured%20Merge%20Key-Value%20Stores%20with%20a%20Non-Volatile%2C%20Concurrency-aware%20Storage%20Hierarchy",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2872-macedo.pdf",
    "session": "Research 54: Database Engines II",
    "authors": [
      {
        "Name": "R√∫ben Ad√£o",
        "Affiliation": "INESC TEC & University of Minho"
      },
      {
        "Name": "Zhongjie Wu",
        "Affiliation": "Yale University"
      },
      {
        "Name": "Changjun Zhou",
        "Affiliation": "McGill University"
      },
      {
        "Name": "Oana Balmau",
        "Affiliation": "McGill, Canada"
      },
      {
        "Name": "Jo√£o Paulo",
        "Affiliation": "INESC TEC & University of Minho"
      },
      {
        "Name": "Ricardo Macedo",
        "Affiliation": "INESC TEC & University of Minho"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a9037dcb-e35c-4b4b-a488-34f74621baa6",
    "title": "On More Efficiently and Versatilely Querying Historical k-Cores",
    "abstract": "The recently proposed historical ùëò-core query introduces a new paradigm of structure analysis for temporal graphs. However, the query processing based on the existing PHC-index, which preserves the distinct ‚Äúcore time‚Äù of each vertex, needs to traverse all vertices for each query, even though the results usually contain only a small subset of vertices. Inspired by the traditional ùëò-shell that ensures the optimal ùëò-core query processing, we propose a novel concept called ‚Äúcore time shell‚Äù, which reveals the hierarchical structure of vertices with respect to their core time. Based on the core time shell, we design a time-space balanced Merged Core Time Shell index (MCTS-index). It is theoretically guaranteed that, the MCTS-index provides the approximately optimal query performance, and has the approximately same space complexity as the PHC-index. Moreover, we leverage the MCTS-index to efficiently address the brand-new ‚Äúwhen‚Äù historical ùëò-core queries orthogonal to the current ‚Äúwhat‚Äù historical ùëò -core queries. Our experimental results on ten real-world temporal graphs demonstrate both the superior efficiency of pro- cessing ‚Äúwhat‚Äù queries and the effectiveness of processing versatile ‚Äúwhen‚Äù queries for the MCTS-index.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/On%20More%20Efficiently%20and%20Versatilely%20Querying%20Historical%20k-Cores",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1335-zhong.pdf",
    "session": "Research 65: Graph Data Management VIII",
    "authors": [
      {
        "Name": "Zhi Wang",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Ming Zhong",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Yuanyuan Zhu",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Tieyun Qian",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Mengchi Liu",
        "Affiliation": "Carleton University"
      },
      {
        "Name": "Jeffrey Xu Yu",
        "Affiliation": "Chinese University of Hong Kong"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8e213f54-844f-4c82-bc38-eb065812583b",
    "title": "Laser: Buffer-Aware Learned Query Scheduling in Master-Standby Databases",
    "abstract": "Master-standby database deployment is a commonly adopted data-Master-standby database deployment is a commonly adopted database architecture in modern production environments, thanks to its fault tolerance and high availability. However, despite the architec-fault tolerance and high availability. However, despite the architecture‚Äôs widespread application in various online services, relatively few research efforts have been made to improve its overall query performance. When a sequence of queries arrive, existing methods of scheduling them across master and standby servers still rely on rules or heuristics, which may overlook some potential opti-on rules or heuristics, which may overlook some potential optimization directions such as buffer utilization. If we can efficiently reuse the database buffers resident in memory through intelligent query scheduling, the average response time of user queries can be significantly reduced as opposed to reading data from disk. \nTo address this issue, in this paper, we introduce a new buffer-To address this issue, in this paper, we introduce a new bufferaware query scheduling system named  Laser . The system integrates a lightweight learned model that can directly map a query to the data blocks it accesses. Then, based on the predictions of the queries, we develop adaptive query scheduling algorithms to perform query allocation as well as query rearrangements, aiming to maximize the overall buffer hit rate while also maintaining load balance. The proposed system requires no pre-training, and can adjust to unseen workloads on the fly through constant model updates and query re-allocation. In our experiments, we observe a reduction of  ‚àº 80% in query completion time compared to other traditional heuristic-in query completion time compared to other traditional heuristicbased methods, with relatively low extra overhead added to the critical path of query execution.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Laser%3A%20Buffer-Aware%20Learned%20Query%20Scheduling%20in%20Master-Standby%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p743-li.pdf",
    "session": "Research 53: Applied ML and AI for Data Management IV",
    "authors": [
      {
        "Name": "Yuwei Huang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "87b6d258-f40c-4a1e-afda-63dd8913728a",
    "title": "Enhancing Transaction Processing through Indirection Skipping",
    "abstract": "In modern database management systems (DBMS), data retrieval typically requires traversing multiple layers‚Äîsuch as secondary indexes, primary indexes, and buffer pools‚Äîwhich introduces significant overhead and creates performance bottlenecks. In this paper, we propose a novel method that minimizes this overhead by establishing more direct access paths during data retrieval. Our experimental results demonstrate substantial efficiency gains across various DBMS components, including secondary indexing and concurrency control mechanisms. Specifically, we observe that implementing direct access paths can boost the throughput of transaction processing systems by up to 19 . 7 √ó  when executing the TPC-Clike benchmark with 40 threads. Furthermore, our approach holds promise for broader applications, potentially transforming data retrieval practices by enabling efficient handling of data movements with minimal overhead.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Enhancing%20Transaction%20Processing%20through%20Indirection%20Skipping",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4104-otaki.pdf",
    "session": "Research 22: Views, Indexing, and Search II",
    "authors": [
      {
        "Name": "Riki Otaki",
        "Affiliation": "University of Chicago"
      },
      {
        "Name": "Jun Hyuk Chang",
        "Affiliation": "University of Chicago"
      },
      {
        "Name": "Aaron Elmore",
        "Affiliation": "University of Chicago"
      },
      {
        "Name": "Goetz Graefe",
        "Affiliation": "Google"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6a455972-88b8-49ab-bd50-971f8eec61e9",
    "title": "GPEmu: A GPU Emulator for Faster and Cheaper Prototyping and Evaluation of Deep Learning System Research",
    "abstract": "Deep learning (DL) system research is often impeded by the limited availability and expensive costs of GPUs. In this paper, we introduce GPEmu, a GPU emulator for faster and cheaper prototyping and evaluation of deep learning system research without using real GPUs. GPEmu comes with four novel features: time emulation, memory emulation, distributed system support, and sharing support. We support over 30 DL models and 6 GPU models, the largest scale to date. We demonstrate the power of GPEmu by successfully reproducing the main results of nine recent publications and easily prototyping three new micro-optimizations.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GPEmu%3A%20A%20GPU%20Emulator%20for%20Faster%20and%20Cheaper%20Prototyping%20and%20Evaluation%20of%20Deep%20Learning%20System%20Research",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1919-wang.pdf",
    "session": "Research 17: Applied ML and AI for Data Management II",
    "authors": [
      {
        "Name": "Meng Wang",
        "Affiliation": "University of Chicago"
      },
      {
        "Name": "Gus Waldspurger",
        "Affiliation": "University of Chicago"
      },
      {
        "Name": "Naufal Ananda",
        "Affiliation": "Telkom University"
      },
      {
        "Name": "Yuyang Huang",
        "Affiliation": "University of Chicago"
      },
      {
        "Name": "Kemas Wiharja",
        "Affiliation": "Telkom University"
      },
      {
        "Name": "John Bent",
        "Affiliation": "LANL"
      },
      {
        "Name": "Swaminathan Sundararaman",
        "Affiliation": "IBM Research"
      },
      {
        "Name": "Vijay Chidambaram",
        "Affiliation": "UT Austin"
      },
      {
        "Name": "Haryadi S. Gunawi",
        "Affiliation": "University of Chicago"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f3475322-23a8-4d71-b114-83d4b1bc76bc",
    "title": "Maximum k-Plex Search: An Alternated Reduction-and-Bound Method",
    "abstract": "ùëò-plexes relax cliques by allowing each vertex to disconnect to at most ùëò vertices. Finding a maximum ùëò-plex in a graph is a funda- mental operator in graph mining and has been receiving significant attention from various domains. The state-of-the-art algorithms all adopt the branch-reduction-and-bound (BRB) framework where a key step, called reduction-and-bound (RB), is used for narrow- ing down the search space. A common practice of RB in existing works is SeqRB, which sequentially conducts the reduction pro- cess followed by the bounding process once at a branch. However, these algorithms suffer from the efficiency issues. In this paper, we propose a new alternated reduction-and-bound method AltRB for conducting RB. AltRB first partitions a branch into two parts and then alternatively and iteratively conducts the reduction process and the bounding process at each part of a branch. With newly- designed reduction rules and bounding methods, AltRB is superior to SeqRB in effectively narrowing down the search space in both theory and practice. Further, to boost the performance of BRB algo- rithms, we develop efficient and effective pre-processing methods which reduce the size of the input graph and heuristically compute a large ùëò-plex as the lower bound. We conduct extensive experi- ments on 664 real and synthetic graphs. The experimental results show that our proposed algorithm kPEX with AltRB and novel pre- processing techniques runs up to two orders of magnitude faster and solves more instances than state-of-the-art algorithms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Maximum%20k-Plex%20Search%3A%20An%20Alternated%20Reduction-and-Bound%20Method",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p363-liu.pdf",
    "session": "Research 51: Information Integration and Data Quality III",
    "authors": [
      {
        "Name": "Shuohao Gao",
        "Affiliation": "Harbin Institute of Technology, Shenzhen"
      },
      {
        "Name": "Kaiqiang Yu",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Shengxin Liu",
        "Affiliation": "Harbin Institute of Technology, Shenzhen"
      },
      {
        "Name": "Cheng Long",
        "Affiliation": "Nanyang Technological University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ef67a47b-f29d-471c-8222-082c1ff41073",
    "title": "GraphAr: An Efficient Storage Scheme for Graph Data in Data Lakes",
    "abstract": "Data lakes, increasingly adopted for their ability to store and ana-Data lakes, increasingly adopted for their ability to store and analyze diverse types of data, commonly use columnar storage formats like Parquet and ORC for handling relational tables. However, these traditional setups fall short when it comes to efficiently managing graph data, particularly those conforming to the Labeled Property Graph (LPG) model. To address this gap, this paper introduces GraphAr , a specialized storage scheme designed to enhance exist-GraphAr , a specialized storage scheme designed to enhance existing data lakes for efficient graph data management. Leveraging the strengths of Parquet,  GraphAr  captures LPG semantics pre-the strengths of Parquet,  GraphAr  captures LPG semantics precisely and facilitates graph-specific operations such as neighbor retrieval and label filtering. Through innovative data organization, encoding, and decoding techniques,  GraphAr  dramatically improves performance. Our evaluations reveal that  GraphAr  outperforms conventional Parquet and Acero-based methods, achieving an aver-conventional Parquet and Acero-based methods, achieving an average speedup of 4452 √ó  for neighbor retrieval, 14 . 8 √ó  for label filter-age speedup of 4452 √ó  for neighbor retrieval, 14 . 8 √ó  for label filtering, and 29 . 5 √ó  for end-to-end workloads. These findings highlight GraphAr ‚Äôs potential to extend the utility of data lakes by enabling efficient graph data management.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GraphAr%3A%20An%20Efficient%20Storage%20Scheme%20for%20Graph%20Data%20in%20Data%20Lakes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p530-li.pdf",
    "session": "Research 13: Graph Data Management II",
    "authors": [
      {
        "Name": "Xue Li",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Weibin Zeng",
        "Affiliation": "Alibaba"
      },
      {
        "Name": "Zhibin Wang",
        "Affiliation": "NJU"
      },
      {
        "Name": "Diwen Zhu",
        "Affiliation": "Alibaba"
      },
      {
        "Name": "Jingbo Xu",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Wenyuan Yu",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Jingren Zhou",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0a0e1afd-cf9c-4a0a-95f2-2d0f6819d5bf",
    "title": "[Industry] From Scale-Up to Scale-Out: PolarDB‚Äôs Journey to Achieving 2 Billion tpmC",
    "abstract": "In the past decade, cloud databases have experienced rapid development and growth. PolarDB, Alibaba‚Äôs cloud-native OLTP database, has evolved significantly to meet the increasing demand for cloud-native architectures and now serves hundreds of thousands of customers across various industries. \nThis paper presents PolarDB‚Äôs evolution over the past eight years, with a focus on scalability, performance, and cost-efficiency. Initially, PolarDB adopted a primary-replica architecture based on disaggregated storage, with an emphasis on enhancing single-node performance for scale-up in modern many-core systems. To achieve this, we co-designed PolarDB with cutting-edge hardware, including RDMA, to improve performance. Meanwhile, we refined the internal architecture, including improvements to B+ tree concurrency control and transaction management, ensuring high scalability in scale-up scenarios. More recently, our focus has shifted to scaling out PolarDB to meet the performance and scalability needs of ultra-large-scale applications. By leveraging RDMA, we optimized distributed transaction processing,  transforming PolarDB into a high-performance, high-scalability and cost-effective distributed database . In the TPC-C benchmark, PolarDB scaled out to 2340 nodes and achieved over 2 billion tpmC, with a jitter rate of no more than 0.16% during the 8-hour stress test. Compared to the second- and third-highest-performing databases in public TPC-C results, PolarDB‚Äôs tpmC is 2.52 √ó  and 2.91 √ó  higher, respectively. In terms of cost-effectiveness, PolarDB‚Äôs per-tpmC cost is 37% and 79.5% lower than that of the other two systems, respectively.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/From%20Scale-Up%20to%20Scale-Out%3A%20PolarDB%E2%80%99s%20Journey%20to%20Achieving%202%20Billion%20tpmC",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5059-chen.pdf",
    "session": "Industry 5: Transactions and Concurrency Control",
    "authors": [
      {
        "Name": "Xinjun Yang",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Yingqiang Zhang",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Hao Chen",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Qingda Hu",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Panfeng Zhou",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Qiang Zhang",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Shuai Li",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Zongzhi Chen",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Zheyu Miao",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Rongbiao Xie",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Chuan Sun",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Zetao Wei",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Jing Fang",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Xingxuan Zhou",
        "Affiliation": "Alibaba Cloud Computing"
      },
      {
        "Name": "Xiaofei Wu",
        "Affiliation": "Alibaba Cloud Computing"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9e6d5efc-dbe9-419d-903f-c9406c6223c1",
    "title": "Graph Compression for Interpretable Graph Neural Network Inference At Scale",
    "abstract": "We demonstrate  ExGIS , a parallel inference query engine to support explainable Graph Neural Network ( GNNs ) inference analysis in large graphs. (1) For a class of  GNNs  M ùêø with at most  ùêø layers, and a graph  ùëÄ ,  ExGIS  performs an o!ine, once-for-all compression of  ùëÄ to a small graph  ùëÄ ùëÄ , such that for any inference query  ùëÅ that requests the output of any  GNN  ùëÇ ‚ÜíM ùêø on any node  ùëÉ in  ùëÄ ,  ùëÄ ùëÄ can be directly queried to yield correct output without decompression. (2) Given a workload  ùëÑ of inference queries that requests the output of  GNNs  from  M  over  ùëÄ ,  ExGIS  perform fast online  GNN inference and interpretation in parallel. It dynamically partitions ùëÑ to balance workloads, and (a) executes inference that only consults compressed graph  ùëÄ ùëÄ without decompression, and (b) directly yields concise, explanatory subgraphs from  ùëÄ ùëÄ that can clarify the query output with high fidelity, all in parallel. Moreover,  ExGIS integrates visual, interactive interfaces for query performance analysis, and a Large Language Models (LLMs)- enabled interpreter to support user-friendly, natural language explanation of query outputs. We demonstrate the compression rate and scalability of ExGIS , and its application in interpretable anomaly detection over bitcoin transaction networks and academic networks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Graph%20Compression%20for%20Interpretable%20Graph%20Neural%20Network%20Inference%20At%20Scale",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5239-fan.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Yangxin Fan",
        "Affiliation": "Case Western Reserve University"
      },
      {
        "Name": "Haolai Che",
        "Affiliation": "Case Western Reserve University"
      },
      {
        "Name": "Mingjian Lu",
        "Affiliation": "Case Western Reserve University"
      },
      {
        "Name": "Yinghui Wu",
        "Affiliation": "Case Western Reserve University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3c8ced67-9174-4777-96e5-d8be3e2f4d99",
    "title": "PrivEval: a tool for interactive evaluation of privacy metrics in synthetic data generation",
    "abstract": "Synthetic data generation (SDG) is the process of generating a new synthetic dataset based on the statistical properties of a confidential existing dataset. Differential privacy is the property of a SDG mechanism that establishes how protected individuals whose sensitive data is part of the confidential dataset are, when sharing such data. To ensure a SDG is differentially private, noise is injected into the statistics learned from the dataset. Depending on the amount of noise injected, we witness a trade-off between privacy and utility. Privacy is then measured via a set of privacy metrics that usually establish a lower bound on a few aspects of the privacy-utility tradeoff. Therefore, it is not possible to assess privacy based only on one metric. To close this gap, we demonstrate PrivEval, a tool to assist users in evaluating the privacy properties of a synthetic dataset. PrivEval implements several privacy metrics and validates them on both a single user and the overall dataset. Besides, PrivEval checks assumptions behind each metric. Hence, PrivEval is a first step to bridge the gap between privacy experts and the general public to make privacy estimation more transparent.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PrivEval%3A%20a%20tool%20for%20interactive%20evaluation%20of%20privacy%20metrics%20in%20synthetic%20data%20generation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5271-trudslev.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Frederik Trudslev",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Matteo Lissandrini",
        "Affiliation": "Universit√° di Verona"
      },
      {
        "Name": "Juan Rodriguez",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Martin B√∏gsted",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Daniele Dell'Aglio",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "28c93740-6ef5-459c-9eea-f71eac800d5c",
    "title": "NeutronTask: Scalable and Efficient Multi-GPU GNN Training with Task Parallelism",
    "abstract": "Graph neural networks (GNNs) have emerged as a promising method for learning from graph data, but large-scale GNN training requires extensive memory and computation resources. To address this, researchers have proposed using multi-GPU processing, which partitions graph data across GPUs for parallel training. However, vertex dependencies in multi-GPU GNN training lead to significant neighbor replications across GPUs, increasing memory consumption. The substantial intermediate data generated during training further exacerbates this issue. Neighbor replication and intermediate data constitute the primary memory consumption in GNN training (i.e., typically accounting for over 80%). In this work, we propose GNN task parallelism for multi-GPU GNN training, which reduces neighbor replication by partitioning training tasks in each layer across different GPUs rather than partitioning the graph structure. This approach only partitions the graph data within individual GPUs, reducing the memory requirements of single tasks while overlapping subgraph computation across different GPUs. Shared neighbor embeddings among different subgraphs can be efficiently reused within a single GPU. Additionally, we employ a task-decoupled GNN training framework, which decouples different training tasks to manage their associated intermediate data independently and release it as early as possible to reduce memory usage. By integrating these techniques, we propose a multi-GPU GNN training system, NeutronTask. Experimental results on a 4 √ó A5000 GPU server show that NeutronTask effectively supports billion-scale full-graph GNN training. For small graphs where the training data fits into the GPUs, NeutronTask achieves 1.27 √ó  - 5.47 √ó  speedup compared to state-of-the-art GNN systems including NeutronStar and Sancus.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/NeutronTask%3A%20Scalable%20and%20Efficient%20Multi-GPU%20GNN%20Training%20with%20Task%20Parallelism",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1705-fu.pdf",
    "session": "Research 45: Graph Data Learning",
    "authors": [
      {
        "Name": "Zhenbo Fu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Xin Ai",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Qiange Wang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Yanfeng Zhang",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Shizhan Lu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Chaoyi Chen",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Chunyu Cao",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Hao Yuan",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Zhewei Wei",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yu Gu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Yingyou Wen",
        "Affiliation": "Neusoft AI Magic Technology Research"
      },
      {
        "Name": "Ge Yu",
        "Affiliation": "Northeastern University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a8174ba4-1fae-42fc-9b42-b6a80afcea9f",
    "title": "Unlocking the Power of CI/CD for Data Pipelines in Distributed Data Warehouses",
    "abstract": "Ensuring the reliability of data pipelines is critical for modern datadriven organizations, yet building robust Continuous Integration (CI) in large, distributed data warehouses remains a signi!cant challenge. Complexities arising from distributed ownership, the high cost of replicating production environments, and the rapid evolution of business logic lead to fragile pipelines and costly failures. This paper introduces a novel CI framework designed to conquer these challenges, achieving 94.5% pre-production issue detection in YouTube‚Äôs data warehouse while dramatically reducing resource consumption. Our key innovation lies in a productioncon!guration-driven testing methodology, that enables scalable, isolated testing directly within the production environment. This approach reduces testing overhead and ensures high test !delity. Furthermore, we present a lineage-aware impact analysis framework that automatically propagates data quality checks across distributed pipeline components based on an algebraic dependency model, ensuring data consistency and promoting cross-team collaboration. This production-proven solution provides a practical blueprint for CI/CD in complex, large-scale environments.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Unlocking%20the%20Power%20of%20CI-slash-CD%20for%20Data%20Pipelines%20in%20Distributed%20Data%20Warehouses",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4887-yang.pdf",
    "session": "Industry 2: Data Platforms for Analytics",
    "authors": [
      {
        "Name": "Hongtao Yang",
        "Affiliation": "Google"
      },
      {
        "Name": "Zhichen Xu",
        "Affiliation": "Google"
      },
      {
        "Name": "Sergey Yudin",
        "Affiliation": "Google"
      },
      {
        "Name": "Andrew Davidson",
        "Affiliation": "Google"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "de597c07-61e3-402e-9adc-81b1891d5647",
    "title": "Datalog with First-Class Facts",
    "abstract": "Datalog is a popular logic programming language for deductive reasoning tasks in a wide array of applications, including business analytics, program analysis, and ontological reasoning. However, Datalog‚Äôs restriction to flat facts over atomic constants leads to challenges in working with tree-structured data, such as derivation trees or abstract syntax trees. To ameliorate Datalog‚Äôs restrictions, popular extensions of Datalog support features such as existential quantification in rule heads (Datalog ¬± , Datalog ‚àÉ ) or algebraic data types (Souffl√©). Unfortunately, these are imperfect solutions for reasoning over structured and recursive data types, with general existentials leading to complex implementations requiring unifi-existentials leading to complex implementations requiring unification, and ADTs unable to trigger rule evaluation and failing to support efficient indexing. \nWe present  DL ‚àÉ ! , a Datalog with first-class facts, wherein every fact is identified with a Skolem term unique to the fact. We show that this restriction offers an attractive price point for Datalog-that this restriction offers an attractive price point for Datalogbased reasoning over tree-shaped data, demonstrating its appli-based reasoning over tree-shaped data, demonstrating its application to databases, artificial intelligence, and programming lan-cation to databases, artificial intelligence, and programming languages. We implemented  DL ‚àÉ !   as a system Slog, which leverages the uniqueness restriction of  DL ‚àÉ !   to enable a communication-the uniqueness restriction of  DL ‚àÉ !   to enable a communicationavoiding, massively-parallel implementation built on MPI. We show that Slog outperforms leading systems (Nemo, Vlog, RDFox, and Souffl√©) on a variety of benchmarks, with the potential to scale to thousands of threads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Datalog%20with%20First-Class%20Facts",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p651-micinski.pdf",
    "session": "Research 42: Data Models and Query Languages",
    "authors": [
      {
        "Name": "Thomas Gilray",
        "Affiliation": "Washington State University - Pullman"
      },
      {
        "Name": "Arash Sahebolamri",
        "Affiliation": "Syracuse Univeristy"
      },
      {
        "Name": "Yihao Sun",
        "Affiliation": "Syracuse University"
      },
      {
        "Name": "Sowmith Kunapaneni",
        "Affiliation": "Washington State University - Pullman"
      },
      {
        "Name": "Sidharth kumar",
        "Affiliation": "University of Illinois at Chicago"
      },
      {
        "Name": "Kristopher Micinski",
        "Affiliation": "Syracuse University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a909fd64-5a8e-4bb8-a17a-fce69a68c0a9",
    "title": "AutoPrep: Natural Language Question-Aware Data Preparation with a Multi-Agent Framework",
    "abstract": "Answering natural language (NL) questions about tables, known as Tabular Question Answering (TQA), is crucial because it allows users to quickly and efficiently extract meaningful insights from structured data, effectively bridging the gap between human language and machine-readable formats. Many of these tables are derived from web sources or real-world scenarios, which require meticulous data preparation (or data prep) to ensure accurate responses. However, preparing such tables for NL questions introduces new requirements that extend beyond traditional data preparation. This question-aware data preparation involves specific tasks such as column derivation and filtering tailored to particular questions, as well as question-aware value normalization or conversion, highlighting the need for a more nuanced approach in this context. Because each of the above tasks is unique, a single model (or agent) may not perform effectively across all scenarios. In this paper, we propose  AutoPrep , a large language model (LLM)-based multiagent framework that leverages the strengths of multiple agents, each specialized in a certain type of data prep, ensuring more accurate and contextually relevant responses. Given an NL question over a table, AutoPrep performs data prep through three key components.  Planner : Determines a logical plan, outlining a sequence of high-level operations.  Programmer : Translates this logical plan into a physical plan by generating the corresponding low-level code. Executor : Executes the generated code to process the table. To support this multi-agent framework, we design a novel Chain-ofClauses reasoning mechanism for high-level operation suggestion, and a tool-augmented method for low-level code generation. Extensive experiments on real-world TQA datasets demonstrate that AutoPrep can significantly improve the state-of-the-art TQA solutions through question-aware data preparation.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/AutoPrep%3A%20Natural%20Language%20Question-Aware%20Data%20Preparation%20with%20a%20Multi-Agent%20Framework",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3504-fan.pdf",
    "session": "Research 36: Data Cleaning and Preparation II",
    "authors": [
      {
        "Name": "Meihao Fan",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Ju Fan",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Nan Tang",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Lei Cao",
        "Affiliation": "University of Arizona/MIT"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xiaoyong Du",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ad6e5bda-1dd1-49f4-af91-af459c344377",
    "title": "Cloudy With a Chance of JSON",
    "abstract": "Couchbase Capella is a scalable document-oriented database service in the cloud. Its existing Capella Operational service is based on a shared-nothing architecture and supports high volumes of lowlatency queries and updates for JSON documents. Its new Capella Columnar cloud service complements the Operational service. The Capella Columnar service supports complex analytical queries (e.g., ad hoc joins and aggregations) over large collections of JSON documents that can originate from a variety of Couchbase and nonCouchbase data sources and formats and can either be stored and managed by the Capella Columnar service or externally stored and accessed on demand at query time. This paper describes the new Capella Columnar service, looking both over and under the hood.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cloudy%20With%20a%20Chance%20of%20JSON",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4938-carey.pdf",
    "session": "Industry 1: Distributed Systems",
    "authors": [
      {
        "Name": "Murtadha Al Hubail",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Ali Alsuliman",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Wail Alkowaileet",
        "Affiliation": "Saudi National Center for AI"
      },
      {
        "Name": "Michael Blow",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Michael Carey",
        "Affiliation": "UC Irvine"
      },
      {
        "Name": "Savyasach Enukonda",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Peeyush Gupta",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Santosh Hegde",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Kamini Jagtiani",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Abhishek Jindal",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Nawazish Kahn",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Mehnaz Mahin",
        "Affiliation": "UC Riverside"
      },
      {
        "Name": "Ian Maxon",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "M Muralikrishna",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Keshav Murthy",
        "Affiliation": "Couchbase, Inc"
      },
      {
        "Name": "Daniel Nagy",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Preetham Poluparthi",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Ankit Prabhu",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Ritik Raj",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Vijay Sarathy",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Shahrzad Shirazi",
        "Affiliation": "UC Riverside"
      },
      {
        "Name": "Utsav Singh",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Hussain Towaileb",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Ayush Tripathi",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Janhavi Tripurwar",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Bo-Chun Wang",
        "Affiliation": "Couchbase, Inc."
      },
      {
        "Name": "Till Westmann",
        "Affiliation": "Couchbase, Inc."
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "70eb4bd8-1a01-4fef-84dc-b6ca4f71cb0a",
    "title": "A Memory Guided Transformer for Time Series Forecasting",
    "abstract": "Accurate long-term forecasting from multivariate time series has important real-world applications. However, achieving this so is challenging. Thus, analyses reveal that time series that span long durations often exhibit dynamic and disrupted correlations. State-of-durations often exhibit dynamic and disrupted correlations. State-ofthe-art methods employ attention mechanisms to capture dynamic correlations, but they often do not contend well with disrupted correlations, which reduces prediction accuracy. We introduce local and global information concepts and then leverage these in a Mem-and global information concepts and then leverage these in a Memory Guided Transformer, called the Memformer. By integrating patch-wise recurrent graph learning and global attention, the Mem-patch-wise recurrent graph learning and global attention, the Memformer aims to capture dynamic correlations and take disrupted correlations into account. We also integrate a so-called Alternating Memory Enhancer into the Memformer to capture correlations be-Memory Enhancer into the Memformer to capture correlations between local and global information. We report on experiments that offer insight into the effectiveness of the Memformer at capturing dynamic correlations and its robustness to disrupted correlations. The experiments offer evidence that the new method is capable of advancing the state-of-the-art in forecasting accuracy on real-world datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Memory%20Guided%20Transformer%20for%20Time%20Series%20Forecasting",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p239-cheng.pdf",
    "session": "Research 39: Analytics over Different Data Types II",
    "authors": [
      {
        "Name": "Yunyao Cheng",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Chenjuan Guo",
        "Affiliation": "ECNU"
      },
      {
        "Name": "Bin Yang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Haomin Yu",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "kai zhao",
        "Affiliation": "AAU"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c94437cc-7ef0-4763-b5dd-a3887ebd92e0",
    "title": "The ParClusterers Benchmark Suite (PCBS): A Fine-Grained Analysis of Scalable Graph Clustering",
    "abstract": "We introduce the ParClusterers Benchmark Suite (PCBS)‚Äîa col-We introduce the ParClusterers Benchmark Suite (PCBS)‚Äîa collection of highly scalable parallel graph clustering algorithms and benchmarking tools that streamline comparing different graph clus-benchmarking tools that streamline comparing different graph clustering algorithms and implementations. The benchmark includes clustering algorithms that target a wide range of modern clustering use cases, including community detection, classification, and dense subgraph mining. The benchmark toolkit makes it easy to run and evaluate multiple instances of different clustering algorithms with respect to both the running time and quality. \nWe evaluate the PCBS algorithms empirically and find that they deliver both the state of the art quality and the running time. In terms of the running time, they are on average over 4x faster than the fastest library we compared to. In terms of quality, the correla-the fastest library we compared to. In terms of quality, the correlation clustering algorithm [Shi et al., VLDB‚Äô21] optimizing for the LambdaCC objective, which does not have a direct counterpart in other libraries, delivers the highest quality in the majority of datasets that we used.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20ParClusterers%20Benchmark%20Suite%20(PCBS)%3A%20A%20Fine-Grained%20Analysis%20of%20Scalable%20Graph%20Clustering",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p836-yu.pdf",
    "session": "Research 57: Graph Data Management VII",
    "authors": [
      {
        "Name": "Shangdi Yu",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Jessica Shi",
        "Affiliation": "MIT"
      },
      {
        "Name": "Jamison Meindl",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "David Eisenstat",
        "Affiliation": "Google"
      },
      {
        "Name": "Xiaoen Ju",
        "Affiliation": "Google"
      },
      {
        "Name": "Sasan Tavakkol",
        "Affiliation": "Google"
      },
      {
        "Name": "Laxman Dhulipala",
        "Affiliation": "University of Maryland, College Park"
      },
      {
        "Name": "Jakub ≈ÅƒÖcki",
        "Affiliation": "Google"
      },
      {
        "Name": "Vahab Mirrokni",
        "Affiliation": "Google"
      },
      {
        "Name": "Julian Shun",
        "Affiliation": "MIT"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c4cd3973-3103-4d5c-99c8-a0e00337b522",
    "title": "G-View: View Management for Graph Databases",
    "abstract": "Graph database systems (GDBS) have become popular for representing real-world entities and their relationships, and offering convenient query languages based on graph pattern matching. As graphs increase in size and complexity, GDBS need to provide the appropriate support for abstraction for which views have demonstrated to be an effective tool, facilitating query writing and improving query execution time via materialization techniques. This paper explores how views can be de≈ëned and used in GDBS. We propose view-based extensions to the widely used graph query language Cypher, explore a wide range of possible view types, and outline several implementation strategies for view materialization. Using a set of micro- and macro-benchmarks, we provide insight into how expressive different view types are and how effective the proposed implementation strategies are for different GDBS. Our results show that views can be a powerful tool for GDBS, offering great ≈ïexibility in query expression and providing performance improvements if materialized.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/G-View%3A%20View%20Management%20for%20Graph%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1730-zheng.pdf",
    "session": "Research 35: Database Engines for Graphs",
    "authors": [
      {
        "Name": "Yunjia Zheng",
        "Affiliation": "McGill University"
      },
      {
        "Name": "Charlotte Sacr√©",
        "Affiliation": "McGill University"
      },
      {
        "Name": "Mohanna Shahrad",
        "Affiliation": "Princeton University"
      },
      {
        "Name": "Owen Lipchitz",
        "Affiliation": "McGill"
      },
      {
        "Name": "YuTing Gu",
        "Affiliation": "McGill University"
      },
      {
        "Name": "Bettina Kemme",
        "Affiliation": "McGill University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6ffea541-8c4d-45c2-ba98-8ddf5e5f217e",
    "title": "Hint-QPT: Hints for Robust Query Performance Tuning",
    "abstract": "Query optimizers rely heavily on selectivity estimates to choose efficient execution plans, but inaccuracies in these estimates often result in poor query performance. We introduce  Hint-QPT ( Hint s for Robust  Q uery  P erformance  T uning), an interactive tool designed to help users diagnose and improve query performance. Hint-QPT  proactively recommends robust plans that are resilient to uncertainty in selectivity estimates, identifies sensitive subqueries for which selectivity estimation errors greatly affect plan quality, and provides intuitive interfaces for targeted selectivity adjustments. Users can either choose the recommended robust plans for execution, or acquire additional statistics on the identified sensitive subqueries to tune query performance. Moreover,  Hint-QPT visualizes the alternative execution plans and their costs under uncertainty, helping users to better understand their robustness.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Hint-QPT%3A%20Hints%20for%20Robust%20Query%20Performance%20Tuning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5327-xiu.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Haibo Xiu",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Yang Li",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Qianyu Yang",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Weihang Guo",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Yuxi Liu",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Pankaj Agarwal",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Sudeepa Roy",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Jun Yang",
        "Affiliation": "Duke University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e8615e12-ed1b-48a9-986d-96088fffbb51",
    "title": "K2: On Optimizing Distributed Transactions in a Multi-region Data Store with True-time Clocks",
    "abstract": "TrueTime clocks (TTCs) that offer accurate and reliable time within limited uncertainty bounds have been increasingly implemented in many clouds. Multi-region data stores that seek decentralized synchronization for high performance represent an ideal application of TTC. However, the co-designs between the two often failed to realize their full potential. This paper proposes K2, a multi-region data store that explores the opportunity of using TTC for distributed transactions. Compared to its pioneer, Google Spanner, K2 augments TTC‚Äôs semantics in three core design pillars. First, K2 carries a new timestamp-generating scheme that is capable of providing a small time uncertainty bound at scale. Second, K2 revitalizes existing multi-version timestamp-ordered concurrency control to realize multi-version properties for read-write transactions. Third, K2 introduces a new TTC-based visibility control protocol that provides efficient reads at replicas. Our evaluation shows that, K2 achieves an order of magnitude higher transaction throughput relative to other geo-distributed transaction protocols while ensuring a lower visibility delay at asynchronous replicas.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/K2%3A%20On%20Optimizing%20Distributed%20Transactions%20in%20a%20Multi-region%20Data%20Store%20with%20True-time%20Clocks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1756-song.pdf",
    "session": "Research 48: Distributed Transactions II",
    "authors": [
      {
        "Name": "Haoze Song",
        "Affiliation": "The University of Hong Kong"
      },
      {
        "Name": "Yongqi Wang",
        "Affiliation": "SJTU"
      },
      {
        "Name": "Xusheng Chen",
        "Affiliation": "The University of Hong Kong"
      },
      {
        "Name": "Hao Feng",
        "Affiliation": "Huawei Cloud"
      },
      {
        "Name": "Yazhi Feng",
        "Affiliation": "HUAWEI"
      },
      {
        "Name": "xieyun fang",
        "Affiliation": "huawei"
      },
      {
        "Name": "Heming Cui",
        "Affiliation": "The University of Hong Kong"
      },
      {
        "Name": "Linghe Kong",
        "Affiliation": "Shanghai Jiao Tong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b855437a-09d2-4eb9-a795-a6740e40ac8b",
    "title": "Large Language Models for Spatial Analysis Queries",
    "abstract": "This tutorial provides a comprehensive overview of the research landscape of employing Large Language Models (LLMs) to spatial analysis queries. The tutorial categorizes the research in this area based on how LLMs are employed to serve such queries. This goes from employing LLMs as is, to fine-tuning LLMs, to completely retrain LLM architectures, to modifying the LLM internals to fit spatial queries. The tutorial concludes by a set of benchmarks and pointing out to research gaps and future research directions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Large%20Language%20Models%20for%20Spatial%20Analysis%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5451-mokbel.pdf",
    "session": "Tutorial 5: Large Language Models for Spatial Analysis Queries",
    "authors": [
      {
        "Name": "Youssef Hussein",
        "Affiliation": "University of Minnesota"
      },
      {
        "Name": "Mohamed Hemdan",
        "Affiliation": "University of Minnesota"
      },
      {
        "Name": "Mohamed Mokbel",
        "Affiliation": "University of Minnesota - Twin Cities"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0bff201a-e8b9-46a0-9f63-01092389edf3",
    "title": "Nitro: Boosting Distributed Reinforcement Learning with Serverless Computing",
    "abstract": "Deep reinforcement learning (DRL) has demonstrated significant potential in various applications, including gaming AI, robotics, and system scheduling. DRL algorithms produce, sample, and learn from training data online through a trial-and-error process, demanding considerable time and computational resources. To address this, distributed DRL algorithms and paradigms have been developed to expedite training using extensive resources. Through carefully designed experiments, we are the first to observe that strategically increasing the actor-environment interactions by spawning more concurrent actors at certain training rounds within ephemeral time frames can significantly enhance training efficiency. Yet, current distributed DRL solutions, which are predominantly server-based (or serverful), fail to capitalize on these opportunities due to their long startup times, limited adaptability, and cumbersome scalability.\nThis paper proposes Nitro, a generic training engine for dis- tributed DRL algorithms that enforces timely and effective boost- ing with concurrent actors instantaneously spawned by serverless computing. With serverless functions, Nitro adjusts data sampling strategies dynamically according to the DRL training demands. Ni- tro seizes the opportunity of real-time boosting by accurately and swiftly detecting an empirical metric. To achieve cost efficiency, we design a heuristic actor scaling algorithm to guide Nitro for cost-aware boosting budget allocation. We integrate Nitro with state-of-the-art DRL algorithms and frameworks and evaluate them on AWS EC2 and Lambda. Experiments with Mujoco and Atari benchmarks show that Nitro improves the final rewards (i.e., train- ing quality) by up to 6√ó and reduces training costs by up to 42%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Nitro%3A%20Boosting%20Distributed%20Reinforcement%20Learning%20with%20Serverless%20Computing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p66-yu.pdf",
    "session": "Research 23: Applied ML and AI for Data Management III",
    "authors": [
      {
        "Name": "Hanfei Yu",
        "Affiliation": "Stevens Institute of Technology"
      },
      {
        "Name": "Jacob Carter",
        "Affiliation": "Louisiana State University"
      },
      {
        "Name": "Hao Wang",
        "Affiliation": "Louisiana State University"
      },
      {
        "Name": "Devesh Tiwari",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Jian Li",
        "Affiliation": "Stony Brook University"
      },
      {
        "Name": "Seung-Jong Park",
        "Affiliation": "Missouri University of Science & Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "78600163-501c-4f87-ab04-bfd35685ec46",
    "title": "Instance-Optimal Acyclic Join Processing Without Regret: Engineering the Yannakakis Algorithm in Column Stores",
    "abstract": "Acyclic join queries can be evaluated instance-optimally using Yannakakis‚Äô algorithm, which avoids needlessly large intermediate results through semi-join passes. Recent work proposes to address the significant hidden constant factors arising from a naive implementation of Yannakakis by decomposing the hash join operator into two suboperators, called Lookup and Expand. We present a novel method for integrating Lookup and Expand plans in interpreted environments, like column stores, formalizing them using Nested Semijoin Algebra (NSA) and implementing them through a shredding approach. We characterize the class of NSA expressions that can be evaluated instance-optimally as those that are 2-phase: no ‚Äòshrinking‚Äô operator is applied after an unnest (i.e., expand). We introduce Shredded Yannakakis ( SYA ), an evaluation algorithm for acyclic joins that, starting from a binary join plan, transforms it into a 2-phase NSA plan, and then evaluates it through the shredding technique. We show that  SYA  is provably robust (i.e., never produces large intermediate results) and without regret (i.e., is never worse than the binary join plan under a suitable cost model) on the class of well-behaved binary join plans. Our experiments on a suite of 1,849 queries show that  SYA  improves performance for 85 . 3% of the queries with speedups up to 62.5x, while remaining competitive on the other queries.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Instance-Optimal%20Acyclic%20Join%20Processing%20Without%20Regret%3A%20Engineering%20the%20Yannakakis%20Algorithm%20in%20Column%20Stores",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2413-vansummeren.pdf",
    "session": "Research 42: Data Models and Query Languages",
    "authors": [
      {
        "Name": "Liese Bekkers",
        "Affiliation": "Hasselt University"
      },
      {
        "Name": "Frank Neven",
        "Affiliation": "Hasselt University"
      },
      {
        "Name": "Stijn Vansummeren",
        "Affiliation": "Hasselt University"
      },
      {
        "Name": "Remy Wang",
        "Affiliation": "University of California, Los Angeles"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "80fb3ea8-7d09-4b98-af85-67b29651f4c3",
    "title": "Styx in Action: Transactional Cloud Applications Made Easy",
    "abstract": "Developing and deploying transactional cloud applications such as banking and e-commerce systems is a daunting task for developers. The reason for this diÔ¨Éculty is twofold. First, developing such applications shifts the developers‚Äô focus from the application logic to considerations of distributed transactions, fault-tolerance, consistency, and scalability. Second, deploying such applications involves multiple systems, such as databases, load balancers, or containerized services, impeding eÔ¨Écient resource management. \nThis demonstration presents Styx, a scalable application runtime that allows developers to build scalable and transactional cloud applications with minimal eÔ¨Äort. It supports serializability and exactly-once guarantees and focuses on the ease of development and deployment, as well as Styx‚Äôs fault-tolerance mechanisms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Styx%20in%20Action%3A%20Transactional%20Cloud%20Applications%20Made%20Easy",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5275-psarakis.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Kyriakos Psarakis",
        "Affiliation": "TU Delft"
      },
      {
        "Name": "Oto Mraz",
        "Affiliation": "TU Delft"
      },
      {
        "Name": "George Christodoulou",
        "Affiliation": "TU Delft"
      },
      {
        "Name": "George Siachamis",
        "Affiliation": "Inria & Institut Polytechnique de Paris"
      },
      {
        "Name": "Marios Fragkoulis",
        "Affiliation": "TU Delft"
      },
      {
        "Name": "Asterios Katsifodimos",
        "Affiliation": "TU Delft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1b9f9c75-30ca-479c-bca1-330227b01538",
    "title": "BIRDIE: Natural Language-Driven Table Discovery Using Differentiable Search Index",
    "abstract": "Natural language (NL)-driven table discovery identifies relevant tables from large table repositories based on NL queries. While current deep-learning-based methods using the traditional dense vector search pipeline, i.e.,  representation-index-search , achieve remarkable accuracy, they face several limitations that impede further performance improvements: (i) the errors accumulated during the table representation and indexing phases affect the subsequent search accuracy; and (ii) insufficient query-table interaction hinders effective semantic alignment, impeding accuracy improvements. In this paper, we propose a novel framework Birdie, using a differentiable search index. It unifies the indexing and search into a single encoder-decoder language model, thus getting rid of error accumulations. Birdie first assigns each table a prefix-aware identifier and leverages a large language model-based query generator to create synthetic queries for each table. It then encodes the mapping between synthetic queries/tables and their corresponding table identifiers into the parameters of an encoder-decoder language model, enabling deep query-table interactions. During search, the trained model directly generates table identifiers for a given query. To accommodate the continual indexing of dynamic tables, we introduce an index update strategy via parameter isolation, which mitigates the issue of catastrophic forgetting. Extensive experiments demonstrate that Birdie outperforms state-of-the-art dense methods by 16.8% in accuracy, and reduces forgetting by over 90% compared to other continual learning approaches.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/BIRDIE%3A%20Natural%20Language-Driven%20Table%20Discovery%20Using%20Differentiable%20Search%20Index",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2070-gao.pdf",
    "session": "Research 30: Information Integration and Data Quality I",
    "authors": [
      {
        "Name": "Yuxiang Guo",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "zhonghao hu",
        "Affiliation": "zhejiang University"
      },
      {
        "Name": "Yuren Mao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Baihua Zheng",
        "Affiliation": "Singapore Management University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Mingwei Zhou",
        "Affiliation": "Zhejiang Dahua Technology Co.,Ltd."
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9398bbac-9ac2-4d0d-a3bd-bf58fe49a041",
    "title": "The Accuracy of Cardinality Estimators: Unraveling the Evaluation Result Conundrum",
    "abstract": "Existing research on the accuracy of cardinality estimators generally suffers from a lack of diversity and sufficient quantity of their experimental datasets, particularly in relation to the claimed scope of the study and the generality of its conclusions. We argue that a sufficiently large number of varied datasets are essential for comprehensive evaluations. However, the prevailing per-dataset evaluation method (PDE), producing one result table per dataset, has so far hindered this necessary expansion of the experiments. Moreover, as we demonstrate, this evaluation method often leaves the reader with contradictory results, where one estimator excels on certain datasets or queries, while the other performs better elsewhere. To address these and similar limitations, we propose a multidimensional evaluation framework. This framework unravels the conundrum of analyzing the evaluation results across multiple datasets through the use of discretization. It establishes a robust foundation for aggregating the evaluation results and conducting pairwise comparisons between estimators. Furthermore, it facilitates informed decision making in the presence of conflicting results through a customizable ranking mechanism. \nTo empirically highlight the shortcomings of the aforementioned per-dataset evaluation and demonstrate the advantages of our proposed framework, we conduct a benchmarking study of cardinality estimators, incorporating both learned and traditional approaches. We focus on a fundamental challenge: estimating the cardinality of range queries on a single 2-D geographical relation in a static environment. Despite the apparent simplicity of this task, our findings reveal that many estimators struggle to handle this challenge effectively. To further enhance the quality of our study, we provide valuable insights by addressing some critical aspects that were overlooked in previous benchmarking studies.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20Accuracy%20of%20Cardinality%20Estimators%3A%20Unraveling%20the%20Evaluation%20Result%20Conundrum",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3744-rashedi.pdf",
    "session": "Research 44: Database Engine Performance and Manageability I",
    "authors": [
      {
        "Name": "Nazanin Rashedi",
        "Affiliation": "University of Mannheim"
      },
      {
        "Name": "Guido Moerkotte",
        "Affiliation": "University of Mannheim"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e33d3b45-cea5-4a05-bbfe-f3132c9eed08",
    "title": "HoliPaxos: Towards More Predictable Performance in State Machine Replication",
    "abstract": "State machine replication (SMR) algorithms ensure redundancy in critical systems and, as a result, underpin fault-tolerant distributed databases. Good SMR protocol performance is essential for capacity planning and meeting desired performance objectives. However, many implementations of popular SMR algorithms, such as MultiPaxos and Raft, have issues that make their performance unpredictable. This unpredictability often arises from certain ‚Äúbolt-on‚Äù additions to core protocols, such as external failure detectors and replication log compaction. In this paper, we argue that tighter integration of such traditionally ad-hoc mechanisms with the core replication protocols can stabilize performance, making the solutions more reliable and more accessible to accurate capacity planning. Moreover, we show that these integrations can be nondisruptive for the underlying consensus algorithm, resulting in systems that preserve the simplicity and safety of traditional singleleader consensus-based SMR. To that order, we integrate the failure and slowdown detectors inside the SMR and achieve better performance and faster fail-over under various network partitions and node slowdown events. We also illustrate that tight integration of replication log management, pruning, and snapshotting can reduce memory and CPU usage while avoiding performance fluctuations associated with traditional log compaction and cleanup approaches.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/HoliPaxos%3A%20Towards%20More%20Predictable%20Performance%20in%20State%20Machine%20Replication",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2505-charapko.pdf",
    "session": "Research 1: Cloud Data Management",
    "authors": [
      {
        "Name": "Zhiying Liang",
        "Affiliation": "The Pennsylvania State University"
      },
      {
        "Name": "Vahab Jabrayilov",
        "Affiliation": "Columbia University"
      },
      {
        "Name": "Abutalib Aghayev",
        "Affiliation": "Penn State"
      },
      {
        "Name": "Aleksey Charapko",
        "Affiliation": "University of New Hampshire"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2bd2f753-3fb0-4ab7-babb-55fdb56673d1",
    "title": "Maximum Inner Product is Query-Scaled Nearest Neighbor",
    "abstract": "Maximum Inner Product Search ( MIPS ) for high-dimensional vectors is pivotal across databases, information retrieval, and artificial intelligence. Existing methods either reduce  MIPS  to Nearest Neighbor Search ( NNS ) while suffering from harmful vector space transformations, or attempt to tackle  MIPS  directly but struggle to mitigate redundant computations due to the absence of the triangle inequality. This paper presents a novel theoretical framework that equates  MIPS  with  NNS  without requiring space transformation, thereby allowing us to leverage advanced graph-based indices for NNS  and efficient edge pruning strategies, significantly reducing unnecessary computations. Despite a strong baseline set by our theoretical analysis, we identify and address two persistent challenges to further refine our method: the introduction of the   P roximity Graph with   S pherical   P athway ( PSP ), designed to mitigate the issue of  MIPS  solutions clustering around large-norm vectors, and the implementation of   A daptive   E arly   T ermination (AET), which efficiently curtails the excessive exploration once an accuracy bottleneck is reached. Extensive experiments reveal that our method is superior to existing state-of-the-art techniques in search efficiency, scalability, and practical applicability. Compared with state-of-theart graph-based methods, it achieves an average 35% speed-up in query processing and a 3 √ó  reduction in index size. Notably, our approach has been validated and deployed in the search engines of Shopee, a well-known online shopping platform. Our code and an industrial-scale dataset for offline evaluation will also be released to address the absence of e-commerce data in public benchmarks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Maximum%20Inner%20Product%20is%20Query-Scaled%20Nearest%20Neighbor",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1770-ke.pdf",
    "session": "Research 3: Vector Data Management I",
    "authors": [
      {
        "Name": "Tingyang Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Cong Fu",
        "Affiliation": "Shopee Pte. Ltd."
      },
      {
        "Name": "Kun Wang",
        "Affiliation": "Shopee Pte. Ltd."
      },
      {
        "Name": "Xiangyu Ke",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Wenchao Zhou",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Yabo Ni",
        "Affiliation": "NTU - NANYANG TECHNOLOGICAL UNIVERSITY"
      },
      {
        "Name": "ANXIANG ZENG",
        "Affiliation": "Nanyang Technological University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0b22918d-5ec0-4ba4-95fc-a4709e3be4a0",
    "title": "LakeVisage: Towards Scalable, Flexible and Interactive Visualization Recommendation for Data Discovery over Data Lakes",
    "abstract": "Data discovery from data lakes is an essential application in modern data science. While many previous studies focused on improving the efficiency and effectiveness of data discovery, little attention has been paid to the usability of such applications. In particular, exploring data discovery results can be cumbersome due to the cognitive load involved in understanding raw tabular results and identifying insights to draw conclusions. To address this challenge, we introduce a new problem: visualization recommendation for data discovery over data lakes, which aims to automatically identify visualizations that highlight relevant or desired trends in the results returned by data discovery engines. We propose  LakeVisage , an end-to-end framework as the first solution to this problem. Given a data lake, a data discovery engine, and a user-specified query table, LakeVisage  intelligently explores the space of visualizations and recommends the most useful and ‚Äúinteresting‚Äù visualization plans. To this end, we developed (i) approaches to smartly construct the candidate visualization plans from the results of the data discovery engine and (ii) effective pruning strategies to filter out less interesting plans so as to accelerate the visual analysis. Experimental results on real data lakes demonstrate that our proposed techniques can achieve an order-of-magnitude speedup in visualization recommendation. We also conduct a comprehensive user study to demonstrate that  LakeVisage  offers convenience to users in real data analysis applications by enabling them seamlessly get started with the tasks and performing explorations flexibly.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LakeVisage%3A%20Towards%20Scalable%2C%20Flexible%20and%20Interactive%20Visualization%20Recommendation%20for%20Data%20Discovery%20over%20Data%20Lakes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3545-wang.pdf",
    "session": "Research 43: Information Integration and Data Quality II",
    "authors": [
      {
        "Name": "Yihao Hu",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Jin Wang",
        "Affiliation": "Megagon Labs"
      },
      {
        "Name": "Sajjadur Rahman",
        "Affiliation": "Megagon Labs"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e8387e98-de68-4109-8bf5-1a6adf32053f",
    "title": "Steiner-Hardness: A Query Hardness Measure for Graph-Based ANN Indexes",
    "abstract": "Graph-based indexes have been widely employed to accelerate approximate similarity search of high-dimensional vectors. However, the performance of graph indexes to answer different queries varies vastly, leading to an unstable quality of service for downstream applications. This necessitates an effective measure to test query hardness on graph indexes. Nonetheless, popular distance-based hardness measures like LID lose their effects due to the ignorance of the graph structure. In this paper, we propose ùëÜùë°ùëíùëñùëõùëíùëü-hardness, a novel connection-based graph-native query hardness measure. Specifically, we first propose a theoretical framework to analyze theminimumqueryeffortongraphindexesandthendefineùëÜùë°ùëíùëñùëõùëíùëü-hardness as the minimum effort on a representative graph. Moreover, we prove that our ùëÜùë°ùëíùëñùëõùëíùëü-hardness is highly relevant to the classical Directed ùëÜùë°ùëíùëñùëõùëíùëü Tree (DST) problems. In this case, we design a novel algorithm to reduce our problem to DST problems and then leverage their solvers to help calculate ùëÜùë°ùëíùëñùëõùëíùëü-hardness efficiently. Compared with LID and other similar measures, ùëÜùë°ùëíùëñùëõùëíùëü-hardness shows a significantly better correlation with the actual query effort on various datasets. Additionally, an unbiased evaluation designed based on ùëÜùë°ùëíùëñùëõùëíùëü-hardness reveals new ranking results, indicating a meaningful direction for enhancing the robustness of graph indexes.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Steiner-Hardness%3A%20A%20Query%20Hardness%20Measure%20for%20Graph-Based%20ANN%20Indexes",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4668-wang.pdf",
    "session": "Research 22: Views, Indexing, and Search II",
    "authors": [
      {
        "Name": "Zeyu Wang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Qitong Wang",
        "Affiliation": "Harvard University"
      },
      {
        "Name": "Xiaoxing Cheng",
        "Affiliation": "Tongji University"
      },
      {
        "Name": "Peng Wang",
        "Affiliation": "\" Fudan University, China\""
      },
      {
        "Name": "Themis Palpanas",
        "Affiliation": "Universit√© Paris Cit√©"
      },
      {
        "Name": "Wei Wang",
        "Affiliation": "\" Fudan University, China\""
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "46cda591-7972-4311-8170-bca199e6c64f",
    "title": "Conformal Prediction for Verifiable Learned Query Optimization",
    "abstract": "Query optimization is critical in relational databases. Recently, numerous Learned Query Optimizers (LQOs) have been proposed, demonstrating superior performance over traditional hand-crafted query optimizers after short training periods. However, the opacity and instability of machine learning models have limited their practical applications. To address this issue, we are the first to formulate the LQO verification as a Conformal Prediction (CP) problem. We first construct a CP model and obtain user-controlled bounded ranges for the actual latency of LQO plans before execution. Then, we introduce CP-based runtime verification along with violation handling to ensure performance prior to execution. For both scenarios, we further extend our framework to handle distribution shifts in the dynamic environment using adaptive CP approaches. Finally, we present CP-guided plan search, which uses actual latency upper bounds from CP to heuristically guide query plan construction. We integrated our verification framework into three LQOs (Balsa, Lero, and RTOS) and conducted evaluations on several workloads. Experimental results demonstrate that our method is both accurate and efficient. Our CP-based approaches achieve tight upper bounds, reliably detect and handle violations. Adaptive CP maintains accurate confidence levels even in the presence of distribution shifts, and the CP-guided plan search improves both query plan quality (up to 9.84x) and planning time, with a reduction of up to 74.4% for a single query and 9.96% across all test queries from trained LQOs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Conformal%20Prediction%20for%20Verifiable%20Learned%20Query%20Optimization",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2653-liu.pdf",
    "session": "Research 61: Learned Query Optimization",
    "authors": [
      {
        "Name": "Hanwen Liu",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Shashank Giridhara",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Ibrahim Sabek",
        "Affiliation": "University of Southern California"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5832c4b5-1e55-4709-a7ce-6f47551da8db",
    "title": "Evaluating Methods for Efficient Entity Count Estimation",
    "abstract": "The problem of estimating the size of a query result has a long history in data management. When the query performs entity resolution (aka record linkage or deduplication), the problem is that of estimating the number of distinct entities, referred to as the  entity count . This problem has received attention from the statistics community but it has been largely overlooked in the data management literature. In this work, we formally define the entity count problem from a data management perspective and decompose it into a framework of fundamental steps. We explore approaches from both statistics and data management, systematically identifying a design space for different pipelines that address this problem. Finally, we provide extensive experiments to highlight the strengths and weaknesses of these approaches on real-world benchmarks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Evaluating%20Methods%20for%20Efficient%20Entity%20Count%20Estimation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2589-mathew.pdf",
    "session": "Research 5: Data Cleaning and Preparation I",
    "authors": [
      {
        "Name": "Jerin George Mathew",
        "Affiliation": "Sapienza University"
      },
      {
        "Name": "Donatella Firmani",
        "Affiliation": "Sapienza University"
      },
      {
        "Name": "Divesh Srivastava",
        "Affiliation": "AT&T Chief Data Office"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9abf7cca-e557-499e-87e6-496f9e973a25",
    "title": "Evaluating Continuous Queries with Inconsistency Annotations",
    "abstract": "Continuous Queries (CQs) run inde!nitely, processing in!nite data streams and producing continuous outputs. They commonly use window functions to segment streams into !nite chunks for compu- tation. Ensuring data integrity in CQs is challenging, involving, for example, streaming joins for binary constraints. Current methods, like dropping or repairing inconsistent data, can harm throughput and increase latency. This paper proposes a novel approach using provenance-based techniques to map violations in input streams to CQ results with minimal overhead. This ensures continuous data flow and maintains the analytical integrity of CQs. Our study explores the feasibility and effciency of this method, addressing a signi!cant gap in applying provenance techniques to streaming data. While provenance-based techniques have proven effective for static data, their application in streaming contexts remains unexplored. Our solution addresses this gap, achieving a stable throughput across increasingly demanding memory loads wrt to the baselines, spacing between a 10% increase for medium-sized buffers (i.e., the windows), up to 80% for heavier loads. Moreover, results show the minimal impact of annotation (up to 25%) in the total execution runtime, demonstrating the effectiveness of our graph-based approach.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Evaluating%20Continuous%20Queries%20with%20Inconsistency%20Annotations",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1321-langhi.pdf",
    "session": "Research 64: Database Engines and Architectures",
    "authors": [
      {
        "Name": "Samuele Langhi",
        "Affiliation": "Lyon 1 University"
      },
      {
        "Name": "Angela Bonifati",
        "Affiliation": "Univ. of Lyon"
      },
      {
        "Name": "Riccardo Tommasini",
        "Affiliation": "INSA Lyon - LIRIS"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7dc15f63-851d-47e1-877b-798a21d82882",
    "title": "LEAP: A Low-cost Spark SQL Query Optimizer using Pairwise Comparison",
    "abstract": "Selecting a good execution plan can significantly improve the query efficiency of Spark SQL. Several machine learning-based techniques have been proposed to select good execution plans for DBMS, but none of them perform well on Spark SQL due to the following issues. (1) Limited compatibility with Spark SQL: these approaches rely on physical operator enumeration, while Spark SQL doesn‚Äôt sup-on physical operator enumeration, while Spark SQL doesn‚Äôt support it; (2) Unreliable cost estimation: they often select execution plans with poor performance due to inaccurate cost estimation; (3) Time-consuming plan enumeration: they take much time to generate a large number of candidate execution plans in Spark SQL. To overcome these issues, in this paper, we propose LEAP, the first learned query optimizer tailored for Spark SQL, which can be inte-learned query optimizer tailored for Spark SQL, which can be integrated seamlessly into Spark SQL and solves the compatibility issue. Also, to avoid the unreliable cost value estimation, LEAP selects execution plans with an estimation-free method, which directly per-execution plans with an estimation-free method, which directly performs comparisons between the plans. Furthermore, LEAP employs an efficient progressive plan enumeration algorithm with pruning techniques to find better plans with fewer enumerations. Extensive experiments on three public benchmarks show the effectiveness of LEAP. It reduces the end-to-end execution time of the native optimizer by up to 54% and other learned methods by up to 94%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LEAP%3A%20A%20Low-cost%20Spark%20SQL%20Query%20Optimizer%20using%20Pairwise%20Comparison",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p675-chen.pdf",
    "session": "Research 61: Learned Query Optimization",
    "authors": [
      {
        "Name": "Junhao Ye",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Jiahui Li",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lu Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yuren Mao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Tianyi Li",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "da813fba-ae97-4974-95f3-8da05c40396d",
    "title": "Substructure-aware Log Anomaly Detection",
    "abstract": "System logs, recording critical information about system opera-System logs, recording critical information about system operations, serve as indispensable tools for system anomaly detection. Graph-based methods have demonstrated superior performance compared to other methods in capturing the interdependencies of log events. However, existing methods often neglect the com-of log events. However, existing methods often neglect the complex substructure patterns of nodes within log graphs, making it challenging to capture the subtle alteration in event type, struc-challenging to capture the subtle alteration in event type, structure, and the location of exceptions that indicate node anomalies. To address this limitation, this paper proposes a novel framework called Substructure-aware Log Anomaly Detection at Code File Level ( SLAD ). It first introduces a Monte Carlo Tree Search strategy tailored specifically for log anomaly detection to discover repre-tailored specifically for log anomaly detection to discover representative substructures. Then,  SLAD  incorporates a substructure distillation way to enhance the efficiency of anomaly inference based on the representative substructures. After that, we introduce a soft pruning to obtain key substructure for nodes. Experimental results show  SLAD  outperforms all baselines. Particularly,  SLAD demonstrates at least 15 times faster than substructure-based graph learning methods in anomaly inference.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Substructure-aware%20Log%20Anomaly%20Detection",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p213-tang.pdf",
    "session": "Research 6: Graph Data Management I",
    "authors": [
      {
        "Name": "Yanni Tang",
        "Affiliation": "The University of Auckland"
      },
      {
        "Name": "Zhuoxing Zhang",
        "Affiliation": "The University of Auckland"
      },
      {
        "Name": "Kaiqi Zhao",
        "Affiliation": "University of Auckland"
      },
      {
        "Name": "Lanting Fang",
        "Affiliation": "Southeast University"
      },
      {
        "Name": "Zhenhua Li",
        "Affiliation": "Southwest University"
      },
      {
        "Name": "Wu Chen",
        "Affiliation": "College of Computer ÔºÜ Information Science, Southwest University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "071ea175-523d-4950-b7f1-0cfb6b982d9c",
    "title": "The LAW theorem: Local Reads and Linearizable Asynchronous Replication",
    "abstract": "Distributed datastores underpin highly concurrent, read-intensive applications, ensuring consistency, availability, and performance. They use crash-tolerant protocols to replicate data and endure replica server crashes. To ensure safety and meet the performance demands, replication must support high-throughput, strongly consistent (i.e., linearizable) reads without assuming any synchrony. However, existing protocols either \n1  relax consistency, or provide linearizable reads that are \n2  fully asynchronous but remote (involving multiple replicas), or \n3  local but require synchrony. This work explores the tradeoÔ¨Äs between consistency, asynchrony, and performance in crash-tolerant protocols, and proves that  in linearizable asynchronous read/write registers tolerating a single crash, no reads can be local . Building on this, we introduce almost-local reads  (ALRs), a new abstraction that ensures crash tolerance and linearizability under asynchrony. While ALRs have slightly higher latency than local reads, they remain lightweight, with computation and network costs close to single-node reads. \nWe present two simple yet eÔ¨Äective ALR schemes that enhance protocols across all three categories. For protocols with local reads, ALRs address consistency or synchrony issues with minimal throughput loss. In asynchronous linearizable protocols, they improve performance without compromises. Our evaluation shows that ALRenhanced ZAB and Hermes achieve within 2% and 5% of their original throughput in 95% reads while ensuring linearizability under asynchrony. On Raft, ALRs deliver over 2.5√ó higher throughput without compromising consistency or asynchrony.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20LAW%20theorem%3A%20Local%20Reads%20and%20Linearizable%20Asynchronous%20Replication",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2831-giortamis.pdf",
    "session": "Research 48: Distributed Transactions II",
    "authors": [
      {
        "Name": "Emmanouil Giortamis",
        "Affiliation": "TU Munich"
      },
      {
        "Name": "Antonios Katsarakis",
        "Affiliation": "Huawei Research"
      },
      {
        "Name": "Vasilis Gavrielatos",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Pramod Bhatotia",
        "Affiliation": "TU Munich"
      },
      {
        "Name": "Aleksandar Dragojevic",
        "Affiliation": "-"
      },
      {
        "Name": "Boris Grot",
        "Affiliation": "University of Edinburgh"
      },
      {
        "Name": "Vijay Nagarajan",
        "Affiliation": "University of Utah"
      },
      {
        "Name": "Panagiota Fatourou",
        "Affiliation": "University of Crete"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "31a33f74-5461-4617-a678-bdbf27ad6054",
    "title": "The Power of Constraints in Natural Language to SQL Translation",
    "abstract": "Current large language model (LLM)-based Natural Language to SQL (NL2SQL) approaches typically rely on the database schema and partial data values for the translation. These approaches are unable to use sufficient data for accurate database understanding due to limitations in data selection methods, and they cannot input the entire database due to the limited context window sizes of LLMs. This insufficient data integration may result in an incomplete understanding of the database, leading to semantically incorrect SQL generation. In this paper, we introduce REDSQL, a novel plugand-play framework that refines the predicted SQL by utilizing the entire database in the refinement process. The core idea of REDSQL is to enhance SQL refinement by identifying potential errors based on the database content, which is achieved by applying constraints on the input relations of query operations. LLMs can refine the SQL using SQL-related information extracted by REDSQL, which provides concise and informative insights into the database. Additionally, REDSQL enhances schema semantics by integrating data profiling for more effective database utilization. Our experiments demonstrate that REDSQL consistently improves the performance of existing NL2SQL approaches across five benchmarks. Specifically, REDSQL elevates the accuracy of CODES to 67.3% (+8.8%) and PURPLE to 67.7% (+11.1%) on the  Bird  benchmark.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20Power%20of%20Constraints%20in%20Natural%20Language%20to%20SQL%20Translation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2097-ren.pdf",
    "session": "Research 7: Natural Language Interfaces to Data",
    "authors": [
      {
        "Name": "Tonghui Ren",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Chen Ke",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Yuankai Fan",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Yinan Jing",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Zhenying He",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Kai Zhang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "X. Sean Wang",
        "Affiliation": "Fudan University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "03a61ecb-7307-4945-ba08-ce4ccbe10027",
    "title": "PrivAGM: Secure Construction of Differentially Private Directed Attributed Graph Models on Decentralized Social Graphs",
    "abstract": "Decentralized social graphs, where no single entity possesses the information of the entire graph, and each user maintains only a limited view of the graph, contain great value for different applications. However, simply collecting local views for analytics raises privacy concerns due to the sensitive information of social relationships they capture. To address this, a canonical approach involves privately fitting a generative graph model to the decentralized social graph, generating a differentially private synthetic graph that serves as a proxy for analytics. Existing solutions, however, often fail to capture the inherent directionality of edges and attributeedge correlations when dealing with decentralized directed social graphs, leading to synthetic graphs with poor utility. To bridge this gap, we present PrivAGM, a new solution that harnesses the synergies among differential privacy, secure multiparty computation, and generative graph models, enabling the secure construction of differentially private directed attributed graph models on decentralized social graphs while ensuring the privacy preservation of individuals. We evaluate PrivAGM on three real-world directed social graph datasets. The results show that PrivAGM outperforms the stateof-the-art methods, generating synthetic graphs with significantly higher utility.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PrivAGM%3A%20Secure%20Construction%20of%20Differentially%20Private%20Directed%20Attributed%20Graph%20Models%20on%20Decentralized%20Social%20Graphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4682-zheng.pdf",
    "session": "Research 66: Access Control and Privacy II",
    "authors": [
      {
        "Name": "Songlei Wang",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Yifeng Zheng",
        "Affiliation": "The Hong Kong Polytechnic University"
      },
      {
        "Name": "Xiaohua Jia",
        "Affiliation": "City University of Hong Kong"
      },
      {
        "Name": "Haibo Hu",
        "Affiliation": "The Hong Kong Polytechnic University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "382bc7d3-13eb-4656-b9f9-8405dba2e3b2",
    "title": "Opening The Black-Box: Explaining Learned Cost Models For Databases",
    "abstract": "Learned Cost Model s ( LCM s) have shown superior results over traditional database cost models as they can significantly improve the accuracy of cost predictions. However,  LCM s still fail for some query plans, as prediction errors can be large in the tail. Unfortunately, recent  LCM s are based on complex deep ne ural models, and thus, there is no easy way to understand where this accuracy drop is rooted, which critically prevents systematic troubleshooting. In this demo paper, we present the very first approach for opening the black box by bringing AI explainability approaches to  LCM s. As a core contribution, we developed new explanation techniques that extend existing methods that are available for the general explainability of AI models and adapt them significantly to be usable for  LCM s. In our demo, we provide an interactive tool to showcase how explainability for  LCM s works. We believe this is a first step for making  LCM s debuggable and thus paving the road for new approaches for systematically fixing problems in LCMs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Opening%20The%20Black-Box%3A%20Explaining%20Learned%20Cost%20Models%20For%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5255-heinrich.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Roman Heinrich",
        "Affiliation": "DFKI Darmstadt & TU Darmstadt"
      },
      {
        "Name": "Oleksandr Havrylov",
        "Affiliation": "TU Darmstadt & exocad GmbH"
      },
      {
        "Name": "Manisha Luthra",
        "Affiliation": "DFKI Darmstadt & TU Darmstadt"
      },
      {
        "Name": "Johannes Wehrstein",
        "Affiliation": "TU Darmstadt"
      },
      {
        "Name": "Carsten Binnig",
        "Affiliation": "DFKI Darmstadt & TU Darmstadt"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "98834254-209d-4c20-83a1-0bbd74a3e97b",
    "title": "A Systematic Study on Early Stopping Metrics in HPO and the Implications of Uncertainty",
    "abstract": "The development of hyperparameter optimization (HPO) algorithms is an important topic within both the machine learning and data management domains. While numerous strategies employing early stopping mechanisms have been proposed to bolster HPO e!ciency, there remains a notable deffciency in understanding how the selection of early stopping metrics inffuences the reliability of early stopping decisions and, by extension, the broader HPO outcomes. This paper undertakes a systematic exploration of the impact of metric selection on the effectiveness of early stopping-based HPO. Specifically, we introduce a set of metrics that incorporate uncertainty and highlight their practical significance in enhancing the reliability of early stopping decisions. Our empirical experiments on HPO and NAS benchmarks show that using training loss as an early stopping metric in the early training stages improves HPO outcomes by up to 24.76% compared to the more widely accepted validation loss. Furthermore, integrating uncertainty into the metric yields an additional improvement of up to 4% under budget constraints, translating into meaningful resource savings and scalability benefits in large-scale HPO scenarios. These findings demonstrate the critical role of metric selection while shedding light on the potential implications of integrating uncertainty as a metric. This research provides empirical insights that serve as a compass for the selection and formulation of metrics, thereby contributing to a more profound comprehension of mechanisms underpinning early stopping-based HPO.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Systematic%20Study%20on%20Early%20Stopping%20Metrics%20in%20HPO%20and%20the%20Implications%20of%20Uncertainty",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1551-guan.pdf",
    "session": "Research 32: Data-centric Machine Learning",
    "authors": [
      {
        "Name": "Jiawei Guan",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Feng Zhang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Jiesong Liu",
        "Affiliation": "North Carolina State University"
      },
      {
        "Name": "Xiaoyong Du",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Xipeng Shen",
        "Affiliation": "North Carolina State University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f7860604-0ed0-48dd-bc44-11db5715619c",
    "title": "Efficient Data-aware Distance Comparison Operations for High-Dimensional Approximate Nearest Neighbor Search",
    "abstract": "High-dimensional approximate  ùêæ nearest neighbor search (AKNN) is a fundamental task for various applications, including informa-is a fundamental task for various applications, including information retrieval. Most existing algorithms for AKNN can be decom-tion retrieval. Most existing algorithms for AKNN can be decomposed into two main components, i.e., candidate generation and distance comparison operations (DCOs). While different methods have unique ways of generating candidates, they all share the same DCO process. In this study, we focus on accelerating the process of DCOs that dominates the time cost in most existing AKNN al-of DCOs that dominates the time cost in most existing AKNN algorithms. To achieve this, we propose an   D ata - A ware   D istance E stimation approach, called  DADE , which approximates the  exact distance in a lower-dimensional space. We theoretically prove that the distance estimation in  DADE  is  unbiased  in terms of data dis-the distance estimation in  DADE  is  unbiased  in terms of data distribution. Furthermore, we propose an optimized estimation based on the unbiased distance estimation formulation. In addition, we propose a hypothesis testing approach to adaptively determine the number of dimensions needed to estimate the  exact  distance with sufficient confidence. We integrate  DADE  into widely-used AKNN search algorithms, e.g.,  IVF  and  HNSW , and conduct extensive ex-search algorithms, e.g.,  IVF  and  HNSW , and conduct extensive experiments to demonstrate the superiority.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Data-aware%20Distance%20Comparison%20Operations%20for%20High-Dimensional%20Approximate%20Nearest%20Neighbor%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p812-zheng.pdf",
    "session": "Research 46: Vector Data Management III",
    "authors": [
      {
        "Name": "Liwei Deng",
        "Affiliation": "University of Electronic Science and Technology of China"
      },
      {
        "Name": "Penghao Chen",
        "Affiliation": "University of Electronic Science and Technology of China"
      },
      {
        "Name": "Ximu Zeng",
        "Affiliation": "University of Electronic Science and Technology of China"
      },
      {
        "Name": "Tianfu Wang",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Yan Zhao",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Kai Zheng",
        "Affiliation": "University of Electronic Science and Technology of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "330d3ae6-9057-4f6d-978c-963e0460e704",
    "title": "TabulaX: Leveraging Large Language Models for Multi-Class Table Transformations",
    "abstract": "The integration of tabular data from diverse sources is often hindered by inconsistencies in formatting and representation, posing significant challenges for data analysts and personal digital assistants. Existing methods for automating tabular data transformations are limited in scope, often focusing on specific types of transformations or lacking interpretability. In this paper, we introduce TabulaX, a novel framework that leverages Large Language Models (LLMs) for multi-class column-level tabular transformations. TabulaX first classifies input columns into four transformation types‚Äî string-based, numerical, algorithmic, and general‚Äîand then applies tailored methods to generate human-interpretable transformation functions, such as numeric formulas or programming code. This approach enhances transparency and allows users to understand and modify the mappings. Through extensive experiments on realworld datasets from various domains, we demonstrate that TabulaX outperforms existing state-of-the-art approaches in terms of accuracy, supports a broader class of transformations, and generates interpretable transformations that can be efficiently applied. KEYWORDS Large Language Models, Heterogeneous Table Join, Data Integration, Data Transformation, Data Cleaning and Transformation",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TabulaX%3A%20Leveraging%20Large%20Language%20Models%20for%20Multi-Class%20Table%20Transformations",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3826-nobari.pdf",
    "session": "Research 43: Information Integration and Data Quality II",
    "authors": [
      {
        "Name": "Arash Dargahi Nobari",
        "Affiliation": "University of Alberta"
      },
      {
        "Name": "Davood Rafiei",
        "Affiliation": "University of Alberta"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5a5c9b49-c381-4df3-a96b-f97918c024a2",
    "title": "Towards Automated Cross-domain Exploratory Data Analysis through Large Language Models",
    "abstract": "Exploratory data analysis (EDA), coupled with SQL, is essential for data analysts involved in data exploration and analysis. However, data analysts often encounter two primary challenges: (1) the need to craft SQL queries skillfully and (2) the requirement to generate suitable visualization types that enhance the interpretation of query results. Due to its signiÔ¨Åcance, substantial research eÔ¨Äorts have been made to explore diÔ¨Äerent approaches to address these challenges, including leveraging large language models (LLMs). However, existing methods fail to meet real-world data exploration requirements primarily due to (1) complex database schema, (2) unclear user intent, (3) limited cross-domain generalization capability, and (4) insuÔ¨Écient end-to-end text-to-visualization capability. \nThis paper presents TiInsight, an automated SQL-based crossdomain exploratory data analysis system. First, we propose a hierarchical data context (i.e., HDC), which leverages LLMs to summarize the contexts related to the database schema, which is crucial for open-world EDA systems to generalize across data domains. Second, the EDA system is divided into four components (i.e., stages): HDC generation, question clariÔ¨Åcation and decomposition, text-toSQL generation (i.e., TiSQL), and data visualization (i.e., TiChart). Finally, we implemented an end-to-end EDA system with a userfriendly GUI in the production environment at PingCAP. We have also open-sourced all APIs of TiInsight to facilitate research within the EDA community. Through extensive evaluations by a real-world user study, we demonstrate that TiInsight oÔ¨Äers remarkable performance compared to human experts. Additionally, TiSQL achieves an execution accuracy of 86.3% on the Spider dataset when using GPT-4 . It also attains an execution accuracy of 60.98% on the Bird test dataset.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Automated%20Cross-domain%20Exploratory%20Data%20Analysis%20through%20Large%20Language%20Models",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5086-zhu.pdf",
    "session": "Industry 3: Document, Graph, and Vector Databases",
    "authors": [
      {
        "Name": "Jun-Peng Zhu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Boyan Niu",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Peng Cai",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Zheming Ni",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Jianwei Wan",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Kai Xu",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Jiajun Huang",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Shengbo Ma",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Bing Wang",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Xuan Zhou",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Guanglei Bao",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Donghui Zhang",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Liu Tang",
        "Affiliation": "PingCAP"
      },
      {
        "Name": "Qi Liu",
        "Affiliation": "PingCAP"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "715ae5fd-c152-45dc-9c90-291d179b3e9d",
    "title": "LASEK: LLM-Assisted Style Exploration Kit for Geospatial Data",
    "abstract": "Geospatial data visualization on a map is an essential tool for modern data exploration tools. However, these tools require users to manually configure the visualization style including color scheme and attribute selection, a process that is both complex and domainspecific. Large Language Models (LLMs) provide an opportunity to intelligently assist in styling based on the underlying data distribution and characteristics. This paper demonstrates LASEK, an LLM-assisted visualization framework that automates attribute selection and styling in large-scale spatio-temporal datasets. The system leverages LLMs to determine which attributes should be highlighted for visual distinction and even suggests how to integrate them in styling options improving interpretability and efficiency. We demonstrate our approach through interactive visualization scenarios, showing how LLM-driven attribute selection enhances clarity, reduces manual effort, and provides data-driven justifications for styling decisions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LASEK%3A%20LLM-Assisted%20Style%20Exploration%20Kit%20for%20Geospatial%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5435-bahadori.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Tarlan Bahadori",
        "Affiliation": "University of California, Riverside"
      },
      {
        "Name": "Ahmed Eldawy",
        "Affiliation": "University of California, Riverside"
      },
      {
        "Name": "Sai Sreekar Sarvepalli",
        "Affiliation": "University of California, Riverside"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1af34e80-6551-426a-85c4-1bf5edf5e741",
    "title": "S^3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor Difference Semantics",
    "abstract": "For the past decades, the  subgraph similarity search  over a largescale data graph has become increasingly important and  crucial in many real-world applications, such as social network analysis, bioinformatics network analytics, knowledge graph discovery, and many others. While previous works on subgraph similarity search used various graph similarity metrics such as the graph isomorphism, graph edit distance, and so on, in this paper, we propose a novel problem, namely  subgraph similarity search under aggregated neighbor difference semantics  (S 3 AND), which identifies subgraphs  ùëî in a data graph  ùê∫ that are similar to a given query graph ùëû by considering both keywords and graph structures (under new keyword/structural matching semantics). To efficiently tackle the S 3 AND problem, we design two effective pruning methods,  keyword set  and  aggregated neighbor difference lower bound pruning , which rule out false alarms of candidate vertices/subgraphs to reduce the S 3 AND search space. Furthermore, we construct an effective indexing mechanism to facilitate our proposed efficient S 3 AND query answering algorithm. Through extensive experiments, we demonstrate the effectiveness and efficiency of our S 3 AND approach over both real and synthetic graphs under various parameter settings.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/S%5E3AND%3A%20Efficient%20Subgraph%20Similarity%20Search%20Under%20Aggregated%20Neighbor%20Difference%20Semantics",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3708-wen.pdf",
    "session": "Research 34: Graph Data Management IV",
    "authors": [
      {
        "Name": "Qi Wen",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Yutong Ye",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Xiang Lian",
        "Affiliation": "Kent State University"
      },
      {
        "Name": "Mingsong Chen",
        "Affiliation": "East China Normal University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d858d05f-cf63-4b5f-994e-45fdfc1e205c",
    "title": "Dynamic Range-Filtering Approximate Nearest Neighbor Search",
    "abstract": "Range-filtering approximate nearest neighbor search ( RFANNS ) has gained significant attention recently. Consider a set  D  of highdimensional vectors, each associated with a numeric attribute value, e.g., price or timestamp. An  RFANNS  query consists of a query vector  ùëû and a query range, reporting the approximate nearest neighbors of  ùëû among data vectors whose attributes fall in the query range. Existing work on  RFANNS  only considers a static set D  of data vectors while in many real-world scenarios, vectors arrive in the system in an arbitrary order. This paper studies dynamic RFANNS  where both data vectors and queries arrive in a mixed stream: a query is posed on all the data vectors that have already arrived in the system. Existing work on  RFANNS  is difficult to be extended to the streaming setting as they construct the index in the order of the attribute values while the vectors arrive in the system in an arbitrary order. The main challenge to the dynamic RFANNS  lies in the difference between the two orders. A naive approach to  RFANNS  maintains multiple hierarchical navigable small-world (HNSW) graphs, one for each of the  ùëÇ (|D| 2 )  possible query ranges ‚Äì too expensive to construct and maintain. To design an index structure that can integrate new data vectors with a low index size increment for efficient and effective query processing, we propose a structure called  dynamic segment graph . It compresses the set of HNSW graphs of the naive approach, proven to be lossless under certain conditions, with only a linear to  log  |D|  new edges in expectation when inserting a new vector. This dramatically reduces the index size while largely preserving the search performance. We further propose heuristics to significantly reduce the index cost of our dynamic segment graph in practice. Extensive experimental results show that our approach outperforms existing methods for static  RFANNS  and is scalable in handling dynamic  RFANNS .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Dynamic%20Range-Filtering%20Approximate%20Nearest%20Neighbor%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3256-deng.pdf",
    "session": "Research 46: Vector Data Management III",
    "authors": [
      {
        "Name": "Zhencan Peng",
        "Affiliation": "Rutgers University - New Brunswick"
      },
      {
        "Name": "Miao Qiao",
        "Affiliation": "The University of Auckland"
      },
      {
        "Name": "Wenchao Zhou",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Dong Deng",
        "Affiliation": "Rutgers University - New Brunswick"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c40e4e1a-eaa4-4dd5-b87c-b408000602d2",
    "title": "Improving Time Series Data Compression in Apache IoTDB",
    "abstract": "Time series data are generated on an unprecedented scale across various domains. Although traditional compression techniques reduce storage costs, they typically require full decompression before querying, leading to increased latency and higher resource consumption. Homomorphic compression (HC), which enables direct computation on the compressed data without decompression, shows the potential for both reduced storage and improved query performance. However, the unique complexities of time series data pose challenges that current HC methods have yet to adequately address. In this paper, we introduce HC theory in the time series domain, transformatively enabling HC to time series database queries. Building on our theory, we develop  CompressIoTDB  ‚Äì a novel homomorphic compression framework integrated into Apache IoTDB. By leveraging our proposed CompColumn structure, our framework supports a wide range of query operators, including filtering, aggregation, and window-based functions, all while maintaining data in its compressed form. Furthermore, we incorporate system-level optimizations such as late decompression and dynamic auxiliary management to further boost query efficiency. Extensive experiments show that CompressIoTDB significantly enhances query processing for time series data, achieving an average throughput improvement of 53.4% and memory usage reduction of 20%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Improving%20Time%20Series%20Data%20Compression%20in%20Apache%20IoTDB",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3406-tang.pdf",
    "session": "Research 25: Time Series Data III",
    "authors": [
      {
        "Name": "Yuxin Tang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Feng Zhang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Jiawei Guan",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yuan Tian",
        "Affiliation": "Timecho"
      },
      {
        "Name": "Xiangdong Huang",
        "Affiliation": "NERCBDS, EIRI, Tsinghua University"
      },
      {
        "Name": "Chen Wang",
        "Affiliation": "NERCBDS, EIRI, Tsinghua University"
      },
      {
        "Name": "Jianmin Wang",
        "Affiliation": "NERCBDS, EIRI, Tsinghua University"
      },
      {
        "Name": "Xiaoyong Du",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eb6999b3-a670-48a4-b127-19409022c243",
    "title": "AdaNDV: Adaptive Number of Distinct Value Estimation via Learning to Select and Fuse Estimators",
    "abstract": "Estimating the Number of Distinct Values (NDV) is fundamental for numerous data management tasks, especially within database applications. However, most existing works primarily focus on introducing new statistical or learned estimators, while identifying the most suitable estimator for a given scenario remains largely unexplored. Therefore, we propose  AdaNDV , a learned method designed to adaptively select and fuse existing estimators to address this issue. Specifically, (1) we propose to use learned models to distinguish between overestimated and underestimated estimators and then select appropriate estimators from each category. This strategy provides a complementary perspective by integrating overestimations and underestimations for error correction, thereby improving the accuracy of NDV estimation. (2) To further integrate the estimation results, we introduce a novel fusion approach that employs a learned model to predict the weights of the selected estimators and then applies a weighted sum to merge them. By combining these strategies, the proposed  AdaNDV  fundamentally distinguishes itself from previous works that directly estimate NDV. Moreover, extensive experiments conducted on real-world datasets, with the number of individual columns being several orders of magnitude larger than in previous studies, demonstrate the superior performance of our method.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/AdaNDV%3A%20Adaptive%20Number%20of%20Distinct%20Value%20Estimation%20via%20Learning%20to%20Select%20and%20Fuse%20Estimators",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1104-zhang.pdf",
    "session": "Research 53: Applied ML and AI for Data Management IV",
    "authors": [
      {
        "Name": "Xianghong Xu",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Tieying Zhang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Xiao He",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Haoyang Li",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Rong Kang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Wang Shuai",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Xu Linhui",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "zhimin liang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Shangyu Luo",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Lei Zhang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Jianjun Chen",
        "Affiliation": "Bytedance"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cb44313c-458a-4892-b238-bf4a4c5a322a",
    "title": "RED: Effective Trajectory Representation Learning with Comprehensive Information",
    "abstract": "Trajectory representation learning (TRL) maps trajectories to vec-Trajectory representation learning (TRL) maps trajectories to vectors that can then be used for various downstream tasks, includ-tors that can then be used for various downstream tasks, including trajectory similarity computation, trajectory classification, and travel-time estimation. However, existing TRL methods often pro-travel-time estimation. However, existing TRL methods often produce vectors that, when used in downstream tasks, yield insuffi-duce vectors that, when used in downstream tasks, yield insufficiently accurate results. A key reason is that they fail to utilize the comprehensive information encompassed by trajectories. We propose a self-supervised TRL framework, called RED, which ef-propose a self-supervised TRL framework, called RED, which effectively exploits multiple types of trajectory information. Overall, RED adopts the Transformer as the backbone model and masks the constituting paths in trajectories to train a masked autoencoder (MAE). In particular, RED considers the moving patterns of trajecto-(MAE). In particular, RED considers the moving patterns of trajectories by employing a  R oad-aware masking strategy  that retains key paths of trajectories during masking, thereby preserving crucial information of the trajectories. RED also adopts a  spatial-temporal-information of the trajectories. RED also adopts a  spatial-temporaluser joint  E mbedding  scheme to encode comprehensive information when preparing the trajectories as model inputs. To conduct train-when preparing the trajectories as model inputs. To conduct training, RED adopts  D ual-objective task learning : the Transformer en-ing, RED adopts  D ual-objective task learning : the Transformer encoder predicts the next segment in a trajectory, and the Transformer decoder reconstructs the entire trajectory. RED also considers the spatial-temporal correlations of trajectories by modifying the at-spatial-temporal correlations of trajectories by modifying the attention mechanism of the Transformer. We compare RED with 9 state-of-the-art TRL methods for 4 downstream tasks on 3 real-state-of-the-art TRL methods for 4 downstream tasks on 3 realworld datasets, finding that RED can usually improve the accuracy of the best-performing baseline by over 5%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RED%3A%20Effective%20Trajectory%20Representation%20Learning%20with%20Comprehensive%20Information",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p80-zhou.pdf",
    "session": "Research 55: Spatial and Temporal Databases",
    "authors": [
      {
        "Name": "Silin Zhou",
        "Affiliation": "UESTC"
      },
      {
        "Name": "Shuo Shang",
        "Affiliation": "UESTC"
      },
      {
        "Name": "Lisi Chen",
        "Affiliation": "University of Electronic Science and Technology of China"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Panos Kalnis",
        "Affiliation": "King Abdullah University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a7bb9977-de9f-40af-853b-99a4216a640e",
    "title": "Synergetic Community Search over Large Multilayer Graphs",
    "abstract": "Community search is a fundamental problem in graph analysis and has attracted much attention for its ability to discover personalized communities. In this paper, we focus on community search over multilayer graphs. We design a novel cohesive subgraph model called synergetic core for multilayer graphs, which requires both local and global cohesiveness. Specifically, the synergetic core man- dates that the vertices within the subgraph are not only densely connected on some individual layers but also form more cohesive connections on the projected graph that considers all layers. The local and global cohesiveness collectively ensure the superiority of the synergetic core. Based on this new model, we formulate the problem of synergetic community search. To efficiently retrieve the community, we propose two algorithms. The first is a progressive search algorithm, which enumerates potential layer combinations to compute the synergetic core. The second is a trie-based search algorithm, leveraging our novel index called dominant layers-based trie (DLT). DLT compactly stores synergetic cores within the trie structure. By traversing the DLT, we can efficiently identify the syn- ergetic core. We conduct extensive experiments on ten real-world datasets. Experimental results demonstrate that (1) the synergetic core can find communities with the best quality among the state- of-the-art models, and (2) our proposed algorithms are up to five orders of magnitude faster than the basic method.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Synergetic%20Community%20Search%20over%20Large%20Multilayer%20Graphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1412-gao.pdf",
    "session": "Research 65: Graph Data Management VIII",
    "authors": [
      {
        "Name": "Chengyang Luo",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Qing Liu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Jianliang Xu",
        "Affiliation": "Hong Kong Baptist University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "114b7ab4-0297-441b-9f9a-84b2d3ed17f3",
    "title": "A Flexible Framework for Query-oriented Interactive Community Search",
    "abstract": "Community search finds query-dependent communities over graphs, which has been investigated broadly. In this work, we focus on the task of returning only a single connected community containing all user input query vertices. Most existing studies in the literature only propose  a single and static model  based on  a particular subgraph (e.g.,  ùëò -core,  ùëò -truss, quasi-clique, and learning-based component). These fixed models are hard to find exact community answers on all datasets and fit with different underlying desires of users and queries. This implies that the  community search  task needs human-in-loop  interactions , which allows users to give feedback and dynamically advise community refinement. \nTo tackle the above issues, we formulate and study the problem of  interactive community search , which allows users to add/delete vertices for improving community answers in a few rounds of interactions. We first summarize dozens of existing community models and develop an integrated notation system  M (G ,  M ,  O ,  P) to describe them all. Then, we propose a flexible approach to i nteractive   c ommunity   s earch over  g raphs called  GICS -framework. The successful principle of  GICS -framework lies on three key components:  personalized adding/deleting recommendation ,  parameter auto-tuning , and  fast partial refinement . We develop efficient algorithms and successfully deploy three community models on our  GICS -framework. We further analyze algorithm complexity of  GICS -framework by illustrating one instance model in detail. Extensive experiments on ground-truth communities demonstrate that our interaction of  GICS -framework improves F1-score accuracy by 22% against state-of-the-art competitors, and gives users real-time responses within one second.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Flexible%20Framework%20for%20Query-oriented%20Interactive%20Community%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1977-sun.pdf",
    "session": "Research 20: Road Networks, Social Networks",
    "authors": [
      {
        "Name": "Longxu SUN",
        "Affiliation": "Hong Kong Baptist University"
      },
      {
        "Name": "Xin Huang",
        "Affiliation": "Hong Kong Baptist University"
      },
      {
        "Name": "Jiannan Wang",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Jianliang Xu",
        "Affiliation": "Hong Kong Baptist University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "09f14bdf-ec56-4468-ba08-f9121bf49d68",
    "title": "LLM-R2: A Large Language Model Enhanced Rule-based Rewrite System for Boosting Query Efficiency",
    "abstract": "Query rewrite, which aims to improve query e!ciency by altering an SQL query‚Äôs structure without changing its result, has been an important research problem. In order to maintain equivalence between the rewritten query and the original one during rewriting, traditional query rewrite methods always rewrite the queries following certain rewrite rules. However, some problems still remain. First, existing methods of finding the optimal choice or sequence of rewrite rules are still limited and the process always costs a lot of resources. Methods involving discovering new rewrite rules typically require complicated proofs of structural logic or extensive user interactions. Second, current query rewrite methods usually rely highly on DBMS cost estimators which are often not accurate. In this paper, we address these problems by proposing a novel query rewrite method named LLM-R2, which leverages a large language model (LLM) to recommend rewrite rules for a database rewrite system. To further enhance the inference ability of the LLM in recommending rewrite rules, we train a contrastive model using a curriculum-based approach to learn query representations and select effective query demonstrations for the LLM. Experimental results show that our method significantly improves the query execution e!ciency and outperforms the baseline methods. In addition, our method exhibits high robustness across different datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LLM-R2%3A%20A%20Large%20Language%20Model%20Enhanced%20Rule-based%20Rewrite%20System%20for%20Boosting%20Query%20Efficiency",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p53-yuan.pdf",
    "session": "Research 2: Applied ML and AI for Data Management I",
    "authors": [
      {
        "Name": "Zhaodonghui Li",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Haitao Yuan",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Huiming Wang",
        "Affiliation": "Singapore University of Technology and Design"
      },
      {
        "Name": "Gao Cong",
        "Affiliation": "Nanyang Technological Univesity"
      },
      {
        "Name": "Lidong Bing",
        "Affiliation": "Alibaba DAMO Academy"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "000e9876-058c-4e63-8a00-2788996ed66b",
    "title": "DIM-SUM: Dynamic IMputation for Smart Utility Management",
    "abstract": "Time series imputation models have traditionally been developed using complete datasets with artiÔ¨Åcial masking patterns to simulate missing values. However, in real-world infrastructure monitoring, practitioners often encounter datasets where large amounts of data are missing and follow complex, heterogeneous patterns. We introduce DIM-SUM, a preprocessing framework for training robust imputation models that bridges the gap between artiÔ¨Åcially masked training data and real missing patterns. DIM-SUM combines pattern clustering and adaptive masking strategies with theoretical learning guarantees to handle diverse missing patterns actually observed in the data. Through extensive experiments on over 2 billion readings from California water districts, electricity datasets, and benchmarks, we demonstrate that DIM-SUM outperforms traditional methods by reaching similar accuracy with lower processing time and signiÔ¨Åcantly less training data. When compared against a large pre-trained model, DIM-SUM averages 2x higher accuracy with signiÔ¨Åcantly less inference time.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DIM-SUM%3A%20Dynamic%20IMputation%20for%20Smart%20Utility%20Management",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4451-hildebrant.pdf",
    "session": "Research 36: Data Cleaning and Preparation II",
    "authors": [
      {
        "Name": "Ryan Hildebrant",
        "Affiliation": "University of California, Irvine"
      },
      {
        "Name": "Rahul Bhope",
        "Affiliation": "University of California, Irvine"
      },
      {
        "Name": "Sharad Mehrotra",
        "Affiliation": "University of California, Irvine"
      },
      {
        "Name": "Christopher Tull",
        "Affiliation": "California Data Collaborative"
      },
      {
        "Name": "Nalini Venkatasubramanian",
        "Affiliation": "University of California, Irvine"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "95f4dcc2-41a2-40ae-abce-b2b0d4e579da",
    "title": "Neural Relational Data: Tabular Foundation Models, LLMs... or both?",
    "abstract": "Recent breakthroughs in artificial intelligence have produced  Large Language Models  (LLMs) and a new wave of  Tabular Foundation Models  (TFMs). Both promise to redefine how we query, integrate, and reason over relational data, yet they embody opposing philosophies: LLMs pursue broad generality through massive text-centric pre-training, whereas TFMs embed inductive biases that mirror table structure and relational semantics. This panel assembles researchers and practitioners from academia and industry to debate which path, specialized TFMs, ever stronger general-purpose LLMs, or a hybrid of the two, will most effectively power the next generation of data management systems. Panelists will confront questions of generality, accuracy, scalability, robustness, cost, and usability across core data management tasks such as Text-to-SQL translation, schema understanding, and entity resolution. The discussion aims to surface critical research challenges and guide the community‚Äôs investment of effort and resources over the coming years.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Panel%20on%20Neural%20Relational%20Data%3A%20Tabular%20Foundation%20Models%2C%20LLMs...%20or%20both%EF%BC%9F",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5513-paolo.pdf",
    "session": "Panel 1: Neural Relational Data: Tabular Foundation Models, LLMs... or both?",
    "authors": [
      {
        "Name": "Paolo Papotti",
        "Affiliation": "Eurecom"
      },
      {
        "Name": "Carsten Binnig",
        "Affiliation": "TU Darmstadt"
      },
      {
        "Name": "Floris Geerts",
        "Affiliation": "University of Antwerp"
      },
      {
        "Name": "Johannes Hoffart",
        "Affiliation": "SAP"
      },
      {
        "Name": "Madelon Hulsebos",
        "Affiliation": "CWI"
      },
      {
        "Name": "Fatma √ñzcan",
        "Affiliation": "Google"
      },
      {
        "Name": "Gael Varoquaux",
        "Affiliation": "INRIA"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8e536fea-db62-4ffb-87c6-fa2201a02862",
    "title": "Cracking Vector Search Indexes",
    "abstract": "Retrieval Augmented Generation (RAG) uses vector databases to expand the expertise of an LLM model without having to retrain it. The idea can be applied over data lakes, leading to the notion of embedding data lakes, i.e., a pool of vector databases ready to be used by RAGs. The key component in these systems is the indexes enabling Approximated Nearest Neighbor Search (ANNS). However, in data lakes, one cannot realistically expect to build indexes for every dataset. Thus, we propose an adaptive, partition-based index, CrackIVF, that performs much better than up-front index building. CrackIVF starts answering as a small index, and only expands to improve performance as it sees enough queries. It does so by progressively adapting the index to the query workload. That way, queries can be answered right away without having to build a full index !rst. After seeing enough queries, CrackIVF will produce an index comparable to those built with conventional techniques. CrackIVF can often answer more than 1 million queries before other approaches have even built the index, achieving 10-1000x faster initialization times. This makes it ideal for cold or infrequently used data and as a way to bootstrap access to unseen datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cracking%20Vector%20Search%20Indexes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3951-mageirakos.pdf",
    "session": "Research 31: Vector Data Management II",
    "authors": [
      {
        "Name": "Vasilis Mageirakos",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Bowen Wu",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Gustavo Alonso",
        "Affiliation": "ETH Zurich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bab19a82-6519-4080-bb36-f872dd58bdfc",
    "title": "Efficient Historical Butterfly Counting in Large Temporal Bipartite Networks via Graph Structure-aware Index",
    "abstract": "Bipartite graphs are ubiquitous in many domains, e.g., e-commerce platforms, social networks, and academia, by modeling in teractions between distinct entity sets. Within these graphs, the butterfly motif, a complete 2 √ó 2 biclique, represents the simplest yet significant subgraph structure, crucial for analyzing complex network patterns. Counting the butterflies offers significant benefits across various applications, including community analysis and recommender systems. Additionally, the temporal dimension of bipartite graphs, where edges activate within specific time frames, introduces the concept of historical butterfly counting, i.e., counting butterflies within a given time interval. This temporal analysis sheds light on the dynamics and evolution of network interactions, offering new insights into their mechanisms. Despite its importance, no existing algorithm can efficiently solve the historical butterfly counting task. To address this, we design two novel indices whose memory footprints are dependent on #butterflies and #wedges, respectively. Combining these indices, we propose a graph structure-aware indexing approach that significantly reduces memory usage while preserving exceptional query speed. To further reduce the index size and boost the query efficiency, we design an index compression strategy, enabling the fast, high-quality, and unbiased approximation of historical butterfly counts. We theoretically prove that our approach is particularly advantageous on power-law graphs, a common characteristic of real-world bipartite graphs, by surpassing traditional complexity barriers for general graphs. Extensive experiments reveal that our query algorithms outperform existing methods by up to five magnitudes, effectively balancing speed with manageable memory requirements.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Historical%20Butterfly%20Counting%20in%20Large%20Temporal%20Bipartite%20Networks%20via%20Graph%20Structure-aware%20Index",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1607-mang.pdf",
    "session": "Research 41: Graph Data Management V",
    "authors": [
      {
        "Name": "Qiuyang Mang",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Jingbang Chen",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Hangrui Zhou",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Yu Gao",
        "Affiliation": "Independent"
      },
      {
        "Name": "Yingli Zhou",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Qingyu Shi",
        "Affiliation": "Independent"
      },
      {
        "Name": "Richard Peng",
        "Affiliation": "Carnegie Mellon University"
      },
      {
        "Name": "Yixiang Fang",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Chenhao Ma",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e365b949-d978-4588-a910-f3e88437ae26",
    "title": "Efficient Maintenance of 2-Hop Labeling Index on Dynamic Small-World Graphs",
    "abstract": "2-hop labeling has been widely utilized to accelerate the efficiency of online shortest distance queries. Given the nature of frequent changes in real-world graphs, the efficient maintenance of 2-hop labeling index has been extensively studied recently. However, existing methods cannot efficiently process large-scale graphs due to their high time and memory costs, and most of them process large batches of updates sequentially, significantly decreasing efficiency. In this paper, we propose a novel algorithm for maintaining the 2-hop labeling index in a parallel manner, called  M2HL , which can efficiently handle both edge insertions and deletions. Moreover, we theoretically prove that M2HL maintains both correctness and minimality for the updated 2-hop labeling index. Our experiments on ten large-scale graphs demonstrate that M2HL outperforms the state-of-the-art 2-hop labeling maintenance methods by up to four orders of magnitude in speed while maintaining correctness and minimality, as well as exhibiting strong scalability and low memory usage.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Maintenance%20of%202-Hop%20Labeling%20Index%20on%20Dynamic%20Small-World%20Graphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2005-zeng.pdf",
    "session": "Research 27: Graph Data Management III",
    "authors": [
      {
        "Name": "Yuanyuan Zeng",
        "Affiliation": "Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Yixiang Fang",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Kun Chen",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Yangfan Li",
        "Affiliation": "Central South University"
      },
      {
        "Name": "Chenhao Ma",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3e835e41-34df-4765-b0a6-1433132fb15f",
    "title": "Automatic Indexing in Oracle",
    "abstract": "Indexes are one of the important access structures that help improve database performance. This paper provides a methodology to automate the entire lifecycle of index creation and management with continuous index tuning based on changing data and workload. We present novel ideas that are critical to ensuring automatic indexing seamlessly works in a production database. Our methodology avoids using an expensive clone; yet o!ers non-intrusive index operations (candidate isolation and evaluation with Oracle resource manager ensuring no visible impact to the user workload), and upon deployment of auto indexes ensures non-disruptive plan invalidations and timely mitigation of performance regressions. The proposed approach is unique in that it is incremental and iterative, continually creating beneficial indexes and dropping unused ones as the workload evolves. The approach even supports indexes on expressions. It performs careful validation ‚Äì including computing overhead of index maintenance incurred during DML while evaluating potential benefit ‚Äì and provides accountability for its actions. Performance regressions are e!ectively managed using Oracle‚Äôs powerful SQL Plan Management (SPM) framework. For example, a new automatic index isn‚Äôt dropped in response to a single statement regressing due to it; SPM instead ensures such regressing statements revert to well-performing plans even in the presence of new indexes that continue to benefit other statements. We also share results of comprehensively evaluating various automatic indexing aspects in publicly available and Oracle customer workloads. Our experiments show benefit with automatic indexing, especially in customer workload, with a 15% improvement in performance and 60% space reclamation potential. This automatic indexing feature is available since Oracle 19c and in Oracle Autonomous Database.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Automatic%20Indexing%20in%20Oracle",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4924-chakkappen.pdf",
    "session": "Industry 6: Database Engines",
    "authors": [
      {
        "Name": "Sunil Chakkappen",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Shreya Kunjibettu",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Daniel McGreer",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Masoomeh Kishi",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Hong Su",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Mohamed Ziauddin",
        "Affiliation": "Oracle"
      },
      {
        "Name": "Mohamed Zait",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Zhan Li",
        "Affiliation": "Meta"
      },
      {
        "Name": "Yuying Zhang",
        "Affiliation": "Google"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "446c936f-8dfa-4889-bd8e-ca5c98daab73",
    "title": "Why Are Learned Indexes So Effective but Sometimes Ineffective?",
    "abstract": "Learned indexes have attracted significant research interest due to their potential to offer better space-time trade-offs compared to B+tree variants. Among various learned indexes, the PGM-Index based on error-bounded piecewise linear approximation is an elegant data structure that has demonstrated  provably  superior performance over conventional B+-tree indexes. However, despite numerous efforts to optimize the design of the PGM-Index, few systematically study the root causes of performance mismatches observed in practice. In this paper, we explore two key research questions. Q1 :  Why are PGM-Indexes theoretically effective?  and  Q2 :  Why do PGM-Indexes underperform in practice?  For  Q1 , we show that for a set of  ùëÅ sorted keys, the PGM-Index can achieve a lookup time of  ùëÇ ( log log  ùëÅ )  while using  ùëÇ(ùëÅ)  space. For  Q2 , we identify that querying PGM-Indexes is highly memory-bound, where the internal index search operations often become the bottleneck. To fill the performance gap, we propose PGM++, a  simple yet effective extension to the original PGM-Index that employs a mixture of different search strategies, with hyper-parameters automatically tuned through a cost model calibrated by theoretical findings. Extensive experiments show that, at comparable space costs, PGM++ speeds up index lookup queries by up to  2.31 √ó  and  1.56 √ó  when compared to the original PGM-Index and SOTA baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Why%20Are%20Learned%20Indexes%20So%20Effective%20but%20Sometimes%20Ineffective%EF%BC%9F",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2886-liu.pdf",
    "session": "Research 53: Applied ML and AI for Data Management IV",
    "authors": [
      {
        "Name": "Qiyu LIU",
        "Affiliation": "Southwest University"
      },
      {
        "Name": "Siyuan HAN",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Yanlin Qi",
        "Affiliation": "Harbin Institute of Technology, shenzhen"
      },
      {
        "Name": "Jingshu Peng",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Jin Li",
        "Affiliation": "Harvard University"
      },
      {
        "Name": "Longlong Lin",
        "Affiliation": "Southwest University"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "830bf204-b61b-484e-b420-7d4efbae3aec",
    "title": "Enter the Warp: Fast and Adaptive Data Transfer with XDBC",
    "abstract": "Fast and scalable data transfer is crucial in today‚Äôs decentralized data ecosystems and data-driven applications, including extractiontransformation-loading (ETL) pipelines, and data science workflows. Transfers often occur across heterogeneous environments‚Äîranging from cloud-hosted systems to local consumer devices‚Äîwith varying compute and network constraints. However, existing solutions struggle to balance performance with generality across such diverse setups. We recently proposed XDBC, a holistic data transfer framework that decomposes the pipeline into logical components with multiple physical implementations per component. Its modular architecture enables seamless system integration and automatic tuning based on workload and environment characteristics. In this demonstration, we present Enter the Warp, an interactive game built around XDBC that visualizes data transfer as a space mission. Players configure transfer parameters, monitor live throughput metrics, and optimize performance to shield Earth from meteor strikes, gaining an understanding and hands-on experience of data transfer challenges in an engaging and intuitive way.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Enter%20the%20Warp%3A%20Fast%20and%20Adaptive%20Data%20Transfer%20with%20XDBC",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5315-gavriilidis.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Haralampos Gavriilidis",
        "Affiliation": "Technische Universit√§t Berlin"
      },
      {
        "Name": "Joel Ziegler",
        "Affiliation": "Technische Universit√§t Berlin"
      },
      {
        "Name": "Midhun Kaippillil Venugopalan",
        "Affiliation": "Technische Universit√§t Berlin"
      },
      {
        "Name": "Benedikt Didrich",
        "Affiliation": "Technische Universit√§t Berlin"
      },
      {
        "Name": "Matthias Boehm",
        "Affiliation": "Technische Universit√§t Berlin"
      },
      {
        "Name": "Volker Markl",
        "Affiliation": "Technische Universit√§t Berlin"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c30db722-a4ce-4958-8fa8-ea95ccbe77fa",
    "title": "Scribe: How Meta transports terabytes per second in real time",
    "abstract": "Millions of web servers and a multitude of applications are producing ever-increasing amounts of data in real time at Meta. Regardless of how data is generated and how it is processed, there is a need for infrastructure that can accommodate the transport of arbitrarily large data streams from their generation location to their processing location with low latency. \nThis paper presents Scribe, a multi-tenant message queue service that natively supports the requirements of Meta‚Äôs data-intensive applications, ingesting  >  15 ùëáùêµ / ùë† and serving  >  110 ùëáùêµ / ùë† to its consumers. Scribe relies on a multi-hop write path and opportunistic data placement to maximise write availability, whereas its read path adapts replica placement and representation based on the incoming workload as a means to minimise resource consumption for both Scribe and its downstreams. The wide range of Scribe use cases can pick from a range of offered guarantees, based on the trade-offs favourable for each one.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Scribe%3A%20How%20Meta%20transports%20zettabytes%20per%20day%20in%20real%20time",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4817-karpathiotakis.pdf",
    "session": "Industry 1: Distributed Systems",
    "authors": [
      {
        "Name": "Manos Karpathiotakis",
        "Affiliation": "Facebook"
      },
      {
        "Name": "Vlassios Rizopoulos",
        "Affiliation": "Facebook"
      },
      {
        "Name": "Artem Gelun",
        "Affiliation": "Facebook"
      },
      {
        "Name": "Tiziano Carotti",
        "Affiliation": "Facebook"
      },
      {
        "Name": "Hazem Nada",
        "Affiliation": "Facebook"
      },
      {
        "Name": "Basri Kahveci",
        "Affiliation": "Facebook"
      },
      {
        "Name": "Yuri Dolgov",
        "Affiliation": "Facebook"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e88ff004-9b0e-4682-9663-27c89c59a2a8",
    "title": "How to Optimize SQL Queries? A Comparison Between Split, Holistic, and Hybrid Approaches",
    "abstract": "",
    "conference_link": "",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3910-gretscher.pdf",
    "session": "Research 9: Query Processing and Optimization I",
    "authors": [
      {
        "Name": "Luca Gretscher",
        "Affiliation": "Saarland University"
      },
      {
        "Name": "Jens Dittrich",
        "Affiliation": "Saarland University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Missing conference link",
      "Empty or missing abstract"
    ]
  },
  {
    "id": "aeaac48b-c4a2-48c9-a0d5-a059c501a037",
    "title": "Unraveling the Impact of Window Semantics: Optimizing Join Order for Efficient Stream Processing",
    "abstract": "Window joins (WJs) are fundamental operators in stream processing systems (SPSs), enabling continuous, time-aware joins over unbounded data streams. Unlike time-agnostic relational joins, WJs incorporate temporal semantics associated with different window types (i.e., sliding, session, and interval windows), which introduce uncertainty in algebraic properties such as commutativity and associativity. As a result, state-of-the-art SPSs exploit only a single, fixed join order, which limits optimization opportunities and often leads to suboptimal performance. In this work, we eliminate this restriction by introducing three transformation rules that enable WJ reordering while preserving query semantics for those window types. Based on them, we propose  WJR , an algorithm that systematically enumerates semantically equivalent join orders, expanding the search space for finding efficient WJ execution plans. Our evaluation shows speedups of up to 10 for multi-way WJ queries under various window configurations and rate ratios, highlighting the performance benefits of flexible join reordering in streaming queries.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Unraveling%20the%20Impact%20of%20Window%20Semantics%3A%20Optimizing%20Join%20Order%20for%20Efficient%20Stream%20Processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2468-ziehn.pdf",
    "session": "Research 16: Query Processing and Optimization II",
    "authors": [
      {
        "Name": "Ariane Ziehn",
        "Affiliation": "Technische Universit√§t Berlin"
      },
      {
        "Name": "Jan Szlang",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Steffen Zeuch",
        "Affiliation": "TU Berlin"
      },
      {
        "Name": "Volker Markl",
        "Affiliation": "Technische Universit√§t Berlin"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "64633963-df00-4b58-b0e5-385348529fed",
    "title": "EVOSCHEMA: TOWARDS TEXT-TO-SQL ROBUSTNESS AGAINST SCHEMA EVOLUTION",
    "abstract": "Neural text-to-SQL models, which translate natural language questions (NLQs) into SQL queries given a database schema, have achieved remarkable performance. However, database schemas frequently evolve to meet new requirements. Such schema evolution often leads to performance degradation for models trained on static schemas. Existing work either mainly focuses on simply paraphrasing some syntactic or semantic mappings among NLQ, DB and SQL, or lacks a comprehensive and controllable way to investigate the model robustness issue under the schema evolution, which is insufficient when facing the increasingly complex and rich database schema changes in reality, especially in the LLM era. \nTo address the challenges posed by schema evolution, we present EvoSchema , a comprehensive benchmark designed to assess and enhance the robustness of text-to-SQL systems under real-world schema changes.  EvoSchema  introduces a novel schema evolution taxonomy, encompassing ten perturbation types across columnlevel and table-level modifications, systematically simulating the dynamic nature of database schemas. Through  EvoSchema , we conduct an in-depth evaluation spanning different open-source and closed-source LLMs, revealing that table-level perturbations have a significantly greater impact on model performance compared to column-level changes. Furthermore,  EvoSchema  inspires the development of more resilient text-to-SQL systems, in terms of both model training and database design. The models trained on EvoSchema ‚Äôs diverse schema designs can force the model to distinguish the schema difference for the same questions to avoid learning spurious patterns, which demonstrate remarkable robustness compared to those trained on unperturbed data on average. This benchmark offers valuable insights into model behavior and a path forward for designing systems capable of thriving in dynamic, real-world environments.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/EVOSCHEMA%3A%20TOWARDS%20TEXT-TO-SQL%20ROBUSTNESS%20AGAINST%20SCHEMA%20EVOLUTION",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3655-zhang.pdf",
    "session": "Research 7: Natural Language Interfaces to Data",
    "authors": [
      {
        "Name": "TIANSHU ZHANG",
        "Affiliation": "The OHIO STATE UNIVERSITY"
      },
      {
        "Name": "Kun Qian",
        "Affiliation": "Adobe"
      },
      {
        "Name": "Siddhartha Sahai",
        "Affiliation": "Adobe"
      },
      {
        "Name": "Yuan Tian",
        "Affiliation": "Purdue University"
      },
      {
        "Name": "Shaddy Garg",
        "Affiliation": "Adobe"
      },
      {
        "Name": "Huan Sun",
        "Affiliation": "The OHIO STATE UNIVERSITY"
      },
      {
        "Name": "Yunyao Li",
        "Affiliation": "Adobe"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "30d2e08f-b47b-44cf-b4df-61c80fb3245e",
    "title": "Fast Graph Vector Search via Hardware Acceleration and Delayed-Synchronization Traversal",
    "abstract": "Vector search systems are indispensable in large language model (LLM) serving, search engines, and recommender systems, where minimizing online search latency is essential. Among various algorithms, graph-based vector search (GVS) is particularly popular due to its high search performance and quality. However, reducing GVS latency by intra-query parallelization remains challenging due to limitations imposed by both existing hardware architectures (CPUs and GPUs) and the inherent difficulty of parallelizing graph traversals. To efficiently serve low-latency GVS, we co-design hardware and algorithm by proposing Falcon and Delayed-Synchronization Traversal (DST). Falcon is a hardware GVS accelerator that implements efficient GVS operators, pipelines these operators, and reduces memory accesses by tracking search states with an onchip Bloom filter. DST is an efficient graph traversal algorithm that simultaneously improves search performance and quality by relaxing traversal orders to maximize accelerator utilization. Evaluation across various graphs and datasets shows that Falcon, prototyped on FPGAs, together with DST, achieves up to 4.3 √ó  and 19.5 √ó  lower latency and up to 8.0 √ó  and 26.9 √ó  improvements in energy efficiency over CPU- and GPU-based GVS systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Fast%20Graph%20Vector%20Search%20via%20Hardware%20Acceleration%20and%20Delayed-Synchronization%20Traversal",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3797-jiang.pdf",
    "session": "Research 46: Vector Data Management III",
    "authors": [
      {
        "Name": "Wenqi Jiang",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Hang Hu",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Torsten Hoefler",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Gustavo Alonso",
        "Affiliation": "ETH Zurich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "57c2262f-7cb1-4fae-92b8-f4e095585404",
    "title": "Weak-to-Strong Prompts with Lightweight-to-Powerful LLMs for High-Accuracy, Low-Cost, and Explainable Data Transformation",
    "abstract": "Data transformation poses significant challenges due to the wide diversity in input data formats and different requirements. Existing approaches‚Äîincluding human-driven, algorithmic, and large language model (LLM)-based solutions‚Äîeach exhibits trade-offs in terms of cost, accuracy, and the range of supported transformations. To address these limitations, we propose  MegaTran , a novel framework for generating accurate and cost-effective data transformation code.  MegaTran  employs a two-stage process:  Weak2StrongPrompt , which converts a user‚Äôs weak prompt (a loosely specified user input) into a strong, structured prompt, and  Prompt2Code , which generates transformation code based on this refined prompt. In Weak2StrongPrompt , a fine-tuned lightweight LLM predicts the transformation type and generates a detailed task description from the user‚Äôs input. In  Prompt2Code , a powerful LLM generates the corresponding transformation code, guided by two key optimizations: (1)  Sanity-check Reflection with checklist , which iteratively debugs and refines the code by addressing errors; and (2)  LazyRAG , a retrieval-augmented generation technique that retrieves relevant code snippets or documentation from external resources ( e.g.,  GitHub, DataPrep) to enhance code quality. Extensive experiments show that  MegaTran  achieves results varying from  +2.2%  to +26.1%  accuracy improvement compared with SoTA methods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Weak-to-Strong%20Prompts%20with%20Lightweight-to-Powerful%20LLMs%20for%20High-Accuracy%2C%20Low-Cost%2C%20and%20Explainable%20Data%20Transformation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2371-tang.pdf",
    "session": "Research 5: Data Cleaning and Preparation I",
    "authors": [
      {
        "Name": "Changlun Li",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Chenyu Yang",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Yuyu Luo",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Ju Fan",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Nan Tang",
        "Affiliation": "HKUST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "849b5b19-8942-40d9-ab59-577b0e8eb52c",
    "title": "Most Similar Biclique Search at Scale",
    "abstract": "The biclique is a fundamental model of bipartite cohesive subgraphs. To analyze a bipartite graph, many existing works seek the maximum biclique, that is, the biclique with the largest number of edges. However, our finding is that the  most similar biclique  (i.e., the biclique whose vertices are the most similar to each other) can be a good alternative for understanding the network. Using the model, we can detect meaningful communities with high similarity and avoid unnecessary searches based on vertex similarity. In particular, we aim to find (i)  local most similar biclique : the biclique that contains a query node  ùëû and the similarity between vertices is the highest, and (ii)  global most similar biclique : the biclique with the highest similarity between vertices. \nDespite the NP-hardness of the problems, this paper presents two efficient algorithms,  Mosib  and  Mosib-GloApp . Specifically, our Mosib  is an exact algorithm for the most similar biclique search. The algorithm incorporates three novel graph reduction rules that can reduce the size of the bipartite graph while preserving the most similar biclique, as well as two similarity-first search rules that can prioritize the bicliques with high similarity in the search. These techniques can significantly improve the practical efficiency of the algorithm. Meanwhile, our  Mosib-GloApp  is an approximate algorithm that adopts a novel MinHash-based dividing method, and it can further improve the efficiency of the global most similar biclique search. We experimentally evaluate our algorithms on realworld networks, and show that the most similar biclique models can find meaningful results while being computed efficiently.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Most%20Similar%20Biclique%20Search%20at%20Scale",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1022-zhang.pdf",
    "session": "Research 34: Graph Data Management IV",
    "authors": [
      {
        "Name": "Deming Chu",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Zhizhi Gao",
        "Affiliation": "Guangzhou University"
      },
      {
        "Name": "Fan Zhang",
        "Affiliation": "Guangzhou University"
      },
      {
        "Name": "Wenjie Zhang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Zhihong Tian",
        "Affiliation": "Guangzhou University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "238269c7-00e4-4489-bc46-4ebe6f63ba10",
    "title": "Efficient ùëò-Clique Densest Subgraph Discovery: Towards Bridging Practice and Theory",
    "abstract": "Densest subgraph discovery (DSD)  is a fundamental topic in graph mining. It has been studied for decades, and is widely used in various areas, including network science, biological analysis, and graph database s. As a typical problem of DSD, the  ùëò -clique densest subgraph (CDS) problem aims to detect a subgraph from a graph, such that the number of  ùëò -cliques over the number of its vertices is maximized. While the CDS problem has received plenty of attention in the literature, existing CDS algorithms that perform best in practice often have weaker theoretical guarantees, while those with the stronger theoretical assurances tend to perform worse in practice. Besides, all the existing CDS algorithms struggle with graphs with high degeneracy values, a characteristic commonly found in real-world graphs. To bridge the huge gap between practice and theory, in this paper, we first introduce a novel graph reduction technique, which locates the CDS into a very small subgraph, with non-trivial theoretical guarantees. We further propose a new efficient approximation algorithm by employing the state-of-the-art ùëò -clique counting algorithm, which shares all the advantages of existing algorithms, achieving both strong practical efficiency and theoretical guarantees. Extensive experiments on 12 real-world large graphs demonstrate the high efficiency of our CDS algorithm. Particularly, our algorithm is up to four orders of magnitude faster than the state-of-the-art algorithm while maintaining the same accuracy guarantees and requiring much less memory.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20%F0%9D%91%98-Clique%20Densest%20Subgraph%20Discovery%3A%20Towards%20Bridging%20Practice%20and%20Theory",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3490-zhou.pdf",
    "session": "Research 49: Graph Data Management VI",
    "authors": [
      {
        "Name": "Yingli Zhou",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen, and Alibaba Group"
      },
      {
        "Name": "Qingshuo Guo",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Yixiang Fang",
        "Affiliation": "The Chinese University of Hong Kong,Shenzhen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6edfd38f-b37e-4ec4-96bb-b378f8f8ccb0",
    "title": "GooseDB: A Database Engine that Optimally Refines Top-k Queries to Satisfy Representation Constraints",
    "abstract": "In many applications, from university rankings to the selection of candidates for a job interview, there exist various ‚Äúreasonable‚Äù ways to filter the data and generate a ranking. When the initial choice lacks certain desirable properties, we want to identify a minimally modified alternative that has those properties. To this end, we demonstrate  GooseDB , a database engine that combines DuckDB with an MILP solver. Given an SQL query, constraints on the output, and modification preferences,  GooseDB  returns a minimally modified SQL query that satisfies the constraints. This demo focuses on representation constraints for top- ùëò queries, i.e., count constraints over groups of tuples, such as the gender distribution of the top- ùëò job candidates.  GooseDB  significantly generalizes previous work in two directions. First, it supports more general modifications of the selection condition and the scoring function. Second, it is the first solution to holistically optimize for both at the same time, as well as for alternative values of limit  ùëò . Conference attendees will be able to interactively refine queries from easy-to-understand applications, observing the impact of their choices.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GooseDB%3A%20A%20Database%20Engine%20that%20Optimally%20Refines%20Top-k%20Queries%20to%20Satisfy%20Representation%20Constraints",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5351-chen.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Zixuan Chen",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Jinyang Li",
        "Affiliation": "University of Michigan"
      },
      {
        "Name": "H. V. Jagadish",
        "Affiliation": "University of Michigan"
      },
      {
        "Name": "Mirek Riedewald",
        "Affiliation": "Northeastern University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ba38d119-3a2b-4284-a9bd-468b5cb58a6d",
    "title": "LobRA: Multi-tenant Fine-tuning over Heterogeneous Data",
    "abstract": "With the breakthrough of Transformer-based pre-trained models, the demand for fine-tuning (FT) to adapt the base pre-trained models to downstream applications continues to grow, so it is essential for service providers to reduce the cost of processing FT requests. Low-rank adaption (LoRA) is a widely used FT technique that only trains small-scale adapters and keeps the base model unaltered, conveying the possibility of processing multiple FT tasks by jointly training different LoRA adapters with a shared base model. \nNevertheless, through in-depth analysis, we reveal the efficiency of joint FT is dampened by two heterogeneity issues in the training data ‚Äî the sequence length variation and skewness. To tackle these issues, we develop  LobRA , a brand new framework that supports processing multiple FT tasks by jointly training LoRA adapters. Two innovative designs are introduced. Firstly,  LobRA  deploys the FT replicas (i.e., model replicas for FT) with heterogeneous resource usages and parallel configurations, matching the diverse workloads caused by the sequence length variation. Secondly, for each training step,  LobRA  takes account of the sequence length skewness and dispatches the training data among the heterogeneous FT replicas to achieve workload balance. We conduct experiments to assess the performance of  LobRA , validating that it significantly reduces the GPU seconds required for joint FT by 45.03%-60.67%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LobRA%3A%20Multi-tenant%20Fine-tuning%20over%20Heterogeneous%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2616-fu.pdf",
    "session": "Research 32: Data-centric Machine Learning",
    "authors": [
      {
        "Name": "Sheng Lin",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Fangcheng Fu",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Haoyang Li",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Hao Ge",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Xuanyu Wang",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Jiawen Niu",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Yaofeng Tu",
        "Affiliation": "ZTE Corporation"
      },
      {
        "Name": "Bin Cui",
        "Affiliation": "Peking University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bc9ec0ae-7d45-4110-8c3f-b2ee8b366411",
    "title": "Design and Modular Verification of Distributed Transactions in MongoDB",
    "abstract": "MongoDB‚Äôs distributed multi-document transactions protocol was designed and developed incrementally, building on WiredTiger, an existing single node multi-version storage engine that provided snapshot isolated key-value storage. This layered approach required meticulous management of concurrency control and timestamping mechanisms across system layers, complicated by intricate component interactions and a large evolving codebase. In this paper, we describe our experience using  modular  formal speciÔ¨Åcation techniques to address this challenge. Our approach formally speciÔ¨Åes the distributed transactions protocol and its interface with the underlying storage layer, allowing us to verify high level protocol properties while also formalizing the contract between these two components. This modular approach also enables an automated, model-based veriÔ¨Åcation technique for testing conformance of the WiredTiger storage implementation to this interface. We use an explicit state model checker to automatically generate test cases from our storage model, which are then executed against the storage implementation, ensuring the implementation matches the interface relied upon by the transactions protocol. Our work highlights the value of formal modeling not only for verifying high-level protocol correctness but also for precisely deÔ¨Åning and validating interactions with lower-level system components in an automated way. Beyond verifying key isolation properties, our speciÔ¨Åcation also enabled us to formally analyze  permissiveness ‚Äìhow well the protocol maximizes concurrency within a given isolation level‚Äìa property not previously examined.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Design%20and%20Modular%20Verification%20of%20Distributed%20Transactions%20in%20MongoDB",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5045-schultz.pdf",
    "session": "Industry 5: Transactions and Concurrency Control",
    "authors": [
      {
        "Name": "William Schultz",
        "Affiliation": "MongoDB"
      },
      {
        "Name": "Murat Demirbas",
        "Affiliation": "MongoDB"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "347a4dc5-a2c3-4e88-84fc-295e44b9dda9",
    "title": "Property Graph Standards: State of the Art & Open Challenges",
    "abstract": "Property Graphs are a versatile and expressive data model that has gained widespread adoption due to their flexibility in supporting labeled and attributed nodes and edges. They are well-established in research communities and are becoming widespread in companies and organizations across various sectors. They have been boosted by a fervent ISO/IEC standardization activity, leading to dedicated query and schema languages. While the current standards are still evolving, opportunities remain to enrich them with features such as composability. The plethora of existing query languages reflects a rich and diverse ecosystem, which ongoing unification efforts aim to align. This tutorial aims to deepen the understanding of Property Graph standards by showcasing their strengths, highlighting recent unification efforts, clarifying the central role of schema constraints, and exploring the rich landscape of research and industrial opportunities shaping the future of graph data management.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Property%20Graph%20Standards%3A%20State%20of%20the%20Art%20%26%20Open%20Challenges",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5477-kondylakis.pdf",
    "session": "Tutorial 1: Property Graph Standards: State of the Art & Open Challenges",
    "authors": [
      {
        "Name": "Haridimos Kondylakis",
        "Affiliation": "FORTH-ICS & Computer Science Department, University of Crete"
      },
      {
        "Name": "Stefania Dumbrava",
        "Affiliation": "ENSIIE & T√©l√©com SudParis"
      },
      {
        "Name": "Matteo Lissandrini",
        "Affiliation": "University of Verona"
      },
      {
        "Name": "Nikolay Yakovets",
        "Affiliation": "Eindhoven University of Technology"
      },
      {
        "Name": "Angela Bonifati",
        "Affiliation": "Lyon 1 University & IUF"
      },
      {
        "Name": "Vasilis Efthymiou",
        "Affiliation": "Harokopio University of Athens"
      },
      {
        "Name": "George Fletcher",
        "Affiliation": "Eindhoven University of Technology"
      },
      {
        "Name": "Dimitris Plexousakis",
        "Affiliation": "FORTH-ICS"
      },
      {
        "Name": "Riccardo Tommasini",
        "Affiliation": "INSA-Lyon"
      },
      {
        "Name": "Georgia Troullinou",
        "Affiliation": "CNRS, Univ. Grenoble Alpes, Grenoble INP, LIG"
      },
      {
        "Name": "Elisjana Ymeralli",
        "Affiliation": "FORTH-ICS"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "db21a2a3-8fa1-4ecc-bf0d-262d8f7feb1b",
    "title": "LEGO-GraphRAG: Modularizing Graph-based Retrieval-Augmented Generation for Design Space Exploration",
    "abstract": "GraphRAG integrates (knowledge) graphs with large language models (LLMs) to improve reasoning accuracy and contextual relevance. Despite its promising applications and strong relevance to multiple research communities, such as databases and natural language processing, GraphRAG currently lacks modular workflow analysis, systematic solution frameworks, and insightful empirical studies. To bridge these gaps, we propose  LEGO-GraphRAG , a modular framework that enables:  1)  fine-grained decomposition of the GraphRAG workflow,  2)  systematic classification of existing techniques and implemented GraphRAG instances, and  3)  creation of new GraphRAG instances. Our framework facilitates comprehensive empirical studies of GraphRAG on large-scale real-world graphs and diverse query sets, revealing insights into balancing reasoning quality, runtime efficiency, and token or GPU cost, that are essential for building advanced GraphRAG systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LEGO-GraphRAG%3A%20Modularizing%20Graph-based%20Retrieval-Augmented%20Generation%20for%20Design%20Space%20Exploration",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3269-cao.pdf",
    "session": "Research 37: Applied ML and AI for Graph, Temporal, and Trajectory Data",
    "authors": [
      {
        "Name": "Yukun Cao",
        "Affiliation": "USTC"
      },
      {
        "Name": "Zengyi Gao",
        "Affiliation": "USTC"
      },
      {
        "Name": "Zhiyang Li",
        "Affiliation": "USTC"
      },
      {
        "Name": "Xike Xie",
        "Affiliation": "USTC"
      },
      {
        "Name": "S. Kevin Zhou",
        "Affiliation": "USTC"
      },
      {
        "Name": "Jianliang Xu",
        "Affiliation": "Hong Kong Baptist University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cfabdda1-0059-44b7-b72a-2e31aaccf733",
    "title": "OmniSQL: Synthesizing High-quality Text-to-SQL Data at Scale",
    "abstract": "Text-to-SQL, the task of translating natural language questions into SQL queries, plays a crucial role in enabling non-experts to interact with databases. While recent advancements in large language models (LLMs) have significantly enhanced text-to-SQL performance, existing approaches face notable limitations in real-world text-to-SQL applications. Prompting-based methods often depend on closed-source LLMs, which are expensive, raise privacy concerns, and lack customization. Fine-tuning-based methods, on the other hand, suffer from poor generalizability due to the limited coverage of publicly available training data. To overcome these challenges, we propose a novel and scalable text-to-SQL data synthesis framework for automatically synthesizing large-scale, high-quality, and diverse datasets without extensive human intervention. Using this framework, we introduce  SynSQL-2.5M , the first million-scale text-to-SQL dataset, containing 2.5 million samples spanning over 16,000 synthetic databases. Each sample includes a database, SQL query, natural language question, and chain-of-thought (CoT) solution. Leveraging  SynSQL-2.5M , we develop  OmniSQL , a powerful open-source text-to-SQL model available in three sizes: 7B, 14B, and 32B. Extensive evaluations across nine datasets demonstrate that  OmniSQL  achieves state-of-the-art performance, matching or surpassing leading closed-source and open-source LLMs, including GPT-4o and DeepSeek-V3, despite its smaller size. We release all code, datasets, and models to support further research.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/OmniSQL%3A%20Synthesizing%20High-quality%20Text-to-SQL%20Data%20at%20Scale",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4695-li.pdf",
    "session": "Research 7: Natural Language Interfaces to Data",
    "authors": [
      {
        "Name": "Haoyang Li",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Shang Wu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Xiaokang Zhang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Xinmei Huang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Jing Zhang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Fuxin Jiang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Shuai Wang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Tieying Zhang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Jianjun Chen",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Rui Shi",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Hong Chen",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Cuiping Li",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9a919b1c-dd62-4e55-8338-81a07a4d4857",
    "title": "Finding Convincing Views to Endorse a Claim",
    "abstract": "Recent studies investigated the challenge of assessing the strength of a given claim extracted from a dataset, particularly the claim‚Äôs potential of being misleading and cherry-picked. We focus on claims that compare answers to an aggregate query posed on a view that selects tuples. The strength of a claim amounts to the question of how likely it is that the view is carefully chosen to support the claim, whereas less careful choices would lead to contradictory claims. We embark on the study of the reverse task that offers a complementary angle in the critical assessment of data-based claims: given a claim, find useful supporting views. The goal of this task is twofold. On the one hand, we aim to assist users in finding significant evidence of phenomena of interest. On the other hand, we wish to provide them with machinery to criticize or counter given claims by extracting evidence of opposing statements. \nTo be effective, the supporting sub-population should be signifi-To be effective, the supporting sub-population should be significant and defined by a ‚Äúnatural‚Äù view. We discuss several measures of naturalness and propose ways of extracting the best views under each measure (and combinations thereof). The main challenge is the computational cost, as na√Øve search is infeasible. We devise anytime algorithms that deploy two main steps: (1) a preliminary construction of a ranked list of attribute combinations that are as-construction of a ranked list of attribute combinations that are assessed using fast-to-compute features, and (2) an efficient search for the actual views based on each attribute combination. We present a thorough experimental study that shows the effectiveness of our algorithms in terms of quality and execution cost. We also present a user study to assess the usefulness of the naturalness measures.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Finding%20Convincing%20Views%20to%20Endorse%20a%20Claim",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p439-agmon.pdf",
    "session": "Research 58: User Interfaces for Data Exploration and Recommender Systems",
    "authors": [
      {
        "Name": "Shunit Agmon",
        "Affiliation": "Technion"
      },
      {
        "Name": "Amir Gilad",
        "Affiliation": "The Hebrew University"
      },
      {
        "Name": "Brit Youngmann",
        "Affiliation": "Technion"
      },
      {
        "Name": "Shahar Zoarets",
        "Affiliation": "Technion"
      },
      {
        "Name": "Benny Kimelfeld",
        "Affiliation": "Technion"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a49062c1-b27d-4c74-846f-2e507acb1975",
    "title": "TMLKD: Few-shot Trajectory Metric Learning via Knowledge Distillation",
    "abstract": "Trajectory metric learning, which supports the trajectory similarity search, is one of the most fundamental tasks in spatial-temporal data analysis. However, existing trajectory metric learning methods rely on massive labels of pairwise trajectory distance, and thus cannot be applied to few-shot scenarios frequently occurring in real-world applications. Though performance drops caused by insuÔ¨Écient labels can be alleviated by knowledge distillation, we demonstrate that they cannot be directly applied to few-shot trajectory metric learning due to the domain shift problem. To this end, this paper proposes invariant and relaxed learning enhanced knowledge distillation method TMLKD for few-shot trajectory metric learning, such that domain-invariant representation and rank knowledge can be distilled. SpeciÔ¨Åcally, in the representation learning phase, it Ô¨Årst employs an adversarial sub-network to distinguish domain-speciÔ¨Åc and domain-invariant information, so as to distill transferable representation knowledge from teacher models. To mitigate the few-shot problem in student model training, we further enrich sparse labels of the target domain by utilizing the rank knowledge revealed in teachers‚Äô predictions. Particularly, TMLKD employs a list-wise learning-to-rank approach to learn the relaxed trajectory ranking orders instead of focusing on all the samples inefÔ¨Åciently. Finally, to guide accurate distillation, we adaptively assign reliability of teacher prediction by utilizing the ground-truth labels, to avoid misleading the student model with low-quality teacher predictions. Extensive experiments on three real-world datasets demonstrate the superiority of our model.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TMLKD%3A%20Few-shot%20Trajectory%20Metric%20Learning%20via%20Knowledge%20Distillation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2308-lai.pdf",
    "session": "Research 10: Data Mining and Analytics",
    "authors": [
      {
        "Name": "Danling Lai",
        "Affiliation": "Soochow university"
      },
      {
        "Name": "Jiajie Xu",
        "Affiliation": "Soochow University"
      },
      {
        "Name": "JIanfeng Qu",
        "Affiliation": "Soochow University"
      },
      {
        "Name": "Pingfu Chao",
        "Affiliation": "Soochow University"
      },
      {
        "Name": "Junhua Fang",
        "Affiliation": "Soochow University"
      },
      {
        "Name": "Chengfei Liu",
        "Affiliation": "Swinburne University of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "854ab70f-104a-4788-87cf-e159a29ff9f4",
    "title": "Demonstrating Matelda for Multi-Table Error Detection",
    "abstract": "Real-world datasets are often fragmented across multiple heterogeneous tables, managed by different teams or organizations. Ensuring data quality in such environments is challenging, as traditional error detection tools typically operate on isolated tables and overlook cross-table relationships. To address this gap, we investigate how cleaning multiple tables simultaneously, combined with structured user collaboration, can reduce annotation effort and enhance the effectiveness and efficiency of error detection. \nWe present Matelda, an interactive system for multi-table error detection that combines automated error detection with human-inthe-loop refinement. Matelda guides users through Inspection & Action, allowing them to explore system-generated insights, refine decisions, and annotate data with contextual support. It organizes tables using domain-based and quality-based folding and leverages semi-supervised learning to propagate labels across related tables efficiently. Our demonstration showcases Matelda‚Äôs capabilities for collaborative error detection and resolution by leveraging shared knowledge, contextual similarity, and structured user interactions across multiple tables.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Demonstrating%20Matelda%20for%20Multi-Table%20Error%20Detection",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5379-ahmadi.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Fatemeh Ahmadi",
        "Affiliation": "Technische Universit√§t Berlin - BIFOLD"
      },
      {
        "Name": "Julian Paulu√üen",
        "Affiliation": "Technische Universit√§t Berlin - BIFOLD"
      },
      {
        "Name": "Ziawasch Abedjan",
        "Affiliation": "Technische Universit√§t Berlin - BIFOLD"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "20d3a4c8-fdd3-4c1a-b407-9135c2965947",
    "title": "Semantic Operators and Their Optimization: Enabling LLM-Based Data Processing with Accuracy Guarantees in LOTUS",
    "abstract": "The semantic capabilities of large language models (LLMs) have the potential to enable rich analytics and reasoning over vast knowledge corpora. Unfortunately, existing systems either empirically optimize expensive LLM-powered operations with  no performance guarantees , or limit their support to simple batched-inference primitives. We introduce  semantic operators , the ÔÄ°rst formalism with statistical accuracy guarantees for general-purpose AI-based operations with natural language parameters (e.g., ÔÄ°ltering, sorting, joining or aggregating records using natural language criteria). Each operator can be implemented by multiple  AI algorithms , which compose individual model invocations to orchestrate the model over the data. Our programming model speciÔÄ°es the expected behavior of each operator with a high-quality  reference algorithm , and we develop an optimization framework that reduces cost, while providing accuracy guarantees for individual operators. Using this approach, we propose several novel optimizations to accelerate semantic ÔÄ°ltering, joining, group-by and top-k operations by up to 1 ,  000 ‚á• . We implement semantic operators in the LOTUS system and demonstrate LOTUS‚Äô eÔÄ¢ectiveness on real, bulk-semantic processing applications, including fact-checking, biomedical multilabel classiÔÄ°cation, search, and topic analysis. We show that the semantic operator model is expressive, capturing state-of-the-art AI pipelines in a few operator calls, and making it easy to express new pipelines that match or exceed quality of recent LLM-based analytic systems by up to 170%, while oÔÄ¢ering accuracy guarantees. Overall, LOTUS programs match or exceed the accuracy of state-ofthe-art AI pipelines for each task while running up to 3 . 6 ‚á• faster than the highest-quality baselines. LOTUS is publicly available at https://github.com/lotus-data/lotus.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Semantic%20Operators%20and%20Their%20Optimization%3A%20%20Towards%20AI-Based%20Data%20Analytics%20with%20Accuracy%20Guarantees",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4171-patel.pdf",
    "session": "Research 17: Applied ML and AI for Data Management II",
    "authors": [
      {
        "Name": "Liana Patel",
        "Affiliation": "Stanford University"
      },
      {
        "Name": "Siddharth Jha",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Melissa Pan",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Harshit Gupta",
        "Affiliation": "Stanford University"
      },
      {
        "Name": "Parth Asawa",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Carlos Guestrin",
        "Affiliation": "Stanford University"
      },
      {
        "Name": "Matei Zaharia",
        "Affiliation": "UC Berkeley"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "57b926f7-8ed0-4f60-bb4d-54db1505f013",
    "title": "Continuous Publication of Weighted Graphs with Local Differential Privacy",
    "abstract": "Although a large amount of valuable knowledge can be obtained from the weighted graph snapshots modeled over time, it may cause privacy issues. Local differential privacy (LDP) provides a strong solution for private graph data publishing in decentralized networks. However, most existing LDP studies over graphs are only applicable to static unweighted graphs. This paper investigates the problem of continuous publication of weighted graph snapshots and proposes a graph publication framework, WGT-LDP, under ùë§-event edge weight LDP, which can protect the privacy of edges and weights over any  ùë§ consecutive time steps. WGT-LDP consists of four key components: population division-based sampling that overcomes the problem of over-segmentation of the privacy budget, data range estimation that mitigates noise on edge weights, aggregate information collection that obtains important information about the graph structure and edge weights, and graph snapshot generation that reconstructs weighted graph snapshot at each time step. We provide theoretical guarantees on privacy and utility, and perform extensive experiments on three real-world and two synthetic datasets, using four commonly used metrics. Our experiments show that WGT-LDP produces high-quality synthetic weighted graphs and significantly outperforms baseline methods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Continuous%20Publication%20of%20Weighted%20Graphs%20with%20Local%20Differential%20Privacy",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4214-li.pdf",
    "session": "Research 66: Access Control and Privacy II",
    "authors": [
      {
        "Name": "Wen Xu",
        "Affiliation": "Jinan University"
      },
      {
        "Name": "Pengpeng Qiao",
        "Affiliation": "Institute of Science Tokyo"
      },
      {
        "Name": "Shang Liu",
        "Affiliation": "China University of Mining and Technology"
      },
      {
        "Name": "Zhirun Zheng",
        "Affiliation": "Ajou University"
      },
      {
        "Name": "Yang Cao",
        "Affiliation": "Institute of Science Tokyo"
      },
      {
        "Name": "Zhetao Li",
        "Affiliation": "Jinan University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "204ebca4-9a77-47a7-9896-76c853da50d8",
    "title": "LEAP: LLM-powered End-to-end Automatic Library for Processing Social Science Queries on Unstructured Data",
    "abstract": "Social scientists are increasingly interested in analyzing the seman-Social scientists are increasingly interested in analyzing the semantic information (e.g., emotion) of unstructured data (e.g., Tweets), where the semantic information is not natively present. Perform-where the semantic information is not natively present. Performing this analysis in a cost-eÔ¨Écient manner requires using machine learning (ML) models to extract the semantic information and sub-learning (ML) models to extract the semantic information and subsequently analyze the now structured data. However, this process remains challenging for domain experts. \nTo demonstrate the challenges in social science analytics, we col-To demonstrate the challenges in social science analytics, we collect a dataset, QUIET-ML, of 120 real-world social science queries in natural language and their ground truth answers. Existing systems struggle with these queries since (1) they require selecting and ap-struggle with these queries since (1) they require selecting and applying ML models, and (2) more than a quarter of these queries are vague, making standard tools like natural language to SQL systems unsuited. To address these issues, we develop LEAP, an end-to-end library that answers social science queries in natural language with ML. LEAP Ô¨Ålters vague queries to ensure that the answers are deter-ML. LEAP Ô¨Ålters vague queries to ensure that the answers are deterministic and selects from internally supported and user-deÔ¨Åned ML functions to extend the unstructured data to structured tables with necessary annotations. LEAP further generates and executes code to respond to these natural language queries. LEAP achieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a $1.06 average end-to-end cost, of which code generation costs $0.02.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LEAP%3A%20LLM-powered%20End-to-end%20Automatic%20Library%20for%20Processing%20Social%20Science%20Queries%20on%20Unstructured%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p253-hu.pdf",
    "session": "Research 11: Graph Data Privacy, Text and Semi-Structured Data Management",
    "authors": [
      {
        "Name": "Chuxuan Hu",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "Name": "Austin Peters",
        "Affiliation": "Stanford"
      },
      {
        "Name": "Daniel Kang",
        "Affiliation": "UIUC"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bfb59c0c-7279-4984-8e17-234bc1cf0d71",
    "title": "Infinite Stream Estimation under Personalized ùë§-Event Privacy",
    "abstract": "Streaming data collection is indispensable for stream data analysis, such as event monitoring. However, publishing these data directly leads to privacy leaks.  ùë§ -event privacy is a valuable tool to protect individual privacy within a given time window while maintaining high accuracy in data collection. Most existing ùë§ -event privacy studies on infinite data stream only focus on homogeneous privacy requirements for all users. In this paper, we propose personalized  ùë§ -event privacy protection that allows different users to have different privacy requirements in private data stream estimation. Specifically, we design a mechanism that allows users to maintain constant privacy requirements at each time slot, namely Personalized Window Size Mechanism (PWSM). Then, we propose two solutions to accurately estimate stream data statistics while achieving ùíò -Event  ùùê -Personalized Differential Privacy (( ùíò , ùùê )-EPDP), namely Personalized Budget Distribution (PBD) and Personalized Budget Absorption (PBA). PBD always provides at least the same privacy budget for the next time step as the amount consumed in the previous release. PBA fully absorbs the privacy budget from the previous  ùëò time slots, while also borrowing from the privacy budget of the next  ùëò time slots, to increase the privacy budget for the current time slot. We prove that both PBD and PBA outperform the state-of-the-art private stream estimation methods while satisfying the privacy requirements of all users. We demonstrate the efficiency and effectiveness of our PBD and PBA on both real and synthetic datasets, compared with the recent uniformity  ùë§ -event approaches, Budget Distribution (BD) and Budget Absorption (BA). Our PBD achieves 68% less error than BD on average on real datasets. Besides, our PBA achieves 24 . 9% less error than BA on average on synthetic datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Infinite%20Stream%20Estimation%20under%20Personalized%20w-Event%20Privacy",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1905-cheng.pdf",
    "session": "Research 66: Access Control and Privacy II",
    "authors": [
      {
        "Name": "Leilei Du",
        "Affiliation": "Hunan University"
      },
      {
        "Name": "Peng Cheng",
        "Affiliation": "Tongji University"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Heng Tao Shen",
        "Affiliation": "Tongji University"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiaotong University"
      },
      {
        "Name": "Wei Xi",
        "Affiliation": "Xi'an Jiaotong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a31974ec-e0e1-4c2d-9bbc-60a0caf106e6",
    "title": "Beyond Incrementalism: How to Change the World Through Data Systems Research",
    "abstract": "In this panel we argue that traditional data systems research remains highly in today‚Äôs world. It aims to spark dialogue on choosing impactful research problems, influencing industry, and improving data system technology.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Beyond%20Incrementalism%3A%20How%20to%20Change%20the%20World%20Through%20Data%20Systems%20Research%20(VLDB%202025%20Panel)",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5508-viktor.pdf",
    "session": "Panel 3: Beyond Incrementalism: How to Change the World Through Data Systems Research",
    "authors": [
      {
        "Name": "Viktor Leis",
        "Affiliation": "TU Munich"
      },
      {
        "Name": "Anastasia Ailamaiki",
        "Affiliation": "EPFL"
      },
      {
        "Name": "Peter Boncz",
        "Affiliation": "CWI"
      },
      {
        "Name": "Badrish Chandramouli",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Andy Pavlo",
        "Affiliation": "CMU"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3a8557d1-9fcd-4658-95cc-4dc0e532f081",
    "title": "HAKES: Scalable Vector Database for Embedding Search Service",
    "abstract": "Modern deep learning models capture the semantics of complex data by transforming them into high-dimensional embedding vectors. Emerging applications, such as retrieval-augmented generation, use approximate nearest neighbor (ANN) search in the embedding vector space to find similar data. Existing vector databases provide indexes for efficient ANN searches, with graph-based indexes being the most popular due to their low latency and high recall in real-world high-dimensional datasets. However, these indexes are costly to build, suffer from significant contention under concurrent read-write workloads, and scale poorly to multiple servers. \nOur goal is to build a vector database that achieves high throughput and high recall under concurrent read-write workloads. To this end, we first propose an ANN index with an explicit two-stage design combining a fast filter stage with highly compressed vectors and a refine stage to ensure recall, and we devise a novel lightweight machine learning technique to fine-tune the index parameters. We introduce an early termination check to dynamically adapt the search process for each query. Next, we add support for writes while maintaining search performance by decoupling the management of the learned parameters. Finally, we design HAKES, a distributed vector database that serves the new index in a disaggregated architecture. We evaluate our index and system against 12 state-of-the-art indexes and three distributed vector databases, using high-dimensional embedding datasets generated by deep learning models. The experimental results show that our index outperforms index baselines in the high recall region and under concurrent read-write workloads. Furthermore,  HAKES  is scalable and achieves up to 16 √ó  higher throughputs than the baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/HAKES%3A%20Scalable%20Vector%20Database%20for%20Embedding%20Search%20Service",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3049-ooi.pdf",
    "session": "Research 46: Vector Data Management III",
    "authors": [
      {
        "Name": "Guoyu Hu",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Shaofeng Cai",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Anh Dinh",
        "Affiliation": "Deakin University"
      },
      {
        "Name": "Zhongle Xie",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Cong Yue",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Beng Chin Ooi",
        "Affiliation": "National University of Singapore"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "84aefac1-76f4-4ed4-9c5f-1164a6aefad2",
    "title": "Suna: Scalable Causal Confounder Discovery over Relational Data",
    "abstract": "Understanding the causal relationships between treatments and outcomes is fundamental in various areas. Causal inference aims to estimate the eÔ¨Äect of one variable on another, and critically relies on access to those variables as well as the key confounders. Unfortunately, data analysts often start with datasets lacking these columns, leading to incorrect estimations. Relational data repositories hold signiÔ¨Åcant potential to augment such datasets with an admissible set of confounders necessary for causal analysis. While recent work has advocated for this potential, these approaches face notable limitations. They either assume the existence of a complete causal diagram over all datasets in the repository, which is impractical; rely on computationally infeasible techniques that do not scale to large data repositories with many features; or can only detect confounders in the absence of causal relations, and are thus ineÔ¨Äective when a causal eÔ¨Äect exists. \nWe observe that the asymmetry between causes and eÔ¨Äects used in causal discovery can be exploited to directly identify confounders for causal queries. In this paper, we establish a connection between the existence of confounders and the presence of unconfounded ancestors of the treatment variable in the underlying causal diagram‚Äîwithout requiring access to the diagram. This makes it feasible to iteratively discover confounders until an admissible set is constructed. We propose  Suna , a highly optimized, GPU-compatible system that implements a novel end-to-end algorithm for discovering confounders within large relational data repositories. Experiments on both real-world and synthetic datasets demonstrate that our system eÔ¨Äectively discovers high-quality confounders. Furthermore,  Suna  employs algorithmic optimizations to accelerate confounder discovery without materializing joins. Our experiments show that  Suna  Ô¨Ånds high-quality confounders while running >100x faster than existing confounder discovery systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Suna%3A%20Scalable%20Causal%20Confounder%20Discovery%20over%20Relational%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4158-liu.pdf",
    "session": "Research 56: Analytics over Different Data Types III",
    "authors": [
      {
        "Name": "Jiaxiang Liu",
        "Affiliation": "Columbia University"
      },
      {
        "Name": "Siyuan Xia",
        "Affiliation": "University of Chicago"
      },
      {
        "Name": "Daniel Alabi",
        "Affiliation": "UIUC"
      },
      {
        "Name": "Eugene Wu",
        "Affiliation": "Columbia University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ab60c760-6e23-4ad4-8069-8deb4f34b65b",
    "title": "DVote: Constraining Committee Voting with Database Dependencies",
    "abstract": "Approval-Based Committee (ABC) voting refers to the task of selecting a committee of a desired size, given voter preferences that state the specific candidates that each voter approves of. A voting rule aggregates the voter preferences into a winning committee. As a special case, an ABC scoring rule determines a score that each voter contributes to the committee based on her approvals. Various ways have been proposed to impose constraints on the elected committee. The demonstration presents DVote‚Äî a tool that implements a recent framework for extending score-based ABC voting with constraints on the context surrounding the candidates, given as a relational database. DVote provides a convenient interface to set up a voting instance and build contextual constraints in the form of Tuple-Generating Dependencies (TGDs) and Denial Constraints (DCs). The computation of the winning committee is done by backend components that encapsulate the contextual database and translate the entire task of constrained election into Mixed Integer Programming. In the demonstration, attendees will experience ABC voting with DVote in different domains and contexts.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DVote%3A%20Constraining%20Committee%20Voting%20with%20Database%20Dependencies",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5235-yona.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Roi Yona",
        "Affiliation": "Technion"
      },
      {
        "Name": "Jonathan Breitman",
        "Affiliation": "Technion"
      },
      {
        "Name": "Benny Kimelfeld",
        "Affiliation": "Technion & RelationalAI"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "db677dab-fb21-4e6e-819c-1e9ceec73daa",
    "title": "Using Read Promotion and Mixed Isolation Levels for Performant Yet Serializable Execution of Transaction Programs",
    "abstract": "We propose a theory that can determine the lowest isolation level that can be allocated to each transaction program in an application in a mixed-isolation-level setting, to guarantee that all executions will be serializable and thus preserve all integrity constraints, even those that are not explicitly declared. This extends prior work applied to completely known transactions, to deal with the realistic situation where transactions are generated by running programs with parameters that are not known in advance. Using our theory, we propose an optimization method that allows for high throughput while ensuring that all executions are serializable. Our method is based on searching for application code modifications that are semantics-preserving while improving the isolation level allocation. We illustrate our approach to the SmallBank benchmark.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Using%20Read%20Promotion%20and%20Mixed%20Isolation%20Levels%20for%20Performant%20Yet%20Serializable%20Execution%20of%20Transaction%20Programs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2846-vandevoort.pdf",
    "session": "Research 52: Transaction Management",
    "authors": [
      {
        "Name": "Brecht Vandevoort",
        "Affiliation": "Hasselt University"
      },
      {
        "Name": "Alan Fekete",
        "Affiliation": "University of Sydney"
      },
      {
        "Name": "Bas Ketsman",
        "Affiliation": "Vrije Universiteit Brussel"
      },
      {
        "Name": "Frank Neven",
        "Affiliation": "Hasselt University"
      },
      {
        "Name": "Stijn Vansummeren",
        "Affiliation": "Hasselt University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3eb4a1ef-fa87-408a-bb38-9f78359ff1cd",
    "title": "Triparts: Scalable Streaming Graph Partitioning to Enhance Community Structure",
    "abstract": "k-way edge based partitioning algorithms for processing large streaming graphs, such as social networks and web crawls, assign each arriving edge to one of the k partitions. This can result in vertices being replicated on multiple partitions. Typically, such partitioning algorithms aim to balance the edge counts across partitions while minimizing the vertex replication. However, such objectives ignore the community structure inherently embedded in the graph, which is an important quality metric for clustering and graph mining applications that subsequently operate on the partitions. To address this gap, we propose a novel optimization goal to maximize the number of local triangles in the partitions as an additional objective. Triangle count is an eÔ¨Äective metric to measure the conservation of community structure. Further, we propose TriParts a family of heuristics for online partitioning over an edge stream. They use three complementary state data structures: Bloom Filters, Triangle Map and High degree Map. Each state adds tangible value to meet our objectives. We validate TriParts on six diverse real world graphs with up to 1.6B edges and varying triangle densities. Our best heuristic outperforms the state-of-the-art DBH and HDRF streaming graph partitioners on the triangle-count metric by up to 4-8.3x while maintaining competitive vertex replication factor and edge-balancing. We achieve an ingest rate of 500k edges/sec on a 16 node cluster. We also oÔ¨Äer detailed results on the conÔ¨Åguration parameters, scalability and overheads of TriParts, and its practical beneÔ¨Åts for distributed graph analytics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Triparts%3A%20Scalable%20Streaming%20Graph%20Partitioning%20to%20Enhance%20Community%20Structure",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2992-simmhan.pdf",
    "session": "Research 57: Graph Data Management VII",
    "authors": [
      {
        "Name": "Ruchi Bhoot",
        "Affiliation": "Indian Institute of Science"
      },
      {
        "Name": "Tuhin Khare",
        "Affiliation": "Georgia Institute of Technology"
      },
      {
        "Name": "Manoj Agarwal",
        "Affiliation": "GiKA.AI"
      },
      {
        "Name": "Siddharth Jaiswal",
        "Affiliation": "Indian Institute of Technology Kharagpur"
      },
      {
        "Name": "Yogesh Simmhan",
        "Affiliation": "Indian Institute of Science"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "72781c14-8025-45d3-b4ea-fd05b353d0ae",
    "title": "Horizon: Robust Checks for SQL Migration Using LLMs",
    "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities in code migration across languages, making them promising for SQL schema migration. However, achieving reliable and accurate SQL migration with LLMs remains a challenge. This paper presents the first comprehensive approach for practical and effective SQL schema migration using LLMs. We highlight the necessity of robust evaluation and iterative query refinement to achieve highly accurate migrations. Building on traditional database tools along with LLMs, we introduce novel checks to guide LLMs towards syntactically complete and functionally equivalent translations. Our approach supports all schema object types, including complex procedural constructs. Our demonstrations offer audience opportunities to explore our system using a variety of configurations, datasets and custom inputs, providing useful insights into the underlying techniques, their strengths, and limitations.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Horizon%3A%20Robust%20Checks%20for%20SQL%20Migration%20Using%20LLMs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5259-emani.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Venkatesh Emani",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Wenjing Wang",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Zi Ye",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Jia He",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Neel Ball",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Kumaraswamy Boora",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Carlo Curino",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Avrilia Floratou",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Manan Goenka",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Paridhi Gupta",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Vivek Gupta",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Katherine Lin",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Nick Litombe",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Jared Meade",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Suryakant Mutnal",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Raghu Ramakrishnan",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Sudhir Raparla",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Dhruv Relwani",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Shyam Sai",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Vaibhave Sekar",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Roneet Shaw",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Harmeet Singh",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Prasanna Sridharan",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Mark Taylor",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Sunidhi Tiwari",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Yiwen Zhu",
        "Affiliation": "Microsoft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "13700b14-ae8e-4f77-bd9f-b762117c0bce",
    "title": "SAIL: A Voyage to Symbolic Approximation Solutions for Time-Series Analysis",
    "abstract": "Symbolic Approximation , a dimensionality reduction technique that transforms time series into discrete symbols, has gained increasing attention in various downstream applications. Despite decades of development, there is a noticeable absence of a comprehensive study in this domain, highlighting a need for more in-depth investigation and well-designed exploration tools. To address this gap, we propose  SAIL , a modular web engine serving two purposes: (i) to provide the first comprehensive study on 7 state-of-the-art methods over 100+ time-series datasets, the largest study in this area; (ii) to evaluate the performance of a recently proposed solution, SPARTAN, that solves two core problems. First, SPARTAN exploits intrinsic dimensionality reduction to effectively model the underlying data distribution for approximation. Second, SPARTAN dynamically allocates alphabet sizes per segment, recognizing the non-uniform distribution of information in practice. Through its interactive interface, SAIL enables users to visualize and explore quantitative assessments across various methods, datasets, and analytical tasks. SAIL‚Äôs exploration reveals that (i) while SAX variants outperform SAX by sacrificing storage, none surpass SAX under the same budget, reinforcing it as a strong baseline; SFA is the only existing method that consistently outperforms SAX within the same budget; and (ii) across diverse scenarios, SPARTAN outperforms competing methods in all evaluated tasks significantly, including classification, clustering, indexing, and anomaly detection, without incurring additional storage or runtime overhead. Overall, SAIL not only facilitates the most comprehensive studies in this field but also provides new insights and concrete solutions for future research. We release the SAIL web engine at https://saildemo.streamlit.app/.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SAIL%3A%20A%20Voyage%20to%20Symbolic%20Approximation%20Solutions%20for%20Time-Series%20Analysis",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5419-yang.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Fan Yang",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "John Paparrizos",
        "Affiliation": "The Ohio State University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c2df88f8-bf55-408f-ba12-0c0333ad9c34",
    "title": "Accelerating Approximate Nearest Neighbor Search in Hierarchical Graphs: Efficient Level Navigation with Shortcuts",
    "abstract": "Approximate Nearest Neighbor (ANN) search is a foundational yet computationally demanding query in vector databases, critical for applications such as information retrieval and generative AI inference. Hierarchical graph-based methods have attracted signiÔ¨Åcant attention due to their promising query performances compared to other indexes for ANN search. However, these methods still face eÔ¨Éciency bottlenecks because they rely on exhaustive and level-bylevel traversals within hierarchical graphs. This paper introduces SHG , a novel hierarchical graph-based index that enhances search eÔ¨Éciency by bypassing intermediate and redundant levels. SpeciÔ¨Åcally,  SHG  leverages a hierarchical vector compression method to reduce the time spent on distance computations, and employs a new data structure called shortcuts to determine the number of intermediate levels that can be safely skipped. Extensive experiments demonstrate that our solution achieves 1.5‚Äì1.8 √ó  speedup compared to state-of-the-art methods. Meanwhile, our method signiÔ¨Åcantly improves the robustness of ANN search, boosting recall by up to 20% for certain queries on benchmark datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Accelerating%20Approximate%20Nearest%20Neighbor%20Search%20in%20Hierarchical%20Graphs%3A%20Efficient%20Level%20Navigation%20with%20Shortcuts",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3518-chen.pdf",
    "session": "Research 31: Vector Data Management II",
    "authors": [
      {
        "Name": "Zengyang Gong",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "yuxiang Zeng",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "The Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1de2726a-ef79-4fd5-9725-794ad8fd32f6",
    "title": "PlanRGCN: Predicting SPARQL Query Performance",
    "abstract": "Query Performance Prediction (QPP) is the task of predicting the query runtime performance prior to its execution. While QPP has been studied in relational database systems, it has received little attention for RDF stores, i.e., triplestores that are queried via the SPARQL query language. Existing methods predict the query performance based on the syntactic similarity between a given query and past queries in the query logs. This means that they are not able to generalize to unseen queries with unseen structures or characteristics. We propose a novel GCNN architecture, PlanRGCN, to generalize to unseen queries, fully exploit statistics on the stored KG, and offer more scalable pre-training than the state of the art methods. Furthermore, our architecture is the first to support nontrivial SPARQL operators. In our experiments, we demonstrate both the superior robustness of our prediction method and its practical effect on two downstream tasks: (1) load balancing, achieving a throughput improvement of up to 207% on real-world query logs and (2) execution control, processing up to 70% more queries.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/PlanRGCN%3A%20Predicting%20SPARQL%20Query%20Performance",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1621-mohanaraj.pdf",
    "session": "Research 62: Information Integration and Data Quality IV",
    "authors": [
      {
        "Name": "Abiram Mohanaraj",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Matteo Lissandrini",
        "Affiliation": "University of Verona"
      },
      {
        "Name": "Katja Hose",
        "Affiliation": "TU Wien"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3d8e054d-66f8-46b3-88c5-e99e8259894f",
    "title": "LLMLog: Advanced Log Template Generation via LLM-driven Multi-Round Annotation",
    "abstract": "Modern computing systems, such as HDFS and Spark, produce vast quantities of logs that developers use for tasks like anomaly detection and error analysis. To simplify log analysis, template generation methods have been proposed to standardize log formats, transforming unstructured data into structured templates. Existing heuristic-based methods and neural network-based methods suffer from low accuracy problems due to the reliance on handcrafted heuristics or specific log patterns in training sets. Recently, large language models (LLMs) have shown great potential in log template generation. However, they often struggle with ambiguous, complex, or highly specific log content, which can lead to errors in generating accurate templates. To address these challenges, we propose LLMLog, a multi-round annotation framework with adaptive in-context learning. We first propose an edit-distance-based similarity metric to evaluate log similarity. Then, we introduce a method to select the most informative  ùëò unlabeled logs for annotation by considering both the representativeness of the logs and the confidence of LLM predictions. Additionally, we design an adaptive context selection strategy that adaptively selects labeled logs to ensure comprehensive keyword coverage for unlabeled logs. These labeled logs serve as the context for LLMs to better understand the unlabeled logs, thereby enhancing the accuracy of template generation. Extensive experiments on sixteen datasets demonstrate that LLMLog outperforms the state-of-the-art approaches.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LLMLog%3A%20Advanced%20Log%20Template%20Generation%20via%20LLM-driven%20Multi-Round%20Annotation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3134-li.pdf",
    "session": "Research 10: Data Mining and Analytics",
    "authors": [
      {
        "Name": "Fei Teng",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Haoyang LI",
        "Affiliation": "The Hong Kong Polytechnic University"
      },
      {
        "Name": "Lei CHEN",
        "Affiliation": "HKUST & HKUST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4ae4c6a0-985a-45ff-a4dc-214b989fc0d9",
    "title": "Accordion: Balancing Performance and Cost in Cloud-Native Data Analysis with Intra-Query Runtime Elasticity",
    "abstract": "Cloud databases empower users to leverage vast computing resources for efficient data analysis. However, achieving cost-effective utilization of these resources remains a challenge. Users often struggle to balance computing resource allocation with their temporal and financial constraints. To address this, we propose the concept of Intra-Query Runtime Elasticity  (IQRE), which allows a cloud-native OLAP engine to dynamically adjust the query  Degree of Parallelism (DOP) during query execution. We introduce Accordion, the first IQRE engine. Accordion features a friendly user interface for parallelism adjustments. It includes an auto-tuner that supports both manual and automatic DOP tuning during query execution. In this demonstration, we present Accordion‚Äôs architecture and provide attendees with hands-on experience, allowing them to execute queries and adjust query parallelism during query execution based on their time or cost budgets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Accordion%3A%20Balancing%20Performance%20and%20Cost%20in%20Cloud-Native%20Data%20Analysis%20with%20Intra-Query%20Runtime%20Elasticity",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5307-zhang.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Xukang Zhang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Huanchen Zhang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xiaofeng Meng",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c3d03634-3f51-4840-8fe2-75edaf8148d4",
    "title": "Enabling Efficient Attack Investigation via Human-in-the-Loop Security Analysis",
    "abstract": "System auditing is a vital technique for collecting system call events as system provenance and investigating complex multi-step attacks such as Advanced Persistent Threats. However, existing attack investigation methods struggle to uncover long attack sequences due to the massive volume of system provenance data and their inability to focus on attack-relevant parts. In this paper, we present Provexa , a defense system that enables human analysts to effectively analyze large-scale system provenance to reveal multi-step attack sequences.  Provexa  introduces an expressive domain-specific language,  ProvQL , that offers essential primitives for various types of attack analyses (e.g., attack pattern search, attack dependency tracking) with user-defined constraints, enabling analysts to focus on attack-relevant parts and iteratively sift through the large provenance data. Moreover,  Provexa  provides an optimized execution engine for efficient language execution. Our extensive evaluations on a wide range of attack scenarios demonstrate the practical effectiveness of  Provexa  in facilitating timely attack investigation.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Enabling%20Efficient%20Attack%20Investigation%20via%20Human-in-the-Loop%20Security%20Analysis",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3771-gao.pdf",
    "session": "Research 42: Data Models and Query Languages",
    "authors": [
      {
        "Name": "Saimon Tsegai",
        "Affiliation": "Virginia Tech"
      },
      {
        "Name": "Xinyu Yang",
        "Affiliation": "Virginia Tech"
      },
      {
        "Name": "Haoyuan Liu",
        "Affiliation": "University of California, Berkeley"
      },
      {
        "Name": "Peng Gao",
        "Affiliation": "Virginia Tech"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a4d61327-6f0d-45bc-af49-5dfb8bc04d88",
    "title": "Efficient Concurrent Updates to Persistent Randomized Binary Search Trees",
    "abstract": "In the era of big data, the demand for historical data analytics is growing across various applications. Simultaneously, range queries have been extensively explored within the domain of databases. Binary search trees are a classic type of in-memory index for facilitating range queries. Persistent binary search trees provide read-only snapshots of these trees, allowing range queries to be processed during updates while ensuring consistency. Additionally, multiple versions of snapshots support queries related to historical moments to meet the demands of numerous applications.\nHowever, existing implementations do not support both high-speed updates and efficient, accurate historical queries on multicore platforms. Motivated by this gap, we propose a novel concurrent update strategy to balance update and query performance. For a binary search tree containing ùëõ elements, our approach completes ùëö updates in O (log ùëõ + ùëö) time using O (log ùëõ) threads. We further implement a hybrid concurrent strategy to improve the scalability and practical performance of our solution.\nThe experimental results demonstrate that our proposal strikes a good balance between update and query performance. In particular, our proposal outperforms existing solutions under workloads with different data distributions and varying update-query ratios.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Concurrent%20Updates%20to%20Persistent%20Randomized%20Binary%20Search%20Trees",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1481-wang.pdf",
    "session": "Research 22: Views, Indexing, and Search II",
    "authors": [
      {
        "Name": "Guanhao Hou",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Jinchao Huang",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Fangyuan Zhang",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Sibo Wang",
        "Affiliation": "The Chinese University of Hong Kong"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fa327e74-b519-4d49-9a14-d8fecd7600eb",
    "title": "DobLIX: A Dual-Objective Learned Index for Log-Structured Merge Trees",
    "abstract": "In this paper, we introduce  DobLIX , a dual-objective learned index (LI) specifically designed for Log-Structured Merge (LSM) treebased key-value stores. Traditional LIs primarily focus on optimizing index lookups, often overlooking the critical role of data access from storage, which can become a significant performance bottleneck. In LSM-based systems, a considerable portion of the index is stored on disk, making lookups highly dependent on the efficient coordination between in-memory structures and disk-resident data. Poorly optimized access patterns can lead to excessive I/O operations, negatively impacting read latency and overall system performance.  DobLIX  addresses this by incorporating a second objective, data access optimization, into the LI training process. This dual-objective approach ensures that both index lookup efficiency and data access costs are minimized, leading to significant improvements in read performance while maintaining write efficiency in real-world LSM systems. Additionally,  DobLIX  features a reinforcement learning agent that dynamically tunes the system parameters, allowing it to adapt to varying workloads in real-time. Experimental results using real-world datasets demonstrate that  DobLIX  reduces indexing overhead and improves throughput by 1 . 19 √ó  to 2 . 21 √ó compared to state-of-the-art methods within RocksDB, a widely used LSM-based storage engine.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DobLIX%3A%20A%20Dual-Objective%20Learned%20Index%20for%20Log-Structured%20Merge%20Trees",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3965-heidari.pdf",
    "session": "Research 22: Views, Indexing, and Search II",
    "authors": [
      {
        "Name": "Alireza Heidari",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Amirhossein Ahmadi",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Wei Zhang",
        "Affiliation": "Huawei"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d158dc30-2c92-4bc8-8b7b-7895e65fd91b",
    "title": "ChatTS: Aligning Time Series with LLMs via Synthetic Data for Enhanced Understanding and Reasoning",
    "abstract": "Understanding time series is crucial for its application in real-world scenarios. Recently, large language models (LLMs) have been increasingly applied to time series tasks, leveragin g their strong language capabilities to enhance various applications. However, research on multimodal LLMs (MLLMs) for time series understanding and reasoning remains limited, primarily due to the scarcity of high-quality datasets that align time series with textual information. This paper introduces ChatTS, a novel MLLM designed for time series analysis. ChatTS treats time series as a modality, similar to how vision MLLMs process images, enabling it to perform both understanding and reasoning with time series. To address the scarcity of training data, we propose an attribute-based method for generating synthetic time series and Time Series Evol-Instruct to generates diverse Q&As for enhanced reasoning capabilities. To the best of our knowledge, ChatTS is the !rst MLLM that takes multivariate time series as input for understanding and reasoning, which is !ne-tuned exclusively on synthetic datasets. We evaluate its performance using benchmark datasets with real-world data, including six alignment tasks and four reasoning tasks. Our results show that ChatTS signi!cantly outperforms existing vision-based MLLMs (e.g., GPT-4o) and text/agent-based LLMs, achieving a 46.0% improvement in alignment tasks and a 25.8% improvement in reasoning tasks. We have open-sourced the source code, model checkpoint and datasets at https://github.com/NetManAIOps/ChatTS.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ChatTS%3A%20Aligning%20Time%20Series%20with%20LLMs%20via%20Synthetic%20Data%20for%20Enhanced%20Understanding%20and%20Reasoning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2385-xie.pdf",
    "session": "Research 19: Time Series Data II",
    "authors": [
      {
        "Name": "Zhe Xie",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Zeyan Li",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Xiao He",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Longlong Xu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xidao Wen",
        "Affiliation": "BizSeer Technology"
      },
      {
        "Name": "Tieying Zhang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Jianjun Chen",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Rui Shi",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Dan Pei",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8d4eb43d-5253-4075-8187-79dfa07a4b2b",
    "title": "Chimera: Mitigating Ownership Transfers in Multi-Primary Shared-Storage Cloud-Native Databases",
    "abstract": "Cloud-native database systems with multi-primary shared-storage architecture have emerged due to their superior performance over primary-secondary architecture on write-intensive workload scenarios. However, these systems face performance degradation as the proportion of shared data increases, adversely affecting their CostPerformance Ratio (CPR). In this paper, we identify frequent page ownership transfers between primaries as a key factor contributing to these performance bottlenecks. To address this challenge, we propose  Chimera , a multi-primary database system that employs a two-phase transaction scheduling mechanism, combined with a delay-fetch ownership transfer strategy to effectively reduce the overhead of ownership transfers. Extensive experiments on SmallBank and TPC-C benchmarks demonstrate that  Chimera outperforms existing schedule methods for multi-primary systems, achieving performance gains of 1.86 √ó ‚àº 19.03 √ó  on throughput.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Chimera%3A%20Mitigating%20Ownership%20Transfers%20in%20Multi-Primary%20Shared-Storage%20Cloud-Native%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3368-chunyue.pdf",
    "session": "Research 26: Distributed Transactions I",
    "authors": [
      {
        "Name": "Huang Chunyue",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Shuang Liu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Xinyi Zhang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Wenhao Li",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Wei Lu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Xiaoyong Du",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b7db64fd-98ee-4c0e-9bf2-fb3b9a9bb2a0",
    "title": "LogCloud: Fast Search of Compressed Logs on Object Storage",
    "abstract": "Large organizations emit terabytes of logs every day in their cloud environment. Efficient data science on these logs via text search is crucial for gleaning operational insights and debugging production outages. Current log management systems either perform full-text indexing on a cluster of dedicated servers to provide efficient search at the expense of high storage cost, or store unindexed compressed logs on object storage at the expense of high search cost. \nWe propose LogCloud, a new object-storage based log management system that supports both cheap compressed log storage and efficient search. LogCloud constructs inverted indices on compressed logs using a novel FM-index implementation that supports efficient querying from object storage directly, removing the need for dedicated indexing servers. Experiments on five public and five production log datasets show that LogCloud can achieve both cheap storage and search, scaling to TB-scale datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LogCloud%3A%20Fast%20Search%20of%20Compressed%20Logs%20on%20Object%20Storage",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2362-wang.pdf",
    "session": "Research 56: Analytics over Different Data Types III",
    "authors": [
      {
        "Name": "Ziheng Wang",
        "Affiliation": "Stanford"
      },
      {
        "Name": "Junyu Wei",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Alex Aiken",
        "Affiliation": "Stanford"
      },
      {
        "Name": "Guangyan Zhang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Jacob Torring",
        "Affiliation": "NTNU"
      },
      {
        "Name": "Rain Jiang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Chenyu Jiang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Wei Xu",
        "Affiliation": "Bytedance"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cb004fe7-8967-457e-98f0-c9c40116f61e",
    "title": "Selective Late Materialization in Modern Analytical Databases",
    "abstract": "Late Materialization (LM) is a critical technique applied in traditional column stores to speed up analytical queries. However, with modern analytical databases evolved to incorporate a vectorized columnar execution engine, LM‚Äôs benefits in I/O reduction and fast columnar query processing have diminished. In this paper, we redefine the concept of Late Materialization in the context of modern analytical databases and propose Selective Late Materialization (SLM) to allow each attribute in a query to choose its own materialization point that yields the minimum cost. SLM expands the solution space of the traditional materialization problem from one unified hard-coded binary decision (i.e., early or late) for all attributes to per attribute per query decisions. By integrating SLM into DuckDB, we show that SLM consistently outperforms the baselines of Early Materialization and Late Materialization by 14.7% and 8.9%, respectively, on average using the Join Order Benchmark (JOB), with up to 76.7% latency reduction for individual queries. We observe similar results for the TPC-DS benchmark.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Selective%20Late%20Materialization%20in%20Modern%20Analytical%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4616-liu.pdf",
    "session": "Research 8: Database Engines for OLAP",
    "authors": [
      {
        "Name": "Yihao Liu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Shaoxuan Tang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Yulong Hui",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Hangrui Zhou",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Huanchen Zhang",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d8aa7bfe-0f4a-4197-9e92-d94e66fc1326",
    "title": "X-Blossom: Massive Parallelization of Graph Maximum Matching",
    "abstract": "The blossom algorithm computes maximum matchings in graphs and has been widely applied across diverse domains, including machine learning, economic analysis, and other essential data analytics applications. As data scales and the demand for real-time processing intensifies, high-performance computing solutions have become indispensable. Over the years, substantial research efforts have been dedicated to improving the sequential blossom algorithm. However, developing an efficient parallel solution remains highly challenging due to the algorithm‚Äôs intricate execution patterns, sequential recursive dependencies, dynamic data structure modifications, and inefficient path search. \nBy thoroughly analyzing existing solutions, we have identified critical issues and proposed a new parallel framework called XBlossom. This framework eliminates recursion entirely, enables efficient searches for multiple disjoint paths, and employs a simple path table to trace paths, removing the need for dynamic graphs and trees. These efforts in algorithm development result in significant performance enhancement. Extensive experiments on real-world datasets show that X-Blossom outperforms all existing solutions, achieving up to 992x speedup compared to the fastest sequential baseline, and an average of 431x speedup over the state-of-theart parallel solution using 8 cores. It also demonstrates excellent scalability, achieving an average speedup of 1.72x when threads double in scalability tests to 64 cores. To the best of our knowledge, X-Blossom is the fastest solution for this class of graph algorithms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/X-Blossom%3A%20Massive%20Parallelization%20of%20Graph%20Maximum%20Matching",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3339-fan.pdf",
    "session": "Research 13: Graph Data Management II",
    "authors": [
      {
        "Name": "Dayi Fan",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "Rubao Lee",
        "Affiliation": "Freelance"
      },
      {
        "Name": "Xiaodong Zhang",
        "Affiliation": "The Ohio State University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a90df067-b1a0-4d54-bf69-c2a395774811",
    "title": "LARGE: A Length-Aggregation-based Grid Structure for Line Density Visualization",
    "abstract": "Line Density Visualization (LDV) is an important operation of geospatial analysis, which has been extensively used in many application domains, e.g., urban planning, criminology, and transportation science. However, LDV is computationally demanding. Therefore, existing exact solutions are not scalable (or even not feasible) to support large-scale datasets and high resolution sizes for generating LDV. To handle the efficiency issues, we develop the first solution to approximately compute LDV with an ùúñ-relative error guarantee, which consists of two main parts. First, we develop the new indexing structure, called length-aggregation-based grid structure (LARGE). Second, based on LARGE, we develop two types of fast bound functions, namely (1) square-shaped lower and upper bound functions and (2) arbitrary-shaped lower and upper bound functions, which can filter a large portion of unnecessary computations. By theoretically analyzing the tightness of our bound functions and experimentally comparing our solution with existing exact solutions on four large-scale datasets, we demonstrate that our solution can be scalable to generate high-resolution LDVs using large-scale datasets. In particular, our solution achieves up to 291.8x speedups over the state-of-the-art solutions.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/LARGE%3A%20A%20Length-Aggregation-based%20Grid%20Structure%20for%20Line%20Density%20Visualization",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4585-chan.pdf",
    "session": "Research 58: User Interfaces for Data Exploration and Recommender Systems",
    "authors": [
      {
        "Name": "Tsz Nam Chan",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Bojian Zhu",
        "Affiliation": "Hong Kong Baptist University"
      },
      {
        "Name": "Dingming Wu",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Yun PENG",
        "Affiliation": "Guangzhou University"
      },
      {
        "Name": "Leong Hou U",
        "Affiliation": "University of Macau"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cc3bae46-afc8-41de-bcd6-50e19a4e3edb",
    "title": "In-depth Analysis of Densest Subgraph Discovery in a Unified Framework",
    "abstract": "As a fundamental topic in graph mining,  Densest Subgraph Discovery (DSD)  has found a wide spectrum of real applications. Several DSD algorithms, including exact and approximation algorithms, have been proposed in the literature. However, these algorithms have not been systematically and comprehensively compared under the same experimental settings. In this paper, we first summarize a unified framework to incorporate all DSD algorithms from a highlevel perspective. We then extensively compare representative DSD algorithms over a range of graphs ‚Äì from small to billion-scale ‚Äì and examine the effectiveness of all methods, providing a thorough analysis of DSD algorithms. As a byproduct of our experimental analysis, we are also able to identify new variants of the DSD algorithms over undirected graphs, by combining existing techniques, which are up to 10 √ó faster than the state-of-the-art algorithm with the same accuracy guarantee. Finally, based on the findings, we offer promising research opportunities. We believe that a deeper understanding of the behavior of existing algorithms can provide new valuable insights for future research.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/In-depth%20Analysis%20of%20Densest%20Subgraph%20Discovery%20in%20a%20Unified%20Framework",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1131-zhou.pdf",
    "session": "Research 27: Graph Data Management III",
    "authors": [
      {
        "Name": "Yingli Zhou",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Qingshuo Guo",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Yi Yang",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Yixiang Fang",
        "Affiliation": "School of Data Science, The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Chenhao Ma",
        "Affiliation": "The Chinese University of Hong Kong, Shenzhen"
      },
      {
        "Name": "Laks Lakshmanan",
        "Affiliation": "The University of British Columbia"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4e334570-7dc1-4831-b852-f4bb7fa2c2b7",
    "title": "Maximum Defective Clique Computation: Improved Time Complexities and Practical Performance",
    "abstract": "ùëò-defective clique is a relaxation of the well-studied clique structure, by allowing up-to ùëò edges missing from a clique. The problem of finding a ùëò-defective clique with the largest number of vertices, although being NP-hard, has been receiving increasing interests recently, with advancements in both the theoretical time complexity and practical efficiency. The state-of-the-art time complexity is O‚àó(ùõæ‚Åø‚Çñ), where O‚àó ignores polynomial factors, ùëõ is the number of ùëò vertices in the input graph ùê∫, and ùõæ‚Çñ < 2 is a constant that only depends on ùëò. In this paper, we first prove, through a more refined and non-trivial analysis, that the time complexity of an existing algorithm can actually bebounded by O‚àó(ùõæùëõ ),where ùõæ‚Çñ‚Çã‚ÇÅ <ùõæ‚Çñ. ùëò‚àí1Then, by utilizing the diameter-two property of large ùëò-defective cliques, we show that for graphs with maximum ùëò-defective clique sizes ùúîùëò(ùê∫) ‚â• ùëò + 2, a maximum ùëò-defective clique can be found in O‚àó((ùõºŒî)·µè‚Å∫¬≤ùõæ·µÖ‚Çñ‚Çã‚ÇÅ ) time when using the degeneracy parameteri- ùëò‚àí1 zation ùõº and in O‚àó((ùõºŒî)·µè‚Å∫¬≤(ùëò + 1)ùõº+k+1‚àíùúî‚Çñ(ùê∫)) time when using thedegeneracy-gapparameterization ùõº+ùëò+1‚àíùúî‚Çñ(ùê∫);here,ùõº and Œî are the degeneracy and maximum degree of ùê∫ , respectively. Note that, most real graphs satisfy ùúîùëò (ùê∫) ‚â• ùëò + 2 and ùõº ‚â™ ùëõ. Lastly, to improve the practical performance, we design a new degree- sequence-based reduction rule that can be efficiently applied, and theoretically demonstrate its effectiveness compared with the exist- ing reduction rules. Extensive empirical studies on three benchmark graph collections, containing 290 graphs in total, show that our algorithm is also practically efficient, by outperforming all existing algorithms by several orders of magnitude. We remark that our proving techniques for reducing the base from ùõæ‚Çñ to ùõæ‚Çñ‚Çã‚ÇÅ and our general principle of designing a new reduction rule may also be beneficial to other problems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Maximum%20Defective%20Clique%20Computation%3A%20Improved%20Time%20Complexities%20and%20Practical%20Performance",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p200-chang.pdf",
    "session": "Research 27: Graph Data Management III",
    "authors": [
      {
        "Name": "Lijun Chang",
        "Affiliation": "The University of Sydney"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2bb5fa3a-c0c1-482f-b642-b30e02a60a5b",
    "title": "Time Series Motif Discovery: A Comprehensive Evaluation",
    "abstract": "Motif Discovery involves identifying recurring patterns and locating their occurrences within a time series without prior knowledge about their shape or location. In practice, Motif Discovery faces several data-related challenges, leading to various deÔ¨Ånitions of the problem and multiple algorithms addressing these challenges to diÔ¨Äerent extents. However, there has been no systematic evaluation and comparison of these diverse approaches. Consequently, this paper presents a comprehensive literature review covering data-related challenges, motif deÔ¨Ånitions, and algorithms. We also analyze the strengths and limitations of algorithms carefully chosen to represent the literature diversity. The analysis is structured around key research questions identiÔ¨Åed from our review. Our experimental Ô¨Åndings provide practical guidelines for selecting Motif Discovery algorithms suitable for a given task and suggest directions for future research.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Time%20Series%20Motif%20Discovery%3A%20A%20Comprehensive%20Evaluation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2226-boniol.pdf",
    "session": "Research 25: Time Series Data III",
    "authors": [
      {
        "Name": "Valerio Guerrini",
        "Affiliation": "Centre Borelli"
      },
      {
        "Name": "Thibaut Germain",
        "Affiliation": "ENS Paris Saclay"
      },
      {
        "Name": "Charles Truong",
        "Affiliation": "Universit√© Paris-Saclay, ENS Paris-Saclay"
      },
      {
        "Name": "Laurent Oudre",
        "Affiliation": "ENS Paris-Saclay"
      },
      {
        "Name": "Paul Boniol",
        "Affiliation": "Inria, Ecole normale sup√©rieure"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f9cc64c6-cfa3-4182-bed8-eea6cf9e72b7",
    "title": "Effective and Efficient Community Search for Complex Network Semantics Capture: From Coarse-Grain to Fine-Grain",
    "abstract": "To analyze the massive social networks for providing personalized services, community search is widely studied to find the densely connected subgraph that can reflect the network properties for a given query. The existing community search methods adopt single community model to make structural constraints on communities, which can only describe single interaction mode. Since they fail to capture the semantics of the network with multiple interaction modes, they struggle to find the representative communities. To solve this issue, we design a novel community model called  ( ùúè, ùúå ) camp to flexibly capture complex network semantics in any level of granularity. We propose the unified support maximized community search problem to find the communities with the densest network semantics, which is proven a NP-hard problem. By constructing a hierarchical index structure, we propose an approximate community search algorithm with approximation ratio of 2 and linear time complexity of the query size. Extensive experiments are conducted on two public datasets and two crawled datasets. The experimental results prove the effectiveness and efficiency of our method.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Effective%20and%20Efficient%20Community%20Search%20for%20Complex%20Network%20Semantics%20Capture%3A%20From%20Coarse-Grain%20to%20Fine-Grain",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3669-wang.pdf",
    "session": "Research 28: Social Networks",
    "authors": [
      {
        "Name": "Shuai Han",
        "Affiliation": "Harbin Engineering University"
      },
      {
        "Name": "Yushi Tao",
        "Affiliation": "Harbin Engineering University"
      },
      {
        "Name": "Jingwen Tan",
        "Affiliation": "Harbin Engineering University"
      },
      {
        "Name": "Huanran Wang",
        "Affiliation": "Harbin Engineering University"
      },
      {
        "Name": "Wu Yang",
        "Affiliation": "Harbin Engineering University"
      },
      {
        "Name": "Yanmei Wang",
        "Affiliation": "China Unicom"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4fee8ff7-02b1-4385-abec-d0175cf61669",
    "title": "Scaling GPU-Accelerated Databases beyond GPU Memory Size",
    "abstract": "There has been considerable interest in leveraging GPUs‚Äô computational power and high memory bandwidth for analytical database workloads. However, their limited memory capacity remains a fundamental limitation for databases whose sizes far exceed the GPU memory size. This challenge is exacerbated by the slow PCIe data transfer speed, that creates a bottleneck in overall system performance. In this work, we introduce a hybrid CPU-GPU query processing strategy that leverages the distinct strengths of CPU and GPU to alleviate the data transfer bottleneck. Our approach performs highly eÔ¨Écient data Ô¨Åltering on the CPU, which substantially reduces the volume of data transferred to the GPU via PCIe, and oÔ¨Ñoads compute-intensive operators such as joins to the GPU for further processing. Our evaluation on the TPC-H benchmark at scale factors up to 1000 (1TB), using a single A100 GPU with 80GB memory, demonstrates that our approach can eÔ¨Äectively handle datasets signiÔ¨Åcantly larger than the GPU memory size. Moreover, it substantially outperforms a state-of-the-art CPU-only database system in both performance and cost-eÔ¨Äectiveness.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Scaling%20GPU-Accelerated%20Databases%20beyond%20GPU%20Memory%20Size",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4518-li.pdf",
    "session": "Research 38: Data Management on Novel Hardware",
    "authors": [
      {
        "Name": "Yinan Li",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Bailu Ding",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Ziyun Wei",
        "Affiliation": "Cornell University"
      },
      {
        "Name": "Lukas Maas",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Momin Al-Ghosien",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Spyros Blanas",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "Nicolas Bruno",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Carlo Curino",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Matteo Interlandi",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Craig Peeper",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Kaushik Rajan",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Surajit Chaudhuri",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Johannes Gehrke",
        "Affiliation": "Microsoft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a324c400-c50e-4f2c-aa66-30c5d8d85492",
    "title": "ML-Asset Management: Curation, Discovery, and Utilization",
    "abstract": "Machine learning ( ML ) assets, such as models, datasets, and metadata‚Äîare central to modern  ML  workflows. Despite their explosive growth in practice, these assets are often underutilized due to fragmented documentation, siloed storage, inconsistent licensing, and lack of unified discovery mechanisms, making  ML -asset management an urgent challenge. This tutorial offers a comprehensive overview of  ML -asset management activities across its lifecycle, including curation, discovery, and utilization. We provide a categorization of  ML  assets, and major management issues, survey stateof-the-art techniques, and identify emerging opportunities at each stage. We further highlight system-level challenges related to scalability, lineage, and unified indexing. Through live demonstrations of systems, this tutorial equips both researchers and practitioners with actionable insights and practical tools for advancing  ML -asset management in real-world and domain-specific settings.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ML-Asset%20Management%3A%20Curation%2C%20Discovery%2C%20and%20Utilization",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5493-wang.pdf",
    "session": "Tutorial 10: ML-Asset Management: Curation, Discovery, and Utilization",
    "authors": [
      {
        "Name": "Mengying Wang",
        "Affiliation": "Case Western Reserve University"
      },
      {
        "Name": "Moming Duan",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Yicong Huang",
        "Affiliation": "University of California, Irvine"
      },
      {
        "Name": "Chen Li",
        "Affiliation": "University of California, Irvine"
      },
      {
        "Name": "Bingsheng He",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Yinghui Wu",
        "Affiliation": "Case Western Reserve University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1573bb23-f2cc-4ae8-b655-222974c13202",
    "title": "Efficient Cost Modeling of Space-filling Curves",
    "abstract": "A space-filling curve (SFC) maps points in a multi-dimensional space to one-dimensional points by discretizing the multi-dimensional space into cells and imposing a linear order on the cells. This way, an SFC enables computing a one-dimensional layout for multidimensional data storage and retrieval. Choosing an appropriate SFC is crucial, as different SFCs have different effects on query performance. Currently, there are two primary strategies: 1) deterministic schemes, which are computationally efficient but often yield suboptimal query performance, and 2) dynamic schemes, which consider a broad range of candidate SFCs based on cost functions but incur significant computational overhead. Despite these strategies, existing methods cannot efficiently measure the effectiveness of SFCs under heavy query workloads and numerous SFC options.\nTo address this problem, we propose means of constant-time cost estimations that can enhance existing SFC selection algorithms, enabling them to learn more effective SFCs. Additionally, we propose an SFC learning method that leverages reinforcement learning and our cost estimations to choose an SFC pattern efficiently. Experimental studies offer evidence of the effectiveness and efficiency of the proposed means of cost estimation and SFC learning.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Efficient%20Cost%20Modeling%20of%20Space-filling%20Curves",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4773-liu.pdf",
    "session": "Research 55: Spatial and Temporal Databases",
    "authors": [
      {
        "Name": "Guanli Liu",
        "Affiliation": "The University of Melbourne"
      },
      {
        "Name": "Lars Kulik",
        "Affiliation": "University of Melbourne"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Tianyi Li",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Renata Borovica-Gajic",
        "Affiliation": "University of Melbourne"
      },
      {
        "Name": "Jianzhong Qi",
        "Affiliation": "The University of Melbourne"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "df3c4c6d-a89e-46ff-a398-caf14254a324",
    "title": "RadlER: Deduplicated Sampling On-Demand",
    "abstract": "Data practitioners often need to sample their datasets to produce representative subsets for their downstream tasks. Unfortunately, real-world datasets frequently contain duplicates, whose presence biases sampling and impacts the quality of the produced subsets, hence the outcome of downstream tasks. While deduplication is therefore fundamental, performing it on the entire dataset to run sampling on its cleaned version might be prohibitively expensive in terms of time and resources. Thus, we recently introduced RadlER, a solution to perform  deduplicated sampling on-demand , i.e., to produce a clean sample of a dirty dataset incrementally, according to a target distribution of some subpopulations, by focusing the cleaning effort only on entities required to appear in the sample. \nIn this demonstration, we interactively show how RadlER can support practitioners in their data science pipelines, allowing them to save a relevant amount of time and resources.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RadlER%3A%20Deduplicated%20Sampling%20On-Demand",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5319-zecchini.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Luca Zecchini",
        "Affiliation": "BIFOLD & TU Berlin"
      },
      {
        "Name": "Ziawasch Abedjan",
        "Affiliation": "BIFOLD & TU Berlin"
      },
      {
        "Name": "Vasilis Efthymiou",
        "Affiliation": "Harokopio University of Athens"
      },
      {
        "Name": "Giovanni Simonini",
        "Affiliation": "University of Modena and Reggio Emilia"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "45c9ca5a-75aa-4bb2-a0e6-8b8295cd29f2",
    "title": "Magneto: Combining Small and Large Language Models for Schema Matching",
    "abstract": "Recent advances in language models (LMs) open new opportunities for schema matching (SM). Recent approaches have shown their potential and key limitations: while small LMs (SLMs) require costly, difficult-to-obtain training data, large LMs (LLMs) demand significant computational resources and face context window constraints. We present  Magneto , a cost-effective and accurate solution for SM that combines the advantages of SLMs and LLMs to address their limitations. By structuring the SM pipeline in two phases, retrieval and reranking,  Magneto  can use computationally efficient SLMbased strategies to derive candidate matches which can then be reranked by LLMs, thus making it possible to reduce runtime while improving matching accuracy. We propose (1) a self-supervised approach to fine-tune SLMs which uses LLMs to generate syntactically diverse training data, and (2) prompting strategies that are effective for reranking. We also introduce a new benchmark, developed in collaboration with domain experts, which includes real biomedical datasets and presents new challenges for SM methods. Through a detailed experimental evaluation, using both our new and existing benchmarks, we show that  Magneto  is scalable and attains high accuracy for datasets from different domains.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Magneto%3A%20Combining%20Small%20and%20Large%20Language%20Models%20for%20Schema%20Matching",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2681-freire.pdf",
    "session": "Research 43: Information Integration and Data Quality II",
    "authors": [
      {
        "Name": "Yurong Liu",
        "Affiliation": "New York University"
      },
      {
        "Name": "Eduardo Pena",
        "Affiliation": "New York University"
      },
      {
        "Name": "Aecio Santos",
        "Affiliation": "New York University"
      },
      {
        "Name": "Eden Wu",
        "Affiliation": "New York University"
      },
      {
        "Name": "Juliana Freire",
        "Affiliation": "New York University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0745d27e-b36b-4b53-bc73-baaebdee92de",
    "title": "How and Why False Denial Constraints are Discovered",
    "abstract": "Denial Constraints (DCs) are a Ô¨Çexible formalism to express many types of data rules, making them a widely adopted tool for many applications. This Ô¨Çexibility led to the development of numerous algorithms to automatically discover DCs directly from data. However, few studies have been conducted on the quality of the discovered DCs. We experimentally quantify the lack of quality in the results obtained by state-of-the-art algorithms, showing how the proportion of discovered DCs that are false is rarely below 95%. We hypothesize that the common source of these erroneous DCs stems from the adoption of the current DC validity deÔ¨Ånition. We use a statistical approach to explain the mechanism leading to these results, and propose a redeÔ¨Ånition of DC validity properties to avoid the acceptance of false DCs. We validate this redeÔ¨Ånition experimentally, showing that it exclusively accepts true constraints of the data, and is reliable enough to discover DCs missed by domain experts. Additionally, we provide curated sets of golden DCs for each dataset used in our study, those generated by domain experts and those discovered using our approach.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/How%20and%20Why%20False%20Denial%20Constraints%20are%20Discovered",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3477-martin.pdf",
    "session": "Research 62: Information Integration and Data Quality IV",
    "authors": [
      {
        "Name": "Albert Martin",
        "Affiliation": "Universitat Polit√®cnica de Catalunya"
      },
      {
        "Name": "Eduardo C. de Almeida",
        "Affiliation": "Federal University of Paran√°"
      },
      {
        "Name": "Oscar Romero",
        "Affiliation": "Universitat Polit√®cnica de Catalunya"
      },
      {
        "Name": "Anna Queralt",
        "Affiliation": "Universitat Polit√®cnica de Catalunya"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "31b8258a-6c7d-4bb1-a5c1-cb35645ac989",
    "title": "Sonata: Multi-Database Transactions Made Fast and Serializable",
    "abstract": "Today, the wide adoption of distributed service-oriented applications has rendered multi-database transactions increasingly important. They protect cross-service workflows that access multiple database systems from concurrency anomalies and failures. This paper presents Sonata, a new multi-database transaction system that provides high performance, global serializability, and seamless integration with existing applications and database systems. Sonata builds on the theory of commitment ordering to ensure global serializability and uses two-phase commit for atomicity and durability. Instead of treating database systems as black box storage, Sonata reuses existing database systems‚Äô concurrency control yet refrains from exposing or modifying their internals. It performs additional non-blocking coordination only at prepare time via applicationlevel shim layers, allowing applications to incorporate Sonata without changing their existing queries or database systems. Evaluation using TPC-C shows that Sonata incurs 7.1% coordination overhead on average and outperforms prior work by up to 1114.3%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Sonata%3A%20Multi-Database%20Transactions%20Made%20Fast%20and%20Serializable",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3449-tang.pdf",
    "session": "Research 48: Distributed Transactions II",
    "authors": [
      {
        "Name": "Chuzhe Tang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Zhaoguo Wang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Jinyang Li",
        "Affiliation": "New York University"
      },
      {
        "Name": "Haibo Chen",
        "Affiliation": "Shanghai Jiao Tong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e612298b-cf35-42f3-ada3-49e9efc08e47",
    "title": "Rebirth-Retire: A Concurrency Control Protocol Adaptable to Different Levels of Contention",
    "abstract": "The Wound-Retire concurrency control protocol was proposed to reduce contention for hotspots in in-memory databases. It enhances throughput under high-contention scenarios by allowing transactions to release their locks earlier (referred to as  Retire ), thereby reducing the wait times for other transactions. However, the proactive early release of locks introduces additional overhead, making it less efficient than other lock-based protocols in low-contention scenarios. Moreover, the wound strategy it adopts, while effective at preventing deadlocks, may lead to unnecessary transaction aborts. \nTo address these issues, this paper proposes the Rebirth-Retire concurrency control protocol as an enhancement to the WoundRetire protocol. In this protocol, a lock is retired by a younger transaction that requests it, which reduces unnecessary retire costs in low-contention scenarios. Additionally, rather than aborting younger transactions, older transactions are assigned larger timestamps (referred to as  Rebirth ), unless doing so would result in a deadlock. Experimental evaluations demonstrate that the RebirthRetire protocol achieves better throughput and lower abort rate than the Wound-Retire protocol across varying levels of contention workloads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Rebirth-Retire%3A%20A%20Concurrency%20Control%20Protocol%20Adaptable%20to%20Different%20Levels%20of%20Contention",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3162-zhang.pdf",
    "session": "Research 33: Data Stream Mining",
    "authors": [
      {
        "Name": "Qian Zhang",
        "Affiliation": "east china normal university"
      },
      {
        "Name": "Yiwen Xiang",
        "Affiliation": "east china normal university"
      },
      {
        "Name": "Jianhao Wei",
        "Affiliation": "east china normal university"
      },
      {
        "Name": "Yang Yang",
        "Affiliation": "east china normal university"
      },
      {
        "Name": "Yifan Li",
        "Affiliation": "east china normal university"
      },
      {
        "Name": "Xueqing Gong",
        "Affiliation": "east china normal university"
      },
      {
        "Name": "Wanggen Liu",
        "Affiliation": "Transwarp Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f618ce14-5fe2-4a40-b230-cfd89a0b1277",
    "title": "Is Integer Linear Programming All You Need for Deletion Propagation? A Unified and Practical Approach for Generalized Deletion Propagation",
    "abstract": "Deletion Propagation (DP) refers to a family of database problems rooted in the classical view-update problem: how to propagate intended deletions in a view (query output) back to the source database while satisfying constraints and minimizing side e!ects. Although studied for over 40 years, DP variants, their complexities, and practical algorithms have been typically explored in isolation. \nThis work presents a uni\"ed and generalized framework for DP with several key bene\"ts: (1) It  uni!es and generalizes  all previously known DP variants, e!ectively subsuming them within a broader class of problems, including new, well-motivated variants. (2) It comes with a practical and general-purpose algorithm that is ‚Äú coarse-grained instance-optimal ‚Äù: it runs in PTIME for all known PTIME cases and can  automatically exploit structural regularities  in the data, i.e. it does not rely on hints about such regularities as part of the input. (3) It is  complete : our framework handles all known DP variants in all settings (including those involving self-joins, unions, and bag semantics), and allows us to provide new complexity results. (4) It is  easy to implement  and, in many cases, outperforms prior variant-speci\"c solutions, sometimes by orders of magnitude. We provide the \"rst experimental results for several DP variants previously studied only in theory.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Is%20Integer%20Linear%20Programming%20All%20You%20Need%20for%20Deletion%20Propagation%EF%BC%9F%20A%20Unified%20and%20Practical%20Approach%20for%20Generalized%20Deletion%20Propagation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2667-makhija.pdf",
    "session": "Research 47: Provenance and Workflows",
    "authors": [
      {
        "Name": "Neha Makhija",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Wolfgang Gatterbauer",
        "Affiliation": "Northeastern University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "88f98a48-42f5-4f18-89fa-e84d868cfa8d",
    "title": "Ursa: A Lakehouse-Native Data Streaming Engine for Kafka",
    "abstract": "Data lakehouse architectures unify the cost-efficiency of data lakes with the transactional guarantees of data warehouses. Yet, real-time ingestion often depends on external streaming systems such as Apache Kafka, along with bespoke connectors that read from Kafka and write into the lakehouse‚Äîleading to increased complexity and high operational costs. In particular, traditional leader-based data streaming platforms are designed for sub-100 ms low-latency workloads; however, when used for data-intensive ingestion in a cloud environment, cross availability-zone (AZ) disk-based replication significantly raises total infrastructure costs due to excessive network traffic and overprovisioned disk storage. This paper introduces Ursa, a leaderless, cloud-native, and Kafka-compatible streaming engine that writes data directly to open lakehouse tables on object storage. By eliminating leader-based replication, disk-based broker storage, and external connectors, Ursa markedly reduces infrastructure costs while preserving high throughput, exactly-once semantics, and near-real-time streaming capabilities. Experimental results show that Ursa matches the performance of traditional Kafka clusters at a fraction of the cost, offering up to a 10x reduction in infrastructure expenses.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Ursa%3A%20A%20Lakehouse-Native%20Data%20Streaming%20Engine%20for%20Kafka",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5184-guo.pdf",
    "session": "Industry 1: Distributed Systems",
    "authors": [
      {
        "Name": "Sijie Guo",
        "Affiliation": "StreamNative"
      },
      {
        "Name": "Matteo Merli",
        "Affiliation": "StreamNative"
      },
      {
        "Name": "Hang Chen",
        "Affiliation": "StreamNative"
      },
      {
        "Name": "Neng Lu",
        "Affiliation": "StreamNative"
      },
      {
        "Name": "Penghui Li",
        "Affiliation": "StreamNative"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6840bcc3-1e47-4800-8986-3e660df6d647",
    "title": "Cardinality Estimation for Similarity Search on High-Dimensional Data Objects: The Impact of Reference Objects",
    "abstract": "In this paper, we study the problem of cardinality estimation for similarity search on high-dimensional data ( CE4HD ). We aim to perform  CE4HD  with high  data robustness  (i.e., robust to di ff erent datasets),  query robustness  (i.e., robust to large cardinality variance and scale) and e ffi ciency. We propose to leverage the cardinality es-and scale) and e ffi ciency. We propose to leverage the cardinality estimation of selected objects (called reference objects) in the database to achieve the above. Speci fi cally, we propose two techniques that adopt di ff erent strategies to select and leverage reference objects, as well as strategies to support e ffi cient computation in dynamic databases. Extensive experiments on datasets from diverse domains show that our methods achieve up to  ‚àº 10x speed-up and up to ‚àº 136x smaller mean Q-error compared to existing studies.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cardinality%20Estimation%20for%20Similarity%20Search%20on%20High-Dimensional%20Data%20Objects%3A%20The%20Impact%20of%20Reference%20Objects",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p544-bao.pdf",
    "session": "Research 23: Applied ML and AI for Data Management III",
    "authors": [
      {
        "Name": "Hai Lan",
        "Affiliation": "RMIT University"
      },
      {
        "Name": "Shixun Huang",
        "Affiliation": "University of Wollongong"
      },
      {
        "Name": "Zhifeng Bao",
        "Affiliation": "RMIT University"
      },
      {
        "Name": "Renata Borovica-Gajic",
        "Affiliation": "University of Melbourne"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "47e7ad20-f668-4299-8c19-86b8c7dc2301",
    "title": "Auto-Prep: Holistic Prediction of Data Preparation Steps for Self-Service Business Intelligence",
    "abstract": "Business Intelligence (BI) plays a critical role in empowering modern enterprises to make informed data-driven decisions, and has grown into a billion-dollar business. Self-service BI tools like Power BI and Tableau have democratized the ‚Äúdashboarding‚Äù phase of BI, by offering user-friendly, drag-and-drop interfaces that are tailored to non-technical enterprise users. However, despite these advances, we observe that the ‚Äúdata preparation‚Äù phase of BI continues to be a key pain point for BI users today. \nIn this work, we systematically study around 2K real BI projects harvested from public sources, focusing on the data-preparation phase of the BI workflows. We observe that users often have to program both (1) data transformation steps and (2) table joins steps, before their raw data can be ready for dashboarding and analysis. A careful study of the BI workflows reveals that transformation and join steps are often intertwined in the same BI project, such that considering both holistically is crucial to accurately predict these steps. Leveraging this observation, we develop an Auto-Prep system to holistically predict transformations and joins, using a principled graph-based algorithm inspired by Steiner-tree, with provable quality guarantees. Extensive evaluations using real BI projects suggest that Auto-Prep can correctly predict over 70% transformation and join steps, significantly more accurate than existing algorithms as well as language-models such as GPT-4.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Auto-Prep%3A%20Holistic%20Prediction%20of%20Data%20Preparation%20Steps%20for%20Self-Service%20Business%20Intelligence",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2212-he.pdf",
    "session": "Research 51: Information Integration and Data Quality III",
    "authors": [
      {
        "Name": "Eugenie Lai",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Yeye He",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Surajit Chaudhuri",
        "Affiliation": "Microsoft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cc1ff184-4fba-4b3f-804d-891bc7fd369b",
    "title": "TreeCat: Standalone Catalog Engine for Large Data Systems",
    "abstract": "With ever-increasing volume and heterogeneity of data, advent of new specialized compute engines, and demand for complex use cases, large-scale data systems require a performant catalog system that can satisfy diverse needs. We argue that existing solutions, including recent lakehouse storage formats, have fundamental limitations and that there is a strong motivation for a specialized database engine, dedicated to serve as the catalog. We present the design and implementation of  TreeCat , a database engine that features a hierarchical data model with a path-based query language, a storage format optimized for efficient range queries and versioning, and a correlated scan operation that enables fast query execution. A key performance challenge is supporting concurrent read and write operations from many different clients while providing strict consistency guarantees. To this end, we present a novel MVOCC (multi-versioned optimistic concurrency control) protocol that guarantees serializable isolation. We conduct a comprehensive experimental evaluation comparing our concurrency control scheme with prior techniques, and evaluating our overall system against Hive Metastore, Delta Lake, and Iceberg.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TreeCat%3A%20Standalone%20Catalog%20Engine%20for%20Large%20Data%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4323-oh.pdf",
    "session": "Research 62: Information Integration and Data Quality IV",
    "authors": [
      {
        "Name": "Keonwoo Oh",
        "Affiliation": "University of Maryland"
      },
      {
        "Name": "Pooja Nilangekar",
        "Affiliation": "University of Maryland"
      },
      {
        "Name": "Amol Deshpande",
        "Affiliation": "University of Maryland"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b189cdee-8a98-40c0-92ea-3079970ebc6f",
    "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
    "abstract": "The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using com-amounts of training data, processing batches of samples using complex transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are be-latency and high throughput. Performant input data systems are becoming increasingly critical due to skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, re-systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources ‚Äì or worse ‚Äì underutilize expensive accelerators. \nTo address these demands, we present  cedar , an optimized and unified programming framework for ML input data pipelines.  cedar allows users to define a training job‚Äôs data pipeline using compos-allows users to define a training job‚Äôs data pipeline using composable operators that support arbitrary ML frameworks and libraries. cedar ‚Äôs extensible optimizer systematically combines and applies performance optimizations to the pipeline.  cedar  then orchestrates pipeline processing across configurable local and distributed com-pipeline processing across configurable local and distributed compute resources to efficiently meet the training job‚Äôs data throughput demands. Across eight pipelines,  cedar  improves performance by up to 1.87√ó  to 10.65√ó  compared to state-of-the-art input data systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/cedar%3A%20Optimized%20and%20Unified%20Machine%20Learning%20Input%20Data%20Pipelines",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p488-zhao.pdf",
    "session": "Research 61: Learned Query Optimization",
    "authors": [
      {
        "Name": "Mark Zhao",
        "Affiliation": "Stanford University"
      },
      {
        "Name": "Emanuel Adamiak",
        "Affiliation": "Stanford University"
      },
      {
        "Name": "Christos Kozyrakis",
        "Affiliation": "Stanford University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4bc3a04c-f89c-4ee9-8176-f50e5b2e6a6a",
    "title": "Parachute: Single-Pass Bi-Directional Information Passing",
    "abstract": "Sideways information passing is a well-known technique for mitigating the impact of large build sides in a database query plan. As currently implemented in production systems, sideways information passing enables only a  uni-directional  information flow, as opposed to instance-optimal algorithms, such as Yannakakis‚Äô. On the other hand, the latter require an additional pass over the input, which hinders adoption in production systems. \nIn this paper, we make a step towards enabling  single-pass bidirectional  information passing during query execution. We achieve this by statically analyzing between which tables the information flow is blocked and by leveraging precomputed join-induced fingerprint columns on FK-tables. On the JOB benchmark, Parachute improves DuckDB v1.2‚Äôs end-to-end execution time without and with semi-join filtering by 1.54x and 1.24x, respectively, when allowed to use 15% extra space.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Parachute%3A%20Single-Pass%20Bi-Directional%20Information%20Passing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3299-stoian.pdf",
    "session": "Research 8: Database Engines for OLAP",
    "authors": [
      {
        "Name": "Mihail Stoian",
        "Affiliation": "University of Technology Nuremberg"
      },
      {
        "Name": "Andreas Zimmerer",
        "Affiliation": "University of Technology Nuremberg"
      },
      {
        "Name": "Skander Krid",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Amadou Ngom",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Jialin Ding",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Tim Kraska",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Andreas Kipf",
        "Affiliation": "University of Technology Nuremberg"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c3833c40-55a2-4448-8f4d-c4df90256d30",
    "title": "Delta Sharing: An Open Protocol for Cross-Platform Data Sharing",
    "abstract": "Organizations across industries increasingly rely on sharing data to drive collaboration, innovation, and business performance. However, securely and eÔ¨Éciently sharing live data across diverse platforms and adhering to varying governance requirements remains a signiÔ¨Åcant challenge. Traditional approaches, such as FTP and proprietary in-data-warehouse solutions, often fail to meet the demands of interoperability, cost, scalability, and low overhead. This paper introduces Delta Sharing, an open protocol we developed in collaboration with industry partners, to overcome these limitations. Delta Sharing leverages open formats like Delta Lake and Apache Parquet alongside simple HTTP APIs to enable seamless, secure, and live data sharing across heterogeneous systems. Since its launch in 2021, Delta Sharing has been adopted by over 4000 enterprises and supported by hundreds of major software and data vendors. We discuss the key challenges in developing Delta Sharing and how our design addresses them. We also present, to our knowledge, the Ô¨Årst large-scale study of production data sharing workloads oÔ¨Äering insights into this emerging data platform capability.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Delta%20Sharing%3A%20An%20Open%20Protocol%20for%20Cross-Platform%20Data%20Sharing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5197-puttaswamy.pdf",
    "session": "Industry 1: Distributed Systems",
    "authors": [
      {
        "Name": "Krishna Puttaswamy",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Abhijit Chakankar",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Tao Tao",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Zaheera Valani",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Ramesh Chandra",
        "Affiliation": "Databricks"
      },
      {
        "Name": "William Chau",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Mengxi Chen",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Akram Chetibi",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Tianyi Huang",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Jonathan Keller",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Celia Kung",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Andy Liu",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Charlene Lyu",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Samarth Shetty",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Xiaotong Sun",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Steve Weis",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Lin Zhou",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Ryan Zhu",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Reynold Xin",
        "Affiliation": "Databricks"
      },
      {
        "Name": "Matei Zaharia",
        "Affiliation": "Databricks"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dcd3f44d-b2a0-410f-897d-76f874e298e9",
    "title": "Bonspiel: Low Tail Latency Transactions in Geo-Distributed Databases",
    "abstract": "Tail latency is crucial as it impacts user satisfaction and servicelevel objectives (SLOs). However, geo-distributed databases have long struggled with this issue due to wide-area network access, resulting in tail latencies of several or even exceeding ten seconds. In this paper, we highlight that further optimizing atomic commit protocols does not help but hit a tail latency wall. Instead, making concurrency control and access method selection geo-aware can mitigate this issue. To this end, we present Bonspiel, a new geo-distributed database equipped with geo-aware concurrency control and access method selection. In our experiments, Bonspiel successfully caps the tail latency of TPC-C at 1.8 seconds. Remarkably, it achieves this while maintaining full generality ‚Äì it is fully SQL-compliant and strongly consistent, with both average latency and system throughput remaining at the top of the field.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Bonspiel%3A%20Low%20Tail%20Latency%20Transactions%20in%20Geo-Distributed%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3840-cui.pdf",
    "session": "Research 1: Cloud Data Management",
    "authors": [
      {
        "Name": "Fan Cui",
        "Affiliation": "Chinese University of Hong Kong"
      },
      {
        "Name": "Eric Lo",
        "Affiliation": "Chinese University of Hong Kong"
      },
      {
        "Name": "Srijan Srivastava",
        "Affiliation": "Chinese University of Hong Kong"
      },
      {
        "Name": "Ziliang Lai",
        "Affiliation": "Chinese University of Hong Kong"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0ea0cef8-e7a0-454d-9256-2627c9bb0233",
    "title": "FaDE: More Than a Million What-ifs Per Second",
    "abstract": "What-if queries are the building blocks for many explanation and analytics applications‚Äîsensitivity analysis, hypothetical reasoning, data cleaning, probabilistic databases‚Äîthat explore how a query‚Äôs output changes due to input data changes. Their response time is bounded by intervention evaluation latency, which can be in the minute or hours for complex queries and large datasets.  FaDE is a compilation engine that uses provenance to evaluate hypothetical deletion and scaling interventions at low latency and high throughput.  FaDE  forgoes conventional provenance representations as symbolic expressions and leverages their underlying relational structure. This accelerates intervention evaluation on average by 1000 √ó  against IVM and 10,000 √ó  against prior provenance-based approaches. In addition,  FaDE  develops a suite of optimizations (e.g., compilation, parallelization, incremental evaluation, sparse representations) that collectively raise evaluation throughput to  > 1 million interventions per sec‚Äîa rate that can brute-force existing applications within 1 ùë† .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/FaDE%3A%20More%20Than%20a%20Million%20What-ifs%20Per%20Second",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p943-mohammed.pdf",
    "session": "Research 47: Provenance and Workflows",
    "authors": [
      {
        "Name": "Haneen Mohammed",
        "Affiliation": "Columbia University"
      },
      {
        "Name": "Alexander Yao",
        "Affiliation": "Columbia University"
      },
      {
        "Name": "Charlie Summers",
        "Affiliation": "Columbia University"
      },
      {
        "Name": "Hongbin Zhong",
        "Affiliation": "Georgia Institute of Technology"
      },
      {
        "Name": "Gromit Yeuk-Yin Chan",
        "Affiliation": "Adobe Research"
      },
      {
        "Name": "Subrata Mitra",
        "Affiliation": "Adobe Research"
      },
      {
        "Name": "Lampros Flokas",
        "Affiliation": "Celonis Inc."
      },
      {
        "Name": "Eugene Wu",
        "Affiliation": "Columbia University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "586b38f1-cc4b-4a56-9b47-377ee557bccc",
    "title": "ClaimIt: Finding Convincing Views to Endorse a Claim",
    "abstract": "The demonstration presents ClaimIt ‚Äî a tool for extracting views that support a user-provided claim. Such views can assist users in Ô¨Ånding evidence of phenomena of interest, criticizing given claims by proposing opposing viewpoints, inspecting the robustness of statements with respect to subpopulations, and so on. To be useful, the view should constitute a ‚Äúnatural‚Äù characterization of a signiÔ¨Åcant subpopulation. In a recently published work, we focused on claims that compare groups by an aggregate query, and explored the measurement of naturalness as well as the algorithmic challenge of handling the plenitude of possible views. ClaimIt realizes the framework as an interactive system that enables users to phrase their claims in a convenient user interface, extract supporting views, sort them by diÔ¨Äerent measures of naturalness, and control the weights of individual measures in a global ranking function. In the demonstration of ClaimIt, the audience will be able to suggest and analyze diÔ¨Äerent claims on various datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ClaimIt%3A%20Finding%20Convincing%20Views%20to%20Endorse%20a%20Claim",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5331-agmon.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Shunit Agmon",
        "Affiliation": "Technion"
      },
      {
        "Name": "David Avigdor",
        "Affiliation": "Technion"
      },
      {
        "Name": "Brit Youngmann",
        "Affiliation": "Technion"
      },
      {
        "Name": "Amir Gilad",
        "Affiliation": "Hebrew University"
      },
      {
        "Name": "Benny Kimelfeld",
        "Affiliation": "Technion"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a87bbba9-3fbc-4ef6-9eba-2b7a3acaa2cc",
    "title": "Representative Time Series Discovery for Data Exploration",
    "abstract": "In this work, we address the critical task of discovering representa-In this work, we address the critical task of discovering representative time series in exploratory data mining. We define a representa-tive time series in exploratory data mining. We define a representative time series, referred to as similarity-bounded representative time series, as one that represents other time series if their similarity meets a user-defined threshold. Building on this definition, we study the problem of finding the smallest set of such time series that can represent a specified proportion of all time series within the dataset. The representativeness of each similarity-bounded representative time series is controllable and determined by the specified level of similarity, and only the minimum number of such representatives needed to collectively represent the specified proportion of entire set are identified. Identifying representative time series over large-set are identified. Identifying representative time series over largescale data in an efficient and effective manner facilitates exploratory data analysis and summary generation, serving a wide range of data exploration applications across diverse domains. We first prove the NP-hardness of this problem and propose a range of approxima-NP-hardness of this problem and propose a range of approximation methods with theoretical guarantees, and we refer to them as non-learning-based methods. While effective, these methods often excel in either running time or memory efficiency, but not both concurrently. To overcome these limitations, we further propose a learning-based method that simultaneously optimizes both time and memory efficiency. This method leverages novel data prepara-and memory efficiency. This method leverages novel data preparation and training strategies, providing adaptability to user-specified representativeness requirements with low memory usage and com-representativeness requirements with low memory usage and computational overhead. We conduct extensive experiments across four real-world datasets to demonstrate that our learning-based method is highly competitive with non-learning-based methods in terms of effectiveness (produces similar number of representative time series), while achieving significantly higher efficiency (up to 21 √ó speedups) and lower memory consumption (saving up to 101 √ó memory space).",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Representative%20Time%20Series%20Discovery%20for%20Data%20Exploration",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p915-bao.pdf",
    "session": "Research 12: Time Series Data I",
    "authors": [
      {
        "Name": "Ge Lee",
        "Affiliation": "RMIT University"
      },
      {
        "Name": "Shixun Huang",
        "Affiliation": "University of Wollongong"
      },
      {
        "Name": "Zhifeng Bao",
        "Affiliation": "RMIT University"
      },
      {
        "Name": "Yanchang Zhao",
        "Affiliation": "CSIRO"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ab6919fb-46d7-4e56-bfc5-572ed2f50e5f",
    "title": "LIMAO: A Framework for Lifelong Modular Learned Query Optimization",
    "abstract": "Query optimizers are crucial for the performance of database systems. Recently, many learned query optimizers (LQOs) have demonstrated significant performance improvements over traditional optimizers. However, most of them operate under a limited assumption: a static query environment. This limitation prevents them from effectively handling complex, dynamic query environments in realworld scenarios. Extensive retraining can lead to the well-known catastrophic forgetting problem which reduces the LQO generalizability over time. In this paper, we address this limitation and introduce  LIMAO  ( Lifelong  Modular Learned Query Optimizer), a framework for lifelong learning of plan cost prediction that can be seamlessly integrated into existing LQOs.  LIMAO  leverages a modular lifelong learning technique, an attention-based neural network composition architecture, and an efficient training paradigm designed to retain prior knowledge while continuously adapting to new environments. We implement  LIMAO  in two LQOs, showing that our approach is agnostic to underlying engines. Experimental results show that  LIMAO  significantly enhances the performance of LQOs, achieving up to a 40% improvement in query execution time and reducing the variance of execution time by up to 60% under dynamic workloads. By leveraging a precise and self-consistent design,  LIMAO  effectively mitigates catastrophic forgetting, ensuring stable and reliable plan quality over time. Compared to Postgres, LIMAO  achieves up to a 4√ó speedup on selected benchmarks, highlighting its practical advantages in real-world query optimization.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LIMAO%3A%20A%20Framework%20for%20Lifelong%20Modular%20Learned%20Query%20Optimization",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4546-zhang.pdf",
    "session": "Research 2: Applied ML and AI for Data Management I",
    "authors": [
      {
        "Name": "Qihan Zhang",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Shaolin Xie",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Ibrahim Sabek",
        "Affiliation": "University of Southern California"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bdadfb47-c717-4228-a519-b3b6d2c9a57e",
    "title": "Dandelion: Smaller Clusters, Bigger Speeds‚ÄîDistributed Transactions Redefined",
    "abstract": "This paper presents an in-memory, RDMA-enabled, highly-available, transactional Key-Value Store (KVS), dubbed Dandelion, that sig- nificantly improves performance in small deployments (e.g., 5-10 machines). Small deployments are motivated by the anticipated memory expansion (e.g., through CXL), which enables the deploy- ment of in-memory KVSes with few machines but lots of memory.\nA small deployment presents locality opportunities that have not been examined by related work. Specifically, it is more likely that at any given time, we must send multiple messages to the same recip- ient. We leverage this by transparently batching multiple requests in the same network packet. Similarly, there is a greater chance of having multiple requests that can be served by the local hashtable without going through the network. Sending all requests to the hashtable as a batch allows it to overlap their memory latencies through software prefetching. Finally, it is more likely that the node that requests a key is itself a backup of that key. We leverage this by allowing strongly-consistent local reads from backups.\nOur evaluation shows that these optimizations result in up to 6.5x throughput improvement over a state-of-the-art system, FaSST, in OLTP workloads in a 5-machine deployment. We characterize the impact and scalability of each of these optimizations with up to 10 machines ‚Äì where Dandelion still offers as much as 3.5√ó higher throughput than FaSST.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Dandelion%3A%20Smaller%20Clusters%2C%20Bigger%20Speeds%E2%80%94Distributed%20Transactions%20Redefined",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1264-katsarakis.pdf",
    "session": "Research 26: Distributed Transactions I",
    "authors": [
      {
        "Name": "Vasilis Gavrielatos",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Antonios Katsarakis",
        "Affiliation": "Huawei Research"
      },
      {
        "Name": "Chris Jensen",
        "Affiliation": "University of Cambridge"
      },
      {
        "Name": "Nikos Ntarmos",
        "Affiliation": "Edinburgh Research Center, Central Software Institute, Huawei"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b4671fce-c82b-4425-9874-06396282afcd",
    "title": "Fair Transaction Processing For Multi-Tenant Databases",
    "abstract": "Multi-tenant transactional databases frequently observe contention on shared data, leading to a need for performance isolation. Databases typically provide performance isolation via a request rate limit or quota per tenant, but this approach can lead to system underutilization. Traditionally, fair sharing has been applied to achieve both performance isolation and high utilization in other domains. In this paper, we address the problem of fair sharing for transactions, which introduces new challenges because client requests do not acquire resources all at once. We propose DRFT, the first fair transaction scheduling algorithm that ensures both the share guarantee and strategy-proofness by accurately accounting for transactional resource usage. We evaluate DRFT on a range of standard benchmarks and real-world workloads, showing that it ensures fairness with less than a 5% throughput overhead compared to state-of-the-art scheduling policies.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Fair%20Transaction%20Processing%20For%20Multi-Tenant%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2602-cheng.pdf",
    "session": "Research 44: Database Engine Performance and Manageability I",
    "authors": [
      {
        "Name": "Audrey Cheng",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Aaron Kabcenell",
        "Affiliation": "Meta"
      },
      {
        "Name": "Xiao Shi",
        "Affiliation": "Unaffiliated"
      },
      {
        "Name": "Jolene Huey",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Peter Bailis",
        "Affiliation": "Google"
      },
      {
        "Name": "Natacha Crooks",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Ion Stoica",
        "Affiliation": "UC Berkeley"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "24819145-0149-469e-999c-0c13820a0942",
    "title": "Sphinteract: Resolving Ambiguities in NL2SQL Through User Interaction",
    "abstract": "Translating natural language questions into SQL queries (NL2SQL) is a challenging task of great practical importance. Prior work has extensively studied how to address NL2SQL using Large Language Models (LLMs) with solutions ranging from careful prompt engineering, to fine-tuning existing LLMs, or even training custom models. However, a remaining challenging problem in NL2SQL is the inherent ambiguity in the natural language questions asked by users. In this paper, we introduce Sphinteract, a framework designed to assist LLMs in generating high-quality SQL answers that accurately reflect the user intent. Our key insight to resolve ambiguity is to take into account minimal user feedback interactively. We introduce the  Summarize, Review, Ask  (SRA) paradigm, which guides LLMs in identifying ambiguities in NL2SQL tasks and generates targeted questions for the user to answer. We propose three different methods of how to process user feedback and generate SQL queries based on user input. Our experiments on the challenging KaggleDBQA and BIRD benchmarks demonstrate that by means of asking clarification questions to the user, LLMs can efficiently incorporate the feedback, resulting in accuracy improvements of up to 42%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Sphinteract%3A%20Resolving%20Ambiguities%20in%20NL2SQL%20Through%20User%20Interaction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1145-zhao.pdf",
    "session": "Research 7: Natural Language Interfaces to Data",
    "authors": [
      {
        "Name": "Fuheng Zhao",
        "Affiliation": "UCSB"
      },
      {
        "Name": "Shaleen Deep",
        "Affiliation": "Microsoft Gray Systems Lab"
      },
      {
        "Name": "Fotis Psallidas",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Avrilia Floratou",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Divy Agrawal",
        "Affiliation": "University of California, Santa Barbara"
      },
      {
        "Name": "Amr El Abbadi",
        "Affiliation": "UC Santa Barbara"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f80ba5f5-f7be-4938-81cc-548208d7b67f",
    "title": "RecForUS: A Recommender System for Uncertain Scores",
    "abstract": "We present RecForUs, a recommender system designed to offer accurate music recommendations through a competition between participants and an algorithmic recommender. Our framework aims to demonstrate the intricate management of uncertain scores in a recommender system, catering to the specific objectives of users. The demonstration showcases our novel RankDist algorithm that efficiently computes rank probabilities for items with uncertain scores, enabling optimal selection of ranking semantics tailored to different user objectives without requiring exhaustive evaluation of all possible worlds. RecForUS is versatile, demonstrating the effectiveness of generating top- ùêæ query results in multiple scenarios.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RecForUS%3A%20A%20Recommender%20System%20for%20Uncertain%20Scores",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5267-gal.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Dvir Cohen",
        "Affiliation": "Technion -- Israel Institute of Technology"
      },
      {
        "Name": "Liad Domb",
        "Affiliation": "Technion -- Israel Institute of Technology"
      },
      {
        "Name": "Avigdor Gal",
        "Affiliation": "Technion -- Israel Institute of Technology"
      },
      {
        "Name": "Lior Ganon",
        "Affiliation": "Technion -- Israel Institute of Technology"
      },
      {
        "Name": "Eliezer Gavriel",
        "Affiliation": "Technion -- Israel Institute of Technology"
      },
      {
        "Name": "Omri Lazover",
        "Affiliation": "Technion -- Israel Institute of Technology"
      },
      {
        "Name": "Coral Scharf",
        "Affiliation": "Technion -- Israel Institute of Technology"
      },
      {
        "Name": "Bar Shterenberg",
        "Affiliation": "Technion -- Israel Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7ef04eb6-1c3e-4b14-94f6-253cfb46f2ae",
    "title": "Cost-Effective, Low Latency Vector Search with Azure Cosmos DB",
    "abstract": "Vector indexing enables semantic search over diverse corpora and has become an important interface to databases for both users and AI agents. Efficient vector search requires deep optimizations in database systems. This has motivated a new class of specialized vector databases that optimize for vector search quality and cost. Instead, we argue that a scalable, high-performance, and cost-efficient vector search system can be built inside a cloud-native operational database like Azure Cosmos DB while leveraging the benefits of a distributed database such as high availability, durability, and scale. We do this by deeply integrating DiskANN, a state-of-the-art vector indexing library, inside Azure Cosmos DB NoSQL. This system uses a single vector index per partition stored in existing index trees, and kept in sync with underlying data. It supports < 20ms query latency over an index spanning 10 million vectors, has stable recall over updates, and offers approximately 43 √ó  and 12 √ó  lower query cost compared to Pinecone and Zilliz serverless enterprise products. It also scales out to billions of vectors via automatic partitioning. This convergent design presents a point in favor of integrating vector indices into operational databases in the context of recent debates on specialized vector databases, and offers a template for vector indexing in other databases.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cost-Effective%2C%20Low%20Latency%20Vector%20Search%20with%20Azure%20Cosmos%20DB",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5166-upreti.pdf",
    "session": "Industry 3: Document, Graph, and Vector Databases",
    "authors": [
      {
        "Name": "Nitish Upreti",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Harsha Simhadri",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Hari Sundar",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Krishnan Sundaram",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Samer Boshra",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Bala Perumalswamy",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Shivam Atri",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Martin Chisholm",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Revti Singh",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Greg Yang",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Tamara Hass",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Nitesh Dudhey",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Subramanyam Pattipaka",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Mark Hildebrand",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Magdalen Manohar",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Jack Moffitt",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Haiyang Xu",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Naren Datha",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Suryansh Gupta",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Ravi Krishnaswamy",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Prashant Gupta",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Abhishek Sahu",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Hemeswari Varada",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Sudhanshu Barthwal",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Ritika Mor",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "James Codella",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Shaun Cooper",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Kevin Pilch",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Simon Moreno",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Aayush Kataria",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Santosh Kulkarni",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Neil Deshpande",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Amar Sagare",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Dinesh Billa",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Zishan Fu",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Vipul Vishal",
        "Affiliation": "Microsoft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "88482148-2f59-4aaa-9005-0e59de5914c3",
    "title": "Democratize MATCH_RECOGNIZE!",
    "abstract": "Row pattern matching  in terms of the  MATCH_RECOGNIZE  clause is a powerful and relatively recent feature in SQL that allows users to deÔøøne regular patterns over ordered rows in a table. As of today, few database systems oÔøøer support for  match recognize , making it unaccessible to a wide range of users. We demonstrate the implementation of a transpiler that translates  match recognize  into a plain SQL query executable by any database system that supports window functions and recursive common table expressions‚Äîno changes to the underlying database systems are required. \nWe evaluate the performance of this approach on the running example to show that the transpiler generates code competitive with contemporary database systems that implement row pattern matching natively. The on-site demonstration is based on DuckDB.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Democratize%20MATCH_RECOGNIZE!",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5251-lambrecht.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Louisa Lambrecht",
        "Affiliation": "University of T√ºbingen"
      },
      {
        "Name": "Tim Findling",
        "Affiliation": "University of T√ºbingen"
      },
      {
        "Name": "Samuel Heid",
        "Affiliation": "University of T√ºbingen"
      },
      {
        "Name": "Marcel Kn√ºdeler",
        "Affiliation": "University of T√ºbingen"
      },
      {
        "Name": "Torsten Grust",
        "Affiliation": "University of T√ºbingen"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "57847e0d-f5a8-453c-ae01-60ca1f81dfc8",
    "title": "TEAM: Topological Evolution-aware Framework for Traffic Forecasting",
    "abstract": "Due to the global trend towards urbanization, people increasingly move to and live in cities that then continue to grow. TraÔ¨Éc fore-move to and live in cities that then continue to grow. TraÔ¨Éc forecasting plays an important role in the intelligent transportation systems of cities as well as in spatio-temporal data mining. State-systems of cities as well as in spatio-temporal data mining. Stateof-the-art forecasting is achieved by deep-learning approaches due to their ability to contend with complex spatio-temporal dynam-to their ability to contend with complex spatio-temporal dynamics. However, existing methods assume the input is Ô¨Åxed-topology road networks and static traÔ¨Éc time series. These assumptions fail to align with urbanization, where time series are collected continuously and road networks evolve over time. In such set-continuously and road networks evolve over time. In such settings, deep-learning models require frequent re-initialization and re-training, imposing high computational costs. To enable much more eÔ¨Écient training without jeopardizing model accuracy, we propose the   T opological   E volution- a ware Fra m ework ( TEAM ) for traÔ¨Éc forecasting that incorporates convolution and attention. This combination of mechanisms enables better adaptation to newly col-combination of mechanisms enables better adaptation to newly collected time series while being able to maintain learned knowledge from old time series.  TEAM  features a continual learning module based on the Wasserstein metric that acts as a buÔ¨Äer that can iden-based on the Wasserstein metric that acts as a buÔ¨Äer that can identify the most stable and the most changing network nodes. Then, only data related to stable nodes is employed for re-training when consolidating a model. Further, only data of new nodes and their adjacent nodes as well as data pertaining to changing nodes are used to re-train the model. Empirical studies with two real-world traÔ¨Éc datasets oÔ¨Äer evidence that  TEAM  is capable of much lower re-training costs than existing methods are, without jeopardizing forecasting accuracy.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TEAM%3A%20Topological%20Evolution-aware%20Framework%20for%20Traffic%20Forecasting",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p265-kieu.pdf",
    "session": "Research 10: Data Mining and Analytics",
    "authors": [
      {
        "Name": "Duc Kieu",
        "Affiliation": "University of Science, Vietnam National University"
      },
      {
        "Name": "Tung Kieu",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Peng Han",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Bin Yang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Bac Le",
        "Affiliation": "University of Science, VNU-HCM"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c347e04b-cf1b-4b69-b123-dcb439a6ef21",
    "title": "Approximation-First Timeseries Query At Scale",
    "abstract": "Timeseries monitoring systems such as Prometheus play a crucial role in gaining observability of the underlying system infrastructure. These systems collect timeseries metrics from various system components and perform monitoring queries over periodic windowbased aggregations (i.e., rule queries). However, despite wide adoption, the operational costs and query latency of rule queries remain high. In this paper, we identify major bottlenecks associated with repeated data scans and query computations concerning window overlaps in rule queries, and present PromSketch, an approximation-Ô¨Årst query framework as intermediate caches for monitoring systems. It enables low operational costs and query latency, by combining approximate window-based query frameworks and sketch-based precomputation. PromSketch is implemented as a standalone module that can be integrated into Prometheus and VictoriaMetrics, covering 70% of Prometheus‚Äô aggregation over time queries. Our evaluation shows that PromSketch achieves up to a two-orderof-magnitude reduction in query latency over Prometheus and VictoriaMetrics, while lowering operational dollar costs of query processing by three orders of magnitude compared to Prometheus and by at least  4 √ó  compared to VictoriaMetrics with at most 5% average errors across statistics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Approximation-First%20Timeseries%20Monitoring%20Query%20At%20Scale",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2348-zhu.pdf",
    "session": "Research 16: Query Processing and Optimization II",
    "authors": [
      {
        "Name": "Zeying Zhu",
        "Affiliation": "University of Maryland"
      },
      {
        "Name": "Jonathan Chamberlain",
        "Affiliation": "Boston University"
      },
      {
        "Name": "Kenny Wu",
        "Affiliation": "University of Maryland"
      },
      {
        "Name": "David Starobinski",
        "Affiliation": "Boston University"
      },
      {
        "Name": "Zaoxing Liu",
        "Affiliation": "University of Maryland"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "45a1294a-2227-498d-8996-b884a8e6da7a",
    "title": "Customization Meets 2-Hop Labeling: Efficient Routing in Road Networks",
    "abstract": "Efficient route planning is crucial for modern navigation systems, yet traditional methods face challenges in scenarios with unknown or frequently changing traffic dynamics. This paper introduces a general labeling framework based on the 2-hop cover property, enabling robust, metric-independent preprocessing. Using this framework, we propose  Customizable Tree Labeling  (CTL), a tree-based method combining three key components: metric-independent preprocessing with tree hierarchies, metric customization for dynamic updates, and efficient query algorithms for fast route computation. To allow trade-offs between customization time, labeling size, and query performance, we further develop a parameterized customization technique by dynamically combining tree labels and shortcut graphs. Our key contributions include the introduction of a customizable labeling framework, a novel tree hierarchy for compact and scalable representation, and a hybrid query algorithm that integrates labels and shortcuts for fast and accurate route computation. We conduct extensive experiments on ten large-scale real-world road networks and a case study on the traffic assignment problem. Our algorithms achieve query response times significantly faster than the state-of-the-art methods, while maintaining competitive customization times and labeling size, making it well-suited for real-time and dynamic routing applications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Customization%20Meets%202-Hop%20Labeling%3A%20Efficient%20Routing%20in%20Road%20Networks",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3326-farhan.pdf",
    "session": "Research 20: Road Networks, Social Networks",
    "authors": [
      {
        "Name": "Muhammad Farhan",
        "Affiliation": "Australian National University"
      },
      {
        "Name": "Henning Koehler",
        "Affiliation": "Massey University"
      },
      {
        "Name": "Qing Wang",
        "Affiliation": "Australian National University"
      },
      {
        "Name": "Jiawen Wang",
        "Affiliation": "Australian National University"
      },
      {
        "Name": "Moritz Laupichler",
        "Affiliation": "Karlsruhe Institute of Technology"
      },
      {
        "Name": "Peter Sanders",
        "Affiliation": "Karlsruhe Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dd95bb84-f462-4d0b-89a5-cdbe991e9879",
    "title": "GREAT: Generalized Reservoir Sampling based Triangle Counting Estimation over Streaming Graphs",
    "abstract": "The number of triangles of a streaming graph is a crucial metric with various applications, such as network evolution analysis, community detection, and anomaly detection. A practical solution for triangle counting in streaming graphs is the sampling-based approximation. Although a lot of research efforts have been devoted to the fixed-sized memory based algorithms, they suffer from the accuracy and the efficiency issues. To tackle these issues, we first propose the generalized reservoir sampling (GRS), which stores less edges for reducing the computational cost and can still generate uniformly random edge sample in the streaming graph. Then, we propose the  GREAT  algorithm based on GRS for efficient and accurate triangle counting estimation. To further improve the estimation accuracy, we propose the  GREAT +   algorithm for considering the dynamic timestamp interval distribution in real-world streaming graphs so that triangles with short and long timestamp intervals will be sampled following the ground-truth distribution. Extensive evaluations on real datasets demonstrate the efficiency and the accuracy of our algorithms. The relative error of our algorithm GREAT+ is significantly (an order of magnitude) better than the competitors.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GREAT%3A%20Generalized%20Reservoir%20Sampling%20based%20Triangle%20Counting%20Estimation%20over%20Streaming%20Graphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2031-wu.pdf",
    "session": "Research 49: Graph Data Management VI",
    "authors": [
      {
        "Name": "Siyue Wu",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Dingming Wu",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "sinhong cheuk",
        "Affiliation": "shenzhen university"
      },
      {
        "Name": "Tsz Nam Chan",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Kezhong Lu",
        "Affiliation": "Shenzhen University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eebc27c9-4261-4be0-9dc1-bab15ffb24d8",
    "title": "The HANA Native Query Engine for Lakehouse Systems",
    "abstract": "Modern enterprise applications and data warehouse systems move data into data lakes for economical and scalability reasons. Data is then stored in popular columnar file formats like Parquet which are optimized for writing using open table formats like Iceberg or Delta. This presents new challenges for existing database systems and their execution engines because excellent performance and scalability when accessing this data in complex analytical queries is expected while data is located in a remote data lake. \nIn this work, we present how we adapted the HANA Cloud Database Engine for efficient processing of files in data lakes, which we call  SQL-on-Files  (SoF). We motivate this evolution by its relevance for Business Data Cloud, SAP‚Äôs Lakehouse, we discuss the viability of general architecture choices like  pushdown  and  direct access  architectures, and give insights into our SoF design decisions towards scalable, analytical query processing around execution engine, optimizer and caching. Our evaluation of SoF shows benefits of direct access over pushdown architectures for a new warehouse benchmark with complex, analytical workloads. KEYWORDS Cloud Data Platform, Database System, Data Lake, Lakehouse",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20HANA%20Native%20Query%20Engine%20for%20Lakehouse%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4831-ritter.pdf",
    "session": "Industry 6: Database Engines",
    "authors": [
      {
        "Name": "Daniel Ritter",
        "Affiliation": "SAP"
      },
      {
        "Name": "Mihnea Andrei",
        "Affiliation": "SAP"
      },
      {
        "Name": "Sukhyeun Cho",
        "Affiliation": "SAP"
      },
      {
        "Name": "Maik Goergens",
        "Affiliation": "SAP"
      },
      {
        "Name": "Taehyung Lee",
        "Affiliation": "SAP"
      },
      {
        "Name": "Norman May",
        "Affiliation": "SAP"
      },
      {
        "Name": "Amit Pathak",
        "Affiliation": "SAP"
      },
      {
        "Name": "Paul Willems",
        "Affiliation": "SAP"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9c35cf7b-df0a-44f1-80e7-5f16ef6295e5",
    "title": "RDPro: Distributed Processing of Big Raster Data",
    "abstract": "Advancements in remote sensing technology allowed for collecting vast amounts of satellite and aerial imagery with up to 1 cm pixel resolutions, stored in raster format crucial for various research fields. However, processing this data poses challenges, including resolving data dependencies when location, resolution, and coor-resolving data dependencies when location, resolution, and coordinate systems do not align and managing large datasets within memory constraints. This paper introduces RDPro, a novel Spark-memory constraints. This paper introduces RDPro, a novel Sparkbased system that efficiently processes and analyzes large raster datasets. RDPro features a new data model tailored for data depen-datasets. RDPro features a new data model tailored for data dependencies in a distributed, shared-nothing environment, complete with tools for loading and writing raster data. It also optimizes core raster operations within Spark, allowing users to integrate com-raster operations within Spark, allowing users to integrate complex data science workflows. Comparative analysis shows RDPro outperforms existing systems by up to two orders of magnitude.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/RDPro%3A%20Distributed%20Processing%20of%20Big%20Raster%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p613-shang.pdf",
    "session": "Research 14: User Interfaces for Data Exploration and Explainable AI",
    "authors": [
      {
        "Name": "Zhuocheng Shang",
        "Affiliation": "University of California, Riverside"
      },
      {
        "Name": "Samriddhi Singla",
        "Affiliation": "Meta Platforms Inc"
      },
      {
        "Name": "Ahmed Eldawy",
        "Affiliation": "University of California, Riverside"
      },
      {
        "Name": "Elia Scudiero",
        "Affiliation": "University of California, Riverside"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3010c890-3ac9-4507-8769-84d742fbcf94",
    "title": "Causal DAG Summarization",
    "abstract": "Causal inference aids researchers in discovering cause-and-effect relationships, leading to scientific insights. Accurate causal estimation requires identifying confounding variables to avoid false discoveries. Pearl‚Äôs causal model uses causal DAGs to identify confounding variables, but incorrect DAGs can lead to unreliable causal conclusions. However, for high dimensional data, the causal DAGs are often complex beyond human verifiability. Graph summarization is a logical next step, but current methods for general-purpose graph summarization are inadequate for causal DAG summarization. This paper addresses these challenges by proposing a causal graph summarization objective that balances graph simplification for better understanding while retaining essential causal information for reliable inference. We develop an efficient greedy algorithm and show that summary causal DAGs can be directly used for inference and are more robust to misspecification of assumptions, enhancing robustness for causal inference. Experimenting with six real-life datasets, we compared our algorithm to three existing solutions, showing its effectiveness in handling high-dimensional data and its ability to generate summary DAGs that ensure both reliable causal inference and robustness against misspecifications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Causal%20DAG%20Summarization",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1933-youngmann.pdf",
    "session": "Research 37: Applied ML and AI for Graph, Temporal, and Trajectory Data",
    "authors": [
      {
        "Name": "Anna Zeng",
        "Affiliation": "MIT"
      },
      {
        "Name": "Michael Cafarella",
        "Affiliation": "MIT CSAIL"
      },
      {
        "Name": "Batya Kenig",
        "Affiliation": "Technion, Israel Institute of Technology"
      },
      {
        "Name": "Markos Markakis",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Brit Youngmann",
        "Affiliation": "Technion"
      },
      {
        "Name": "Babak Salimi",
        "Affiliation": "University of California at San Diego"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "41f7555b-fc2a-4863-aeb3-0511cd5697b9",
    "title": "Can Graph Reordering Speed Up Graph Neural Network Training? An Experimental Study",
    "abstract": "Graph neural networks (GNNs) are a type of neural network capa-Graph neural networks (GNNs) are a type of neural network capable of learning on graph-structured data. However, training GNNs on large-scale graphs is challenging due to iterative aggregations of high-dimensional features from neighboring vertices within sparse graph structures combined with neural network operations. The sparsity of graphs frequently results in suboptimal memory access patterns and longer training time.  Graph reordering  is an optimiza-patterns and longer training time.  Graph reordering  is an optimization strategy aiming to improve the graph data layout. It has shown to be effective to speed up graph analytics workloads, but its effect on the performance of GNN training has not been investigated yet. The generalization of reordering to GNN performance is nontrivial, as multiple aspects must be considered: GNN hyper-parameters such as the number of layers, the number of hidden dimensions, and the feature size used in the GNN model, neural network operations, large intermediate vertex states, and GPU acceleration. \nIn our work, we close this gap by performing an empirical evalua-In our work, we close this gap by performing an empirical evaluation of 12 reordering strategies in two state-of-the-art GNN systems, PyTorch Geometric and Deep Graph Library. Our results show that graph reordering is effective in reducing training time for CPU-graph reordering is effective in reducing training time for CPUand GPU-based training, respectively. Further, we find that GNN hyper-parameters influence the effectiveness of reordering, that reordering metrics play an important role in selecting a reordering strategy, that lightweight reordering performs better for GPU-based than for CPU-based training, and that invested reordering time can in many cases be amortized.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Can%20Graph%20Reordering%20Speed%20Up%20Graph%20Neural%20Network%20Training%EF%BC%9F%20An%20Experimental%20Study",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p293-merkel.pdf",
    "session": "Research 45: Graph Data Learning",
    "authors": [
      {
        "Name": "Nikolai Merkel",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Pierre Toussing",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Ruben Mayer",
        "Affiliation": "University of Bayreuth"
      },
      {
        "Name": "Hans-Arno Jacobsen",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "43075127-9b1a-4d85-be8f-be5bd3a44e47",
    "title": "A Demonstration of Q2O: Quantum-augmented Query Optimizer",
    "abstract": "The join order (JO) optimization problem is a key challenge in query optimization. Classical approaches can compute the optimal solution for smaller queries. For larger queries, some heuristic methods trade off plan quality to reduce the exponential search space. Recently, quantum-based methods have been proposed to leverage quantum mechanisms to accelerate exploration; however, encoding problem-specific constraints as penalty terms introduces extra overhead. Moreover, quantum-inspired methods on classical hardware do not harness the true advantages of quantum computation. Furthermore, these methods remain at the simulation stage. \nIn this demonstration, we present the first Quantum-augmented Query Optimizer ( Q 2 O ) that integrates a hybrid quantum-classical approach to solve the JO problem in a real database setup. This demonstration allows conference attendees to interact directly with Q 2 O  by executing queries and viewing detailed execution results (Scenario 1). Users can easily compare plan quality between  Q 2 O and PostgreSQL (Scenario 2). Additionally, they can experiment with quantum parameters to observe their impact (Scenario 3). Experimental results show that the query plan generated by our approach achieves up to a 13x speedup in query execution. The video corresponding to this demonstration is available at   this link .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Demonstration%20of%20Q2O%3A%20Quantum-augmented%20Query%20Optimizer",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5439-liu.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Hanwen Liu",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Federico Spedalieri",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Ibrahim Sabek",
        "Affiliation": "University of Southern California"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f20d6100-891a-439a-822a-1dc0338646af",
    "title": "From FASTER to F2: Evolving Concurrent Key-Value Store Designs for Large Skewed Workloads",
    "abstract": "Modern large-scale services such as search engines, messaging platforms, and serverless functions, rely on key-value (KV) stores to maintain high performance at scale. When such services are deployed in constrained memory environments, they present challenging requirements: point operations requiring high throughput, working sets  much larger  than main memory, and natural  skew in key access patterns. Traditional KV stores, based on LSM- and B-Trees, have been widely used to handle such use cases, but they often suffer from suboptimal use of modern hardware resources. The FASTER project, developed as a high-performance open-source KV storage library, has demonstrated remarkable success in both inmemory and hybrid storage environments. However, when tasked with serving large skewed workloads, it faced challenges, including high indexing and compactions overheads, and inefficient management of non-overlapping read-hot and write-hot working sets. \nIn this paper, we introduce F2 (for FASTER v2), an evolution of FASTER designed to meet the requirements of large skewed workloads common in industry applications. F2 adopts a two-tier recordoriented design to handle larger-than-memory skewed workloads, along with new concurrent latch-free mechanisms and components to maximize performance on modern hardware. To realize this design, F2 tackles key challenges and introduces several innovations, including new latch-free algorithms for multi-threaded log compaction, a two-level hash index to reduce indexing overhead for cold records, and a read-cache for serving read-hot records. Our evaluation shows that F2 achieves 2-11 . 9 √ó  better throughput compared to existing KV stores, effectively serving the target workload. F2 is open-source and available as part of the FASTER project.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/From%20FASTER%20to%20F2%3A%20Evolving%20Concurrent%20Key-Value%20Store%20Designs%20for%20Large%20Skewed%20Workloads",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4910-kanellis.pdf",
    "session": "Industry 5: Transactions and Concurrency Control",
    "authors": [
      {
        "Name": "Konstantinos Kanellis",
        "Affiliation": "University of Wisconsin-Madison"
      },
      {
        "Name": "Badrish Chandramouli",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Ted Hart",
        "Affiliation": "Microsoft Research"
      },
      {
        "Name": "Shivaram Venkataraman",
        "Affiliation": "University of Wisconsin-Madison"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dd42672d-3db1-4a8e-a60e-8cd9a6d82abf",
    "title": "Scalable Pre-Training of Compact Urban Spatio-Temporal Predictive Models on Large-Scale Multi-Domain Data",
    "abstract": "Spatio-Temporal Prediction (STP) is crucial for various smart city applications, such as traffic management and resource allocation. However, training samples can be scarce in data-constrained scenarios, which often degrades the predictive capability of existing deep STP models. Although recent STP foundation models excel in few-shot and zero-shot learning through extensive pre-training on large-scale, multi-domain spatio-temporal data, they often rely on large parameter scale to achieve enhanced performance, resulting in high computational demands that hinder practical deployment. In response, we develop CompactST, an efficient, compact, and versatile pre-trained model for STP in data-scarce settings. Recognizing the complexities posed by large-scale, heterogeneous pre-training datasets, CompactST integrates three specialized components: (1) a mixture-of-normalizers module to address domain and spatial heterogeneity, (2) a multi-scale spatio-temporal mixer that captures diverse patterns from datasets with varying spatio-temporal resolutions, and (3) an adaptive dataset-oriented tuning module that transfers the handling of dataset-specific parameters from pre-training to fine-tuning stage. These tailored designs enable CompactST to maximize generalizability across diverse datasets while maintaining a compact model size ( i.e. , only 300K parameters). To validate its effectiveness, we pre-train CompactST on a substantial corpus of public spatio-temporal datasets spanning over 10 domains and encompassing 300 million data points. Extensive experimental results on ten real-world datasets demonstrate CompactST‚Äôs significantly improved prediction accuracy and efficiency in data-scarce scenarios.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Scalable%20Pre-Training%20of%20Compact%20Urban%20Spatio-Temporal%20Predictive%20Models%20on%20Large-Scale%20Multi-Domain%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2149-han.pdf",
    "session": "Research 4: Analytics over Different Data Types I",
    "authors": [
      {
        "Name": "Jindong Han",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Hao Wang",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Hui Xiong",
        "Affiliation": "Hong Kong University of Science and Tech"
      },
      {
        "Name": "Hao Liu",
        "Affiliation": "HKUST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4ce544ec-64ce-4b58-babb-d5b048935617",
    "title": "Doctopus: Budget-aware Structural Table Extraction from Unstructured Documents",
    "abstract": "To fulfill the potential great value of unstructured documents, it is critical to extract structural data (e.g., attributes) from them, which can benefit various applications such as analytical SQL queries and decision-making. Multiple strategies, such as pre-trained language models (PLMs), can be employed for this task. However, these methods often struggle to achieve high-quality results, particularly when dealing with attribute extraction that requires intricate reasoning or semantic comprehension. Recently, large language models (LLMs) have proven to be effective in extracting attributes but incur substantial costs caused by token consumption, making them impractical for large-scale document set. \nTo best trade off quality and cost, we present  Doctopus , a system designed for accurate attribute extraction from unstructured documents with a user-specified cost constraint. Overall,  Doctopus combines LLMs with non-LLM strategies to achieve a good tradeoff. First, the system employs an index-based approach to efficiently identify and process only relevant text chunks, thereby reducing the LLM cost. Afterwards, it further estimates the quality of multiple strategies for each attribute. Finally, based on the cost and estimated quality,  Doctopus  dynamically selects the optimal strategies through budget-aware optimization. We have built a comprehensive benchmark including 4 document sets with various characteristics and manually labeled ground truth using 1000 human hours. Extensive experiments on the benchmark show that compared with state-of-the-art baselines,  Doctopus  can improve the quality by 11% given the same cost constraint.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Doctopus%3A%20Budget-aware%20Structural%20Table%20Extraction%20from%20Unstructured%20Documents",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3695-chai.pdf",
    "session": "Research 51: Information Integration and Data Quality III",
    "authors": [
      {
        "Name": "Chengliang Chai",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Jiajun Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Yuhao Deng",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Yuanhao Zhong",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Ye Yuan",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guoren Wang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Lei Cao",
        "Affiliation": "University of Arizona/MIT"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "97af2d81-fdfc-4700-ba22-2b85fab334d3",
    "title": "Fused Gromov-Wasserstein Alignment for Graph Edit Distance Computation and Beyond",
    "abstract": "Graph Edit Distance (GED) is a widely recognized metric for measuring graph similarity, yet its NP-complete nature poses challenges for fast and accurate computation. This paper introduces FGWAlign, an Optimal Transport (OT)-based approach for graph alignment and GED computation. We take the first step to theoretically demonstrate and that computing GED can be transformed into optimizing a particular OT variant‚Äîthe Fused Gromov-Wasserstein distance. Tailored to the GED problem structure, we further implement three key enhancements to the standard FGW solver: (1) a random exploration scheme to better locate the global optimum, (2) a diverse projection strategy for post-processing the transportation plan to escape local optima, and (3) a novel extension to accommodate multi-relational graphs with edge labels. With  O(| ùëΩ || ùë¨ |)  time complexity and  O(| ùëΩ | 2 )  space complexity, where  | ùëΩ |  and  | ùë¨ |  are the maximum number of nodes and edges between the two compared graphs, FGWAlign achieves a superior balance of efficiency, accuracy, and scalability. Empirical results show that, compared with 12 representative GED computation methods across different categories on 4 real-world graph datasets, FGWAlign reduces computation errors by over 80% and achieves 15-60 √ó  speedup. It also demonstrates promising resutls on downstream applications including labeled graph alignment and graph-level anomaly detection, highlighting its versatility. FGWAlign opens up promising avenues for future applications in graph data management.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Fused%20Gromov-Wasserstein%20Alignment%20for%20Graph%20Edit%20Distance%20Computation%20and%20Beyond",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3641-tang.pdf",
    "session": "Research 13: Graph Data Management II",
    "authors": [
      {
        "Name": "Jianheng Tang",
        "Affiliation": "Hong Kong University of Science and Technology"
      },
      {
        "Name": "Xi Zhao",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Lemin Kong",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Xiaofang Zhou",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Jia Li",
        "Affiliation": "Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "10d8a11d-e116-43df-87d6-973168b6ebd3",
    "title": "Twisted Twin: A Collaborative and Competitive Memory Management Approach in HTAP Systems",
    "abstract": "Many GaussDB customers, particularly small and medium-sized enterprises (SMEs), require high transaction throughput with occasional analytical queries. HTAP systems that deploy both OLTP and OLAP engines on a single server to manage hybrid workloads have become increasingly popular among customers for achieving high cost-efficiency and data freshness. However, co-locating these systems can lead to resource contention, particularly for memory, potentially degrading overall system performance and causing Service-Level Agreements (SLA) violations. To address this issue, we propose  ùëá 2   ( T wisted  T win), an adaptive memory management approach that dynamically allocates memory between OLTP and OLAP components. This approach ensures OLTP meets SLA while optimizing the efficiency of OLAP query processing. However, this is non-trivial, as memory allocation triggers a cascade of effects, including in-memory column selection and data synchronization, both critical in HTAP systems. To overcome these challenges, we introduce a Bayesian optimization framework tailored for fluctuating workloads that adjusts memory allocation responsively. Experiments conducted on the real-world HTAP system, GaussDB-HTAP, demonstrate the effectiveness and efficiency of  ùëá 2 .",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Twisted%20Twin%3A%20A%20Collaborative%20and%20Competitive%20Memory%20Management%20Approach%20in%20HTAP%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3312-wu.pdf",
    "session": "Research 8: Database Engines for OLAP",
    "authors": [
      {
        "Name": "JIANI YANG",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "SAI WU",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "YONG WANG",
        "Affiliation": "Huawei"
      },
      {
        "Name": "DONGXIANG ZHANG",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "YIFEI LIU",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "XIU TANG",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "GANG CHEN",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "07c61125-39bf-47e2-92f9-ec1e8428a5b3",
    "title": "Smart SPARQL Advisor: Guiding Users in Query Formulation with Performance Prediction",
    "abstract": "Writing SPARQL queries is often an iterative process, where users refine queries until they meet their information needs. However, long-running query executions can lead to inefficient workflows, as users must wait idly for results √ê potentially without success due to strict timeouts imposed by public endpoints. In this demo, we present the  Smart SPARQL Advisor  (SSA), a system that integrates Query Performance Prediction (QPP) to proactively mitigate these issues. By predicting query runtimes prior to execution, SSA alerts users to potentially slow or timeout-prone queries and, when necessary, employs a large language model (LLM) guided by latent representations from the QPP model to suggest alternative query formulations. We demonstrate that SSA enables users to identify performant queries and understand performance bottlenecks, thereby reducing idle time and avoiding unproductive query executions. Through this approach, SSA fosters more responsive and resource-efficient interactions with triplestores, enhancing both user experience and triplestore utilization.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Smart%20SPARQL%20Advisor%3A%20Guiding%20Users%20in%20Query%20Formulation%20with%20Performance%20Prediction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5295-mohanaraj.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Abiram Mohanaraj",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Matteo Lissandrini",
        "Affiliation": "University of Verona"
      },
      {
        "Name": "Katja Hose",
        "Affiliation": "TU Wien"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0b0736d0-b062-4ff3-afb9-cdfba22f3ac0",
    "title": "The LDBC Financial Benchmark: Transaction Workload",
    "abstract": "Graph databases play a pivotal role in the FinTech industry. However, existing graph benchmarks fail to capture the unique characteristics of financial datasets and workloads, rendering them inadequate for evaluating graph databases in financial scenarios. This paper presents the LDBC Financial Benchmark (FinBench) Transaction Workload, a novel benchmark that adopts a choke point-driven design methodology, emphasizing performance bottlenecks, and incorporates distinct features such as dataset skewness, edge multiplicity, temporal window filtering, recursive path filtering, read-write query patterns, and truncation on hub vertices. Key contributions include a scalable data generator that synthesizes datasets with financial-specific features, a parameter generator that leverages bucketed data statistics for runtime consistency across queries, and a scalable benchmark driver that biases query execution by time windows. Experimental evaluations on graph databases demonstrate the benchmark‚Äôs capability to reveal novel choke points and provide insights into system performance in financial scenarios.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20LDBC%20Financial%20Benchmark%3A%20Transaction%20Workload",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3007-qi.pdf",
    "session": "Research 60: Database Engine Performance and Manageability II",
    "authors": [
      {
        "Name": "Shipeng Qi",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Bing Tong",
        "Affiliation": "CreateLink Technology"
      },
      {
        "Name": "Jiatao Hu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Heng Lin",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Yue Pang",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Wei Yuan",
        "Affiliation": "China Software Testing Center"
      },
      {
        "Name": "Songlin Lyu",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Zhihui Guo",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Ke Huang",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Xujin Ba",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Qiang Yin",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Youren Shen",
        "Affiliation": "Beijing Haizhixingtu Technology"
      },
      {
        "Name": "Yan Zhou",
        "Affiliation": "CreateLink Technology"
      },
      {
        "Name": "Tao Lv",
        "Affiliation": "China Software Testing Center"
      },
      {
        "Name": "Jia Li",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Lei Zou",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Yongwei Wu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "G√°bor Sz√°rnyas",
        "Affiliation": "LDBC"
      },
      {
        "Name": "Xiaowei Zhu",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Wenguang Chen",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Chuntao Hong",
        "Affiliation": "Ant Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "33f5f520-ddb9-437f-bf37-e4fa5799654c",
    "title": "VIDEX: A Disaggregated and Extensible Virtual Index for Cloud-Native and AI-Driven Databases",
    "abstract": "Virtual indexes play a crucial role in database query optimization. However, with the rapid advancement of cloud computing and AIdriven models for database optimization, traditional virtual index approaches face significant challenges. Cloud-native environments often prohibit direct conducting query optimization process on production databases due to stability requirements and data privacy concerns. Moreover, while AI models show promising progress, their integration with database systems poses challenges in system complexity, inference acceleration, and model hot updates. In this paper, we present VIDEX, a three-layer disaggregated architecture that decouples database instances, the virtual index optimizer, and algorithm services, providing standardized interfaces for AI model integration. Users can configure VIDEX by either collecting production statistics or loading from a prepared file, enabling highaccuracy what-if analysis using virtual indexes that yield query plans identical to production instances. Additionally, users can freely integrate new AI-driven algorithms into VIDEX. VIDEX has been deployed at ByteDance, serving thousands of MySQL instances daily and over millions of SQL queries for index optimization tasks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/VIDEX%3A%20A%20Disaggregated%20and%20Extensible%20Virtual%20Index%20for%20Cloud-Native%20and%20AI-Driven%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5231-zhang.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Rong Kang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Shuai Wang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Tieying Zhang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Xianghong Xu",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Linhui Xu",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Zhimin Liang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Lei Zhang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Rui Shi",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Jianjun Chen",
        "Affiliation": "ByteDance"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ac57f6c2-7d94-4e0d-a0ca-68063cf3088d",
    "title": "Unsupervised Anomaly Detection in Multivariate Time Series across Heterogeneous Domains",
    "abstract": "The widespread adoption of digital services, along with the scale and complexity at which they operate, has made incidents in IT operations increasingly more likely, diverse, and impactful. This has led to the rapid development of a central aspect of ‚ÄúArtificial Intelligence for IT Operations\" (AIOps), focusing on detecting anomalies in vast amounts of multivariate time series data generated by service entities. In this paper, we begin by introducing a unifying framework for benchmarking unsupervised anomaly detection (AD) methods, and highlight the problem of shifts in normal behaviors that can occur in practical AIOps scenarios. To tackle anomaly detection under domain shift, we then cast the problem in the framework of domain generalization and propose a novel approach, Domain-Invariant VAE for Anomaly Detection (DIVAD), to learn domain-invariant representations for unsupervised anomaly detection. Our evaluation results using the Exathlon benchmark show that the two main DIVAD variants significantly outperform the best unsupervised AD method in maximum performance, with 20% and 15% improvements in maximum peak F1-scores, respectively. Evaluation using the Application Server Dataset further demonstrates the broader applicability of our domain generalization methods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Unsupervised%20Anomaly%20Detection%20in%20Multivariate%20Time%20Series%20across%20Heterogeneous%20Domains",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1691-jacob.pdf",
    "session": "Research 12: Time Series Data I",
    "authors": [
      {
        "Name": "Vincent Jacob",
        "Affiliation": "Ecole Polytechnique"
      },
      {
        "Name": "Yanlei Diao",
        "Affiliation": "Ecole Polytechnique"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "50267d2d-b061-4822-887f-e1efb73b5038",
    "title": "Are Joins over LSM-trees Ready: Take RocksDB as an Example",
    "abstract": "LSM-tree-based data stores are widely adopted in industries for their excellent performance. As data scale increases, disk-based join operations become indispensable yet costly for the database, making the selection of suitable join methods crucial for system optimization. Current LSM-based stores generally adhere to conventional relational database practices and support only a limited number of join methods. However, the LSM-tree delivers distinct read and write eÔ¨Éciency compared to the relational databases, which could accordingly impact the performance of various join methods. Therefore, it is necessary to reconsider the selection of join methods in this context to fully explore the potential of various join algorithms and index designs. In this work, we present a systematic study and an exhaustive benchmark for joins over LSM-trees. We deÔ¨Åne a conÔ¨Åguration space for join methods, encompassing various join algorithms, secondary index types, and consistency strategies. We also summarize a theoretical analysis to evaluate the overhead of each join method for an in-depth understanding. Furthermore, we implement all join methods in the conÔ¨Åguration space on a uniÔ¨Åed platform and compare their performance through extensive experiments. Our theoretical and experimental results yield several insights and takeaways tailored to joins in LSM-based stores that aid developers in choosing proper join methods based on their working conditions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Are%20Joins%20over%20LSM-trees%20Ready%3A%20Take%20RocksDB%20as%20an%20Example",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1077-luo.pdf",
    "session": "Research 60: Database Engine Performance and Manageability II",
    "authors": [
      {
        "Name": "Weiping Yu",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "FAN WANG",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Xuwei Zhang",
        "Affiliation": "Nanyang Technological University"
      },
      {
        "Name": "Siqiang Luo",
        "Affiliation": "Nanyang Technological University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a0eb3d4a-0dd1-418f-852c-86cb70dea6be",
    "title": "UFGTime: Mining Intertwined Dependencies in Multivariate Time Series via an Efficient Pure Graph Approach",
    "abstract": "Graph Neural Networks (GNNs) have become a cornerstone in multivariate time series forecasting by addressing the challenge of modeling inter-series dependencies often overlooked by traditional temporal approaches. However, real-world temporal dependencies (inter- and intra-dependencies) are inherently intertwined, making it difficult to treat them as separate processes. Recent pure graph paradigms attempt to capture these dependencies holistically by transforming time series into fully connected graphs. While effective, these methods suffer from prohibitive computational complexity  O(( ùëÅùëá ) 2 ) , limiting their scalability for large-scale data and long-term forecasting. To address these challenges, we propose UFGTime, a novel framework that leverages spectral signals to construct a \"spectral-variate graph,\" embedding multivariate temporal dependencies in a compact spectral representation and modeling inter- and intra-signal connections through frequency similarities. Empowered by our proposed graph framelet message-passing function, UFGTime efficiently aggregates global information, avoids over-smoothing, and achieves near-linear complexity  O( ùëòùëÅùëá ) . Extensive experiments on diverse datasets demonstrate that UFGTime consistently outperforms state-of-the-art baselines, offering a scalable, accurate, and resource-efficient pure graph solution for multivariate time series forecasting.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/UFGTime%3A%20Mining%20Intertwined%20Dependencies%20in%20Multivariate%20Time%20Series%20via%20an%20Efficient%20Pure%20Graph%20Approach%20(Flavor%3A%20Foundations%20and%20Algorithms%20Papers)",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3175-gao.pdf",
    "session": "Research 4: Analytics over Different Data Types I",
    "authors": [
      {
        "Name": "Ruikun Li",
        "Affiliation": "The University of Sydney"
      },
      {
        "Name": "Dai Shi",
        "Affiliation": "The University of Sydney"
      },
      {
        "Name": "Ye Xiao",
        "Affiliation": "The University of Sydney"
      },
      {
        "Name": "Junbin Gao",
        "Affiliation": "The University of Sydney"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f6287163-3bcf-4ace-9a63-19d14158128b",
    "title": "MD-MVCC: Multi-version Concurrency Control for Schema Changes in Azure SQL Database",
    "abstract": "As applications and data evolve over time, the database schema  must be adjusted to accommodate their needs. Schema changes  in  relational  databases  have  traditionally  required  synchronization with concurrent read and write access, causing  signiÔ¨Åcant interruptions to user applications. Although, most  commercial databases have optimized common schema changes  to reduce their runtime, they have not fundamentally addressed  the requirement for synchronization which can lead to data  being inaccessible for minutes or even hours in the presence of  long running queries. MD-MVCC is a new technology in Azure  SQL Database that enables multi-version concurrency control for  schema changes. ÓÅâis allows schema changes to occur without  any synchronization with concurrent queries which can operate  on the earlier version of the schema until ongoing operations are  commiÓÄºed, following Snapshot Isolation semantics. Schema  deployments can now occur with minimal impact, increasing  data availability but also Ô¨Çexibility for application developers.  ÓÅâis required a comprehensive redesign of the schema  management and metadata components of the RDBMS that are  now multi-versioned across all layers, from query execution and  in-memory caches to the system tables where metadata is  persisted. ÓÅâis paper presents the overall design of MD-MVCC  and demonstrates how it fundamentally improves data  availability during schema changes without incurring any  performance overheads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/MD-MVCC%3A%20Multi-version%20Concurrency%20Control%20for%20Schema%20Changes%20in%20Azure%20SQL%20Database",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4791-antonopoulos.pdf",
    "session": "Industry 5: Transactions and Concurrency Control",
    "authors": [
      {
        "Name": "Panagiotis Antonopoulos",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Mansi Chauhan",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Shailender Dabas",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Rajat Jain",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Darshan Kattera",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Wonseok Kim",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Hanuma Kodavalla",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Nikolas Ogg",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Prashanth Purnananda",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Rahul Ranjan",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Alex Swanson",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Divyesh Tikmani",
        "Affiliation": "Microsoft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f57006db-e7fc-4246-bceb-0ef397e60cdb",
    "title": "SiriusBI: A Comprehensive LLM-powered Solution for Data Analytics in Business Intelligence",
    "abstract": "With the proliferation of Large Language Models (LLMs) in Business Intelligence (BI), existing solutions face critical challenges in industrial deployments: functionality deficiencies from legacy systems failing to meet evolving LLM-era user demands, interaction limitations from single-round SQL generation paradigms inadequate for multi-round clarification, and cost for domain adaptation arising from cross-domain methods migration. \nWe present SiriusBI, a practical LLM-powered BI system addressing the challenges of industrial deployments through three key innovations: (a) An end-to-end architecture integrating multi-module coordination to overcome functionality gaps in legacy systems; (b) A multi-round dialogue with querying mechanism, consisting of semantic completion, knowledge-guided clarification, and proactive querying processes, to resolve interaction constraints in SQL generation; (c) A data-conditioned SQL generation method selection strategy that supports both an efficient one-step Fine-Tuning approach and a two-step method leveraging Semantic Intermediate Representation for low-cost cross-domain applications. Experiments on both real-world datasets and public benchmarks demonstrate the effectiveness of SiriusBI. User studies further confirm that SiriusBI enhances both productivity and user experience. \nAs an independent service on Tencent‚Äôs data platform, SiriusBI is deployed across finance, advertising, and cloud sectors, serving dozens of enterprise clients. It achieves over 93% accuracy in SQL generation and reduces data analysts‚Äô query time from minutes to seconds in real-world applications.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SiriusBI%3A%20A%20Comprehensive%20LLM-powered%20Solution%20for%20Data%20Analytics%20in%20Business%20Intelligence",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4860-xie.pdf",
    "session": "Industry 4: Machine Learning, AI, and Databases",
    "authors": [
      {
        "Name": "Jie Jiang",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Haining Xie",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Siqi Shen",
        "Affiliation": "Center of Machine Learning Research, Peking University"
      },
      {
        "Name": "Yu Shen",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Zihan Zhang",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Meng Lei",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Yifeng Zheng",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Yang Li",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Chunyou Li",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Danqing Huang",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Yinjun Wu",
        "Affiliation": "School of Computer Science, Peking University"
      },
      {
        "Name": "Wentao Zhang",
        "Affiliation": "Center of Machine Learning Research, Peking University"
      },
      {
        "Name": "Xiaofeng Yang",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Bin Cui",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      },
      {
        "Name": "Peng Chen",
        "Affiliation": "Department of Data Platform, TEG, Tencent Inc."
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1f0dacdd-67b3-4a1f-919b-9d9e8333a874",
    "title": "Federated and Balanced Clustering for High-dimensional Data",
    "abstract": "Balanced  ùëò -means ensures representative centroids by forming equal-sized clusters, but struggles with slow clustering of massive distributed attributes and data-sharing restrictions. A common approach is adapting it to a vertical federated learning (VFL) framework, preventing raw data exposure by only intermediate result exchange and accelerating clustering via parallelism, yet it remains unexplored. In this paper, we propose a   t ime-efficient, f e derated, and   b alanced  ùëò - means  algorithm, called  Teb-means , to bridge the gap. We first formulate the balanced  ùëò -means problem as a trace maximization problem (TMP) and propose an efficient coordinatewise optimization (CO) scheme to solve it. We then integrate TMP and CO into the VFL framework by demonstrating that TMP can be decomposed into multiple subproblems based on each party‚Äôs data, which can be solved using CO while exchanging only intermediate results. Notably, we build a trade-off between utility and communication efficiency by designing a greedy block-based strategy for CO (GBCO). Our theoretical analysis shows that  Teb-means achieves linear time complexity on each client, and our communication round is constant in the mild condition. Experiments show that  Teb-means  is on average 12.18 √ó  faster than other balanced clustering algorithms that can be federated, while achieving better balance without disrupting the cluster structure.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Federated%20and%20Balanced%20Clustering%20for%20High-dimensional%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4032-wang.pdf",
    "session": "Research 40: Time Series and Vector Data",
    "authors": [
      {
        "Name": "Yushuai Ji",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Shengkun Zhu",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Shixun Huang",
        "Affiliation": "The University of Wollongong"
      },
      {
        "Name": "Zepeng Liu",
        "Affiliation": "Wuhan University"
      },
      {
        "Name": "Sheng Wang",
        "Affiliation": "Wuhan"
      },
      {
        "Name": "Zhiyong Peng",
        "Affiliation": "Wuhan University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c9614fb2-f169-4200-a167-e02acb6d8728",
    "title": "Sectric: Towards Accurate, Privacy-preserving and Efficient Triangle Counting",
    "abstract": "Graph data analysis, particularly local triangle counting, plays a pivotal role in deciphering complex relationships within graph data. This method is invaluable across diverse fields such as social networks, transportation, and cybersecurity. However, this process often involves handling sensitive information, necessitating that the relationship between any two nodes is considered private. Differential privacy (DP) is a formal model to address privacy concerns and can be categorized into two types: the central DP (CDP) model, which achieves better result accuracy, and the local DP (LDP) model, which does not assume a trusted server. To bridge the gap between the two models, we propose  Sectric , a   se rver-aided   c rypto-assisted local   tri angle   c ounting protocol, in this paper. It can achieve the same result accuracy with the same privacy budget as the CDP model without assuming a trusted server.  Sectric  also explores a new approach in crypto-assisted graph data analysis algorithms that represents a node‚Äôs neighbors using a set instead of an adjacency vector, and successfully achieves higher efficiency compared to other crypto-assisted solutions. We also conduct theoretical and empirical evaluations to demonstrate that  Sectric  achieves the design principles.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Sectric%3A%20Towards%20Accurate%2C%20Privacy-preserving%20and%20Efficient%20Triangle%20Counting",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3382-xu.pdf",
    "session": "Research 11: Graph Data Privacy, Text and Semi-Structured Data Management",
    "authors": [
      {
        "Name": "MinZe Xu",
        "Affiliation": "Nanjing University"
      },
      {
        "Name": "Zhentai Xie",
        "Affiliation": "Nanjing University"
      },
      {
        "Name": "Zhibin Wang",
        "Affiliation": "Nanjing University"
      },
      {
        "Name": "Guangzhan Wang",
        "Affiliation": "Nanjing University"
      },
      {
        "Name": "Longbin Lai",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Yuan Zhang",
        "Affiliation": "Nanjing University"
      },
      {
        "Name": "Chen Tian",
        "Affiliation": "Nanjing University"
      },
      {
        "Name": "Sheng Zhong",
        "Affiliation": "Nanjing University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7603469a-dd8f-474e-b9fd-4bebd5be4909",
    "title": "CENTS: A Flexible and Cost-Effective Framework for LLM-Based Table Understanding",
    "abstract": "Large Language Models (LLMs) have recently shown impressive capabilities in a variety of applications including table understanding tasks such as column type annotation. Existing LLM-based solutions for table understanding, however, focus on developing speciÔ¨Åc framework for each individual task, or do not consider the cost-eÔ¨Äectiveness tradeoÔ¨Ä. In this paper, we present  Cents , a uniÔ¨Åed and cost-eÔ¨Äective framework for LLM-based solutions for table understanding tasks.  Cents ‚Äôs key capability is an eÔ¨Écient and eÔ¨Äective approach to compress the tabular LLM input in a way that reduces input token cost while improving performance compared with state-of-the-art methods. Experiment results show that  Cents  outperforms other LLM-based baselines on a variety of table understanding tasks at the same or lower cost.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/CENTS%3A%20A%20Flexible%20and%20Cost-Effective%20Framework%20for%20LLM-Based%20Table%20Understanding",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4574-xiao.pdf",
    "session": "Research 43: Information Integration and Data Quality II",
    "authors": [
      {
        "Name": "Guorui Xiao",
        "Affiliation": "University of Washington"
      },
      {
        "Name": "Dong He",
        "Affiliation": "University of Washington"
      },
      {
        "Name": "Jin Wang",
        "Affiliation": "Megagon Labs"
      },
      {
        "Name": "Magdalena Balazinska",
        "Affiliation": "University of Washington"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f609854c-4088-4bcf-a831-1206a21cdce9",
    "title": "How SMPC Query Execution can be sped up through Efficient and Flexible Intermediate Result Size Trimming",
    "abstract": "There is growing interest in Secure Collaborative Analytics, but fully oblivious query execution in Secure Multi-Party Computation (MPC) settings is prohibitively expensive. Recent related works proposed different approaches to trimming the size of intermediate results between oblivious query operators, resulting in significant speedups at the cost of some controlled information leakage. In Reflex, we generalize these ideas into a flexible and efficient trimming method for the output of the oblivious operators, that we call Resizer. Resizers can be seamlessly integrated between MPCbased query operators. This allows for precisely controlling the security/performance trade-off on a per-operator and per-query basis. Our method has the potential to accelerate the performance of current oblivious query execution by up to 200 times compared to fully oblivious query execution, and by approximately 7 times compared to existing approaches with the same security guarantees. Our work lays down the foundation for a future MPC query planner that can pick different performance and security targets when composing physical plans. \nThis demonstration showcases the benefits of Reflex. More precisely, it focuses on the integration of our proposed resizers into the oblivious query plan, significantly enhancing performance. Conference attendees will have the opportunity to observe the efficient trimming of intermediate results and, additionally, they will be able to configure the oblivious execution settings, ranging from fully oblivious to fully revealed. This hands-on experience will highlight the benefits of our proposal in various obliviousness scenarios.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Demonstration%20of%20Reflex%3A%20How%20SMPC%20Query%20Execution%20can%20be%20sped%20up%20through%20Efficient%20and%20Flexible%20Intermediate%20Result%20Size%20Trimming",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5311-gu.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Long Gu",
        "Affiliation": "TU Darmstadt"
      },
      {
        "Name": "Shaza Zeitouni",
        "Affiliation": "TU Darmstadt"
      },
      {
        "Name": "Carsten Binnig",
        "Affiliation": "TU Darmstadt"
      },
      {
        "Name": "Zsolt Istv√°n",
        "Affiliation": "TU Darmstadt"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dcd04672-d734-42fa-b58c-2676ad59060d",
    "title": "veDB-HTAP: a Highly Integrated, Efficient and Adaptive HTAP System",
    "abstract": "In this paper, we describe veDB-HTAP, a highly integrated, efficient, and adaptive HTAP system recently built in ByteDance. veDB-HTAP adopts a highly integrated system architecture by leveraging the Secondary Engine mechanism provided by MySQL and provides a seamless query processing experience across OLTP and OLAP engines. In addition, we introduce a cost-based and machine-learning-based smart query router that significantly outperforms the rule-based query router used in ByteHTAP, a precursor of veDB-HTAP. A key design principle of veDB-HTAP is the collaboration and adaptability of major system components, including query planning, query execution, and unified storage. Our adaptive query execution can be classified into two categories: 1) adaptive execution that dynamically collects and utilizes runtime statistics for better query performance; 2) utilizing runtime resource information to achieve a high quality of service even under heavy workloads. The experiments show that veDB-HTAP can achieve more than 3 √ó  speedup for TPC-H while consuming only one-third of the resources compared to ByteHTAP.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/veDB-HTAP%3A%20a%20Highly%20Integrated%2C%20Efficient%20and%20Adaptive%20HTAP%20System",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4896-chen.pdf",
    "session": "Industry 2: Data Platforms for Analytics",
    "authors": [
      {
        "Name": "Jianjun Chen",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Li Zhang",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Yu Xie",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Wei Ding",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Lixun Cao",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Ye Liu",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Yonghua Ding",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Fangshi Li",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Ke Wu",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Haibo Xiu",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Kui Wei",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Le Cai",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Rui Chang",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Yuxiang Chen",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Yuanjin Lin",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Shangyu Luo",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Jianfeng Qian",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Xu Wang",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Zikang Wang",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Jian Zhang",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Mingyi Zhang",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Shicai Zeng",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Jason Sun",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Lei Zhang",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Rui Shi",
        "Affiliation": "ByteDance Inc"
      },
      {
        "Name": "Pengwei Zhao",
        "Affiliation": "ByteDance Inc"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5e62e9ed-c1c7-40bf-91a3-2577622a936e",
    "title": "Workload Insights From the Snowflake Data Cloud: What Do Production Analytic Queries Really Look Like?",
    "abstract": "Capturing the characteristics of real-world analytical workloads is challenging yet critical for advancing industry practices and academic research. Historically, obtaining accurate query and data characteristics has been difficult, largely because detailed workload information has often been confined to on-premises database systems. With the rise of cloud-native databases like Snowflake, it has become possible to analyze production query workloads at scale and in greater detail. Leveraging this capability, this study presents a comprehensive analysis of analytics workloads across diverse customers and industries. In particular, we investigate the query characteristics of 667 million queries issued by the most popular BI tools against Snowflake over a two-week period. Based on this dataset, this paper makes two primary contributions: first, we conduct a detailed examination of query properties, with particular attention to filters, joins, aggregations, and other previously underexplored aspects. Second, we uncover unique and practically relevant query patterns that are typically absent from standard database benchmarks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Workload%20Insights%20From%20the%20Snowflake%20Data%20Cloud%3A%20What%20Do%20Production%20Analytic%20Queries%20Really%20Look%20Like%EF%BC%9F",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5126-bress.pdf",
    "session": "Industry 2: Data Platforms for Analytics",
    "authors": [
      {
        "Name": "Jan Vincent Szlang",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Sebastian Bre√ü",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Sebastian Cattes",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Jonathan Dees",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Florian Funke",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Max Heimel",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Michel Oleynik",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Ismail Oukid",
        "Affiliation": "Snowflake"
      },
      {
        "Name": "Tobias Maltenberger",
        "Affiliation": "Google"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8b0e5c14-4100-4e89-9a8d-827f0bbbe60b",
    "title": "A Demonstration of POLARIS: An Interactive and Scalable Data Infrastructure for Polar Science",
    "abstract": "This demonstration presents Polaris; a novel open-source system infrastructure for   Polar  science that is highly   I nteractive and S calable. Polaris is designed based on three observations that distinguish the query workload of polar scientists, namely, all queries are spatio-temporal, not all data are equal, and the large majority of queries are aggregates. With this, Polaris is equipped with a hierarchical spatio-temporal index structure that stores precomputed aggregates for data of interest. Audience will be able to experience Polaris through various scenarios that show the interactivity and scalability as well as Polaris optimized query processes.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Demonstration%20of%20POLARIS%3A%20An%20Interactive%20and%20Scalable%20Data%20Infrastructure%20for%20Polar%20Science",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5347-mokbel.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Yuchuan Huang",
        "Affiliation": "University of Minnesota - Twin Cities"
      },
      {
        "Name": "Ana Elena Uribe",
        "Affiliation": "University of Minnesota - Twin Cities"
      },
      {
        "Name": "Youssef Hussein",
        "Affiliation": "University of Minnesota - Twin Cities"
      },
      {
        "Name": "Grant Ogren",
        "Affiliation": "University of Minnesota - Twin Cities"
      },
      {
        "Name": "Kareem Eldahshoury",
        "Affiliation": "University of Minnesota - Twin Cities"
      },
      {
        "Name": "Mohamed Mokbel",
        "Affiliation": "University of Minnesota - Twin Cities"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d51df30f-52ba-49cb-897f-96f3dfa89dff",
    "title": "Migration-Free Elastic Storage of Time Series in Apache IoTDB",
    "abstract": "In distributed time series databases (TSDBs), time series data are typically partitioned by both series and time. These partitions are then allocated to shards, whose replicas determine the storage location, with the leader managing the write load. In Internet of Things (IoT) scenarios, clusters expand as the number of sensors continues to grow. A common approach to re-balancing storage is migrating existing partitions, yet it incurs additional overhead. Fortunately, Time to Live (TTL) is often implemented in time series databases to automatically unload expired data. As a result, dynamically expanding shards rather than migrating existing partitions can also restore storage balance. In addition, the cluster‚Äôs fault tolerance depends on replica placement schemes, and an expanding cluster complicates this issue. Finally, the intensive write load in IoT scenarios requires balanced leader selection, which becomes difficult due to fault-tolerant placement schemes. To address these IoT challenges, this paper presents the migration-free data partitioning and allocation strategies, a storage-balanced replica placement algorithm with proven fault tolerance, and a write-balanced leader selection algorithm. Our proposals have been deployed in Apache IoTDB since version 1.3. Extensive evaluation of the system demonstrates its superiority in availability and performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Migration-Free%20Elastic%20Storage%20of%20Time%20Series%20in%20Apache%20IoTDB",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1784-song.pdf",
    "session": "Research 19: Time Series Data II",
    "authors": [
      {
        "Name": "Rongzhao Chen",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xiangpeng Hu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xiangdong Huang",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Chen Wang",
        "Affiliation": "\" Tsinghua University, China\""
      },
      {
        "Name": "Shaoxu Song",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Jianmin Wang",
        "Affiliation": "\"Tsinghua University, China\""
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "67fdd7dd-614c-4a7c-a2c8-28be0cdec365",
    "title": "OpenFGL: A Comprehensive Benchmark for Federated Graph Learning",
    "abstract": "Federated graph learning (FGL) is a promising distributed training paradigm for graph neural networks across multiple local systems without direct data sharing. This approach inherently involves large-scale distributed graph processing, which closely aligns with the challenges and research focuses of graph-based data systems. Despite the proliferation of FGL, the diverse motivations from real- world applications, spanning various research backgrounds and settings, pose a signi!cant challenge to fair evaluation. To !ll this gap, we propose OpenFGL, a uni!ed benchmark designed for the primary FGL scenarios: Graph-FL and Subgraph-FL. Speci!cally, OpenFGL includes 42 graph datasets from 18 application domains, 8 federated data simulation strategies that emphasize different graph properties, and 5 graph-based downstream tasks. Additionally, it offers 18 recently proposed SOTA FGL algorithms through a user- friendly API, enabling a thorough comparison and comprehensive evaluation of their effectiveness, robustness, and effciency. Our empirical results demonstrate the capabilities of FGL while also highlighting its potential limitations, providing valuable insights for future research in this growing !eld, particularly in fostering greater interdisciplinary collaboration between FGL and data systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/OpenFGL%3A%20A%20Comprehensive%20Benchmark%20for%20Federated%20Graph%20Learning",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1305-li.pdf",
    "session": "Research 45: Graph Data Learning",
    "authors": [
      {
        "Name": "Xunkai Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Yinlin Zhu",
        "Affiliation": "Sun Yat-sen University"
      },
      {
        "Name": "Boyang Pang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guochen Yan",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Yeyu Yan",
        "Affiliation": "Beijing Jiaotong University"
      },
      {
        "Name": "Zening Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Zhengyu Wu",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Wentao Zhang",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Ronghua Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guoren Wang",
        "Affiliation": "Beijing Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8574f7d6-72b8-494c-b2eb-658d97873300",
    "title": "SDG-KG: A Framework to Compute SDG Indicators with Open Data",
    "abstract": "Monitoring Sustainable Development Goal (SDG) indicators requires integrating heterogeneous open datasets from sources such as relational databases, NoSQL stores, and APIs. While SDG indicators follow standardized deÔ¨Ånitions, open data sources are often fragmented, schema-less, and inconsistent, making both integration and computation challenging. In this demonstration, we present SDG-KG , a spatio-temporal Knowledge Graph (KG) framework designed to structure metadata, guide data retrieval, and formalize indicator computation workÔ¨Çows. Our approach leverages graphbased modeling to construct a Metadata Graph, apply conÔ¨Çict resolution techniques when multiple sources provide overlapping data, and dynamically generate query-driven execution plans. Through an interactive interface, users can explore United Nations speciÔ¨Åcations, inspect data provenance and the generated KG, and visualize the computed indicators.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SDG-KG%3A%20A%20Framework%20to%20Compute%20SDG%20Indicator%20using%20Open%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5367-benjira.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Wissal BENJIRA",
        "Affiliation": "DVRC - IGN"
      },
      {
        "Name": "Nicolas TRAVERS",
        "Affiliation": "DVRC - CNAM"
      },
      {
        "Name": "Faten ATIGUI",
        "Affiliation": "CNAM"
      },
      {
        "Name": "B√©n√©dicte BUCHER",
        "Affiliation": "IGN"
      },
      {
        "Name": "Malika GRIM-YEFSAH",
        "Affiliation": "IGN"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eb2417d3-6659-4e27-a873-7cc74d881e9e",
    "title": "Accelerating Subgraph Matching through Fine-grained and Powerful Equivalences",
    "abstract": "Subgraph matching, a cornerstone of graph analytics, crit ically suÔ¨Äers from redundant computations during the search process. Existing methods primarily target identical computations redundant operations that are localized to individual query vertices but fail to address similar redundancies that recur across multiple query vertices. In this paper, we present a novel algorithm, called FiPE, that accelerates subgraph matching through   Fi ne-grained and   P owerful E quivalences. FiPE redeÔ¨Ånes redundancy elimination by shifting the optimization granularity from isolated vertices to vertex pairs and multiple vertex patterns. It introduces  vertex-pair equivalence  to cluster candidate pairs with isomorphic neighbor structures, even if their individual vertices diÔ¨Äer, enabling pruning of similar computations between these vertex pairs. FiPE proposes  group equivalence  to defer equivalence checks to later search depths, capturing potential redundancies incrementally. To fully exploit the advantages of the equivalence, we introduce two optimization techniques: a matching order generation method to reduce the overall search space and an efficient conÔ¨Çict resolution mechanism to avoid two query vertices being mapped to the same data vertex. Experiments on real-world graphs highlight the superiority of FiPE. FiPE achieves a speedup of 2 to 3 orders of magnitude on various graphs under the EPS (embeddings per second) metric.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Accelerating%20Subgraph%20Matching%20through%20Fine-grained%20and%20Powerful%20Equivalences",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3896-zheng.pdf",
    "session": "Research 28: Social Networks",
    "authors": [
      {
        "Name": "Yujie Lu",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Zhijie Zhang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Weiguo Zheng",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Lei Zou",
        "Affiliation": "Peking University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bc018a1a-1a54-4c01-bc90-c5eca059f4dd",
    "title": "Discovering Approximate Inclusion Dependencies",
    "abstract": "Inclusion dependencies ( INDs ) are widely used in data management tasks. The discovery techniques of  INDs  have thus received a lot of attention, for discovering  INDs  valid in data. However, real-world data quality issues may lead to partial violations of  INDs . This paper makes the first effort to provide a comprehensive study on the discovery of approximate  INDs  ( AINDs ), aiming to identify  INDs with error rates below a given threshold. This paper introduces a new definition of  AIND  based on deletion semantics, in addition to the existing definition based on insertion semantics. A discovery method is developed that can be configured to identify  AINDs based on either of these semantics. The method combines partitioning techniques to handle tables that cannot all fit into memory simultaneously, with novel approaches to quantify  AIND  violations based on partitioned tables. To improve efficiency, the method employs a novel three-layer filtering structure and techniques that can potentially prune invalid candidate  AINDs  and identify valid AINDs  without necessarily processing all tuples. We conduct an extensive experimental evaluation and verify the following: the proposed method significantly outperforms existing methods for AIND  discovery based on insertion semantics, the  AIND  discoveries with insertion and deletion semantics can provide complementary results, and our discovery method can effectively deal with dirty dataset containing various types of errors.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Discovering%20Approximate%20Inclusion%20Dependencies",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1210-tan.pdf",
    "session": "Research 4: Analytics over Different Data Types I",
    "authors": [
      {
        "Name": "Qingdong Su",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Zhikang Wang",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Zijing Tan",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Shuai Ma",
        "Affiliation": "Beihang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f4eb41ea-279d-4d25-b6bd-d99ca12f2209",
    "title": "SCompression: Enhancing Database Knob Tuning Efficiency Through Slice-Based OLTP Workload Compression",
    "abstract": "Workload execution can account for 90% of the total database knob tuning time, which is often the bottleneck for efficient knob tuning in practice. Reducing the tuning time by using a compressed workload is a natural solution. However, many existing workload compression methods are designed for OLAP workloads, which reduce the number of queries needed for analysis tasks by sampling a small subset of queries. These methods are less effective for OLTP workloads in knob-tuning tasks, as they often disregard essential contextual details, including query sequence and concurrency. As a result, configurations that perform well on the compressed OLTP workload may not deliver similar competitive performance on the original workload. To address these challenges, we first define the objective of OLTP workload compression for knob tuning. We then propose a slice-based compression method,  SCompression , which compresses workloads by slicing based on time intervals while preserving concurrency.  SCompression  achieves the objective by focusing on generating a compressed workload that (1) executes faster than the original workload and (2) produces performance variations similar to the source workload under different configurations. SCompression  works in three steps: (1) dividing the workload into segments to capture regular performance fluctuations, (2) slicing each segment to preserve concurrency and transaction context, and (3) sampling slices under execution time constraints using a clusterbased approach to ensure representativeness. Finally,  SCompression replays the compressed workload to produce the performance that mirrors the source workload. Extensive experiments on real-world and benchmark OLTP workloads show that  SCompression  is a costeffective solution for knob tuning, accelerating tuning by up to 40 √ó with only a 5% performance reduction.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SCompression%3A%20Enhancing%20Database%20Knob%20Tuning%20Efficiency%20Through%20Slice-Based%20OLTP%20Workload%20Compression",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1865-cai.pdf",
    "session": "Research 60: Database Engine Performance and Manageability II",
    "authors": [
      {
        "Name": "Baoqing Cai",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Yu Liu",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Lin Ma",
        "Affiliation": "University of Michigan"
      },
      {
        "Name": "Pingqi Huang",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Bingcheng Lian",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Ke Zhou",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Jia Yuan",
        "Affiliation": "University of Arizona"
      },
      {
        "Name": "Jie yang",
        "Affiliation": "Tencent"
      },
      {
        "Name": "Xiaofan Cai",
        "Affiliation": "Tencent"
      },
      {
        "Name": "peijun wu",
        "Affiliation": "tencent.com"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3b79af6f-a9c9-40c0-8ebf-f3d4f93634c6",
    "title": "Data-Agnostic Cardinality Learning from Imperfect Workloads",
    "abstract": "Cardinality estimation (CardEst) is a critical aspect of query optimization. Traditionally, it leverages statistics built directly over the data. However, organizational policies (e.g., regulatory compliance) may restrict global data access. Fortunately,  query-driven  cardinality estimation can learn CardEst models using query workloads. However, existing query-driven models often require access to data or summaries for best performance, and they assume  perfect  training workloads with complete and balanced join templates (or join graphs). Such assumptions rarely hold in real-world scenarios, in which join templates are incomplete and imbalanced. \nWe present GRASP, a  data-agnostic  cardinality learning system designed to work under these real-world constraints. GRASP‚Äôs compositional design  generalizes to unseen join templates and is robust to join template imbalance. It also introduces a new pertable CardEst model that handles value distribution shifts for range predicates, and a novel  learned count sketch  model that captures join correlations across base relations. Across three database instances, we demonstrate that GRASP consistently outperforms existing query-driven models on imperfect workloads, both in terms of estimation accuracy and query latency. Remarkably, GRASP achieves performance comparable to, or even surpassing, traditional approaches built over the underlying data on the complex CEB-IMDb-full benchmark ‚Äî despite operating without any data access and using only 10% of all possible join templates.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Data-Agnostic%20Cardinality%20Learning%20from%20Imperfect%20Workloads",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2519-wu.pdf",
    "session": "Research 61: Learned Query Optimization",
    "authors": [
      {
        "Name": "Peizhi Wu",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Rong Kang",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Tieying Zhang",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Jianjun Chen",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Ryan Marcus",
        "Affiliation": "University of Pennsylvania"
      },
      {
        "Name": "Zack Ives",
        "Affiliation": "University of Pennsylvania"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "12f3657d-cac3-43d0-9c76-b34761977469",
    "title": "Decentralized Actor Scheduling and Reference-based Storage in Xorbits: a Native Scalable Data Science Engine",
    "abstract": "Data science pipelines consist of data preprocessing and transformation, and a typical pipeline comprises a series of operators, such as DataFrame filtering and  groupby . As practitioners seek tools to handle larger-scale data while maintaining APIs compatible with popular single-machine libraries (e.g., pandas), scaling such a pipeline requires efficient distribution of decomposed tasks across the cluster and fine-grained, key-level intermediate storage management, two challenges that existing systems have not effectively addressed. Motivated by the requirements of scaling diverse data science applications, we present the design and implementation of Xorbits, a native scalable data science engine built on our decentralized actor model, Xoscar. Our actor model can eliminate dependency on a global scheduler and enable fast actor task scheduling. We also provide reference-based distributed storage with unified access across heterogeneous memory resources. Our evaluation demonstrates that Xorbits achieves up to 3.22 √ó  speedup on 3 machine learning pipelines and 22 data analysis workloads compared to state-of-the-art solutions. Xorbits is available on PyPI with nearly 1k daily downloads and has been successfully deployed in production environments.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Decentralized%20Actor%20Scheduling%20and%20Reference-based%20Storage%20in%20Xorbits%3A%20a%20Native%20Scalable%20Data%20Science%20Engine",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2955-lu.pdf",
    "session": "Research 59: Distributed and Streaming Data Processing",
    "authors": [
      {
        "Name": "Weizheng Lu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Chao Hui",
        "Affiliation": "Shandong University"
      },
      {
        "Name": "Yunhai Wang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Feng Zhang",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yueguo Chen",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Bao Liu",
        "Affiliation": "Xorbits Inc."
      },
      {
        "Name": "Chengjie Li",
        "Affiliation": "Xorbits Inc."
      },
      {
        "Name": "Zhaoxin Wu",
        "Affiliation": "Xorbits Inc."
      },
      {
        "Name": "Xuye Qin",
        "Affiliation": "Xorbits Inc."
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eaa07fb5-1759-41b5-ba3d-66cd55d0184e",
    "title": "NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance",
    "abstract": "There is an increasing demand for extending existing DBMSs with vector indices to become unified systems that can support modern predictive applications, which require joint querying of vector embeddings and structured properties and connections of objects. We present NaviX, a  Na tive  v ector  i nde X  for graph DBMSs (GDBMSs) that has two main design goals. First, we aim to implement a diskbased vector index that leverages the core storage and query processing capabilities of the underlying GDBMS. To this end, NaviX is a  hierarchical navigable small world  (HNSW) index, which is itself a graph-based structure. Second, we aim to evaluate  predicateagnostic  filtered vector search queries, where the k nearest neighbors (kNNs) of a query vector  ùë£ ùëÑ are searched across an arbitrary subset  ùëÜ of vectors that is specified by an ad-hoc selection sub-query ùëÑ ùëÜ . We adopt a prefiltering-based approach that evaluates  ùëÑ ùëÜ first and passes the full information about  ùëÜ to the kNN search operator. We study how to design a prefiltering-based search algorithm that is robust under different selectivities as well as correlations of  ùëÜ with  ùë£ ùëÑ . We propose an adaptive algorithm that utilizes local selectivity of each vector in the HNSW graph to pick a suitable heuristic at each iteration of the kNN search algorithm. We demonstrate NaviX‚Äôs robustness and efficiency through extensive experiments against both existing prefiltering- and postfiltering-based baselines.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/NaviX%3A%20A%20Native%20Vector%20Index%20Design%20for%20Graph%20DBMSs%20With%20Robust%20Predicate-Agnostic%20Search%20Performance",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4438-sehgal.pdf",
    "session": "Research 31: Vector Data Management II",
    "authors": [
      {
        "Name": "Gaurav Sehgal",
        "Affiliation": "University Of Waterloo"
      },
      {
        "Name": "Semih Salihoglu",
        "Affiliation": "University Of Waterloo"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9815408e-baed-46ff-af7b-bcd9a1e7549d",
    "title": "mLoRA: Fine-Tuning LoRA Adapters via Highly-Efficient Pipeline Parallelism in Multiple GPUs",
    "abstract": "Transformer-based large language models (LLMs) have demonstrated outstanding performance across diverse domains, particularly in the emerging  pretrain-then-finetune  paradigm. LoRA, a parameter-efficient fine-tuning method, is commonly used to adapt a base LLM to multiple downstream tasks. Further, LLM platforms enable developers to fine-tune multiple models and develop various domain-specific applications simultaneously. However, existing model parallelism schemes suffer from high communication overhead and inefficient GPU utilization. \nIn this paper, we present mLoRA, a parallelism-efficient finetuning system designed for training multiple LoRA across GPUs and machines. mLoRA introduces a novel LoRA-aware pipeline parallelism scheme that efficiently pipelines LoRA adapters and their distinct fine-tuning stages across GPUs and machines, along with a new LoRA-efficient operator to enhance GPU utilization. Our extensive evaluation shows that mLoRA can significantly reduce average fine-tuning task completion time, e.g., by 30%, compared to state-of-the-art methods like FSDP. More importantly, mLoRA enables simultaneous fine-tuning of larger models, e.g., two Llama2-13B models on four NVIDIA RTX A6000 48GB GPUs, which is not feasible for FSDP due to high memory requirements. Hence, mLoRA not only increases fine-tuning efficiency but also makes it more accessible on cost-effective GPUs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/mLoRA%3A%20Fine-Tuning%20LoRA%20Adapters%20via%20Highly-Efficient%20Pipeline%20Parallelism%20in%20Multiple%20GPUs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1948-tang.pdf",
    "session": "Research 23: Applied ML and AI for Data Management III",
    "authors": [
      {
        "Name": "Zhengmao Ye",
        "Affiliation": "Sichuan University"
      },
      {
        "Name": "Dengchun Li",
        "Affiliation": "Sichuan University"
      },
      {
        "Name": "Zetao Hu",
        "Affiliation": "SiChuan University"
      },
      {
        "Name": "Tingfeng Lan",
        "Affiliation": "University of Virginia"
      },
      {
        "Name": "Jian Sha",
        "Affiliation": "AntGroup"
      },
      {
        "Name": "Shicong Zhang",
        "Affiliation": "Hunan university"
      },
      {
        "Name": "Lei Duan",
        "Affiliation": "Sichuan University"
      },
      {
        "Name": "Jie Zuo",
        "Affiliation": "Sichuan University"
      },
      {
        "Name": "Hui Lu",
        "Affiliation": "The University of Texas at Arlington"
      },
      {
        "Name": "Yuanchun Zhou",
        "Affiliation": "Computer Network Information Center, Chinese Academy of Sciences"
      },
      {
        "Name": "Mingjie Tang",
        "Affiliation": "Sichuan University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7f3bbbda-4bb4-43bb-910f-4aed6afc9463",
    "title": "MTSClean: Efficient Constraint-based Cleaning for Multi-Dimensional Time Series Data",
    "abstract": "The widespread existence of time series data in information systems poses significant challenges to data cleaning due to its quality issues, particularly the complex interdependencies among attributes and the persistence of errors. Existing semantic constraints, such as conditional regression rules and speed constraints, though helpful, remain insufficient for this task. This paper introduces two novel online cleaning methods: MTSClean and MTSClean-soft, designed to improve cleaning efficiency and robustness. By combining row and column constraints, we significantly accelerate the cleaning process, reducing the time complexity of the exact solution MTSClean from ùëÇ(Ô∏Å(ùëÅùëÄ)3.5|Œ£|)Ô∏Å to ùëÇ(Ô∏ÅùëÅùëÄ3.5|Œ£|)Ô∏Å. Meanwhile, MTSClean-softachievesùëÇ(Ô∏ÅùëÅùëÄ2)Ô∏Åandmorepreciserepairsthrough optimized search for key cells and a novel repair cost function. Comparative experiments against nine benchmark methods highlight our approach‚Äôs superiority in multiple metrics, completing cleaning tasks faster and performing better than state-of-the-art methods. This demonstrates the practicality and advantage of the proposed methods in cleaning multidimensional time series data.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/MTSClean%3A%20Efficient%20Constraint-based%20Cleaning%20for%20Multi-Dimensional%20Time%20Series%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4840-wang.pdf",
    "session": "Research 19: Time Series Data II",
    "authors": [
      {
        "Name": "Xiaoou Ding",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Song YiChen",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Hongzhi Wang",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Chen Wang",
        "Affiliation": "\" Tsinghua University, China\""
      },
      {
        "Name": "Donghua Yang",
        "Affiliation": "Harbin Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f3ef658d-2937-429d-9b6e-453dd54e5e50",
    "title": "IcedTea: Efficient and Responsive Time-Travel Debugging in Dataflow Systems",
    "abstract": "DataÔ¨Çow systems have an increasing need to support a wide range of tasks in data-centric applications using latest techniques such as machine learning. These tasks often involve custom functions with complex internal states. Consequently, users need enhanced debugging support to understand runtime behaviors and investigate internal states of dataÔ¨Çows. Traditional forward debuggers allow users to follow the chronological order of operations in an execution. Therefore, a user cannot easily identify a past runtime behavior  after an unexpected result is produced. In this paper, we present a novel time-travel debugging paradigm called  IcedTea , which supports reverse debugging. In particular, in a dataÔ¨Çow‚Äôs execution, which is inherently distributed across multiple operators, the user can periodically interact with the job and retrieve the global states of the operators. After the execution, the system allows the user to roll back the dataÔ¨Çow state to any past interactions. The user can use step instructions to repeat the past execution to understand how data was processed in the original execution. We give a full speciÔ¨Åcation of this powerful paradigm, study how to reduce its runtime overhead and develop techniques to support debugging instructions responsively. Our experiments on real-world datasets and workÔ¨Çows show that  IcedTea  can support responsive time-and workÔ¨Çows show that  IcedTea  can support responsive timetravel debugging with low time and space overhead.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/IcedTea%3A%20Efficient%20and%20Responsive%20Time-Travel%20Debugging%20in%20Dataflow%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p902-ni.pdf",
    "session": "Research 47: Provenance and Workflows",
    "authors": [
      {
        "Name": "Shengquan Ni",
        "Affiliation": "U C Irvine"
      },
      {
        "Name": "Yicong Huang",
        "Affiliation": "UC Irvine"
      },
      {
        "Name": "Zuozhi Wang",
        "Affiliation": "U C Irvine"
      },
      {
        "Name": "Chen Li",
        "Affiliation": "UC Irvine"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "941554d1-ee35-461e-a065-69c55a811580",
    "title": "mlidea: Interactively Improving ML Data Preparation Code via \"Shadow Pipelines\"",
    "abstract": "",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/mlidea%3A%20Interactively%20Improving%20ML%20Data%20Preparation%20Code%20via%20%22Shadow%20Pipelines%22",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5359-grafberger.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Stefan Grafberger",
        "Affiliation": "BIFOLD & TU Berlin"
      },
      {
        "Name": "Paul Groth",
        "Affiliation": "University of Amsterdam"
      },
      {
        "Name": "Sebastian Schelter",
        "Affiliation": "BIFOLD & TU Berlin"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Empty or missing abstract"
    ]
  },
  {
    "id": "336d05a3-2a13-4c6d-a812-175e275afb8e",
    "title": "TCO2: Analyzing the Carbon Footprint of Database Server Replacements",
    "abstract": "Data centers produce a significant and increasing amount of CO 2 emissions. In the past, these have been predominantly due to energy generation for powering data centers. With the transition to energy sources with lower carbon production, the embodied carbon (i.e., CO 2  and other greenhouse gas emissions during production, transport, and end-of-life) plays an increasing role when planning server lifecycles. While replacing an old server with newer hardware will typically reduce the power consumption of individual tasks, due to better efficiency of modern CPUs, offsetting the embodied carbon of new hardware can take months to tens of years, depending on the grid carbon intensity. In this demo, we invite attendees to interactively analyze the ecological lifecycles of modern database servers for different workloads and grid carbon intensities. Attendees can compare servers with different CPU architectures and estimate ecological deployment cycles for database servers.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TCO2%3A%20Analyzing%20the%20Carbon%20Footprint%20of%20Database%20Server%20Replacements",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5223-rabl.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Marc Baeuerle",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Thomas Bodner",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Martin Boissier",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Tilmann Rabl",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Ricardo Salazar Diaz",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Florian Schmeller",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Nils Stra√üenburg",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Ilin Tolovski",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Marcel Weisgut",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      },
      {
        "Name": "Wang Yue",
        "Affiliation": "Hasso Plattner Institute, University of Potsdam"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bccda7bf-0534-442d-941b-a2b8340cb5de",
    "title": "Mining Meaningful Keys and Foreign Keys with High Precision and Recall",
    "abstract": "We demonstrate a next-generation Entity/Relationship (E/R) ProÔ¨Åler that mines meaningful key/foreign key relationships from a given data repository. Core novelties include a strict hierarchy of key variants ranging from candidate keys to SQL unique constraints that represent diÔ¨Äerent ways to identify incomplete entities, a measure of orthogonality that separates accidental from meaningful keys, and algorithms for mining approximate keys for all these variants under diÔ¨Äerent thresholds of arity, completeness, dirtiness, and orthogonality. We showcase the high precision and recall achieved by our tool and how it facilitates the users‚Äô understanding which entity and referential integrity constraints govern their data.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Mining%20Meaningful%20Keys%20and%20Foreign%20Keys%20with%20High%20Precision%20and%20Recall",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5363-link.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Henning Koehler",
        "Affiliation": "Massey University"
      },
      {
        "Name": "Sebastian Link",
        "Affiliation": "University of Auckland"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "57ce585b-ea50-40c3-b63b-eee62c70a983",
    "title": "SSD-iq: Uncovering the Hidden Side of SSD Performance",
    "abstract": "SSDs are hardware and software systems whose design involves complex and undocumented trade-oÔ¨Äs between cost, energy consumption and performance. This complexity is hidden behind standard interfaces and a few headline speciÔ¨Åcations, such as capacity, sequential, and random performance. As a result, database system designers often assume that SSDs are interchangeable commodities and regularly use a single SSD model to evaluate database performance. Does it matter which SSD model is provisioned for a database system? If yes, how to choose the right one? These are the questions we address in this paper. We study the performance characteristics of commercial data center SSDs, highlighting the limitations of current standard metrics in capturing their true behavior. We conduct experiments on nine SSDs from major vendors, revealing signiÔ¨Åcant diÔ¨Äerences in performance despite similar headline speciÔ¨Åcations. We show that the choice of SSD matters for database system performance. We propose a new benchmark,  SSD-iq , which introduces four additional metrics to better characterize SSD performance, particularly for write-intensive workloads. Incidentally, our work should encourage vendors to optimize SSD controllers using more comprehensive and transparent performance criteria.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SSD-iq%3A%20Uncovering%20the%20Hidden%20Side%20of%20SSD%20Performance",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4295-haas.pdf",
    "session": "Research 54: Database Engines II",
    "authors": [
      {
        "Name": "Gabriel Haas",
        "Affiliation": "TUM"
      },
      {
        "Name": "Bohyun Lee",
        "Affiliation": "TUM"
      },
      {
        "Name": "Philippe Bonnet",
        "Affiliation": "University of Copenhagen"
      },
      {
        "Name": "Viktor Leis",
        "Affiliation": "TUM"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "63c40aaa-7c08-49f5-85c0-b71e92f9b9d3",
    "title": "GTI: Graph-based Tree Index with Logarithm Updates for Nearest Neighbor Search in High-Dimensional Spaces",
    "abstract": "Nearest neighbor search (NNS) is fundamental for high-dimensional space retrieval and impacts various fields, such as pattern recognition, information retrieval, recommendation systems, and vector database management. Among existing NNS methods, graph-based methods often excel in query accuracy and efficiency. However, these methods face significant challenges, including high construction costs and difficulties with dynamic data updates. Recent efforts have focused on combining graph methods with hashing, quantization, and tree-based approaches to address these issues, but problems with large index sizes and update performance remain unresolved. In response, this paper proposes  GTI , a novel, lightweight, and dynamic graph-based tree index for high-dimensional NNS.  GTI  constructs a tree index built across the entire dataset and employs a lightweight graph index at the level 1 of the tree to significantly reduce graph construction costs. It also features effective data insertion and deletion algorithms that enable logarithmic realtime updates. Additionally, we have developed an effective NNS algorithm for  GTI , which not only achieves approximate search performance on par with SOTA graph-based methods but also supports exact NNS. Extensive experiments on six real-world datasets demonstrate that  GTI  achieves an approximately 10 √ó  improvement in update efficiency compared to SOTA tree-based methods, while achieving search effectiveness comparable to SOTA approximate NNS methods. These results underscore the potential of  GTI  for effective application in dynamic and evolving scenarios.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GTI%3A%20Graph-based%20Tree%20Index%20with%20Logarithm%20Updates%20for%20Nearest%20Neighbor%20Search%20in%20High-Dimensional%20Spaces",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p986-gao.pdf",
    "session": "Research 24: Database Engines I",
    "authors": [
      {
        "Name": "Ruiyao Ma",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yifan Zhu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Baihua Zheng",
        "Affiliation": "Singapore Management University"
      },
      {
        "Name": "Lu Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Congcong Ge",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Yunjun Gao",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7378a792-1f8f-4f6c-bb70-d41de98be1c7",
    "title": "OpenForge: Probabilistic Metadata Integration",
    "abstract": "Modern data stores increasingly rely on metadata to enable diverse activities such as data cataloging and search. However, metadata curation remains a labor-intensive task, and the broader challenge of metadata maintenance√êensuring its consistency and usefulness√êhas been largely overlooked. In this work, we tackle the problem of resolving relationships among metadata concepts from disparate sources. Inferring these relationships are critical for creating clean and consistent metadata repositories, and a central challenge for metadata integration. \nWe propose  OpenForge , a two-stage prior-posterior framework for metadata integration. In the first stage,  OpenForge  exploits multiple methods including fine-tuned large language models to obtain prior beliefs about concept relationships. In the second stage, OpenForge  refines these predictions using the Markov Random Field, a probabilistic graphical model. We formalize metadata integration as an optimization problem, where the objective is to identify the relationship assignments that maximize the joint probability of assignments. The MRF formulation allows  OpenForge  to capture prior beliefs while encoding critical relationship properties, such as transitivity, in probabilistic inference. Experiments on four datasets show the effectiveness and efficiency of  OpenForge . In a use case of matching two metadata vocabularies,  OpenForge outperforms GPT-4, the second-best method, by 25 F1 points.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/OpenForge%3A%20Probabilistic%20Metadata%20Integration",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2914-cong.pdf",
    "session": "Research 62: Information Integration and Data Quality IV",
    "authors": [
      {
        "Name": "Tianji Cong",
        "Affiliation": "University of Michigan"
      },
      {
        "Name": "Fatemeh Nargesian",
        "Affiliation": "University of Rochester"
      },
      {
        "Name": "Junjie Xing",
        "Affiliation": "University of Michigan"
      },
      {
        "Name": "H. V. Jagadish",
        "Affiliation": "University of Michigan"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d31836bc-c2eb-4b7a-b00d-425b01240ac3",
    "title": "Accio: Bolt-on Query Federation",
    "abstract": "Data scientists today often need to analyze data from various places. This makes it necessary for corresponding engines to support query federation (i.e., the ability to perform SQL queries over data hosted in different sources). Although many systems come with federation capabilities, their implementations are tightly coupled with the core engine design. This not only increases complexity and reduces portability across engines, but also often leads to performance issues by missing optimization opportunities. This paper proposes Accio, a new ‚Äúbolt-on‚Äù approach to query federation. Accio is a middleware library that decouples query federation from the target system. It enables two key optimizations‚Äîjoin pushdown and query partitioning‚Äîvia a declarative interface that can be easily leveraged by different engines. Our experience of adapting five popular data science query engines shows that Accio can outperform existing approaches by orders of magnitude in various scenarios without the need for any intrusive changes or extra maintenance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Accio%3A%20Bolt-on%20Query%20Federation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2126-wang.pdf",
    "session": "Research 62: Information Integration and Data Quality IV",
    "authors": [
      {
        "Name": "Xiaoying Wang",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Jiannan Wang",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Tianzheng Wang",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Yong Zhang",
        "Affiliation": "Huawei Technologies Canada Co., Ltd."
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "19a6b156-dc9a-4756-b950-fe3790b57cd2",
    "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
    "abstract": "Knowledge-intensive analytical applications retrieve context from both structured tabular data and unstructured free text documents for effective decision-making. Large language models (LLMs) have significantly simplified the prototyping of such retrieval and reasoning data pipelines. However, implementing them efficiently remains challenging and demands significant effort. Developers must often orchestrate heterogeneous systems, manage data movement, and handle low-level concerns such as LLM context management. \nTo address these challenges, we introduce FlockMTL: an extension for DBMSs that integrates LLM capabilities and enables retrieval-augmented generation (RAG) within SQL. FlockMTL provides LLM-powered scalar and aggregate functions, enabling chained predictions over tuples. It further provides data fusion functions to support hybrid search. Drawing inspiration from the relational model, FlockMTL incorporates: (i) seamless optimizations such as batching and meta-prompting; and (ii) resource independence through novel SQL DDL abstractions: PROMPT and MODEL, introduced as first-class schema objects alongside TABLE.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Beyond%20Quacking%3A%20Deep%20Integration%20of%20Language%20Models%20and%20RAG%20into%20DuckDB",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5415-mhedhbi.pdf",
    "session": "Demo A1:",
    "authors": [
      {
        "Name": "Anas Dorbani",
        "Affiliation": "Polytechnique Montr√©al"
      },
      {
        "Name": "Sunny Yasser",
        "Affiliation": "Polytechnique Montr√©al"
      },
      {
        "Name": "Jimmy Lin",
        "Affiliation": "University of Waterloo"
      },
      {
        "Name": "Amine Mhedhbi",
        "Affiliation": "Polytechnique Montr√©al"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "092517ab-d15c-4647-938d-48a384c42ea1",
    "title": "GaussDB-Vector: A Large-Scale Persistent Real-Time Vector Database for LLM Applications",
    "abstract": "Vector databases are widely used as a fundamental tool for addressing the weaknesses of large language model (LLM) applications, speciÔ¨Åcally hallucinations and the high cost of inference. However, existing vector databases either cater to niche applications with lowlatency in-memory search, or oÔ¨Äer sophisticated data management capabilities but at the cost of low performance. \nTo address these limitations, we propose  GaussDB-Vector , a high-performance, real-time persistent vector database that excels in low-latency scalable search, real-time inserts and deletes, high availability, large-scale distributed search, and hybrid scalar-vector Ô¨Åltered search capabilities. These features are primarily achieved through an innovative storage architecture designed for a graphbased vector index, optimized for I/O operations and adaptable across various dataset sizes and dimensions, complemented by novel buÔ¨Äering strategies to further reduce I/O burdens.  GaussDBVector  supports product quantization, parallel search, and hardware acceleration via SIMD, GPUs, and NPUs in order to further accelerate queries. Experimental results show that  GaussDB-Vector outperforms competitive baselines by a factor of 1 to 5 times.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GaussDB-Vector%3A%20A%20Large-Scale%20Persistent%20Real-Time%20Vector%20Database%20for%20LLM%20Applications",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4951-sun.pdf",
    "session": "Industry 3: Document, Graph, and Vector Databases",
    "authors": [
      {
        "Name": "Ji Sun",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "James Pan",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Jiang Wang",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Yongqing Xie",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Ruicheng Liu",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Wen Nie",
        "Affiliation": "Huawei"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ea952dbd-9c9f-4f4c-9c46-582f4731cd4a",
    "title": "BigVectorBench: Heterogeneous Data Embedding and Compound Queries are Essential in Evaluating Vector Databases",
    "abstract": "Vector databases are designed to eÔøøectively store, organize, and re- trieve high-dimensional vectors, enabling faster and more accurate querying and analysis. This study highlights that the performance of cutting-edge vector databases hinges on their proÔøøciency in managing heterogeneous data embedding and handling compound queries. The former task revolves around converting varied data types into a cohesive vector format, while the latter involves pro- cessing multimodal or single-modal queries with precise constraints. The paper advocates for evaluating these dual tasks within an in- tegrated benchmark framework. However, state-of-the-art vector database benchmarks overlook heterogeneous data embedding and compound queries, creating a gap in evaluating vector database performance.\nTo address this gap, we introduce BigVectorBench, a benchmark suite designed to evaluate vector database performance. BigVec- torBench contributes by deÔøøning and evaluating the embedding performance of heterogeneous data. Additionally, it abstracts com- pound queries, which are increasingly used in real-world appli- cations, replacing unimodal vector searches. Our rigorous evalu- ations validate the two design decisions of BigVectorBench and identify performance bottlenecks of mainstream vector databases. Its source code and user manual are available from https://github. com/BenchCouncil/BigVectorBench.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/BigVectorBench%3A%20Heterogeneous%20Data%20Embedding%20and%20Compound%20Queries%20are%20Essential%20in%20Evaluating%20Vector%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1536-zhan.pdf",
    "session": "Research 40: Time Series and Vector Data",
    "authors": [
      {
        "Name": "Guoxin Kang",
        "Affiliation": "Institute of Computing Technology, Chinese Academy of Sciences"
      },
      {
        "Name": "Zhongxin Ge",
        "Affiliation": "Institute of Computing Technology, Chinese Academy of Sciences"
      },
      {
        "Name": "Jingpei Hu",
        "Affiliation": "Institute of Computing Technology, Chinese Academy of Sciences"
      },
      {
        "Name": "Xueya Zhang",
        "Affiliation": "University of Chinese Academy of Sciences"
      },
      {
        "Name": "Lei Wang",
        "Affiliation": "Institute of Computing Technology, Chinese Academy of Sciences"
      },
      {
        "Name": "Jianfeng Zhan",
        "Affiliation": "Institute of Computing Technology, Chinese Academy of Sciences"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a6cb4759-0189-4100-93db-339433802f8b",
    "title": "Efficient and Adaptive Estimation of Local Triadic Coefficients",
    "abstract": "Characterizing graph properties is fundamental to the analysis and to our understanding of real-world networked systems. The  local clustering coefficient , and the more-recent,  local closure coefficient , capture powerful properties that are essential in a large number of applications, ranging from graph embeddings to graph partitioning. Such coefficients capture the local density of the neighborhood of each node, considering incident triangle structures and paths of size 2. For this reason, we refer to these coefficients collectively as local triadic coefficients . \nIn this work, we consider the novel and fundamental problem of efficiently computing the  average  of local triadic coefficients, over a given  partition  of the nodes of the input graph into a set of disjoint  buckets . The  average local triadic coefficients  of the nodes in each bucket provide a better insight into the interplay of graph structure and the properties of the nodes associated to each bucket. Unfortunately, exact computation, which requires listing all triangles in a graph, is infeasible for large networks. Hence, we focus on obtaining  highly-accurate probabilistic estimates . \nWe develop T riad , an adaptive algorithm based on sampling, which can be used to estimate the average local triadic coefficients for a partition of the nodes into buckets. T riad  is based on a new class of unbiased estimators, and non-trivial bounds on its sample complexity, enabling the efficient computation of highly accurate estimates. Finally, we show how T riad  can be efficiently used in practice on large networks, and we present a case study showing that average local triadic coefficients can capture highorder patterns over collaboration networks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20and%20Adaptive%20Estimation%20of%20Local%20Triadic%20Coefficients",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2561-sarpe.pdf",
    "session": "Research 56: Analytics over Different Data Types III",
    "authors": [
      {
        "Name": "Ilie Sarpe",
        "Affiliation": "KTH Royal Institute of Technology"
      },
      {
        "Name": "Aristides Gionis",
        "Affiliation": "KTH Royal Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2428b7c3-da2b-4616-bd8f-653dec172718",
    "title": "TUCKET: A Tensor Time Series Data Structure for Efficient and Accurate Factor Analysis over Time Ranges",
    "abstract": "Given an evolving tensor time series and multiple time ranges, how can we compute Tucker decomposition for each time range efficiently and accurately? Tucker decomposition has been widely used in a variety of applications to obtain latent factors of tensor data. For example, Tucker decomposition on air pollution data allows us to analyze and compare air pollution patterns between different locations during different periods of time. In these applications, a common need is to compute Tucker decomposition for a given time range. Furthermore, real-world tensor time series are typically evolving in the time dimension. Such needs call for a data structure that can efficiently and accurately support range queries of Tucker decomposition and stream updates. Unfortunately, existing methods do not support either range queries or stream updates. For methods that do not support range queries, they have to re-compute from scratch for each query. Not until 2021 has a data structure called Zoom-Tucker been proposed to support range queries via block-wise preprocessing. However, Zoom-Tucker does not support stream updates and, more critically, suffers from a reluctant efficiency‚Äìaccuracy tradeoff ‚Äî a large block size causes inaccuracy, while a small block size leads to inefficiency. This challenging problem has remained open for years prior to our work. To solve this challenging problem, we propose TUCKET, a data structure that can efficiently and accurately handle both range queries and stream updates. Our key idea is to design a new data structure that we call a stream segment tree by generalizing the segment tree, a data structure that was originally invented for computational geometry. For a range query of length ùêø, our TUCKET can find ùëÇ (log ùêø) nodes (called the hit set) from the tree and efficiently stitch their preprocessed decompositions to answer the range query. We also propose an algorithm to optimally prune the hit set via an approximation of subtensor decomposition. For the ùëá -th stream update, our TUCKET modifiesonlyamortizedùëÇ(1)nodesandonlyùëÇ(logùëá)nodesinthe worst case. Extensive evaluation demonstrates that our TUCKET consistently achieves the highest efficiency and accuracy across four large-scale datasets. Our TUCKET achieves at least 3 times lower latency and at least 1.4 times smaller reconstruction error than Zoom-Tucker on all datasets. The full version can be found at https://github.com/q-rz/TUCKET/blob/main/TUCKET-Full.pdf.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/TUCKET%3A%20A%20Tensor%20Time%20Series%20Data%20Structure%20for%20Efficient%20and%20Accurate%20Factor%20Analysis%20over%20Time%20Ranges",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4746-qiu.pdf",
    "session": "Research 19: Time Series Data II",
    "authors": [
      {
        "Name": "Ruizhong Qiu",
        "Affiliation": "University of Illinois Urbana-Champaign"
      },
      {
        "Name": "Jun-Gi Jang",
        "Affiliation": "University of Illinois Urbana-Champaign"
      },
      {
        "Name": "Xiao Lin",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "Name": "Lihui Liu",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "Name": "Hanghang Tong",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "3aff4449-b186-4ba1-917a-365675529421",
    "title": "CEDAR: A System for Cost-Efficient Data-Driven Claim Verification",
    "abstract": "We present CEDAR, a system for cost-efficient, data-driven claim verification. CEDAR takes as input a collection of text documents, containing claims that can be verified from relational data. The system uses large language models (LLMs) to map claims to SQL queries that can be used for claim verification. While LLMs like GPT4 are nowadays able to map claims to queries with high accuracy, using them is expensive. This is why CEDAR implements multiple verification approaches, ranging from zero-shot LLM invocations to iterative, agent-based approaches, that realize different tradeoffs between accuracy and costs. The system may apply multiple methods to the same claim, starting with cheaper methods and resorting to more expensive versions in case of failures. CEDAR uses cost-based optimization to derive an optimal order of verification methods and an optimal number of re-tries (with randomization) for each method, enabling users to trade costs for accuracy via tuning parameters. The experiments on real data, including newspaper and Wikipedia articles, show that CEDAR achieves significantly higher accuracy than prior methods for data-driven fact-checking.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/CEDAR%3A%20A%20System%20for%20Cost-Efficient%20Data-Driven%20Claim%20Verification",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4492-jayasekara.pdf",
    "session": "Research 7: Natural Language Interfaces to Data",
    "authors": [
      {
        "Name": "Tharushi Jayasekara",
        "Affiliation": "Cornell University"
      },
      {
        "Name": "Immanuel Trummer",
        "Affiliation": "Cornell University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8962dde8-16f6-412e-8bf4-933fe381170f",
    "title": "Demonstration of ModelarDB: Model-Based Management of High-Frequency Time Series Across Edge, Cloud, and Client",
    "abstract": "Renewable Energy Sources ( RES s) are monitored by many highquality sensors that produce vast amounts of high-frequency time series data. This can be used to increase the renewable energy production and longevity of the  RES s, e.g., yaw misalignment detection and predictive maintenance for wind turbines. It is currently not possible for wind turbine manufacturers and owners to use this data due to limits on bandwidth and storage that are infeasible to increase. Thus, they store simple aggregates which remove valuable outliers and fluctuations. As a remedy, we demonstrate the new model-based Time Series Management System ( TSMS ) ModelarDB. The participants can experience how ModelarDB ingests time series on the edge and compresses them as  segments  with metadata and so-called  models . The models represent values within a user-defined absolute or relative error bound (even 0 or 0%). Participants can adjust many parameters and see how the segments are transferred to the cloud using much less bandwidth and storage than other popular solutions like Apache Parquet and Apache TsFile, e.g., up to 90%‚Äì99% less than Apache Parquet. Participants can analyze the time series on the edge, in the cloud, and on the client using SQL or Python. On the client, ModelarDB runs in-process to integrate with, e.g., Python. Thus, participants can see how ModelarDB efficiently manages high-frequency time series across edge, cloud, and client.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Demonstration%20of%20ModelarDB%3A%20Model-Based%20Management%20of%20High-Frequency%20Time%20Series%20Across%20Edge%2C%20Cloud%2C%20and%20Client",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5247-jensen.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "S√∏ren Kejser Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Christian Schmidt Godiksen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Christian Thomsen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Torben Bach Pedersen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c12ce8ff-70c8-4e2e-b054-e3460ed8cb04",
    "title": "Advancing Fact Attribution for Query Answering: Aggregate Queries and Novel Algorithms",
    "abstract": "In this paper, we introduce a novel approach to computing the contribution of input tuples to the result of the query, quantified by the Banzhaf and Shapley values. In contrast to prior algorithmic work that focuses on Select-Project-Join-Union queries, ours is the first practical approach for queries with aggregates. It relies on two novel optimizations that are essential for its practicality and significantly improve the runtime performance already for queries without aggregates. The first optimization exploits the observation that many input tuples have the same contribution to the query result, so it is enough to compute the contribution of one of them. The second optimization uses the gradient of the query lineage to compute the contributions of all tuples with the same complexity as for one of them. Experiments with a million instances over 3 databases show that our approach achieves up to 3 orders of magnitude runtime improvements over the state-of-the-art for queries without aggregates, and that it is practical for aggregate queries.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Advancing%20Fact%20Attribution%20for%20Query%20Answering%3A%20Aggregate%20Queries%20and%20Novel%20Algorithms",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3996-abramovich.pdf",
    "session": "Research 47: Provenance and Workflows",
    "authors": [
      {
        "Name": "Omer Abramovich",
        "Affiliation": "Tel Aviv University"
      },
      {
        "Name": "Daniel Deutch",
        "Affiliation": "Tel Aviv University"
      },
      {
        "Name": "Nave Frost",
        "Affiliation": "eBay Research"
      },
      {
        "Name": "Ahmet Kara",
        "Affiliation": "OTH Regensburg"
      },
      {
        "Name": "Dan Olteanu",
        "Affiliation": "University of Zurich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7d072f70-151f-474e-8f20-441a917032cc",
    "title": "Scalable Model-Based Management of Massive High Frequency Wind Turbine Data with ModelarDB",
    "abstract": "Modern wind turbines are monitored by sensors that generate massive amounts of high frequency time series that are ingested on the edge and then transferred to the cloud where they are stored and analyzed. This results in at least four challenges: (1) Limited hardware makes efficient ingestion necessary to keep up; (2) Limited bandwidth makes data compression necessary; (3) High storage costs as all data must be stored; and (4) Low data quality due to lossy compression methods without error bounds. Practitioners currently use solutions that only solve some of these. In this paper, we evaluate the Time Series Management System ModelarDB, a solution that meets all four challenges by efficiently managing time series across the entire pipeline. We compare it to three commonly used alternatives and evaluate different aspects of them in a realistic edge-to-cloud scenario with real-life datasets. For lossless compression, ModelarDB achieves up to 2x better compression and 1.2x better transfer efficiency. For lossy compression, ModelarDB achieves up to 4.6x better compression and 10x better transfer efficiency, or similar compression with orders of magnitude less error.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Scalable%20Model-Based%20Management%20of%20Massive%20High%20Frequency%20Wind%20Turbine%20Data%20with%20ModelarDB",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4723-abduvakhobov.pdf",
    "session": "Research 55: Spatial and Temporal Databases",
    "authors": [
      {
        "Name": "Abduvoris Abduvakhobov",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "S√∏ren Kejser Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Torben Bach Pedersen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Christian Thomsen",
        "Affiliation": "Aalborg University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "99c1cf51-33a6-4e09-a51b-1196bb1f1e0d",
    "title": "DBPecker: A Graph-Based Compound Anomaly Diagnosis System for Distributed RDBMSs",
    "abstract": "This demonstration introduces DBPecker, an integrated diagnostic platform tailored for distributed relational database systems. DBPecker leverages a graph-based anomaly modeling approach to capture inter-node dependencies and effectively localize compound anomalies, while a causality-aware metric prioritization module automatically isolates critical performance indicators. By unifying anomaly detection with a comprehensive root cause analysis pipeline, the system facilitates rapid and precise diagnosis in distributed database environments. Evaluated on a multinode OceanBase cluster, DBPecker not only accelerates the identification of underlying anomalies but also substantially improves operational reliability, offering practical insights and actionable recommendations for real-world distributed database management.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DBPecker%3A%20A%20Graph-Based%20Compound%20Anomaly%20Diagnosis%20System%20for%20Distributed%20RDBMSs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5383-wu.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Qingliu Wu",
        "Affiliation": "Beijing University of Posts and Telecommunications"
      },
      {
        "Name": "Qingfeng Xiang",
        "Affiliation": "Beijing University of Posts and Telecommunications"
      },
      {
        "Name": "Yingxia Shao",
        "Affiliation": "Beijing University of Posts and Telecommunications"
      },
      {
        "Name": "Qiyao Luo",
        "Affiliation": "Indenpendent Researcher"
      },
      {
        "Name": "Quanqing Xu",
        "Affiliation": "Independent Researcher"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d9e8087f-f404-40c5-976c-9ece50f634a3",
    "title": "LEADRE: Multi-Faceted Knowledge Enhanced LLM Empowered Display Advertisement Recommender System",
    "abstract": "Display advertising plays a crucial role in benefiting advertisers, publishers, and users. Traditional display advertising systems employ a multi-stage architecture comprising retrieval, coarse ranking, ranking, and re-ranking. However, conventional retrieval methods primarily rely on ID-based learning-to-rank mechanisms, often underutilizing the content information of ads, like ads‚Äô title, and description. This limitation reduces the ability to generate diverse and relevant recommendation lists. \nTo address this challenge, we propose leveraging the extensive world knowledge of large language models (LLMs). However, effectively integrating LLMs into advertising systems presents three key challenges:  (i) How to accurately capture user interests ,  (ii) How to bridge the knowledge gap between LLMs and advertising systems , and (iii) How to efficiently deploy LLMs at scale . To overcome these challenges, we introduce  LEADRE ‚Äîthe  L LM  E mpowered Display  AD vertisement  RE commender system. LEADRE consists of three core components. The  Intent-Aware Prompt Engineering module introduces multi-faceted knowledge and constructs intentaware  <Prompt, Response>  pairs, fine-tuning LLMs to generate ads tailored to users‚Äô personal interests. The  Advertising-Specific Knowledge Alignment  module incorporates auxiliary fine-tuning tasks and Direct Preference Optimization (DPO) to align LLMs with advertising semantics and business objectives. The  Latency-Aware Model Deployment  module integrates a hybrid service framework that balances latency-tolerant and latency-sensitive service, ensuring seamless online deployment. \nExtensive offline experiments validate the effectiveness of LEADRE, demonstrating significant improvements across multiple evaluation metrics. Furthermore, online A/B tests reveal a  1.57%  and  1.17% ‚àó Work was done while Fengxin Li was intern at Tencent. ‚Ä† Corresponding authors. \nincrease in Gross Merchandise Value (GMV) for serviced users on WeChat Channels and Moments, respectively. LEADRE has been successfully deployed on both platforms, handling tens of billions of requests daily.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LEADRE%3A%20Multi-Faceted%20Knowledge%20Enhanced%20LLM%20Empowered%20Display%20Advertisement%20Recommender%20System",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4763-he.pdf",
    "session": "Industry 4: Machine Learning, AI, and Databases",
    "authors": [
      {
        "Name": "Fengxin Li",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yi Li",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Yue Liu",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Chao Zhou",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Yuan Wang",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Xiaoxiang Deng",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Wei Xue",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Dapeng Liu",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Lei Xiao",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Haijie Gu",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Jie Jiang",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Hongyan Liu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Biao Qin",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Jun He",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6c20ce4c-94ec-438d-94c9-8069387e45dd",
    "title": "LETIndex: A Secure Learned Index with TEE",
    "abstract": "Trusted execution environment (TEE) oÔ¨Äers a promising approach to building encrypted databases, which keep data conÔ¨Ådential for users. However, designing an eÔ¨Écient index for TEE databases remains a signiÔ¨Åcant challenge. Due to the limited enclave memory and system call support in enclaves, traditional indexes incur massive context switches (including enclave entry and exiting), which cause performance regression. Existing approaches, such as introducing rich execution environment (REE) buÔ¨Äer pools or index parameter optimization, may not alleviate these problems eÔ¨Äectively. To address these limitations, we propose  LETIndex , an eÔ¨Écient learned dynamic index designed for TEE databases.  LETIndex adopts LSM-structured Piecewise Geometric Model (PGM) indexes and an adaptive prefetch mechanism to support lookup, range queries, and updates with signiÔ¨Åcantly reduced context switches and disk I/O overhead. Experimental results show that  LETIndex achieves superior performance compared to existing approaches on the SOSD benchmark. We demonstrate  LETIndex  with two real scenarios, binary join and multi-tale join.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LETIndex%3A%20A%20Secure%20Learned%20Index%20with%20TEE",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5403-li.pdf",
    "session": "Demo C2:",
    "authors": [
      {
        "Name": "Shuting Cao",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Zeping Niu",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5f0aca76-fe43-4356-bd46-354fcf74bf78",
    "title": "LogLite: Lightweight Plug-and-Play Streaming Log Compression",
    "abstract": "Log data is a vital resource for capturing system events and states. With the increasing complexity and widespread adoption of modern software systems and IoT devices, the daily volume of log generation has surged to tens of petabytes, leading to significant collection and storage costs. To address this challenge, lossless log compression has emerged as an effective solution, enabling substantial resource savings without compromising log information. In this paper, we first conduct a characterization study on extensive public log datasets and identify four key observations. Building on these insights, we propose LogLite, a lightweight, plug-and-play, streaming lossless compression algorithm designed to handle both TEXT and JSON logs throughout their life cycle. LogLite requires no predefined rules or pre-training and is inherently adaptable to evolving log structures. Our evaluation shows that, compared to state-of-the-art baselines, LogLite achieves Pareto optimality in most scenarios, delivering an average improvement of up to 67.8% in compression ratio and up to  2 . 7 √ó  in compression speed.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/LogLite%3A%20Lightweight%20Plug-and-Play%20Streaming%20Log%20Compression",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3757-yang.pdf",
    "session": "Research 11: Graph Data Privacy, Text and Semi-Structured Data Management",
    "authors": [
      {
        "Name": "benzhao tang",
        "Affiliation": "Guangzhou University"
      },
      {
        "Name": "shiyu yang",
        "Affiliation": "Guangzhou University"
      },
      {
        "Name": "zhitao shen",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "wenjie zhang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "zhihong tian",
        "Affiliation": "Guangzhou University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2e09f3d0-f011-45ec-8e98-040d4d45d89a",
    "title": "OmniMatch: Joinability Discovery in Data Products",
    "abstract": "We propose  OmniMatch , a novel joinability discovery technique, specifically tailored for the needs of  data products : cohesive curated collections of tabular datasets.  OmniMatch  combines multiple column-pair similarity measures leveraging self-supervised Graph Neural Networks (GNNs).  OmniMatch ‚Äôs GNN captures column relatedness by leveraging graph neighborhood information, significantly improving the recall of joinability discovery tasks. At the same time,  OmniMatch  increases its precision by augmenting its training data with negative column join examples through an automated negative example generation process. Compared to the state-of-the-art,  OmniMatch  exhibits up to 14% higher effectiveness in F1 score and AUC without relying on individual, user-provided thresholds for each similarity metric.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/OmniMatch%3A%20Joinability%20Discovery%20in%20Data%20Products",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4588-koutras.pdf",
    "session": "Research 30: Information Integration and Data Quality I",
    "authors": [
      {
        "Name": "Christos Koutras",
        "Affiliation": "New York University"
      },
      {
        "Name": "Jiani Zhang",
        "Affiliation": "Amazon"
      },
      {
        "Name": "Xiao Qin",
        "Affiliation": "Amazon"
      },
      {
        "Name": "Chuan Lei",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Vassilis Ioannidis",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Christos Faloutsos",
        "Affiliation": "Amazon"
      },
      {
        "Name": "George Karypis",
        "Affiliation": "Amazon"
      },
      {
        "Name": "Asterios Katsifodimos",
        "Affiliation": "TU Delft"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ec5bf7c0-fa2c-440c-8ce4-d37fc04be50d",
    "title": "HAWK: A Workload-driven Hierarchical Deadlock Detection Approach in Distributed Database System",
    "abstract": "Distributed databases are widely used in various fields, such as financial services and e-commerce. These businesses generally exhibit characteristics of large-scale and rapid growth. However, these business systems often suffer from deadlocks that prevent them from operating normally for extended periods. Traditional deadlock detection methods face challenges in scalability and efficiency, especially as the number of nodes increases. Therefore, deadlock detection has always been a research area in distributed databases. \nIn this paper, we introduce an efficient deadlock detection algorithm called HAWK, leveraging a   H ierarchical   A pproach based on   W or K load modeling. Our algorithm addresses these issues by constructing a dynamic hierarchical detection tree that adapts to transaction patterns, significantly reducing time complexity and communication overhead. HAWK first models the workload and generates a  predicted access graph  (PAG), transforming the problem of partitioning detection task in the basic hierarchical detection into partition detection zone (DZ) in the PAG by a graph-cutting algorithm. Then, leveraging the properties of strongly connected components (SCCs) and deadlock cycles, the SCC-cut algorithm naturally partitions the system-wide deadlock detection into multiple non-intersecting detection zones, thereby enhancing detection efficiency. We used the greedy SCC-cut algorithm to perform a more fine-grained partitioning of the complex PAG. Finally, by periodically sampling and updating the hierarchical structure, the algorithm remains responsive to dynamic workload variations, ensuring efficient detection. Our approach outperforms both centralized and distributed methods, offering a more efficient and adaptive solution. Extensive experimental results demonstrate the effectiveness of the HAWK algorithm, showing significant reductions in the duration of the deadlock and improved system throughput. This work is licensed under the Creative Commons BY-NC-ND 4.0 International License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of this license. For any use beyond those covered by this license, obtain permission by emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights licensed to the VLDB Endowment. Proceedings of the VLDB Endowment, Vol. 18, No. 10 ISSN 2150-8097. doi:10.14778/3748191.3748224",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/HAWK%3A%20A%20Workload-driven%20Hierarchical%20Deadlock%20Detection%20Approach%20in%20Distributed%20Database%20System",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3682-cai.pdf",
    "session": "Research 48: Distributed Transactions II",
    "authors": [
      {
        "Name": "Rongrong Zhang",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Zhiwei Ye",
        "Affiliation": "China Mobile Cloud Center"
      },
      {
        "Name": "Jun-Peng Zhu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Peng Cai",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Xuan Zhou",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Dunbo Cai",
        "Affiliation": "China Mobile Cloud Center"
      },
      {
        "Name": "Ling Qian",
        "Affiliation": "China Mobile Cloud Center"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eb758d97-73a6-408d-8616-13bd3aab43f1",
    "title": "Streaming View: An Efficient Data Processing Engine for Modern Real-time Data Warehouse of Alibaba Cloud",
    "abstract": "Real-time data warehouses are essential for modern applications. Extract-Transform-Load (ETL) as a fundamental component of ofÔ¨Çine data warehouses also provides crucial support within realtime data warehouses. Among various traditional ETL approaches, Lambda and Kappa have emerged as classic real-time data processing solutions due to their freshness and query performance, which best meet business demands. However, both of them often require the integration of external stream processing engines, introducing challenges related to complexity, eÔ¨Éciency, and consistency. ZeroETL has emerged as an approach to address these issues. Nevertheless, existing ZeroETL-based solutions primarily emphasize the implementation of extraction and loading, resulting in limitations in handling transformation. Incremental View Maintenance (IVM) oÔ¨Äers an alternative that can enhance ZeroETL. However, existing IVM implementations often focus on query acceleration rather than supporting high-throughput, complex real-time workloads. \nTo address these challenges, we propose Streaming View, an eÔ¨Écient real-time data processing engine integrated within AnalyticDB of Alibaba Cloud. Unlike existing solutions, Streaming View supports high-throughput, complex data processing for realtime streaming ETL workloads. Furthermore, it can be leveraged to optimize ZeroETL-based approaches by enhancing transformation capabilities. We design tailored algorithms and optimizations for diverse syntaxes and high-throughput scenarios, ensuring the system meets complex application needs. By integrating incremental computation into the data warehouse, Streaming View reduces complexity, ensures data consistency, and boosts performance, offering a robust solution for real-world applications. Experiments show Streaming View improves processing performance by up to 7x and 20x over traditional ETL and IVM methods, respectively, and addresses complex scenarios unsolved by existing solutions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Streaming%20View%3A%20An%20Efficient%20Data%20Processing%20Engine%20for%20Modern%20Real-time%20Data%20Warehouse%20of%20Alibaba%20Cloud",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5153-zhou.pdf",
    "session": "Industry 2: Data Platforms for Analytics",
    "authors": [
      {
        "Name": "Fangyuan Zhang",
        "Affiliation": "The Chinese University of Hong Kong"
      },
      {
        "Name": "Mengqi Wu",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Chunlei Xu",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Yunong Bao",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Jiyu Qiao",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Yingli Zhou",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Hua Fan",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Caihua Yin",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Wenchao Zhou",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Cloud"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "19b62618-dc43-4c00-b8dd-cf62b533f449",
    "title": "Asymmetric Linearizable Local Reads",
    "abstract": "Many linearizable local read algorithms have been proposed to minimize the read latency of strongly consistent distributed databases deployed in geo-distributed networks. These algorithms do so by enabling reads to be performed immediately against any process‚Äô copy of the database in the best case. However, as our analysis shows, worst-case read latency at every process with all existing algorithms is at least the network‚Äôs relative diameter in terms of the maximum message delay minus a known lower bound on message delay between any two processes. We then show that by leveraging the asymmetric message delays of geo-distributed networks, worstcase read latency can be below the network‚Äôs relative diameter at processes close to the leader or the network‚Äôs center by presenting two new linearizable local read algorithms. Our experimental evaluation shows that these new algorithms reduce worst-case read latency by up to 50x compared to existing ones.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Asymmetric%20Linearizable%20Local%20Reads",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2427-thiessen.pdf",
    "session": "Research 48: Distributed Transactions II",
    "authors": [
      {
        "Name": "Myles Thiessen",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Guy Khazma",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Sam Toueg",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Eyal de Lara",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "c304e004-6444-4dc9-991c-fa66606b112b",
    "title": "Semantic Integrity Constraints: Declarative Guardrails for AI-Augmented Data Processing Systems",
    "abstract": "AI-augmented data processing systems (DPSs)  integrate large language models (LLMs) into query pipelines, allowing powerful semantic operations on structured and unstructured data. However, the reliability (a.k.a. trust) of these systems is fundamentally challenged by the potential for LLMs to produce errors, limiting their adoption in critical domains. To help address this reliability bottleneck, we introduce  semantic integrity constraints (SICs) ‚Äîa declarative abstraction for specifying and enforcing correctness conditions over LLM outputs in semantic queries. SICs generalize traditional database integrity constraints to semantic settings, supporting common types of constraints, such as grounding, soundness, and exclusion, with both reactive and proactive enforcement strategies. \nWe argue that SICs provide a foundation for building reliable and auditable AI-augmented data systems. Specifically, we present a system design for integrating SICs into query planning and runtime execution and discuss its realization in AI-augmented DPSs. To guide and evaluate our vision, we outline several design goals‚Äîcovering criteria around expressiveness, runtime semantics, integration, performance, and enterprise-scale applicability‚Äîand discuss how our framework addresses each, along with open research challenges.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Semantic%20Integrity%20Constraints%3A%20Declarative%20Guardrails%20for%20AI-Augmented%20Data%20Processing%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4073-lee.pdf",
    "session": "Research 2: Applied ML and AI for Data Management I",
    "authors": [
      {
        "Name": "Alexander Lee",
        "Affiliation": "Brown University"
      },
      {
        "Name": "Justin Chan",
        "Affiliation": "Brown University"
      },
      {
        "Name": "Michael Fu",
        "Affiliation": "Brown University"
      },
      {
        "Name": "Nicolas Kim",
        "Affiliation": "Brown University"
      },
      {
        "Name": "Akshay Mehta",
        "Affiliation": "Brown University"
      },
      {
        "Name": "Deepti Raghavan",
        "Affiliation": "Brown University"
      },
      {
        "Name": "Ugur Cetintemel",
        "Affiliation": "Brown University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "25a15146-93f6-43b0-a946-aa349d117218",
    "title": "DumpKV: Learning based lifetime aware garbage collection for key value separation in LSM-tree",
    "abstract": "Key-value separation is used in LSM-tree to store large values in separate log files to reduce write amplification but requires garbage collection to recycle invalid values. Existing LSM-tree typically adopts a static policy to recycle obsolete values, struggling to achieve low write amplification as it is challenging to predefine the static parameters for garbage collection. In this work we propose DumpKV, a learning-based lifetime-aware garbage collection mechanism which achieves lower write amplification. DumpKV trains a machine learning model based on the access history of keys and accordingly uses the lightweight model to predict the lifetime of each key, where the predicted lifetime can be used to guide the garbage collection. To reduce the interference to write throughput introduced by garbage collection, DumpKV conducts feature collection during L0-L1 compaction, leveraging the fact that LSM-tree is small under KV separation. Experimental results show that DumpKV reduces GC write size by 25.7%-53.3% in real-world workloads and 19%-65% in synthetic workloads compared to baseline key-value separation LSM-tree KV stores with small feature storage overhead.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DumpKV%3A%20Learning%20based%20lifetime%20aware%20garbage%20collection%20for%20key%20value%20separation%20in%20LSM-tree",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1223-zhuang.pdf",
    "session": "Research 17: Applied ML and AI for Data Management II",
    "authors": [
      {
        "Name": "ZHUTAO ZHUANG",
        "Affiliation": "Microsoft"
      },
      {
        "Name": "Xinqi Zeng",
        "Affiliation": "Sun Yat-sen University"
      },
      {
        "Name": "Zhiguang Chen",
        "Affiliation": "Sun Yat-sen university"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "af70c3e4-5d92-47f0-aab7-27d5b229ce65",
    "title": "Agent-OM: Leveraging LLM Agents for Ontology Matching",
    "abstract": "Ontology matching (OM) enables semantic interoperability be-Ontology matching (OM) enables semantic interoperability between different ontologies and resolves their conceptual hetero-tween different ontologies and resolves their conceptual heterogeneity by aligning related entities. OM systems currently have two prevailing design paradigms: conventional knowledge-based expert systems and newer machine learning-based predictive sys-expert systems and newer machine learning-based predictive systems. While large language models (LLMs) and LLM agents have revolutionised data engineering and have been applied creatively in many domains, their potential for OM remains underexplored. This study introduces a novel agent-powered LLM-based design paradigm for OM systems. With consideration of several specific challenges in leveraging LLM agents for OM, we propose a generic framework, namely Agent-OM (Agent for Ontology Matching), con-framework, namely Agent-OM (Agent for Ontology Matching), consisting of two Siamese agents for retrieval and matching, with a set of OM tools. Our framework is implemented in a proof-of-concept system. Evaluations of three Ontology Alignment Evaluation Ini-system. Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks over state-of-the-art OM systems show that our system can achieve results very close to the long-standing best performance on simple OM tasks and can significantly improve the performance on complex and few-shot OM tasks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Agent-OM%3A%20Leveraging%20LLM%20Agents%20for%20Ontology%20Matching",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p516-qiang.pdf",
    "session": "Research 51: Information Integration and Data Quality III",
    "authors": [
      {
        "Name": "Zhangcheng Qiang",
        "Affiliation": "Australian National University"
      },
      {
        "Name": "Weiqing Wang",
        "Affiliation": "Monash University"
      },
      {
        "Name": "Kerry Taylor",
        "Affiliation": "Australian National University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fb78ebf5-be41-4031-9f50-0c7815dae335",
    "title": "Efficient Graph Data Access for Out-of-Memory GPU Streaming Graph Processing",
    "abstract": "Leveraging GPUs‚Äô high parallelism can signi!cantly improve the real-time computation effciency of streaming graph processing. However, when a large-scale graph exceeds GPU memory capacity, CPU-GPU cooperative processing often results in substantial and irregular CPU-to-GPU data transfer overhead. This stems from the extensive redundant graph accesses during continuous computation, which can hardly be addressed by existing solutions. In this work, we present Grapin, an out-of-memory GPU streaming graph processing system designed to minimize graph data transfer via two effective techniques for eliminating redundant accesses: (1) Extending advanced incremental processing algorithms to GPUs by converting their heavyweight data dependency processing into GPU-friendly forms, eliminating redundant graph accesses from the computation side; and (2) providing a lightweight yet efficient GPU hot subgraph management framework that !nely caches the frequently accessed dynamic subgraphs in a vertex-centric manner. Experimental results demonstrate that Grapin can efficiently process large-scale streaming graphs with billions of edges on a single NVIDIA A5000 GPU. Enabling incremental computation reduces data transfer by 61%, and the integration of GPU hot subgraph reuse further reduces the remaining transfer by 72%, resulting in a total reduction of 89%. Compared with CPU-based solutions, Grapin achieves speedups ranging from 1.8x to 96.9x (17.9x on average).",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficient%20Graph%20Data%20Access%20for%20Out-of-Memory%20GPU%20Streaming%20Graph%20Processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3854-wang.pdf",
    "session": "Research 34: Graph Data Management IV",
    "authors": [
      {
        "Name": "Qiange Wang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Yongze Yan",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Hongshi Tan",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Cheng Chen",
        "Affiliation": "Bytedance"
      },
      {
        "Name": "Cheng Zhao",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Jiaming Tian",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Jiaxin Jiang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Xiaoliang Cong",
        "Affiliation": "ByteDance"
      },
      {
        "Name": "Yanfeng Zhang",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Ge Yu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Weng-Fai Wong",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Bingsheng He",
        "Affiliation": "National University of Singapore"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9ee1fc58-2b17-4717-93dd-0617856d9ca0",
    "title": "TELESAFE - Detecting Private/Work Boundary Crossings in Energy Consumption Trails in Telework",
    "abstract": "",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TELESAFE%20-%20Detecting%20Private%2FWork%20Boundary%20Crossings%20in%20Energy%20Consumption%20Trails%20in%20Telework",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1565-anciaux.pdf",
    "session": "Research 40: Time Series and Vector Data",
    "authors": [
      {
        "Name": "Haoying ZHANG",
        "Affiliation": "INSA CVL, Inria, LIFO"
      },
      {
        "Name": "Mariem Brahem",
        "Affiliation": "INRIA"
      },
      {
        "Name": "Nicolas Anciaux",
        "Affiliation": "INRIA"
      },
      {
        "Name": "Benjamin NGUYEN",
        "Affiliation": "INSA Centre Val de Loire"
      },
      {
        "Name": "Jose Maria De Fuentes",
        "Affiliation": "Universidad Carlos III de Madrid"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": [
      "Empty or missing abstract"
    ]
  },
  {
    "id": "b4965ad5-72a8-4a87-b1bd-b9184db17e97",
    "title": "Accurate and Fast Approximate Graph Pattern Mining at Scale",
    "abstract": "Approximate graph pattern mining (A-GPM) is an important data analysis tool for numerous graph-based applications. There exist sampling-based A-GPM systems to provide automation and general-sampling-based A-GPM systems to provide automation and generalization over a wide variety of use cases. Despite improved usability, there are two major obstacles that prevent existing A-GPM systems being adopted in practice. First, the termination mechanism that decides when to terminate sampling lacks theoretical backup on confidence, and performs significantly unstable and thus slow in practice. Second, they particularly suffer poor performance when dealing with the ‚Äúneedle-in-the-hay‚Äù cases, because a huge number of samples are required to converge, given the extremely low hit rate of their lazy-pruning strategy and fixed sampling schemes. \nWe build ScaleGPM, an accurate and fast A-GPM system that removes the two obstacles. First, we propose a novel on-the-fly con-removes the two obstacles. First, we propose a novel on-the-fly convergence detection mechanism to achieve stable termination and provide theoretical guarantee on the confidence, with negligible online overhead. Second, we propose two techniques to deal with the ‚Äúneedle-in-the-hay‚Äù problem,  eager-verify  and  hybrid sampling . Our eager-verify method drastically improves sampling hit rate by pruning unpromising candidates as early as possible. Hybrid sam-pruning unpromising candidates as early as possible. Hybrid sampling further improves performance by automatically choosing the better scheme between fine-grained and coarse-grained sampling schemes. Experiments show that our online convergence detection mechanism can precisely detect convergence, and results in stable and rapid termination with theoretically guaranteed confidence. We also show the effectiveness of eager-verify in improving the hit rate, and the scheme-selection mechanism in correctly choosing the better scheme for various cases. Overall, ScaleGPM achieves an  geomean  average of 565 √ó  (up to 610169 √ó ) speedup over the state-of-the-art A-GPM system, Arya. In particular, ScaleGPM han-state-of-the-art A-GPM system, Arya. In particular, ScaleGPM handles billion-scale graphs in seconds, where existing systems either run out of memory or fail to complete in hours.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Accurate%20and%20Fast%20Approximate%20Graph%20Pattern%20Mining%20at%20Scale",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p93-chen.pdf",
    "session": "Research 6: Graph Data Management I",
    "authors": [
      {
        "Name": "Anna Arpaci-Dusseau",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Zixiang Zhou",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Xuhao Chen",
        "Affiliation": "MIT"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8859c941-a2bb-4250-a5f2-dd193933a027",
    "title": "SQL:Trek Automated Index Design at Airbnb",
    "abstract": "Automating index design has been an active area of research for  decades due to the significant impact that indexes have on query  performance and database efficiency. Existing approaches range  from brute-force search to cost-based optimizations and, more  recently, machine learning techniques. However, many suffer from  high computational costs, reliance on inaccurate cost models, or the  need for deep integration with database internals.      We introduce SQL:Trek, a time-efficient tool for automated index  design that operates entirely as an external utility. SQL:Trek  leverages query compiler cost models to identify effective indexes  while mitigating false positives through execution on a lightweight  simulation database. This approach enables fast, iterative index  selection without modifying database internals, making it broadly  applicable across relational databases, including most MySQL √¢  and  PostgreSQL √¢  derivative databases.    Our evaluation demonstrates that SQL:Trek delivers significant  query performance improvements while keeping index selection  computationally efficient, with most workloads analyzed in under  five minutes. Unlike many cost-based what-if analysis methods,  SQL:Trek significantly improved performance of many production  workloads while avoiding the majority of detrimental index  recommendations caused by optimizer misestimates.  These results  highlight SQL:Trek as a practical, scalable solution for automated  index tuning in modern database environments.    This work is licensed under the Creative Commons BY-NC-ND 4.0 International  License. Visit https://creativecommons.org/licenses/by-nc-nd/4.0/ to view a copy of  this license. For any use beyond those covered by this license, obtain permission by  emailing info@vldb.org. Copyright is held by the owner/author(s). Publication rights  licensed to the VLDB Endowment.  Proceedings of the VLDB Endowment, Vol. 18, No. 12 ISSN 2150-8097.  doi:10.14778/3750601.3750638",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SQL%3ATrek%20Automated%20Index%20Design%20at%20Airbnb",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5210-lightstone.pdf",
    "session": "Industry 6: Database Engines",
    "authors": [
      {
        "Name": "Sam Lightstone",
        "Affiliation": "airbnb"
      },
      {
        "Name": "Ping Wang",
        "Affiliation": "airbnb"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "364df874-c16b-43f6-b8df-15e6d47d6631",
    "title": "ShaRP: Explaining Rankings and Preferences with Shapley Values",
    "abstract": "Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Given the impact of these decisions on individuals, organizations, and population groups, it is essential to understand them‚Äîto help individuals improve their ranking position, design better ranking procedures, and ensure legal compliance. In this paper, we argue that explainability methods for classification and regression, such as SHAP, are insufficient for ranking tasks, and present ShaRP‚ÄîShapley Values for Rankings and Preferences‚Äîa framework that explains the contributions of features to various aspects of a ranked outcome. \nShaRP computes feature contributions for various ranking-specific profit functions, such as rank and top- ùëò , and also includes a novel Shapley value-based method for explaining pairwise preference outcomes. We provide a flexible implementation of ShaRP, capable of efficiently and comprehensively explaining ranked and pairwise outcomes over tabular data, in score-based ranking and learning-torank tasks. Finally, we develop a comprehensive evaluation methodology for ranking explainability methods, showing through qualitative, quantitative, and usability studies that our rank-aware QoIs offer complementary insights, scale effectively, and help users interpret ranked outcomes in practice. KEYWORDS ranking, interpretability, feature importance, Shapley values, evaluation, responsible data management, responsible AI",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ShaRP%3A%20Explaining%20Rankings%20and%20Preferences%20with%20Shapley%20Values",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4131-stoyanovich.pdf",
    "session": "Research 14: User Interfaces for Data Exploration and Explainable AI",
    "authors": [
      {
        "Name": "Venetia Pliatsika",
        "Affiliation": "New York University"
      },
      {
        "Name": "Joao Fonseca",
        "Affiliation": "New York University"
      },
      {
        "Name": "Kateryna Akhynko",
        "Affiliation": "Ukrainian Catholic University"
      },
      {
        "Name": "Ivan Shevchenko",
        "Affiliation": "Ukrainian Catholic University"
      },
      {
        "Name": "Julia Stoyanovich",
        "Affiliation": "New York University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "abbec8fd-a3c3-464a-a80d-cbe4fde2980e",
    "title": "VecCity: A Taxonomy-guided Library for Map Entity Representation Learning",
    "abstract": "Electronic maps consist of diverse entities, such as points of interest (POIs), road segments, and land parcels, playing a vital role in applications like ITS and LBS. Map entity representation learning (MapRL) generates versatile and reusable data representations, providing essential tools for efficiently managing and utilizing map entity data. Despite the progress in MapRL, two key challenges constrain further development. First, existing research is fragmented, with models classified by the type of map entity, limiting the reusability of techniques across different tasks. Second, the lack of unified benchmarks makes systematic evaluation and comparison of models difficult. To address these challenges, we propose a novel taxonomy for MapRL that organizes models based on functional modules‚Äîsuch as encoders, pre-training tasks, and downstream tasks‚Äîrather than by entity type. Building on this taxonomy, we present a taxonomy-driven library,  VecCity , which offers easy-to-use interfaces for encoding, pre-training, fine-tuning, and evaluation. The library integrates datasets from nine cities and reproduces 21 mainstream MapRL models, establishing the first standardized benchmarks for the field. VecCity also allows users to modify and extend models through modular components, facilitating seamless experimentation. Our comprehensive experiments cover multiple types of map entities and evaluate 21  VecCity  pre-built models across various downstream tasks. Experimental results demonstrate the effectiveness of  VecCity  in streamlining model development and provide insights into the impact of various components on performance. By promoting modular design and reusability,  VecCity  offers a unified framework to advance research and innovation in MapRL. The code is available at https://github.com/Bigscity-VecCity/VecCity.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/VecCity%3A%20A%20Taxonomy-guided%20Library%20for%20Map%20Entity%20Representation%20Learning%20%5BExperiment%2C%20Analysis%20%26%20Benchmark%5D",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2575-wang.pdf",
    "session": "Research 4: Analytics over Different Data Types I",
    "authors": [
      {
        "Name": "Wentao Zhang",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Jingyuan Wang",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Yifan Yang",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Leong Hou U",
        "Affiliation": "University of Macau"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f7977a96-0ecb-4309-accb-5266d3c07ab5",
    "title": "Authenticated Aggregate Queries with Boolean Range Predicates on Blockchains",
    "abstract": "Blockchains have gained wide adoption for secure data processing. As blockchain data volumes grow, the demand for efficient data analysis, especially aggregate queries, becomes increasingly critical. However, current blockchains lack native support for efficient analytical query processing, forcing users to either maintain full replicas or rely on third-party services without integrity guarantees. \nIn this paper, we propose an efficient framework, Merkle Bloom Filter Tree ( MBFT ), for authenticated aggregate queries that combine boolean keywords and range predicates on blockchains. At its core is a Bloom filter-based authenticated data structure that supports both types of predicates, constructed per block for efficient transaction indexing. For temporal predicates, we optimize time window queries through value pruning and block consolidation. We design a novel Merge Bloom Filter ( MBF ) for space-efficient handling of dynamic sets during query authentication. We provide a theoretical analysis of the storage overhead caused by the Bloom filter‚Äôs false positive rates. Our framework employs data sketches to support various aggregate operations. Extensive experiments demonstrate that  MBFT  has improved the query speed by up to 286 √ó  compared to state-of-the-art authenticated query solutions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Authenticated%20Aggregate%20Queries%20with%20Boolean%20Range%20Predicates%20on%20Blockchains",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3615-sun.pdf",
    "session": "Research 50: Access Control and Privacy, Blockchain",
    "authors": [
      {
        "Name": "Weijie Sun",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Zihuan Xu",
        "Affiliation": "Shenzhen Institute of Computing Sciences"
      },
      {
        "Name": "Wangze Ni",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Lei Chen",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Peng Cheng",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Chen Jason Zhang",
        "Affiliation": "The Hong Kong Polytechnic University}"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f5eaa29e-334c-43e0-9fa7-93c8903cf636",
    "title": "HADES: Range-Filtered Private Aggregation on Public Data",
    "abstract": "In aggregation queries, predicate parameters often reveal user intent. Protecting these parameters is critical for user privacy, regardless of whether the database is public or private. While most existing works focus on private data settings, we address a public data setting where the server has access to the database. Current solutions for this setting either require additional setups (e.g., noncolluding servers, hardware enclaves) or are ineÔ¨Écient for practical workloads. Furthermore, they often do not support range predicates or boolean combinations commonly seen in real-world use cases. \nTo address these limitations, we built HADES, a fully homomorphic encryption (FHE) based private aggregation system for public data that supports point, range predicates, and boolean combinations. Our one-round HADES protocol eÔ¨Éciently generates predicate indicators by leveraging the plaintext form of public data records. It introduces a novel elementwise-mapping operation and an optimized reduction algorithm, achieving latency eÔ¨Éciency within a limited noise budget. Our highly scalable, multi-threaded implementation improves performance over previous one-round FHE solutions by 204x to 6574x on end-to-end TPC-H queries, reducing aggregation time on 1M records from 15 hours to 38 seconds.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/HADES%3A%20Range-Filtered%20Private%20Aggregation%20on%20Public%20Data",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2159-liu.pdf",
    "session": "Research 66: Access Control and Privacy II",
    "authors": [
      {
        "Name": "Xiaoyuan Liu",
        "Affiliation": "University of California, Berkeley"
      },
      {
        "Name": "Ni Trieu",
        "Affiliation": "Arizona State University"
      },
      {
        "Name": "Trinabh Gupta",
        "Affiliation": "UCSB"
      },
      {
        "Name": "Ishtiyaque Ahmad",
        "Affiliation": "University of California Santa Cruz"
      },
      {
        "Name": "Dawn Song",
        "Affiliation": "UC Berkeley"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0b051c02-e70f-4b42-9d30-1dc414e17c4b",
    "title": "Benchmarking Adaptive Multidimensional Indices",
    "abstract": "By adaptive indexing , an index grows dynamically and progressively through query processing. This mode of index-building, well explored over the past fifteen years, proves especially useful in exploratory scenarios where prebuilt indexes do not pay off the time to construct them, as the query workload variably focuses on particular areas of the search space, or the data become quickly obsolete. Despite a significant body of work in multidimensional adaptive indexing, there remains a gap in compa rative studies that evaluate these methods on equal terms in a wide spectrum of settings, including data types, distributions, sizes, and workload patterns. This work fills this gap with a comprehensive benchmark to thoroughly evaluate the performance, strengths, and limitations of existing multidimensional adaptive indexing methods across diverse scenarios, contributing valuable insights that complement previous works. Further, we suggest supplementary technical extensions that enhance the efficiency of existing methods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Benchmarking%20Adaptive%20Multidimensional%20Indices",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4505-lampropoulos.pdf",
    "session": "Research 15: Views, Indexing, and Search I",
    "authors": [
      {
        "Name": "Konstantinos Lampropoulos",
        "Affiliation": "University of Ioannina"
      },
      {
        "Name": "Fatemeh Zardbani",
        "Affiliation": "Aarhus University"
      },
      {
        "Name": "Nikos Mamoulis",
        "Affiliation": "University of Ioannina"
      },
      {
        "Name": "Panagiotis Karras",
        "Affiliation": "Copenhagen University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dc57e3ef-731f-41cc-aa50-9fc0394bd4e6",
    "title": "Disaggregated State Management in Apache Flink¬Æ 2.0",
    "abstract": "We present Apache Flink 2.0, an evolution of the popular stream processing system‚Äôs architecture that decouples computation from state management. Flink 2.0 relies on a remote distributed Ô¨Åle system (DFS) for primary state storage and uses local disks as a secondary cache, with state updates streamed continuously and directly to the DFS. To address the latency implications of remote storage, Flink 2.0 incorporates an asynchronous runtime execution model. Furthermore, Flink 2.0 introduces ForSt, a novel state store featuring a uniÔ¨Åed Ô¨Åle system that enables faster and lightweight checkpointing, recovery, and reconÔ¨Åguration with minimal intrusion to the existing Flink runtime architecture. Using a comprehensive set of Nexmark benchmarks and a large-scale stateful production workload, we evaluate Flink 2.0‚Äôs large-state processing, checkpointing, and recovery mechanisms. Our results show signiÔ¨Åcant performance improvements and reduced resource utilization compared to the baseline Flink 1.20 implementation. SpeciÔ¨Åcally, we observe up to  94%  reduction in checkpoint duration, up to  49 √ó  faster recovery after failures or a rescaling operation, and up to  50%  cost savings.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Disaggregated%20State%20Management%20in%20Apache%20Flink%202.0",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4846-mei.pdf",
    "session": "Industry 1: Distributed Systems",
    "authors": [
      {
        "Name": "Yuan Mei",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Zhaoqian Lan",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Lei Huang",
        "Affiliation": "Boston University"
      },
      {
        "Name": "Yanfei Lei",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Han Yin",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Rui Xia",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Kaitian Hu",
        "Affiliation": "Alibaba Cloud"
      },
      {
        "Name": "Paris Carbone",
        "Affiliation": "KTH Royal Institute of Technology"
      },
      {
        "Name": "Vasiliki Kalavri",
        "Affiliation": "Boston University"
      },
      {
        "Name": "Feng Wang",
        "Affiliation": "Alibaba Cloud"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6395fc21-4006-4c31-b447-dfe5c1918fc9",
    "title": "Agamotto: Scheduling of Deadline-Oriented Incremental Query Execution under Uncertain Resource Price",
    "abstract": "Incremental query processing is widely used in data warehouses and streaming systems. While many optimization techniques are developed to generate incremental query plans, the scheduling support for incremental processing remains preliminary. Typically, execution is triggered with fixed frequencies specified by the user. In this paper, we propose a novel scheduling problem for incremental query execution under a deadline, assuming the resource has a fluctuating and unforeseen price. We propose two naive solutions as well as a prophet scheduler that foresees the future. We present an end-to-end system  Agamotto  that models future probabilities offline with a Markov Decision Process (MDP) and makes cost-based and dynamic scheduling decisions online. We show how  Agamotto can be extended to handle a workflow of dependent queries, so that they can all incrementally execute in an asynchronous fashion. Experiments show that  Agamotto  consistently outperforms the naive solutions, and the achieved cost is on average 10x closer to the theoretical lower bound provided by the prophet scheduler.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Agamotto%3A%20Scheduling%20of%20Deadline-Oriented%20Incremental%20Query%20Execution%20under%20Uncertain%20Resource%20Price",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1852-huang.pdf",
    "session": "Research 59: Distributed and Streaming Data Processing",
    "authors": [
      {
        "Name": "Botong Huang",
        "Affiliation": "Alibaba"
      },
      {
        "Name": "weng lianggui",
        "Affiliation": "alibaba"
      },
      {
        "Name": "Wei Chen",
        "Affiliation": "alibaba"
      },
      {
        "Name": "Zuozhi Wang",
        "Affiliation": "U C IRVINE"
      },
      {
        "Name": "Kai Zeng",
        "Affiliation": "Huawei Technologies Co. Ltd."
      },
      {
        "Name": "Chen Li",
        "Affiliation": "UC Irvine"
      },
      {
        "Name": "Yihui Feng",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Bolin Ding",
        "Affiliation": "\"Data Analytics and Intelligence Lab, Alibaba Group\""
      },
      {
        "Name": "Jingren Zhou",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "df247602-e4d5-443a-8cf2-cf32f3070649",
    "title": "ParSEval: Plan-aware Test Database Generation for SQL Equivalence Evaluation",
    "abstract": "Deciding query equivalence has played an essential role in many real-world applications, including evaluating the accuracy of textto-SQL models, where one needs to compare model-generated queries against ground truth queries. Although query equivalence is undecidable in general, researchers have developed two significant approaches to check query equivalence: formal verification-based and test-case-based. Verification-based solutions ensure correctness but may lack support for advanced SQL features and cross-database adaptability. Test cases are versatile but suffer from ad-hoc constraints and potential incorrectness (false positives). \nIn this paper, we propose  ParSEval , a  Plan-aware SQL Equivalence evaluation framework to generate test database instances for given queries. We observed that existing test data generation methods fail to fully explore the query structure. To address this limitation, ParSEval  formally models specific behaviors of each query operator and considers all possible execution paths of the logical query plan by adapting the notion of branch coverage. We validated the effectiveness and efficiency of  ParSEval  on four datasets with AI-generated and human-crafted queries. The experimental results show that  ParSEval  supports up to 40% more query pairs than state-of-the-art verification-based approaches. Compared to existing test-case-based approaches,  ParSEval  reveals more nonequivalent pairs while being 21 √ó  faster.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ParSEval%3A%20Plan-aware%20Test%20Database%20Generation%20for%20SQL%20Equivalence%20Evaluation",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4750-miao.pdf",
    "session": "Research 42: Data Models and Query Languages",
    "authors": [
      {
        "Name": "Chunyu Chen",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Zhengjie Miao",
        "Affiliation": "Simon Fraser University"
      },
      {
        "Name": "Yong Zhang",
        "Affiliation": "Huawei Technologies Canada Co., Ltd"
      },
      {
        "Name": "Jiannan Wang",
        "Affiliation": "Simon Fraser University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8c0c000f-1c6c-4e32-b55e-f9629e4609c2",
    "title": "Path-centric Cardinality Estimation for Subgraph Matching",
    "abstract": "This paper presents  PathCE , a path-centric cardinality estimation framework for subgraph matching.  PathCE  improves estimation accuracy by utilizing statistics from short graph queries. At its core is a novel data structure called the  path-centric summary graph  ( PSG ), which captures short path query statistics from a data graph  ùê∫ and represents them in a new graph  G . Given a graph query  ùëÑ and a PSG  graph  G  for  ùê∫ ,  PathCE  decomposes  ùëÑ into a simpler query  Q , where each edge in  Q  corresponds to a sub-path query in  ùëÑ with statistics included in  G .  PathCE  estimates the cardinality using  Q and  G , requiring significantly fewer estimation iterations while ensuring that the estimate remains an upper bound on the true cardinality of  ùëÑ ( ùê∫ ) . It also includes  PSGBuilder , a parallelly scalable algorithm that constructs  PSG ‚Äôs for any given graph in linear time, efficiently scaling with the number of processors. Empirical results on real-world and synthetic datasets show that  PathCE  outperforms state-of-the-art baselines in accuracy, estimation latency, and summary construction efficiency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Path-centric%20Cardinality%20Estimation%20for%20Subgraph%20Matching",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3063-yin.pdf",
    "session": "Research 35: Database Engines for Graphs",
    "authors": [
      {
        "Name": "Zhengdong Wang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Qiang Yin",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Longbin Lai",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "81b32bfd-e460-459d-b304-36be4c50df88",
    "title": "Effective and Efficient Distributed Temporal Graph Learning through Hotspot Memory Sharing",
    "abstract": "Memory-based temporal graph neural network (MTGNN) models are eÔ¨Äective for predicting temporal graphs by using node memory and message-passing modules to capture temporal and structural information, respectively. However, distributed training for large graphs presents challenges such as accuracy loss and decreased eÔ¨Éciency due to remote features and memory transmission. Despite improvements in MTGNN system optimizations, issues like dynamic load imbalances, communication overhead, and memory staleness persist. To tackle these challenges, we introduce MemShare, a distributed MTGNN system. MemShare introduces a novel shared node memory paradigm that utilizes a small subset of shared nodes across machines and GPUs to reduce distributed communication for memory management. It incorporates techniques like shared nodescentric graph partitioning, shared nodes-aware boundary decay sampling, and shared nodes-targeted synchronous smoothing aggregation. Experiments show that MemShare outperforms existing distributed MTGNN systems in accuracy and training eÔ¨Éciency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Effective%20and%20Efficient%20Distributed%20Temporal%20Graph%20Learning%20through%20Hotspot%20Memory%20Sharing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3093-wang.pdf",
    "session": "Research 37: Applied ML and AI for Graph, Temporal, and Trajectory Data",
    "authors": [
      {
        "Name": "Longjiao Zhang",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Rui Wang",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Tongya Zheng",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Ziqi Huang",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Wenjie Huang",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Xinyu Wang",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Can Wang",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Mingli Song",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Sai Wu",
        "Affiliation": "ZhejiangUniversity"
      },
      {
        "Name": "Shuibing He",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8eaa150c-4317-48ae-8ca5-b727929ad2e1",
    "title": "Machine Learning for Graph Data Management and Query Processing",
    "abstract": "Machine learning techniques have been proposed to optimize the performance of graph databases in recent years. Due to the NPhardness of graph database tasks and the complexity of graph data, traditional exact solutions usually encounter efficiency issues, while the performance of approximation solutions can be affected by issues like sampling failure and local optimality. Empowered by the inherent advantages of machine learning, the learning-based techniques show the generalization ability and better performance in many scenarios, including graph data management and graph query processing. Despite the efficiency and accuracy brought by machine learning techniques, machine learning for graph database models still face several critical challenges, including scalability and adaptability. In this tutorial, we first provide an in-depth survey of learning-based graph data management and query processing techniques published in recent database and data mining conferences to sketch the frontier of the research of Machine Learning for Graph Database. We also discuss the open challenges and provide future directions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Machine%20Learning%20for%20Graph%20Data%20Management%20and%20Query%20Processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5499-zhang.pdf",
    "session": "Tutorial 7: Machine Learning for Graph Data Management and Query Processing",
    "authors": [
      {
        "Name": "Hanchen Wang",
        "Affiliation": "University of Technology Sydney"
      },
      {
        "Name": "Ying Zhang",
        "Affiliation": "University of Technology Sydney"
      },
      {
        "Name": "Wenjie Zhang",
        "Affiliation": "University of New South Wales"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0bcaf78a-d25f-4ef3-b2fe-104a48b4682f",
    "title": "NeutronTP: Load-Balanced Distributed Full-Graph GNN Training with Tensor Parallelism",
    "abstract": "Graph neural networks (GNNs) have emerged as a promising direc-Graph neural networks (GNNs) have emerged as a promising direction. Training large-scale graphs that relies on distributed comput-tion. Training large-scale graphs that relies on distributed computing power poses new challenges. Existing distributed GNN systems leverage data parallelism by partitioning the input graph and dis-leverage data parallelism by partitioning the input graph and distributing it to multiple workers. However, due to the irregular nature of the graph structure, existing distributed approaches suf-nature of the graph structure, existing distributed approaches suffer from unbalanced workloads and high overhead in managing cross-worker vertex dependencies. \nIn this paper, we leverage tensor parallelism for distributed GNN training. GNN tensor parallelism eliminates cross-worker vertex dependencies by partitioning features instead of graph structures. Different workers are assigned training tasks on different feature slices with the same dimensional size, leading to a complete load balance. We achieve efficient GNN tensor parallelism through two critical functions. Firstly, we employ a generalized decoupled train-critical functions. Firstly, we employ a generalized decoupled training framework to decouple NN operations from graph aggregation operations, significantly reducing the communication overhead caused by NN operations which must be computed using complete features. Secondly, we employ a memory-efficient task scheduling strategy to support the training of large graphs exceeding single GPU memory, while further improving performance by overlapping communication and computation. By integrating the above tech-communication and computation. By integrating the above techniques, we propose a distributed GNN training system NeutronTP. Our experimental results on a 16-node Aliyun cluster demonstrate that NeutronTP achieves 1.29√ó -8.72√ó speedup over state-of-the-art GNN systems including DistDGL, NeutronStar, and Sancus.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/NeutronTP%3A%20Load-Balanced%20Distributed%20Full-Graph%20GNN%20Training%20with%20Tensor%20Parallelism",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p173-ai.pdf",
    "session": "Research 45: Graph Data Learning",
    "authors": [
      {
        "Name": "Xin Ai",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Hao Yuan",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Zeyu Ling",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Qiange Wang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Yanfeng Zhang",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Zhenbo Fu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Chaoyi Chen",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Yu Gu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Ge Yu",
        "Affiliation": "Northeastern University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f71b100c-1cda-4a32-9997-3a5e3e9d9315",
    "title": "SQLStorm: Taking Database Benchmarking into the LLM Era",
    "abstract": "In this paper, we introduce a new methodology for constructing database benchmarks using Large Language Models (LLMs), as well as SQLStorm v1.0, a concrete benchmark on a real-world dataset of three sizes (1 GB, 12 GB, 220 GB) consisting of over 18 K queries. This methodology of using AI to generate query workloads breaks new ground, not only in its ability to cheaply ($15) generate huge volumes (22 MB) of realistic queries but especially because it greatly expands the amount of SQL functionality and query constructions that is covered, compared to human-written SQL benchmarks such as TPC-H, TPC-DS, and JOB. The use cases of SQLStorm that we think will advance data systems most are: (i) improving SQL compatibility between systems, (ii) increasing system quality by identifying crashes/errors and Ô¨Åxing those, (iii) improving cardinality estimators and query optimizers, by identifying trends and opportunities (queries where other systems do much better), as well as (iv) overall system performance, both in terms of speed and robustness.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SQLStorm%3A%20Taking%20Database%20Benchmarking%20into%20the%20LLM%20Era",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4144-schmidt.pdf",
    "session": "Research 44: Database Engine Performance and Manageability I",
    "authors": [
      {
        "Name": "Tobias Schmidt",
        "Affiliation": "TUM"
      },
      {
        "Name": "Leis Viktor",
        "Affiliation": "TUM"
      },
      {
        "Name": "Peter Boncz",
        "Affiliation": "CWI"
      },
      {
        "Name": "Thomas Neumann",
        "Affiliation": "TUM"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "71969eac-1f96-4575-a44f-50a59d281af5",
    "title": "DortDB: Bridging Query Languages for Multi-Model Data Ponds",
    "abstract": "Multi-model data encompasses structurally distinct data, including relational, document, graph, key/value, columnar, etc., managed within a single system, such as a multi-model database or a data lake. Querying multi-model data requires strategies that balance unification and integration across diverse models and query languages. This paper presents DortDB, an extensible framework enabling cross-model queries combining well-known query languages and offering intuitive flexibility and optimization via a unified algebra. Though a small-scale in-memory prototype is to be demonstrated, its principles can be extended to distributed systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/DortDB%3A%20Bridging%20Query%20Languages%20for%20Multi-Model%20Data%20Ponds",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5335-koupil.pdf",
    "session": "Demo C1:",
    "authors": [
      {
        "Name": "Filip Je≈æek",
        "Affiliation": "Charles University"
      },
      {
        "Name": "Pavel Koupil",
        "Affiliation": "Charles University"
      },
      {
        "Name": "Michal Kopeck√Ω",
        "Affiliation": "Charles University"
      },
      {
        "Name": "J√°chym B√°rt√≠k",
        "Affiliation": "Charles University"
      },
      {
        "Name": "Irena Holubov√°",
        "Affiliation": "Charles University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2eb8aa4a-7aed-4573-9dae-927d155551fc",
    "title": "Detecting Schema-Related Logic Bugs in Relational DBMSs via Equivalent Database Construction",
    "abstract": "Relational Database Management Systems (DBMSs) provide flexible DDL (Data Definition Language) statements that enable the creation, modification, and deletion of database schemas. In addition to database schemas, relational DBMSs typically manage various schema-related information internally, e.g., schema changes, tablespace allocation, and block-level data layout. However, incorrect implementations related to schema-related information maintenance and utilization can introduce schema-related logic bugs. These bugs can cause DQL (Data Query Language) statements to return incorrect query results and DML (Data Manipulation Language) statements to create incorrect database states. Existing approaches mainly focus on detecting logic bugs in DQL statements, but are ineffective in detecting schema-related logic bugs. \nIn this paper, we propose a novel and general testing approach, DDLCheck , to effectively detect schema-related logic bugs in relational DBMSs. We first generate a complex DDL sequence  ùë†ùëíùëû ùëîùëíùëõ that consists of various types of DDL statements, and then synthesize a rather simple DDL sequence  ùë†ùëíùëû ùë†ùë¶ùëõ , which utilizes  CREATE statements to create the same database schema as  ùë†ùëíùëû ùëîùëíùëõ . Executing the same SQL statements on the two databases created by  ùë†ùëíùëû ùëîùëíùëõ and  ùë†ùëíùëû ùë†ùë¶ùëõ should yield the same execution results. Any discrepancy between their execution results indicates a schema-related logic bug. To improve the testing efficiency of  DDLCheck , we further design a DDL-sequence-oriented testing optimization strategy, which can help  DDLCheck  explore diverse schema-related information and detect schema-related logic bugs quickly. We implement and evaluate  DDLCheck  on six widely-used relational DBMSs. We have detected 34 bugs in these DBMSs, of which 29 bugs have been confirmed as previously unknown bugs and 9 bugs have been fixed.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Detecting%20Schema-Related%20Logic%20Bugs%20in%20Relational%20DBMSs%20via%20Equivalent%20Database%20Construction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2281-song.pdf",
    "session": "Research 16: Query Processing and Optimization II",
    "authors": [
      {
        "Name": "Jiansen Song",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Wensheng Dou",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Yingying Zheng",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Yu Gao",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Ziyu Cui",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Wei Wang",
        "Affiliation": "Institute of Software, Chinese Academy of Sciences"
      },
      {
        "Name": "Jun Wei",
        "Affiliation": "Institute of Software, Chinese Academy of Sciences"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dd9b8d16-e0ff-4bb4-97e9-64ec09b3976e",
    "title": "QueryArtisan: Generating Data Manipulation Codes for Ad-hoc Analysis in Data Lakes",
    "abstract": "Query processing over data lakes is a challenging task, often requir-Query processing over data lakes is a challenging task, often requiring extensive data pre-processing activities such as data cleaning, transformation, and loading. However, the advent of Large Lan-transformation, and loading. However, the advent of Large Language Models (LLMs) has illuminated a new pathway to address these complexities by offering a unified approach to understand-these complexities by offering a unified approach to understanding the diverse datasets submerged in data lakes. In this paper, we introduce QueryArtisan, a novel LLM-powered analytic tool specif-introduce QueryArtisan, a novel LLM-powered analytic tool specifically designed for data lakes. QueryArtisan transcends traditional ETL (Extract, Transform, Load) processes by generating just-in-ETL (Extract, Transform, Load) processes by generating just-intime code for dataset-specific queries. It eliminates the need for an intermediary schema, enabling users to query the data lake di-an intermediary schema, enabling users to query the data lake directly using natural language. To achieve this, we have developed a suite of heterogeneous operators capable of processing data across various modalities. Additionally, QueryArtisan incorporates a cost model-based query optimization technique, significantly enhanc-model-based query optimization technique, significantly enhancing its code generation capabilities for efficient query resolution. Our extensive experimental evaluations, conducted with real-life datasets, demonstrate that QueryArtisan markedly outperforms existing solutions in terms of effectiveness, efficiency and usability.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/QueryArtisan%3A%20Generating%20Data%20Manipulation%20Codes%20for%20Ad-hoc%20Analysis%20in%20Data%20Lakes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p108-yao.pdf",
    "session": "Research 17: Applied ML and AI for Data Management II",
    "authors": [
      {
        "Name": "Xiu Tang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Wenhao Liu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Sai Wu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Chang Yao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Gongsheng Yuan",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Shanshan Ying",
        "Affiliation": "ApeCloud"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8b8876c0-43e8-4464-bdf9-57a1ab923bd8",
    "title": "Quantifying Point Contributions: A Lightweight Framework for Efficient and Effective Query-Driven Trajectory Simplification",
    "abstract": "As large volumes of trajectory data accumulate, simplifying trajec-As large volumes of trajectory data accumulate, simplifying trajectories to reduce storage and querying costs is increasingly studied. Existing proposals face three main problems. First, they require numerous iterations to decide which GPS points to delete. Second, they focus only on the relationships between neighboring points (local information) while neglecting the overall structure (global information), reducing the global similarity between the simplified and original trajectories and making it difficult to maintain con-and original trajectories and making it difficult to maintain consistency in query results, especially for similarity-based queries. Finally, they fail to differentiate the importance of points with simi-Finally, they fail to differentiate the importance of points with similar features, leading to suboptimal selection of points to retain the original trajectory information. \nWe propose  MLSimp , a novel Mutual Learning query-driven trajectory simplification framework that integrates two distinct models: GNN-TS, based on graph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluates the importance of a point according to its globality, capturing its correlation with the entire trajectory, and its uniqueness, capturing its differences from neighboring points. It also incorporates attention mechanisms in the GNN layers, enabling simultaneous data integration from all points within the same trajectory and refining representations, thus avoiding iterative processes. Diff-TS generates amplified sig-thus avoiding iterative processes. Diff-TS generates amplified signals to enable the retention of the most important points at low compression rates. Experiments involving eight baselines on three databases show that  MLSimp  reduces the simplification time by 42%‚Äì70% and improves query accuracy over simplified trajectories by up to 34.6%.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Quantifying%20Point%20Contributions%3A%20A%20Lightweight%20Framework%20for%20Efficient%20and%20Effective%20Query-Driven%20Trajectory%20Simplification",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p453-gu.pdf",
    "session": "Research 2: Applied ML and AI for Data Management I",
    "authors": [
      {
        "Name": "Yumeng Song",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Yu Gu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Tianyi Li",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Yushuai Li",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Ge Yu",
        "Affiliation": "Northeastern University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "78a31c49-7962-4eec-9c15-2763aee11073",
    "title": "TuskFlow: An Efficient Graph Database for Long-Running Transactions",
    "abstract": "Mammoth transactions, which involve long-running operations that access many items, are common in graph workloads. Graph analytics tasks, including pattern matching and graph algorithms, can generate large read-write operations that impact signi!cant portions of data, which makes their execution challenging under strict isolation guarantees. Consequently, we face an apparent trade-off between ensuring high isolation and achieving high performance, forcing users to choose between the two. \nIn this work, we present Tuskflow, an experimental graph database based on Neo4j, designed to e#ciently handle mammoth transactions on graphs (the technique is applicable to other models such as relational) while maintaining existing transactional semantics. Tuskflow employs a deterministic protocol that safely reorders regular transactions around mammoths within an epoch. Our protocol supports parallel mammoth execution inspired by graph-parallel algorithms. To minimize con$icts with regular transactions, Tuskflow introduces query- and workload-aware optimizations, including graph entity tagging and partitioning. Our experiments demonstrate that, unlike traditional protocols like two-phase locking or MVCC, Tuskflow avoids blocking write transactions and improves tail latency by up to 45X",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TuskFlow%3A%20An%20Efficient%20Graph%20Database%20for%20Long-Running%20Transactions",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4777-theodorakis.pdf",
    "session": "Industry 5: Transactions and Concurrency Control",
    "authors": [
      {
        "Name": "Georgios Theodorakis",
        "Affiliation": "Neo4j"
      },
      {
        "Name": "Hugo Firth",
        "Affiliation": "Neo4j"
      },
      {
        "Name": "James Clarkson",
        "Affiliation": "Neo4j"
      },
      {
        "Name": "Natacha Crooks",
        "Affiliation": "UC Berkeley"
      },
      {
        "Name": "Jim Webber",
        "Affiliation": "Neo4j"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "397aae26-8db4-4c30-b8b6-c3ec9100a71b",
    "title": "Powerful GPUs or Fast Interconnects: Analyzing Relational Workloads on Modern GPUs",
    "abstract": "In this study we explore the impact of different combinations of GPU models (RTX3090, A100, H100, GraceHoppers - GH200) and interconnects (PCIe 3.0, PCIe 4.0, PCIe 5.0, and NVLink 4.0) on various relational data analytics workloads (TPC-H, H2O-G, ClickBench). We present MaxBench, a comprehensive framework designed for benchmarking, profiling, and modeling these workloads on GPUs. Beyond delivering detailed performance metrics, MaxBench estimates query execution performance using a novel cost model. With this model, we move beyond traditional metrics such as arithmetic intensity and GFlop/s and suggest using instead the notions of characteristic query complexity  and  characteristic GPU efficiency , as more suitable metrics for data analytics workloads. We conduct an extensive experimental analysis with MaxBench across different combinations of GPU models and interconnects on various data analytics workloads. The insights from this analysis reveal the trade-offs between GPU computing capacity and interconnect bandwidth on query processing. Using this cost model, we also examine future trends by investigating how enhancements in interconnect bandwidth or GPU efficiency would affect performance in the future.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Powerful%20GPUs%20or%20Fast%20Interconnects%3A%20Analyzing%20Relational%20Workloads%20on%20Modern%20GPUs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4350-kabic.pdf",
    "session": "Research 38: Data Management on Novel Hardware",
    "authors": [
      {
        "Name": "Marko Kabiƒá",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Bowen Wu",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Jonas Dann",
        "Affiliation": "ETH Zurich"
      },
      {
        "Name": "Gustavo Alonso",
        "Affiliation": "ETH Zurich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ccd41e88-42ba-4eca-956a-f459fb031177",
    "title": "How Reliable Are Streams? End-to-End Processing-Guarantee Validation and Performance Benchmarking of Stream Processing Systems",
    "abstract": "Stream processing systems (SPSs) provide processing guarantees to ensure reliability under failure. However, no related work ex-to ensure reliability under failure. However, no related work exists that empirically validates these guarantees. In this paper, we present PGVal, a tool that can end-to-end validate guarantees of SPSs. Additionally, we introduce new metrics for SPSs, such as reliability, reliable throughput, and failure cost, in addition to a refined definition of latency that results in improved measurements. We benchmark three popular SPSs, namely  Kafka Streams ,  Apache Storm , and  Apache Flink . Our results show that the reliability of SPSs depends on many characteristics, such as data rate, data partitions, processing topology, and parallelism factor. An SPS configuration may not continue to provide reliable outputs when any of these characteristics vary. PGVal can also inject faults into SPSs to ob-characteristics vary. PGVal can also inject faults into SPSs to observe their impact on reliability and performance. We provide a comprehensive failure model for fault-tolerance benchmarking of SPSs and report on the impact of faults on the reliability and per-SPSs and report on the impact of faults on the reliability and performance of SPSs. Our experiments show that SPSs‚Äô reliability and performance drop varies by fault. Lastly, we provide suggestions to increase the reliability and performance of these systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/How%20Reliable%20Are%20Streams%EF%BC%9F%20End-to-End%20Processing-Guarantee%20Validation%20and%20Performance%20Benchmarking%20of%20Stream%20Processing%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p585-tahir.pdf",
    "session": "Research 33: Data Stream Mining",
    "authors": [
      {
        "Name": "Jawad Tahir",
        "Affiliation": "Techinical University of Munich"
      },
      {
        "Name": "Ruben Mayer",
        "Affiliation": "University of Bayreuth"
      },
      {
        "Name": "Christoph Doblander",
        "Affiliation": "TU M√ºnchen"
      },
      {
        "Name": "Hans-Arno Jacobsen",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2eea98bb-5319-4cad-af46-bdb2de677d95",
    "title": "Approximate $2$-hop neighborhoods on incremental graphs: An efficient lazy approach",
    "abstract": "In this work, we propose, analyze and empirically validate a lazyupdate approach to maintain accurate approximations of the 2-hop neighborhoods of dynamic graphs resulting from sequences of edge insertions. \nWe first show that under random input sequences, our algorithm exhibits an optimal trade-off between accuracy and insertion cost: it only performs  ùëÇ ( 1/ùúÄ )  (amortized) updates per edge insertion, while the estimated size of any vertex‚Äôs 2-hop neighborhood is at most a factor  ùúÄ away from its true value in most cases,  regardless  of the underlying graph topology and for any  ùúÄ >  0. \nAs a further theoretical contribution, we explore adversarial scenarios that can force our approach into a worst-case behavior at any given time  ùë° of interest. We show that while worst-case input sequences do exist, a necessary condition for them to occur is that the  girth  of the graph released up to time  ùë° be at most 4. \nFinally, we conduct extensive experiments on a collection of real, incremental social networks of different sizes, which typically have low girth. Empirical results are consistent with and typically better than our theoretical analysis anticipates. This further supports the robustness of our theoretical findings: forcing our algorithm into a worst-case behavior not only requires topologies characterized by a low girth, but also carefully crafted input sequences that are unlikely to occur in practice. \nCombined with standard sketching techniques, our lazy approach proves an effective and efficient tool to support key neighborhood queries on large, incremental graphs, including neighborhood size, Jaccard similarity between neighborhoods and, in general, functions of the union and/or intersection of 2-hop neighborhoods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Approximate%202-hop%20neighborhoods%20on%20incremental%20graphs%3A%20An%20efficient%20lazy%20approach",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3937-straziota.pdf",
    "session": "Research 33: Data Stream Mining",
    "authors": [
      {
        "Name": "Luca Becchetti",
        "Affiliation": "Sapienza University of Rome"
      },
      {
        "Name": "Andrea Clementi",
        "Affiliation": "University of Rome \"Tor Vergata\""
      },
      {
        "Name": "Luciano Gual√†",
        "Affiliation": "University of Rome \"Tor Vergata\""
      },
      {
        "Name": "Luca Pep√® Sciarria",
        "Affiliation": "University of Rome \"Tor Vergata\""
      },
      {
        "Name": "Alessandro Straziota",
        "Affiliation": "University of Rome \"Tor Vergata\""
      },
      {
        "Name": "Matteo Stromieri",
        "Affiliation": "University of Rome \"Tor Vergata\""
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6cf8a6c6-c7a4-4b25-808f-4a7b0aa91127",
    "title": "Powering In-Database Dynamic Model Slicing for Structured Data Analytics",
    "abstract": "Relational database management systems (RDBMS) are widely used for the storage of structured data. To derive insights beyond statistical aggregation, we typically have to extract specific subdatasets from the database using conventional database operations, and then apply deep neural networks (DNN) training and inference on these subdatasets in a separate analytics system. The process can be prohibitively expensive, especially when there are various subdatasets extracted for different analytical purposes. This calls for efficient in-database support of advanced analytical methods.\nIn this paper, we introduce LEADS, a novel SQL-aware dynamic model slicing technique to customize models for specified SQL queries. LEADS improves the predictive modeling of structured data via the mixture of experts (MoE) and maintains efficiency by a SQL-aware gating network. At the core of LEADS is the construction of a general model with multiple expert sub-models trained over the database. The MoE scales up the modeling capacity, enhances effectiveness, and preserves efficiency by activating necessary experts via the SQL-aware gating network during inference. To support in-database analytics, we build an inference extension that integrates LEADS onto PostgreSQL. Our extensive experiments on real-world datasets demonstrate that LEADS consistently outperforms the baseline models, and the in-database inference extension delivers a considerable reduction in inference latency compared to traditional solutions.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/Powering%20In-Database%20Dynamic%20Model%20Slicing%20for%20Structured%20Data%20Analytics",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4813-zeng.pdf",
    "session": "Research 53: Applied ML and AI for Data Management IV",
    "authors": [
      {
        "Name": "Lingze Zeng",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Naili Xing",
        "Affiliation": "national university of singapore"
      },
      {
        "Name": "Shaofeng Cai",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Beng Chin Ooi",
        "Affiliation": "NUS"
      },
      {
        "Name": "Jian Pei",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Yuncheng Wu",
        "Affiliation": "Renmin University of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f6473cdc-24d2-4304-a92a-6d6cb240a223",
    "title": "Towards Designing Future-Proof Data Processing Systems",
    "abstract": "Data processing systems find themselves crushed between two moving tectonic plates: the usage plate driven by the system‚Äôs users and their requirements; and the environment plate driven by various technological changes. We argue that the existing status quo of constantly adapting and thus bloating the system‚Äôs implementation is simply unsustainable in the long run. We further argue that now is the right time to take a step back and establish the foundations of future-proof data processing systems that can easily adapt to different workloads and input formats, and that can run efficiently in any type of environment, today and in the future. \nWith our paper, we analyze and learn from prior attempts, identify key design principles, and present our vision on how to design such systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Designing%20Future-Proof%20Data%20Processing%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3988-jungmair.pdf",
    "session": "Research 42: Data Models and Query Languages",
    "authors": [
      {
        "Name": "Michael Jungmair",
        "Affiliation": "Technical University of Munich"
      },
      {
        "Name": "Jana Giceva",
        "Affiliation": "Tehnical University of Munich"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "1363ac3d-633f-4339-8361-b9437e8cf5c2",
    "title": "CUBIT: Concurrent Updatable Bitmap Indexing",
    "abstract": "Bitmap indexes are widely used for read-intensive analytical work-Bitmap indexes are widely used for read-intensive analytical workloads because they are clustered and offer efficient reads with a small memory footprint. However, they are generally inefficient to update. As analytical applications are increasingly fused with transactional applications, leading to the emergence of hybrid trans-transactional applications, leading to the emergence of hybrid transactional/analytical processing (HTAP), it is desirable that bitmap indexes support efficient concurrent real-time updates. In this pa-indexes support efficient concurrent real-time updates. In this paper, we propose  C oncurrent  U pdatable  Bit map indexing (CUBIT) that offers efficient real-time updates that scale with the number of CPU cores used and do not interfere with queries. Our design relies on three principles. First, we employ a horizontal bitwise rep-relies on three principles. First, we employ a horizontal bitwise representation of updated bits, which enables efficient atomic updates without locking entire bitvectors. Second, we propose a lightweight snapshotting mechanism that allows queries to run on separate snapshots and provides a wait-free progress guarantee. Third, we consolidate updates in a latch-free manner, providing a strong progress guarantee. Our evaluation shows that CUBIT offers 3‚Äì16√ó higher throughput and 3‚Äì220√ó  lower latency than state-of-the-art updatable bitmap indexes. CUBIT‚Äôs update-friendly nature widens the applicability of bitmap indexing. Experimenting with OLAP workloads with standard, batched updates shows that  CUBIT  over-workloads with standard, batched updates shows that CUBIT  overcomes the maintenance downtime and outperforms DuckDB by 1.2‚Äì2.7√ó  on TPC-H. For HTAP workloads with real-time updates, CUBIT achieves 2‚Äì11√ó  performance improvement over the state-CUBIT  achieves 2‚Äì11√ó  performance improvement over the stateof-the-art approaches.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/CUBIT%3A%20Concurrent%20Updatable%20Bitmap%20Indexing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p399-athanassoulis.pdf",
    "session": "Research 24: Database Engines I",
    "authors": [
      {
        "Name": "Junchang Wang",
        "Affiliation": "Nanjing University of Posts and Telecommunications"
      },
      {
        "Name": "Manos Athanassoulis",
        "Affiliation": "Boston University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "966ead1a-d56c-486f-8c05-70fa50ee5e90",
    "title": "WeShap: Weak Supervision Source Evaluation with Shapley Values",
    "abstract": "Efficient data annotation stands as a significant bottleneck in training contemporary machine learning models. The Programmatic Weak Supervision (PWS) pipeline presents a solution by utilizing multiple weak supervision sources to automatically label data, thereby expediting the annotation process. Given the varied contributions of these weak supervision sources to the accuracy of PWS, it is imperative to employ a robust and efficient metric for their evaluation. This is crucial not only for understanding the behavior and performance of the PWS pipeline but also for facilitating corrective measures. \nIn this paper, we introduce WeShap values as an evaluation metric. This metric quantifies the average contribution of weak supervision sources within a proxy PWS pipeline, leveraging the theoretical underpinnings of Shapley values. We demonstrate efficient computation of WeShap values using dynamic programming, achieving quadratic computational complexity relative to the number of weak supervision sources. \nOur experiments demonstrate the versatility of WeShap values across various applications, including the identification of beneficial or detrimental labeling functions, refinement of the PWS pipeline, comprehension of the pipeline‚Äôs behavior, and scrutinizing specific instances of mislabeled data. Although initially derived from a specific proxy PWS pipeline, we empirically demonstrate the generalizability of WeShap values to other PWS pipeline configurations. Our findings indicate a noteworthy average improvement of 5.0 points in downstream model accuracy through the revision of the PWS pipeline compared to previous state-of-the-art methods, underscoring the efficacy of WeShap values in enhancing data quality for training machine learning models.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/WeShap%3A%20Weak%20Supervision%20Source%20Evaluation%20with%20Shapley%20Values",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1063-guan.pdf",
    "session": "Research 23: Applied ML and AI for Data Management III",
    "authors": [
      {
        "Name": "Naiqing Guan",
        "Affiliation": "University of Toronto"
      },
      {
        "Name": "Nick Koudas",
        "Affiliation": "University of Toronto"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d9273808-03f8-4f4b-8584-705e360be991",
    "title": "IncrCP: Decomposing and Orchestrating Incremental Checkpoints for Effective Recommendation Model Training",
    "abstract": "Training large models for modern recommendation systems requires a substantial number of computational devices and extended periods. Since it is essential to store model checkpoints throughout the training progress for accuracy debugging or mitigating potential failures, checkpointing systems are widely used. However, given that recommendation models can scale to hundreds of gigabytes or more, existing solutions often introduce significant overhead in terms of both storage and I/O. \nIn this paper, we present IncrCP, a checkpointing system specifically designed for recommendation models. Given that only a small fraction of model parameters are modified in each iteration, IncrCP creatively leverages the incremental checkpointing strategy and overcomes the inherent slow recovery problem. To support recovering all states throughout the training process, while also ensuring efficient storage utilization and rapid recovery, IncrCP proposes the 2-D chunk approach. It proactively records changed parameters in the training process as well as their indexes, extracts parameters according to duplicated indexes as independent chunk files, and then orchestrates these chunks in the 2-dimensional linked list. In this way, IncrCP achieves fast recovery by loading less unnecessary parameters and performing less deduplication during recovery. Furthermore, IncrCP includes a selective extraction approach to reduce I/O by avoiding worthless extractions and a concatenate approach to reduce random disk access when recovery. Evaluations show that IncrCP achieves up to 6.6√ó  recovery speedup compared to the naive incremental strategy and saves storage space by 60.4% with slight overhead compared to another recovery-friendly strategy.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/IncrCP%3A%20Decomposing%20and%20Orchestrating%20Incremental%20Checkpoints%20for%20Effective%20Recommendation%20Model%20Training",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1049-du.pdf",
    "session": "Research 29: Learned Database Systems",
    "authors": [
      {
        "Name": "Qingyin Lin",
        "Affiliation": "Sun Yat-sen University"
      },
      {
        "Name": "Jiangsu Du",
        "Affiliation": "Sun Yat-sen University"
      },
      {
        "Name": "RUI LI",
        "Affiliation": "Peng Cheng Laboratory"
      },
      {
        "Name": "Zhiguang Chen",
        "Affiliation": "Sun Yat-sen university"
      },
      {
        "Name": "Wenguang Chen",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Nong Xiao",
        "Affiliation": "Sun Yat-sen University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "27856118-0e81-45aa-8d3c-eac29bcaae9b",
    "title": "Unleash the Power of Ellipsis: Accuracy-enhanced Sparse Vector Technique with Exponential Noise",
    "abstract": "The Sparse Vector Technique (SVT) is one of the most fundamental tools in differential privacy (DP). It works as a backbone for adap- tive data analysis by answering a sequence of queries on a given dataset, and gleaning useful information in a privacy-preserving manner. Unlike the typical private query releases that directly pub- licize the noisy query results, SVT is less informative‚Äîit keeps the noisy query results to itself and only reveals a binary bit for each query, indicating whether the query result surpasses a predeÔÄ¢ned threshold. To provide a rigorous DP guarantee for SVT, prior works in the literature adopt a conservative privacy analysis by assuming the direct disclosure of noisy query results as in typical private query releases. This approach, however, hinders SVT from achiev- ing higher query accuracy due to an overestimation of the privacy risks, which further leads to an excessive noise injection using the Laplacian or Gaussian noise for perturbation. Motivated by this, we provide a new privacy analysis for SVT by considering its less informative nature. Our analysis results not only broaden the range of applicable noise types for perturbation in SVT, but also identify the exponential noise as optimal among all evaluated noises (which, however, is usually deemed non-applicable in prior works). The main challenge in applying exponential noise to SVT is mitigating the sub-optimal performance due to the bias introduced by noise distributions. To address this, we develop a utility-oriented optimal threshold correction method and an appending strategy, which enhances the performance of SVT by increasing the precision and recall, respectively. The eÔÄ°ectiveness of our proposed methods is substantiated both theoretically and empirically, demonstrating signiÔÄ¢cant improvements up to 50% across evaluated metrics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Accuracy-enhanced%20Sparse%20Vector%20Technique%20with%20Exponential%20Noise%20and%20Optimal%20Threshold%20Correction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p187-liu.pdf",
    "session": "Research 18: Data Privacy and Security I",
    "authors": [
      {
        "Name": "Yuhan Liu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Sheng Wang",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Yixuan Liu",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Feifei Li",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Hong Chen",
        "Affiliation": "\" Renmin University, China\""
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d5cb1168-5e31-4b4d-a9e9-6bdba919104f",
    "title": "The Cost of Representation by Subset Repairs",
    "abstract": "Datasets may include errors, and specifically violations of integrity constraints, for various reasons. Standard techniques for ‚Äúminimal-constraints, for various reasons. Standard techniques for ‚Äúminimalcost‚Äù database repairing resolve these violations by aiming for a minimum change in the data, and in the process, may sway rep-minimum change in the data, and in the process, may sway representations of different sub-populations. For instance, the repair may end up deleting more females than males, or more tuples from a certain age group or race, due to varying levels of inconsistency in different sub-populations. Such repaired data can mislead con-in different sub-populations. Such repaired data can mislead consumers when used for analytics, and can lead to biased decisions for downstream machine learning tasks. We study the ‚Äúcost of repre-downstream machine learning tasks. We study the ‚Äúcost of representation‚Äù in subset repairs for functional dependencies. In simple terms, we target the question of how many additional tuples have to be deleted if we want to satisfy not only the integrity constraints but also representation constraints for given sub-populations. We study the complexity of this problem and compare it with the complexity of optimal subset repairs without representations. While the prob-of optimal subset repairs without representations. While the problem is NP-hard in general, we give polynomial-time algorithms for special cases, and efficient heuristics for general cases. We perform a suite of experiments that show the effectiveness of our algorithms in computing or approximating the cost of representation.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/The%20Cost%20of%20Representation%20by%20Subset%20Repairs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p475-liu.pdf",
    "session": "Research 21: Specialized and Domain-Specific Data Management",
    "authors": [
      {
        "Name": "Yuxi Liu",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Fangzhu Shen",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Kushagra Ghosh",
        "Affiliation": "Duke University"
      },
      {
        "Name": "Amir Gilad",
        "Affiliation": "The Hebrew University"
      },
      {
        "Name": "Benny Kimelfeld",
        "Affiliation": "Technion"
      },
      {
        "Name": "Sudeepa Roy",
        "Affiliation": "Duke University, USA"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "9e77e927-4530-4493-bf2c-d10275303690",
    "title": "Efficiently Joining Large Relations on Multi-GPU Systems",
    "abstract": "Growing data volumes present a mounting challenge to relational joins. GPUs have gained widespread adoption as database accelerators for operators such as joins due to their high instruction throughput and memory bandwidth. Most published GPU-accelerated joins are single-GPU algorithms that do not leverage modern multi-GPU platforms effectively. The few proposed multi-GPU algorithms either fail to exploit the high-speed P2P interconnects between the GPUs or to handle large out-of-core data natively. In this paper, we present a heterogeneous multi-GPU sort-merge join that overcomes both limitations. It is composed of a merge- or radix partitioningbased P2P-enabled multi-GPU  sort  phase, a parallel CPU-based multiway  merge  phase, and a hybrid  join  phase that combines a CPU merge path partition with a binary search-based multi-GPU join strategy. We evaluate our novel multi-GPU join on two platforms with fast NVLink- and NVSwitch-based P2P interconnects. We show that our join outperforms state-of-the-art CPU and GPU baselines regardless of the workload. It outperforms parallel CPU sort-merge and radix-hash joins by up to  15.2 √ó  and  5.5 √ó , respectively. Compared to non-P2P-enabled multi-GPU joins, it achieves speedups of  8.7 √ó  (sort-merge) and  2.5 √ó  (hybrid-radix). We measure that our join‚Äôs hybrid join phase with overlapped copy and compute operations contributes as little as  22%  to its end-to-end runtime. If the input relations are pre-sorted, it is up to  14.4 √ó  faster than the hybrid-radix join. Our join scales well with the number of GPUs and benefits from data skew with as much as  12%  shorter join durations.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Efficiently%20Joining%20Large%20Relations%20on%20Multi-GPU%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4653-maltenberger.pdf",
    "session": "Research 24: Database Engines I",
    "authors": [
      {
        "Name": "Tobias Maltenberger",
        "Affiliation": "Hasso Plattner Institute"
      },
      {
        "Name": "Ilin Tolovski",
        "Affiliation": "Hasso Plattner Institute"
      },
      {
        "Name": "Tilmann Rabl",
        "Affiliation": "Hasso Plattner Institute"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "21ed67f6-6184-4f9d-9abf-fae37d045da9",
    "title": "SIEVE: Effective Filtered Vector Search with Collection of Indexes",
    "abstract": "Real-world tasks such as recommending videos tagged  kids  can be reduced to finding  similar  vectors associated with  hard  predicates. This task,  filtered vector search , is challenging as prior state-of-theart graph-based (unfiltered) similarity search techniques degenerate when hard constraints are considered: effective graph-based filtered similarity search relies on sufficient connectivity for reaching similar items within a few hops. To consider predicates, recent works propose modifying graph traversal to visit only items that satisfy predicates. However, they fail to offer the just-a-few-hops property for a wide range of predicates: they must restrict predicates significantly or lose efficiency if only few items satisfy predicates. \nWe propose an opposite approach: instead of constraining traversal, we build many indexes each serving different predicate forms. For effective construction, we devise a three-dimensional analytical model capturing relationships among index size, search time, and recall, with which we follow a workload-aware approach to pack as many useful indexes as possible into a collection. At query time, the analytical model is employed yet again to discern the one that offers the fastest search at a given recall. We show superior performance and support on datasets with varying selectivities and forms: our approach achieves up to 8.06 √ó  speedup while having as low as 1% build time versus other indexes, with less than 2.15 √ó  memory of a standard HNSW graph and modest knowledge of past workloads.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/SIEVE%3A%20Effective%20Filtered%20Vector%20Search%20with%20Collection%20of%20Indexes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4723-li.pdf",
    "session": "Research 40: Time Series and Vector Data",
    "authors": [
      {
        "Name": "Zhaoheng Li",
        "Affiliation": "University of Illinois at Urbana-Champaign"
      },
      {
        "Name": "Silu Huang",
        "Affiliation": "Bytedance Inc"
      },
      {
        "Name": "Wei Ding",
        "Affiliation": "Bytedance Inc."
      },
      {
        "Name": "Yongjoo Park",
        "Affiliation": "University of Illinois Urbana-Champaign"
      },
      {
        "Name": "Jianjun Chen",
        "Affiliation": "Bytedance Inc."
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "03ac15e9-0905-4a5f-bd68-57db24bca70c",
    "title": "MILLION: A General Multi-Objective Framework with Controllable Risk for Portfolio Management",
    "abstract": "Portfolio management is an important yet challenging task in AI for FinTech, which aims to allocate investors‚Äô budgets among dif-for FinTech, which aims to allocate investors‚Äô budgets among different assets to balance the risk and return of an investment. In this study, we propose a general  M ulti-object I ve framework with contro LL able r I sk for p O rtfolio ma N agement ( MILLION ), which consists of two main phases, i.e., return-related maximization and risk control. Specifically, in the return-related maximization phase, we introduce two auxiliary objectives, i.e., return rate prediction, and return rate ranking, combined with portfolio optimization to remit the overfitting problem and improve the generalization of the trained model to future markets. Subsequently, in the risk control phase, we propose two methods, i.e., portfolio interpolation and portfolio improvement, to achieve fine-grained risk control and fast risk adaption to a user-specified risk level. For the portfolio interpo-risk adaption to a user-specified risk level. For the portfolio interpolation method, we theoretically prove that the risk can be perfectly controlled if the to-be-set risk level is in a proper interval. In ad-controlled if the to-be-set risk level is in a proper interval. In addition, we also show that the return rate of the adjusted portfolio after portfolio interpolation is no less than that of the min-variance optimization, as long as the model in the reward maximization phase is effective. Furthermore, the portfolio improvement method can achieve greater return rates while keeping the same risk level compared to portfolio interpolation. Extensive experiments are con-compared to portfolio interpolation. Extensive experiments are conducted on three real-world datasets. The results demonstrate the effectiveness and efficiency of the proposed framework.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/MILLION%3A%20A%20General%20Multi-Objective%20Framework%20with%20Controllable%20Risk%20for%20Portfolio%20Management",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p466-zheng.pdf",
    "session": "Research 56: Analytics over Different Data Types III",
    "authors": [
      {
        "Name": "Liwei Deng",
        "Affiliation": "University of Electronic Science and Technology of China"
      },
      {
        "Name": "Tianfu Wang",
        "Affiliation": "University of Science and Technology of China"
      },
      {
        "Name": "Yan Zhao",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Kai Zheng",
        "Affiliation": "University of Electronic Science and Technology of China"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "6ef7a9d1-a9b2-4285-bf15-f107bbfb3656",
    "title": "BURST: Rendering Clustering Techniques Suitable for Evolving Streams",
    "abstract": "Identifying patterns or clusters in streaming time-series data is crucial for decision-making, and underpins applications such as anomaly detection, forecasting, and data quality monitoring. While numerous clustering algorithms have been proposed, many remain unexplored in the time-series domain, and others are unsuitable for streaming scenarios. Moreover, many effective methods require prior knowledge of the number of clusters, a significant limitation when dealing with evolving data streams. To address these challenges, we propose BURST, a principled and general-purpose framework that enables the application of partition-based clustering methods in streaming time-series settings. At its core, BURST integrates AutoKC, a novel, adaptive algorithm for automatically estimating the number of clusters, enhancing robustness to evolving time-series streams. Experimental analyses show that BURST is a robust strategy for real-time time-series clustering, effectively generalizing across different partitioning methods, and achieving state-of-the-art performance compared to existing algorithms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/BURST%3A%20Rendering%20Clustering%20Techniques%20Suitable%20for%20Evolving%20Streams",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4054-giannoulidis.pdf",
    "session": "Research 33: Data Stream Mining",
    "authors": [
      {
        "Name": "Apostolos Giannoulidis",
        "Affiliation": "Aristotle University of Thessaloniki"
      },
      {
        "Name": "Anastasios Gounaris",
        "Affiliation": "Aristotle University of Thessaloniki"
      },
      {
        "Name": "John Paparrizos",
        "Affiliation": "The Ohio State University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "2062ae33-7a6a-4a10-8603-f33ef7a7b3f9",
    "title": "Oze: Decentralized Graph-based Concurrency Control for Long-running Update Transactions",
    "abstract": "This paper proposes Oze, a concurrency control protocol that handles heterogeneous workloads, including long-running update transactions. Oze explores a large scheduling space using a multi-version serialization graph to reduce false positives. Oze manages the graph in a decentralized manner to exploit many cores in modern servers. We further propose an OLTP benchmark, BoMB (Bill of Materials Benchmark), based on a use case in an actual manufacturing company. BoMB consists of one long-running update transaction and five short transactions that conflict with each other. Experiments using BoMB show that Oze can handle the long-running update transaction while achieving four orders of magnitude higher throughput than state-of-the-art optimistic and multi-version protocols and up to five times higher throughput than pessimistic protocols. We also show Oze performs comparably with existing techniques even in a typical OLTP workload, TPC-C, thanks to a protocol switching mechanism.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Oze%3A%20Decentralized%20Graph-based%20Concurrency%20Control%20for%20Long-running%20Update%20Transactions",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2321-nemoto.pdf",
    "session": "Research 44: Database Engine Performance and Manageability I",
    "authors": [
      {
        "Name": "Jun Nemoto",
        "Affiliation": "Scalar, Inc."
      },
      {
        "Name": "Taksahi Kambayashi",
        "Affiliation": "Nautilus-technologies.co.ltd"
      },
      {
        "Name": "Takashi Hoshino",
        "Affiliation": "Cybozu Labs, Inc."
      },
      {
        "Name": "Hideyuki Kawashima",
        "Affiliation": "Keio University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "af590efe-652d-4550-917d-42800d972c3b",
    "title": "A Comprehensive Survey and Experimental Study of Learning-based Community Search",
    "abstract": "Given a graph  G and a query node  q , the goal of community search (CS) is to Ô¨Ånd a structurally cohesive subgraph from  G that contains q. SigniÔ¨Åcant progress has been made in community search using deep learning in recent years. To the best of our knowledge, no existing work has provided a comprehensive review of learning-based community search methods. Additionally, we Ô¨Ånd that: (1) Existing methods oÔ¨Äer diverse deÔ¨Ånitions or descriptions of communities, which require systematic summarization. (2) The methods rely on distinct metrics for limited community assessment. (3) Overhead evaluations of the methods vary and exhibit certain biases. \nTherefore, a comprehensive survey and experimental study are essential to achieve four key objectives: designing a uniÔ¨Åed pipeline, clarifying community deÔ¨Ånitions, enriching community evaluation, and establishing overhead assessment. In this paper, we Ô¨Årst propose a uniÔ¨Åed pipeline for these methods, highlighting techniques. We categorize community deÔ¨Ånitions and analyze the relationships between identiÔ¨Åed communities. Beyond that, we proposed several community metrics to evaluate the communities comprehensively. Moreover, we introduce a more detailed overhead evaluation approach that considers resource consumption during both the training and search phases. Finally, we employ the proposed community evaluation metrics and overhead assessment framework to evaluate and analyze the methods, examine correlations among metrics, and explore the eÔ¨Äects of several commonly used techniques.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Comprehensive%20Survey%20and%20Experimental%20Study%20of%20Learning-based%20Community%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2941-gou.pdf",
    "session": "Research 28: Social Networks",
    "authors": [
      {
        "Name": "Xiaoxuan Gou",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Weiguo Zheng",
        "Affiliation": "Fudan University"
      },
      {
        "Name": "Yuxiang Wang",
        "Affiliation": "Hangzhou Dianzi University"
      },
      {
        "Name": "Xiaoliang Xu",
        "Affiliation": "Hangzhou Dianzi University"
      },
      {
        "Name": "Zhiyuan Yu",
        "Affiliation": "Hangzhou Dianzi University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fd84e6c4-8a83-4965-8c03-976168c4e589",
    "title": "Privacy-Enhanced Database Synthesis for Benchmark Publishing",
    "abstract": "Benchmarking is crucial for evaluating a DBMS, yet existing bench-Benchmarking is crucial for evaluating a DBMS, yet existing benchmarks often fail to reflect the varied nature of user workloads. As a result, there is increasing momentum toward creating databases that incorporate real-world user data to more accurately mirror business environments. However, privacy concerns deter users from directly sharing their data, underscoring the importance of creat-directly sharing their data, underscoring the importance of creating synthesized databases for benchmarking that also prioritize privacy protection. Differential privacy (DP)-based data synthesis has become a key method for safeguarding privacy when sharing data, but the focus has largely been on minimizing errors in aggre-data, but the focus has largely been on minimizing errors in aggregate queries or downstream ML tasks, with less attention given to benchmarking factors like query runtime performance. This paper delves into differentially private database synthesis specifically for benchmark publishing scenarios, aiming to produce a synthetic database whose benchmarking factors closely resemble those of the original data. Introducing  PrivBench , an innovative synthesis framework based on sum-product networks (SPNs), we support the synthesis of high-quality benchmark databases that maintain fidelity in both data distribution and query runtime performance while preserving privacy. We validate that PrivBench can ensure database-level DP even when generating multi-relation databases with complex reference relationships. Our extensive experiments show that PrivBench efficiently synthesizes data that maintains privacy and excels in both data distribution similarity and query runtime similarity.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Privacy-Enhanced%20Database%20Synthesis%20for%20Benchmark%20Publishing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p413-zheng.pdf",
    "session": "Research 50: Access Control and Privacy, Blockchain",
    "authors": [
      {
        "Name": "Yunqing Ge",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Jianbin Qin",
        "Affiliation": "Shenzhen Institute of Computing Sciences, Shenzhen University"
      },
      {
        "Name": "Shuyuan Zheng",
        "Affiliation": "Osaka University"
      },
      {
        "Name": "Yongrui Zhong",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Bo Tang",
        "Affiliation": "Southern University of Science and Technology"
      },
      {
        "Name": "Yu-Xuan Qiu",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Rui Mao",
        "Affiliation": "Shenzhen University"
      },
      {
        "Name": "Ye Yuan",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Makoto Onizuka",
        "Affiliation": "Osaka University"
      },
      {
        "Name": "Chuan Xiao",
        "Affiliation": "Osaka University, Nagoya University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "29eedeef-3ac9-4ba2-861e-ed25497ddd3b",
    "title": "ThriftLLM: On Cost-Effective Selection of Large Language Models for Classification Queries",
    "abstract": "Recently, large language models (LLMs) have demonstrated remarkable capabilities in understanding and generating natural language content, attracting widespread attention in both industry and academia. An increasing number of services oÔ¨Äer LLMs for various tasks via APIs. DiÔ¨Äerent LLMs demonstrate expertise in diÔ¨Äerent domains of queries (e.g., text classiÔ¨Åcation queries). Meanwhile, LLMs of diÔ¨Äerent scales, complexities, and performance are priced diversely. Driven by this, several researchers are investigating strategies for selecting an ensemble of LLMs, aiming to decrease overall usage costs while enhancing performance. However, to our best knowledge, none of the existing works addresses the problem, how to Ô¨Ånd an LLM ensemble subject to a cost budget, which maximizes the ensemble performance with guarantees. \nIn this paper, we formalize the performance of an ensemble of models (LLMs) using the notion of correctness probability, which we formally deÔ¨Åne. We develop an approach for aggregating responses from multiple LLMs to enhance ensemble performance. Building on this, we formulate the Optimal Ensemble Selection (OES) problem of selecting a set of LLMs subject to a cost budget that maximizes the overall correctness probability. We show that the correctness probability function is non-decreasing and nonsubmodular and provide evidence that the OES problem is likely to be NP-hard. By leveraging a submodular function that upper bounds correctness probability, we develop an algorithm,  ThrifLLM , and prove that it achieves an instance-dependent approximation guarantee with high probability. Our framework functions as a data processing system that selects appropriate LLM operators to deliver high-quality results under budget constraints. It achieves state-ofthe-art performance for text classiÔ¨Åcation and entity matching queries on multiple real-world datasets against various baselines in our extensive experimental evaluation, while using a relatively lower cost budget, strongly supporting the eÔ¨Äectiveness and superiority of our method.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ThriftLLM%3A%20On%20Cost-Effective%20Selection%20of%20Large%20Language%20Models%20for%20Classification%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4410-huang.pdf",
    "session": "Research 2: Applied ML and AI for Data Management I",
    "authors": [
      {
        "Name": "Keke HUANG",
        "Affiliation": "The University of British Columbia"
      },
      {
        "Name": "Yimin Shi",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Dujian Ding",
        "Affiliation": "University of British Columbia"
      },
      {
        "Name": "Yifei Li",
        "Affiliation": "University of British Columbia"
      },
      {
        "Name": "Yang Fei",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Laks Lakshmanan",
        "Affiliation": "University of British Columbia"
      },
      {
        "Name": "Xiaokui Xiao",
        "Affiliation": "National University of Singapore"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "ae24ebc1-bd15-4536-9d37-46c53c05ebab",
    "title": "Meaningful Data Erasure in the Presence of Dependencies",
    "abstract": "Data regulations like GDPR require systems to support data erasure but leave the definition of \"erasure\" open to interpretation. This ambiguity makes compliance challenging, especially in databases where data dependencies can lead to erased data being inferred from remaining data. We formally define a precise notion of data erasure that ensures any inference about deleted data, through dependencies, remains bounded to what could have been inferred before its insertion. We design erasure mechanisms that enforce this guarantee at minimal cost. Additionally, we explore strategies to balance cost and throughput, batch multiple erasures, and proactively compute data retention times when possible. We demonstrate the practicality and scalability of our algorithms using both real and synthetic datasets.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Meaningful%20Data%20Erasure%20in%20the%20Presence%20of%20Dependencies",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3435-chakraborty.pdf",
    "session": "Research 18: Data Privacy and Security I",
    "authors": [
      {
        "Name": "Vishal Chakraborty",
        "Affiliation": "University of California, Irvine"
      },
      {
        "Name": "Youri Kaminsky",
        "Affiliation": "HPI"
      },
      {
        "Name": "Sharad Mehrotra",
        "Affiliation": "UCI"
      },
      {
        "Name": "Felix Naumann",
        "Affiliation": "HPI"
      },
      {
        "Name": "Faisal Nawab",
        "Affiliation": "UCI"
      },
      {
        "Name": "Primal Pappachan",
        "Affiliation": "Portland State University"
      },
      {
        "Name": "Mohammad Sadoghi",
        "Affiliation": "UC, Davis"
      },
      {
        "Name": "Nalini Venkatasubramanian",
        "Affiliation": "UCI"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "31de9926-3b85-449c-95dc-6b22d2dd8623",
    "title": "TAB: Unified Benchmarking of Time Series Anomaly Detection Methods",
    "abstract": "Time series anomaly detection (TSAD) plays an important role in many domains such as finance, transportation, and healthcare. With the ongoing instrumentation of reality, more time series data will be available, leading also to growing demands for TSAD. While many TSAD methods already exist, new and better methods are still desirable. However, effective progress hinges on the availability of reliable means of evaluating new methods and comparing them with existing methods. We address deficiencies in current evaluation procedures related to datasets and experimental settings and protocols. Specifically, we propose a new time series anomaly detection benchmark, called TAB. First, TAB encompasses 29 public multivariate datasets and 1,635 univariate time series from different domains to facilitate more comprehensive evaluations on diverse datasets. Second, TAB covers a variety of TSAD methods, including Non-learning, Machine learning, Deep learning, LLM-based, and Time-series pre-trained methods. Third, TAB features a unified and automated evaluation pipeline that enables fair and easy evaluation of TSAD methods. Finally, we employ TAB to evaluate existing TSAD methods and report on the outcomes, thereby offering a deeper insight into the performance of these methods.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/TAB%3A%20Unified%20Benchmarking%20of%20Time%20Series%20Anomaly%20Detection%20Methods",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2775-hu.pdf",
    "session": "Research 12: Time Series Data I",
    "authors": [
      {
        "Name": "xiangfei qiu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "zhe li",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "wanghui qiu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "shiyan Hu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "lekui zhou",
        "Affiliation": "Huawei Cloud"
      },
      {
        "Name": "xingjian wu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "zhengyu li",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Chenjuan Guo",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Aoying Zhou",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Zhenli Sheng",
        "Affiliation": "Huawei Cloud"
      },
      {
        "Name": "Jilin Hu",
        "Affiliation": "East China Normal University"
      },
      {
        "Name": "Christian S. Jensen",
        "Affiliation": "Aalborg University"
      },
      {
        "Name": "Bin Yang",
        "Affiliation": "East China Normal University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "bbe31d35-359c-4e57-88ba-66f9cb03c25b",
    "title": "EinDecomp: Decomposition of Declaratively-Specified Machine Learning and Numerical Computations for Parallel Execution",
    "abstract": "We consider the problem of automatic parallelism in high-performance, tensor-based systems. Our focus is on  intra-operator parallelism  for inference tasks on a single GPU server or CPU cluster, where each operator is automatically broken op so that it runs on multiple devices. We assert that tensor-based systems should offer a programming abstraction based on an  extended Einstein summation notation , which is a fully declarative, mathematical specification for tensor computations. We show that any computation specified in the Einstein summation notation can be re-written into an equivalent  tensor-relational  computation that facilitates intra-operator parallelism, and this re-write generalizes existing notations of tensor parallelism such as ‚Äúdata parallel‚Äù and ‚Äúmodel parallel.‚Äù We consider the algorithmic problem of optimally computing a tensorrelational decomposition of a graph of operations specified in our extended Einstein summation notation.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/EinDecomp%3A%20Decomposition%20of%20Declaratively-Specified%20Machine%20Learning%20and%20Numerical%20Computations%20for%20Parallel%20Execution",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2240-bourgeois.pdf",
    "session": "Research 23: Applied ML and AI for Data Management III",
    "authors": [
      {
        "Name": "Daniel Bourgeois",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Zhimin Ding",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Dimitrije Jankov",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Jiehui Li",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Mahmoud Sleem",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Yuxin Tang",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Jiawen Yao",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Xinyu Yao",
        "Affiliation": "Rice University"
      },
      {
        "Name": "Chris Jermaine",
        "Affiliation": "Rice University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "666c8bb0-f715-4b01-b622-38cbeeb9bdeb",
    "title": "Simple Testing Can Expose Most Critical Transaction Bugs: Understanding and Detecting Write-Specific Serializability Violations in Database Systems",
    "abstract": "Database Management Systems (DBMSs) utilize transactions to guarantee data consistency and integrity. Incorrect implementations of transaction processing mechanisms can introduce critical transaction bugs, which can lead to incorrect database states after the involved transactions complete. However, we lack an effective test oracle to determine whether a DBMS produces a correct database state for a given concurrent transaction schedule. \nIn this paper, we propose a general property for concurrent transaction schedules,  write-specific serializability , in which a schedule of concurrent transactions should produce the same database state as a corresponding serial schedule of the same transactions. Through our empirical study on 35 critical transaction bugs collected from six widely-used DBMSs, we find that write-specific serializability can be an effective test oracle to expose critical transaction bugs in DBMSs. We further develop a simple and general transaction testing approach,  WriteCheck , to automatically detect write-specific serializability violations by identifying inconsistencies in the final database states produced by the original transaction schedule and its corresponding serial schedule. We evaluate  WriteCheck  on the latest versions of six production-grade DBMSs, and have found 22 write-specific serializability violations, 11 of which have been confirmed as new critical transaction bugs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Simple%20Testing%20Can%20Expose%20Most%20Critical%20Transaction%20Bugs%3A%20Understanding%20and%20Detecting%20Write-Specific%20Serializability%20Violations%20in%20Database%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2547-cui.pdf",
    "session": "Research 52: Transaction Management",
    "authors": [
      {
        "Name": "Ziyu Cui",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Wensheng Dou",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Yu Gao",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Rui Yang",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Yingying Zheng",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Jiansen Song",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      },
      {
        "Name": "Yuan Feng",
        "Affiliation": "Wuhan Dameng Database Co.,Ltd"
      },
      {
        "Name": "Jun Wei",
        "Affiliation": "Institute of Software Chinese Academy of Sciences"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a515297e-8431-429c-ab14-31f888c47554",
    "title": "Ranking Indicator Discovery from Very Large Knowledge Graphs",
    "abstract": "Ranking indicators are essential tools for comparing the importance of various entities such as cities or scientists. While extensively used in fields like econometrics and scientometrics, many other domains lack systematic approaches for developing these indicators. In this paper, we introduce a novel method for automatically discovering ranking indicators from very large knowledge graphs. To this end, we formalize the notion of  counting graph pattern  (CG) as a special SPARQL query, and the concept of  ideal ranking indicator  as a CG whose result induces a strict total order on a set of entities. To assess the interestingness of ranking indicators, we employ the proportion of covered entities  along with an inequality measure, namely the Gini coefficient. We further present Algorithm  Ranking Indicator Pattern Miner ( RIPM ) , to efficiently identify interesting ranking indicators for a given field, thanks to pruning techniques for handling the very large search space. Our experimental study shows the effectiveness of our optimizations. It also validates that  RIPM extracts  transparent, diverse, and understandable  indicators through a user survey and a comparison with two baselines. This work has significant implications for fields lacking dedicated communities working on ranking tasks, providing a robust tool to automatically produce ranking indicators, and the associated rankings.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Ranking%20Indicator%20Discovery%20from%20Very%20Large%20Knowledge%20Graphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1183-abdallah.pdf",
    "session": "Research 43: Information Integration and Data Quality II",
    "authors": [
      {
        "Name": "Hassan Abdallah",
        "Affiliation": "University of Tours"
      },
      {
        "Name": "B√©atrice Markhoff",
        "Affiliation": "University of Tours"
      },
      {
        "Name": "Arnaud Soulet",
        "Affiliation": "University of Tours, France"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e08c0aea-07d0-407c-be52-cce53c7787b3",
    "title": "GRewriter: Practical Query Rewriting with Automatic Rule Set Expansion in GaussDB",
    "abstract": "Effectively rewriting a wide range of complex and diverse queries is critical for database systems. Huawei GaussDB has been experiencing limited extensibility of its existing query rewriter. The problem is rooted in the need for one-size-fits-all rewrites by its pipelined rewrite workflow and the source code-level coupling of rewrite logic. This makes it not only difficult to identify generic, broadly applicable rewrites but also engineering-intensive to program them into the system. \nThis paper presents GRewriter, GaussDB‚Äôs new bolt-on extensible query rewriter powered by automated rewrite rule discovery. GRewriter sits atop the existing optimizer stack to explore useful rewrites, allowing a variety of rules to coexist and be selected on a per-query basis. A new rule language, G-DSL, is used to express rewrite rules so that the rewrite engine is not coupled with specific rules. To improve rewrite efficiency, a new rule index structure and a rewrite history cache are introduced. Rules in GRewriter are produced by an offline rule generator. With novel enumeration techniques and a new equivalence theorem, our rule generator can efficiently discover formally verified rules that are much more expressive than prior research prototypes. For operational convenience, GRewriter also supports manual rule authoring and interactive management of rules through familiar SQL interfaces. \nGRewriter has been integrated into GaussDB and is gradually rolling out to customers. GRewriter equips GaussDB with over a hundred rules while maintaining negligible overhead ( < 1%). These new rewrite rules have enhanced query performance for two key customer applications, an ERP system and a Banking transaction system, reducing production query latency by up to 99.9%‚Äîfrom 26 seconds to just 17 milliseconds.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/GRewriter%3A%20Practical%20Query%20Rewriting%20with%20Automatic%20Rule%20Set%20Expansion%20in%20GaussDB",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4991-jiang.pdf",
    "session": "Industry 6: Database Engines",
    "authors": [
      {
        "Name": "Zhe Jiang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Zhaoguo Wang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Haoning Lan",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Chuzhe Tang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Haoran Ding",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Lefeng Wang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Songyun Zou",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Zhuoran Wei",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Yongcun Liu",
        "Affiliation": "Huawei Technologies Co."
      },
      {
        "Name": "Xiang Yu",
        "Affiliation": "Huawei Technologies Co."
      },
      {
        "Name": "Yang Ren",
        "Affiliation": "Huawei Technologies Co."
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Haibo Chen",
        "Affiliation": "Shanghai Jiao Tong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "dd2eb019-1c1e-4af9-8668-78db468d1381",
    "title": "Continuous Lifelong Conflict-Aware AGV Routing with Kinematic Constraints",
    "abstract": "Automated Guided Vehicles (AGV) are becoming increasingly important in modern warehouses to cope with the enormous logistic demands of developing e-commerce and the growing operational costs. The key component of implementing such a system is planning the paths of a large horde of AGVs to deliver orders from shelves to packing locations. The existing solutions regard it as a Multi Agent PathFinding (MAPF)  problem, but they can hardly be applied in practice because none of them could satisfy the continuous (temporal), lifelong (future task unknown and keeps appearing), kinematic (acceleration/deceleration/rotation), online (fast response), and scalability (large network, large AGV number, large task number) at the same time. Therefore, we Ô¨Årst propose an AGV routing framework that can satisfy all these properties with its corresponding routing algorithm. Then, to improve the eÔ¨Éciency, we propose the Multi-Hop ConÔ¨Çict-Aware Search method (MHCAS) with action combination, MHSC to reduce the search space, and OHSMD to decompose motions such that routing time is reduced by three orders of magnitude. Extensive experimental studies verify the superiority of our methods compared with the state-of-the-art.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Continuous%20Lifelong%20Conflict-Aware%20AGV%20Routing%20with%20Kinematic%20Constraints",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2254-li.pdf",
    "session": "Research 55: Spatial and Temporal Databases",
    "authors": [
      {
        "Name": "Ruizhong Wu",
        "Affiliation": "The Hong Kong University of Science and Technology"
      },
      {
        "Name": "Mengxuan Zhang",
        "Affiliation": "Australian National University"
      },
      {
        "Name": "Shuxin Wang",
        "Affiliation": "FLAIR"
      },
      {
        "Name": "Kin Sun Chan",
        "Affiliation": "HKFLAIR"
      },
      {
        "Name": "Yan Nei Law",
        "Affiliation": "Hong Kong Industrial Artificial Intelligence & Robotics Centre"
      },
      {
        "Name": "Lei Li",
        "Affiliation": "The Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "fe2742be-0c30-48ed-b109-9a56575a18e2",
    "title": "FedVSE: A Privacy-Preserving and Efficient Vector Search Engine for Federated Databases",
    "abstract": "Efficient vector search is a foundational capability of vector databases. However, most prior research overlooks its critical role in federated databases for applications like financial risk control and smart healthcare. In these privacy-sensitive scenarios, a vector search engine must not only deliver high performance but also guarantee privacy across federated databases. Current solutions, however, struggle with scalability for high-dimensional vectors, and offer limited query support. To bridge this gap, this paper introduces FedVSE, a privacy-preserving vector search engine for federated databases. FedVSE supports both KNN and hybrid queries, matching the versatility of modern vector databases. It leverages Intel SGX for hardware-enabled security and offers highly optimized query processing via indexing and pruning. Conference audiences can interact with FedVSE in real time and observe how it enables real-world services like cross-platform trajectory similarity search.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/FedVSE%3A%20A%20Privacy-Preserving%20and%20Efficient%20Vector%20Search%20Engine%20for%20Federated%20Databases",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5371-tong.pdf",
    "session": "Demo C2:",
    "authors": [],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "038f75aa-3846-4350-bf72-5d6c1c01b2bd",
    "title": "Towards Ideal Temporal Graph Neural Networks: Evaluations and Conclusions after 10,000 GPU Hours",
    "abstract": "Temporal Graph Neural Networks (TGNNs) have emerged as powerful tools for modeling dynamic interactions across various domains. The design space of TGNNs is notably complex, given the unique challenges in runtime efficiency and scalability raised by the evolving nature of temporal graphs. We contend that many of the existing works on TGNN modeling inadequately explore the design space, leading to suboptimal designs. Viewing TGNN models through a performance-focused lens often obstructs a deeper understanding of the advantages and disadvantages of each technique. Specifically, benchmarking efforts inherently evaluate models in their original designs and implementations, resulting in unclear accuracy comparisons and misleading runtime. To address these shortcomings, we propose a practical comparative evaluation framework that performs a design space search across well-known TGNN modules based on a unified, optimized code implementation. Using our framework, we make the first efforts towards addressing three critical questions in TGNN design, spending over 10,000 GPU hours: (1) investigating the efficiency of TGNN module designs, (2) analyzing how the effectiveness of these modules correlates with dataset patterns, and (3) exploring the interplay between multiple modules. Key outcomes of this directed investigative approach include demonstrating that the most recent neighbor sampling and attention aggregator outperform uniform neighbor sampling and MLP-Mixer aggregator; Assessing static node memory as an effective node memory alternative, and showing that the choice between static or dynamic node memory should be based on the repetition patterns in the dataset. Our in-depth analysis of the interplay between TGNN modules and dataset patterns should provide a deeper insight into TGNN performance along with potential research directions for designing more general and effective TGNNs.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Ideal%20Temporal%20Graph%20Neural%20Networks%3A%20Evaluations%20and%20Conclusions%20after%2010%2C000%20GPU%20Hours",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p956-yang.pdf",
    "session": "Research 37: Applied ML and AI for Graph, Temporal, and Trajectory Data",
    "authors": [
      {
        "Name": "Yuxin Yang",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "hongkuan zhou",
        "Affiliation": "University of Southern California"
      },
      {
        "Name": "Rajgopal Kannan",
        "Affiliation": "Devcom Army Research Lab"
      },
      {
        "Name": "Viktor Prasanna",
        "Affiliation": "Unversity of Southern California"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "10b834c1-9862-4436-8b8c-1fd1036ba75d",
    "title": "On LLM-Enhanced Mixed-Type Data Imputation with High-Order Message Passing",
    "abstract": "Missing data imputation, which aims to impute the missing values in the raw datasets, is crucial for modern data-driven models like large language models (LLMs). Despite its importance, existing solutions either 1) only support numerical and categorical data or 2) show an unsatisfactory performance due to their design prioritizing text data and overlooking intrinsic characteristics of tabular data. \nIn this paper, we propose UnIMP, a   Un ified   IMP utation framework that leverages LLM and high-order message passing to enhance the imputation of mixed-type data, including numerical, categorical, and text data. Specifically, we first introduce a celloriented hypergraph to model the table. We then propose BiHMP, an efficient   Bi directional   H igh-order   M essage - P assing network to aggregate global-local and high-order information while capturing the inter-column heterogeneity and intra-column homogeneity. To align the capacity of the LLM with the information aggregated by BiHMP, we introduce Xfusion, which, together with BiHMP, acts as adapters for the LLM. We follow a pre-training and fine-tuning pipeline to train UnIMP, integrating two optimizations: chunking technique, which divides tables into smaller chunks to enhance efficiency; and progressive masking technique, which gradually adapts the model to learn more complex data patterns. Both theoretical proofs and empirical experiments on 10 real-world datasets highlight the superiority of UnIMP over existing techniques.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/On%20LLM-Enhanced%20Mixed-Type%20Data%20Imputation%20with%20High-Order%20Message%20Passing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3421-wang.pdf",
    "session": "Research 5: Data Cleaning and Preparation I",
    "authors": [
      {
        "Name": "Jianwei Wang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Kai Wang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Ying Zhang",
        "Affiliation": "University of Technology Sydney"
      },
      {
        "Name": "Wenjie Zhang",
        "Affiliation": "University of New South Wales"
      },
      {
        "Name": "Xiwei Xu",
        "Affiliation": "Data61, CSIRO"
      },
      {
        "Name": "Xuemin Lin",
        "Affiliation": "Shanghai Jiao Tong University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "cf0e0fea-ae41-44a3-bbf9-94e0fc542e85",
    "title": "Truss Decomposition in Hypergraphs",
    "abstract": "Truss decomposition is a fundamental approach in graph theory that focuses on uncovering cohesive subgraphs within networks. However, many networks involve groupwise rather than pairwise relationships and are often represented as hypergraphs. Modeling and capturing  k-truss  in hypergraphs is essential for uncovering tight-knit relationships in such multi-relational networks. In this paper, we tackle the problem of truss decomposition in hypergraph. A  hyper  ùëò -truss  is a subgraph in which each node is part of at least  ùëò hyper-triangles. We first introduce a framework for hypertruss decomposition and determine that the most time-consuming component is counting hyper-triangles. To count all hyper-triangles efficiently, we propose an edge-iterator algorithm. To further reduce redundant computations, we present an improved algorithm that combines edge-iterator and node-iterator techniques to prune nonpromising nodes. Next, to handle common nodes in hypergraphs, we develop a novel prefix forest technique to encode all hyperedges and count triangles within this prefix forest. We also propose several optimization strategies that reorder nodes and hyperedges to improve work balancing. Finally, we conduct extensive experiments on real-world hypergraph datasets, demonstrating the efficiency and effectiveness of our algorithms.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Truss%20Decomposition%20in%20Hypergraphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p2185-qin.pdf",
    "session": "Research 49: Graph Data Management VI",
    "authors": [
      {
        "Name": "Hongchao Qin",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "guang zeng",
        "Affiliation": "Ant Group"
      },
      {
        "Name": "Ronghua Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Longlong Lin",
        "Affiliation": "Southwest University"
      },
      {
        "Name": "Ye Yuan",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guoren Wang",
        "Affiliation": "Beijing Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "abd025ec-3504-4231-aaf3-225b634881e4",
    "title": "Approximate Anchored Densest Subgraph Search on Large Static and Dynamic Graphs",
    "abstract": "Densest subgraph search, aiming to identify a subgraph with max-Densest subgraph search, aiming to identify a subgraph with maximum edge density, faces limitations as the edge density inade-imum edge density, faces limitations as the edge density inadequately reflects biases towards a given vertex set  ùëÖ . To address this, the  ùëÖ -subgraph density was introduced, refining the doubled edge density by penalizing vertices in a subgraph but not in  ùëÖ , us-edge density by penalizing vertices in a subgraph but not in  ùëÖ , using the degree as a penalty factor. This advancement leads to the Anchored Densest Subgraph (ADS) search problem, which finds the subgraph  ùëÜ ÀÜ   with the highest  ùëÖ -subgraph density for a given set  ùëÖ . Nonetheless, current algorithms for ADS search face signifi-set  ùëÖ . Nonetheless, current algorithms for ADS search face significant inefficiencies in handling large-scale graphs or the sizable  ùëÖ set. Furthermore, these algorithms require re-computing the ADS whenever the graph is updated, complicating the efficient main-whenever the graph is updated, complicating the efficient maintenance within dynamic graphs. To tackle these challenges, we propose the concept of integer  ùëÖ -subgraph density and study the problem of finding a subgraph  ùëÜ ‚àó ‚äÜ ùëâ with the highest integer ùëÖ -subgraph density. We reveal that the  ùëÖ -subgraph density of  ùëÜ ‚àó provides an additive approximation to that of ADS with a difference of less than 1, and hence  ùëÜ ‚àó is termed the Approximate Anchored Densest Subgraph (AADS). For searching the AADS, we present an efficient global algorithm incorporating the re-orientation network flow technique and binary search, operating in a time polynomial to the graph‚Äôs size. Additionally, we propose a novel local algorithm using shortest-path-based methods for the max-flow computation from  ùë† to  ùë° around  ùëÖ , markedly boosting performance in scenarios with larger  ùëÖ sets. For dynamic graphs, both basic and improved algorithms are developed to efficiently maintain the AADS when an edge is updated. Extensive experiments and a case study demon-an edge is updated. Extensive experiments and a case study demonstrate the efficiency, scalability, and effectiveness of our solutions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Approximate%20Anchored%20Densest%20Subgraph%20Search%20on%20Large%20Static%20and%20Dynamic%20Graphs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p623-li.pdf",
    "session": "Research 27: Graph Data Management III",
    "authors": [
      {
        "Name": "Qi Zhang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Yalong Zhang",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Ronghua Li",
        "Affiliation": "Beijing Institute of Technology"
      },
      {
        "Name": "Guoren Wang",
        "Affiliation": "Beijing Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "237aeeec-a8eb-407c-89d9-53eddcfd60a4",
    "title": "Cuckoo Heavy Keeper and the balancing act of maintaining heavy hitters in stream processing",
    "abstract": "Finding heavy hitters in databases and data streams is a fundamental problem with applications ranging from network monitoring to database query optimization, machine learning, and more. Approximation algorithms offer practical solutions, but they present tradeoffs involving throughput, memory usage, and accuracy. Moreover, modern applications further complicate these trade-offs by demanding capabilities beyond sequential processing that require both parallel scaling and support for concurrent queries and updates. \nAnalysis of these trade-offs led us to the key idea behind our proposed streaming algorithm,  Cuckoo Heavy Keeper  (CHK). The approach introduces an inverted process for distinguishing frequent from infrequent items, which unlocks new algorithmic synergies that were previously inaccessible with conventional approaches. By further analyzing the competing metrics with a focus on parallelism, we propose an algorithmic framework that balances scalability aspects and provides options to optimize query and insertion efficiency based on their relative frequencies. The framework is capable of parallelizing any heavy-hitter detection algorithm. \nBesides the algorithms‚Äô analysis, we present an extensive evaluation on both real-world and synthetic data across diverse distributions and query selectivity, representing the broad spectrum of application needs. Compared to state-of-the-art methods, CHK improves throughput by 1.7-5.7 √ó  and accuracy by up to four orders of magnitude even under low-skew data and tight memory constraints. These properties allow its parallel instances to achieve near-linear scale-up and low latency for heavy-hitter queries, even under a high query rate. We expect the versatility of CHK and its parallel instances to impact a broad spectrum of tools and applications in large-scale data analytics and stream processing systems.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Cuckoo%20Heavy%20Keeper%20and%20the%20balancing%20act%20of%20maintaining%20heavy%20hitters%20in%20stream%20processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3149-ngo.pdf",
    "session": "Research 39: Analytics over Different Data Types II",
    "authors": [
      {
        "Name": "Vinh Ngo",
        "Affiliation": "Chalmers University of Technology"
      },
      {
        "Name": "Marina Papatriantafilou",
        "Affiliation": "Chalmers University of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "10f0f6d6-5d0a-4efe-9d54-b9066aaabfd8",
    "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for Dynamic Graph Processing",
    "abstract": "An efficient data structure is fundamental to meeting the growing demands in dynamic graph processing. However, the dual requirements for graph computation efficiency (with contiguous structures) and graph update efficiency (with linked list-like structures) present a conflict in the design principles of graph structures. After experimental studies of state-of-the-art dynamic graph structures, we observe that the overhead of cache misses accounts for a major portion of the graph computation time. This paper presents GastCoCo, a system with graph storage and coroutine-based prefetch co-design. By employing software prefetching via stackless coroutines and designing a prefetch-friendly data structure CBList, GastCoCo significantly alleviates the performance degradation caused by cache misses. Our results show that GastCoCo outperforms state-of-the-art graph storage systems by 1.3√ó- 180√óin graph updates and 1.4√ó- 41.1√óin graph computation.",
    "conference_link": "https://www.vldb.org/pvldb/volumes/17/paper/GastCoCo%3A%20Graph%20Storage%20and%20Coroutine-Based%20Prefetch%20Co-Design%20for%20Dynamic%20Graph%20Processing",
    "pdf_link": "https://www.vldb.org/pvldb/vol17/p4827-li.pdf",
    "session": "Research 41: Graph Data Management V",
    "authors": [
      {
        "Name": "Hongfu Li",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Qian Tao",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Song Yu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Shufeng Gong",
        "Affiliation": "NorthEastern University"
      },
      {
        "Name": "Yanfeng Zhang",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Feng Yao",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Wenyuan Yu",
        "Affiliation": "Alibaba Group"
      },
      {
        "Name": "Ge Yu",
        "Affiliation": "Northeastern University"
      },
      {
        "Name": "Jingren Zhou",
        "Affiliation": "Alibaba Group"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "d73f7816-9a17-4265-b93a-5c25988d979a",
    "title": "POLARIS: An Interactive and Scalable Data Infrastructure for Polar Science",
    "abstract": "Though polar scientists entertain having huge amounts of publicly available datasets, they face the challenge that working with such data is a cumbersome process that requires downloading tons of unnecessary data and writing various scripts on top of it. This hinders their ability to perform any kind of interactive analysis. This paper presents Polaris; a novel open-source system infrastructure for   Polar  science that is highly   I nteractive and   S calable. Polaris is designed based on three observations that distinguish the query workload of polar scientists, namely, all queries are spatio-temporal, not all data are equal, and the large majority of queries are aggregates. Polaris is equipped with a hierarchical spatio-temporal index structure that stores precomputed aggregates for data of interest. Experimental results with a real Polaris prototype and real scientific data show that it achieves highly interactive and scalable data access, enabling interactive analysis of polar science data.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/POLARIS%3A%20An%20Interactive%20and%20Scalable%20Data%20Infrastructure%20for%20Polar%20Science",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4644-mokbel.pdf",
    "session": "Research 55: Spatial and Temporal Databases",
    "authors": [
      {
        "Name": "Yuchuan Huang",
        "Affiliation": "University of Minnesota"
      },
      {
        "Name": "Ana Elena Uribe",
        "Affiliation": "University of Minnesota"
      },
      {
        "Name": "Youssef Hussein",
        "Affiliation": "University of Minnesota"
      },
      {
        "Name": "Kareem Eldahshoury",
        "Affiliation": "University of Minnesota"
      },
      {
        "Name": "Grant Ogren",
        "Affiliation": "University of Minnesota"
      },
      {
        "Name": "Mohamed Mokbel",
        "Affiliation": "University of Minnesota - Twin Cities"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "8f1a6c58-e801-4cff-89ef-8d7a03229c62",
    "title": "D√©j√† Vu: Efficient Video-Language Query Engine with Learning-based Inter-Frame Computation Reuse",
    "abstract": "Recently, Video-Language Models (VideoLMs) have demonstrated remarkable capabilities, offering significant potential for flexible and powerful video query systems. These models typically rely on Vision Transformers (ViTs), which process video frames individually to extract visual embeddings. However, generating embeddings for large-scale videos requires ViT inferencing across numerous frames, posing a major hurdle to real-world deployment and necessitating solutions for integration into scalable video data management systems. This paper introduces  D√©j√† Vu , a video-language query engine that accelerates ViT-based VideoLMs by reusing computations across consecutive frames. At its core is  ReuseViT , a modified ViT model specifically designed for VideoLM tasks, which learns to detect inter-frame reuse opportunities, striking an effective balance between accuracy and reuse. Although  ReuseViT significantly reduces computation, these savings do not directly translate into performance gains on GPUs. To overcome this,  D√©j√† Vu  integrates memory-compute joint compaction techniques that convert the FLOP savings into tangible performance gains. Evaluations on three VideoLM tasks show that  D√©j√† Vu  accelerates embedding generation by up to a 2.64 √ó  within a 2% error bound, dramatically enhancing the practicality of VideoLMs for large-scale video analytics.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/D%C3%A9j%C3%A0%20Vu%3A%20Efficient%20Video-Language%20Query%20Engine%20with%20Learning-based%20Inter-Frame%20Computation%20Reuse",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3284-hwang.pdf",
    "session": "Research 64: Database Engines and Architectures",
    "authors": [
      {
        "Name": "Jinwoo Hwang",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Daeun Kim",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Sangyeop Lee",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Yoonsung Kim",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Guseul Heo",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Hojoon Kim",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Yunseok Jeong",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Tadiwos Meaza",
        "Affiliation": "KAIST"
      },
      {
        "Name": "Eunhyeok Park",
        "Affiliation": "POSTECH"
      },
      {
        "Name": "Jeongseob Ahn",
        "Affiliation": "Korea University"
      },
      {
        "Name": "Jongse Park",
        "Affiliation": "KAIST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "88613823-ade5-4cf4-a208-5c0427fc97d9",
    "title": "Avoiding Materialisation for Guarded Aggregate Queries",
    "abstract": "Optimising queries with many joins is known to be a hard problem. The explosion of intermediate results as opposed to a much smaller final result poses a serious challenge to modern database man-final result poses a serious challenge to modern database management systems (DBMSs). This is particularly glaring in case of analytical queries that join many tables but ultimately only output comparatively small aggregate information. Analogous problems are faced by graph database systems when processing analytical queries with aggregates on top of complex path queries. \nIn this work, we propose novel optimisation techniques, both on the logical, and physical level, that allow us to avoid the material-the logical, and physical level, that allow us to avoid the materialisation of join results for certain types of aggregate queries. The key to these optimisations is the notion of  guardedness , by which we impose restrictions on the occurrence of attributes in  GROUP BY clauses and in aggregate expressions. The efficacy of our optimisa-clauses and in aggregate expressions. The efficacy of our optimisations is validated through their implementation in Spark SQL and extensive empirical evaluation on various standard benchmarks.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Avoiding%20Materialisation%20for%20Guarded%20Aggregate%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1398-selzer.pdf",
    "session": "Research 9: Query Processing and Optimization I",
    "authors": [
      {
        "Name": "Matthias Lanzinger",
        "Affiliation": "TU Wien"
      },
      {
        "Name": "Reinhard Pichler",
        "Affiliation": "TU Wien"
      },
      {
        "Name": "Alexander Selzer",
        "Affiliation": "TU Wien"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "0c086179-f665-491d-b97b-dfb21ba2828a",
    "title": "APEX-DAG: Library and Language independent Pipeline EXtraction",
    "abstract": "Modern data-driven systems often rely on complex pipelines to process and transform data for downstream machine learning (ML) tasks. Extracting these pipelines and understanding their structure is critical for ensuring transparency, performance optimization, and maintainability, especially in large-scale projects. In this work, we introduce a novel system,  APEX-DAG  ( A utomating  P ipeline EX traction with  D ataflow, Static Code  A nalysis, and  G raph Attention Networks), which automates the extraction of data pipelines from computational notebooks or scripts. Unlike execution-based methods,  APEX-DAG  leverages static code analysis to identify the dataflow, transformations, and dependencies within ML workflows without executing the code or the need to alter the code. Further, after an initial training phase, our system can identify pipelines that built with previously unseen libraries.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/APEX-DAG%3A%20Library%20and%20Language%20independent%20Pipeline%20EXtraction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5375-eggers.pdf",
    "session": "Demo B2:",
    "authors": [
      {
        "Name": "Sebastian Eggers",
        "Affiliation": "BIFOLD/TU Berlin"
      },
      {
        "Name": "Nina ≈ªukowska",
        "Affiliation": "BIFOLD/TU Berlin"
      },
      {
        "Name": "Ziawasch Abedjan",
        "Affiliation": "BIFOLD/TU Berlin"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "7f167bc8-7088-487b-bc6f-214d61ca0b5f",
    "title": "A Demonstration of QueryArtisan: Real-Time Data Lake Analysis via Dynamically Generated Data Manipulation Code",
    "abstract": "Querying and analyzing data in data lakes requires substantial manual intervention, including numerous data preprocessing steps, and often demands complex domain expertise. However, the advent of Large Language Models (LLMs) has introduced a promising solution to these challenges by providing a unified framework for interpreting the heterogeneous datasets within data lakes. In this paper, we demonstrate QueryArtisan, a novel LLM-powered analytical system tailored for data lakes. It enables users to issue complex queries in natural language without the need for domain-specific expertise. The system automatically executes user-submitted queries and performs data processing and analysis based on the query results. QueryArtisan extends beyond traditional ETL (Extract, Transform, Load) processes by generating just-in-time code customized for dataset-specific tasks. A suite of heterogeneous operators is developed to process data across various modalities. In addition, a cost-based query optimization mechanism is integrated to improve the efficiency of the generated code. Furthermore, QueryArtisan can dynamically instantiate multiple agents in response to user-defined analytical requirements to perform further in-depth analysis of the retrieved data.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Demonstration%20of%20QueryArtisan%3A%20%20Real-Time%20Data%20Lake%20Analysis%20via%20Dynamically%20Generated%20Data%20Manipulation%20Code",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5263-tang.pdf",
    "session": "Demo A2:",
    "authors": [
      {
        "Name": "Wenhao Liu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Xiu Tang",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Sai Wu",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Chang Yao",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Gongsheng Yuan",
        "Affiliation": "Zhejiang University"
      },
      {
        "Name": "Gang Chen",
        "Affiliation": "Zhejiang University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "009e3bcb-7df8-4519-b760-5bc8e26d838f",
    "title": "Improving DBMS Scheduling Decisions with Accurate Performance Prediction on Concurrent Queries",
    "abstract": "Query scheduling is a critical task that directly impacts query performance in database management systems (DBMS). Deeply integrated schedulers, which require changes to DBMS internals, are usually customized for a specific engine and can take months to implement. In contrast, non-intrusive schedulers make coarse-grained decisions, such as controlling query admission and re-ordering query execution, without requiring modifications to DBMS internals. They require much less engineering effort and can be applied across a wide range of DBMS engines, offering immediate benefits to end users. However, most existing non-intrusive scheduling systems rely on simplified cost models and heuristics that cannot accurately model query interactions under concurrency and different system states, possibly leading to suboptimal scheduling decisions. \nThis work introduces  IconqSched , a new, principled non-intrusive scheduler that optimizes the execution order and timing of queries to enhance total end-to-end runtime as experienced by the user ‚Äî query queuing time plus system runtime. Unlike previous approaches,  IconqSched  features a novel predictor,  Iconq , which treats the DBMS as a black box and accurately estimates the system runtime of concurrently executed queries under different system states. Using these predictions,  IconqSched  is able to capture system runtime variations across different query mixes and system loads. It then employs a greedy scheduling algorithm to effectively determine which queries to submit and when to submit them. We compare  IconqSched  to other schedulers in terms of end-to-end runtime using realistic workload traces. On Postgres, IconqSched reduces end-to-end runtime by up to 16 . 5% on average and 33 . 6% in the tail. Similarly, on Redshift, it reduces end-to-end runtime by up to 14 . 4% on average and 22 . 9% in the tail.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Improving%20DBMS%20Scheduling%20Decisions%20with%20Accurate%20Performance%20Prediction%20on%20Concurrent%20Queries",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4185-wu.pdf",
    "session": "Research 29: Learned Database Systems",
    "authors": [
      {
        "Name": "Ziniu Wu",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Markos Markakis",
        "Affiliation": "MIT"
      },
      {
        "Name": "Chunwei Liu",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Peter Chen",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Balakrishnan Narayanaswamy",
        "Affiliation": "Amazon Web Services"
      },
      {
        "Name": "Tim Kraska",
        "Affiliation": "Massachusetts Institute of Technology"
      },
      {
        "Name": "Samuel Madden",
        "Affiliation": "Massachusetts Institute of Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "e17ebc3b-2ca7-4c7d-b44f-26e761c63e66",
    "title": "Calibrating Noise for Group Privacy in Subsampled Mechanisms",
    "abstract": "Given a group size  ùëö and a sensitive dataset ùê∑, group privacy (GP) releases information about ùê∑ (e.g., weights of a neural network trained on ùê∑) with the guarantee that the adversary cannot infer with high confidence whether the underlying data is ùê∑ or a neigh-with high confidence whether the underlying data is ùê∑ or a neighboring dataset ùê∑‚Ä≤ that differs from ùê∑ by ùëö records. GP generalizes the well-established notion of differential privacy (DP) for protect-the well-established notion of differential privacy (DP) for protecting individuals‚Äô privacy; in particular, when  ùëö =  1 , GP reduces to DP. Compared to DP, GP is capable of protecting the sensitive aggregate  information of a group of up to  ùëö individuals, e.g., the average annual income among members of a yacht club. Despite its longstanding presence in the research literature and its promising applications, GP is often treated as an afterthought, with most ap-applications, GP is often treated as an afterthought, with most approaches first developing a differential privacy (DP) mechanism and then using a generic conversion to adapt it for GP, treating the DP solution as a black box. As we point out in the paper, this method-solution as a black box. As we point out in the paper, this methodology is suboptimal when the underlying DP solution involves subsampling, e.g., in the classic DP-SGD method for training deep learning models. In this case, the DP-to-GP conversion is overly pessimistic in its analysis, leading to high error and low utility in the published results under GP. \nMotivated by this, we propose a novel analysis framework that provides tight privacy accounting for subsampled GP mechanisms. Instead of converting a black-box DP mechanism to GP, our solution carefully analyzes and utilizes the inherent randomness in subsam-carefully analyzes and utilizes the inherent randomness in subsampled mechanisms, leading to a substantially improved bound on the privacy loss with respect to GP. The proposed solution applies to a wide variety of foundational mechanisms with subsampling. Exten-wide variety of foundational mechanisms with subsampling. Extensive experiments with real datasets demonstrate that compared to the baseline convert-from-blackbox-DP approach, our GP mecha-the baseline convert-from-blackbox-DP approach, our GP mechanisms achieve noise reductions of over an order of magnitude in several practical settings, including deep neural network training.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Calibrating%20Noise%20for%20Group%20Privacy%20in%20Subsampled%20Mechanisms",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p322-jiang.pdf",
    "session": "Research 18: Data Privacy and Security I",
    "authors": [
      {
        "Name": "Yangfan Jiang",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Xinjian Luo",
        "Affiliation": "National University of Singapore"
      },
      {
        "Name": "Yin Yang",
        "Affiliation": "Hamad bin Khalifa University"
      },
      {
        "Name": "Xiaokui Xiao",
        "Affiliation": "National University of Singapore"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b5d55baf-4d6f-45df-8819-8a0cc48b3ead",
    "title": "ScaleCache: Scalable and Production-grade Buffer Management for Disk-based Database Systems",
    "abstract": "Bu!er management is critical for DBMSs but often suffers from scalability bottlenecks and poor cache locality, which stems from centralized reference counting in page access and intensive locking in page-to-buffer translation. However, prior radical approaches like pointer swizzling or optimistic lock can hardly be adopted in production-grade DBMSs due to its inherent complexity and incompatibility. \nThis paper proposes  ScaleCache , a scalable, highly-efficient and production-grade buffer management system with three key designs. ScaleCache  #rst incorporates a novel compact per-group bu!er reference counting technique, which enables scalable buffer pinning and unpinning by concurrent threads on many-core servers. It then devised a novel read-write lock based on copy-on-write and per-group reference counting, which is suitable for B-link tree. At last, we propose an optimistic, CPU-cache friendly and SIMDaccelerated hash table for fast and scalable page-to-bffer translation, which eliminates most contention on modern many-core hardware. ScaleCache has been adopted in  Huawei GaussDB , a commercial high-performance DBMS. Evaluation on a 128-core server demonstrates that  ScaleCache  exhibits near-linear scalability and can signi#cantly improve index query throughput of both classic B-link tree index and complex graph-based vector index.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/ScaleCache%3A%20Scalable%20and%20Production-grade%20Buffer%20Management%20for%20Disk-based%20Database%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5073-liu.pdf",
    "session": "Industry 6: Database Engines",
    "authors": [
      {
        "Name": "Mingyu Liu",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Junbin Kang",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Kai Wang",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Lu Zhang",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Haibo Chen",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Xiuchang Li",
        "Affiliation": "Huawei"
      },
      {
        "Name": "Tianhong Ding",
        "Affiliation": "Huawei"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a8a2372b-8cb8-4648-9004-ffc8293c35ea",
    "title": "Towards Scalable and Practical Batch-Dynamic Connectivity",
    "abstract": "We study the problem of dynamically maintaining the connected components of an undirected graph subject to edge insertions and deletions. We give the first parallel algorithm for the problem that is work-efficient, supports batches of updates, runs in polylogarithmic depth, and uses only linear total space. The existing algorithms for the problem either use super-linear space, do not come with strong theoretical bounds, or are not parallel. \nOn the empirical side, we provide the first implementation of the cluster forest algorithm , the first linear-space and polylogarithmic update time algorithm for dynamic connectivity. Experimentally, we find that our algorithm uses up to  19 . 7 √ó  less space and is up to  6 . 2 √ó  faster than the level-set algorithm of Holm, de Lichten-to  6 . 2 √ó  faster than the level-set algorithm of Holm, de Lichtenberg, and Thorup, arguably the most widely-implemented dynamic connectivity algorithm with strong theoretical guarantees.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Scalable%20and%20Practical%20Batch-Dynamic%20Connectivity",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p889-dhulipala.pdf",
    "session": "Research 27: Graph Data Management III",
    "authors": [
      {
        "Name": "Quinten De Man",
        "Affiliation": "University of Maryland"
      },
      {
        "Name": "Laxman Dhulipala",
        "Affiliation": "University of Maryland, College Park"
      },
      {
        "Name": "Adam Karczmarz",
        "Affiliation": "University of Warsaw and IDEAS NCBR"
      },
      {
        "Name": "Jakub ≈ÅƒÖcki",
        "Affiliation": "Google"
      },
      {
        "Name": "Julian Shun",
        "Affiliation": "MIT"
      },
      {
        "Name": "Zhongqi Wang",
        "Affiliation": "University of maryland"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "a44ed6d6-8234-4e9f-a44d-b87ae3b38b41",
    "title": "Beyond Compression: A Comprehensive Evaluation of Lossless Floating-Point Compression",
    "abstract": "Modern data-intensive applications generate vast amounts of floating-point data, essential for fields like databases and machine learning. While many compression techniques focus on space efficiency, there is a lack of benchmarks evaluating both compression and query performance, especially in areas like in-situ query execution on compressed data and machine learning tasks such as distance measurement and k-nearest neighbors (k-NN) in RetrievalAugmented Generation (RAG) systems. This paper addresses this gap by evaluating popular lossless floating-point compression methods on three key factors: compression efficiency, database operations performance, and machine learning query performance. We implemented these techniques in Rust and integrated them into an open-source library for use with columnar engines. Our comparison highlights trade-offs between compression efficiency and query performance, showing that no single approach excels in all areas, and some methods trade off compression for slower performance.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Beyond%20Compression%3A%20A%20Comprehensive%20Evaluation%20of%20Lossless%20Floating-Point%20Compression",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p4396-hishida.pdf",
    "session": "Research 31: Vector Data Management II",
    "authors": [
      {
        "Name": "Kaisei Hishida",
        "Affiliation": "Keio University"
      },
      {
        "Name": "Chunwei Liu",
        "Affiliation": "MIT CSAIL"
      },
      {
        "Name": "John Paparrizos",
        "Affiliation": "The Ohio State University"
      },
      {
        "Name": "Aaron Elmore",
        "Affiliation": "University of Chicago"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "52bbd76b-9cc9-40c3-90b8-f434b30cf0e7",
    "title": "Data Imputation with Limited Data Redundancy Using Data Lakes",
    "abstract": "Data imputation is essential for many data science applications. Existing methods rely heavily on sufficient data redundancy from within-table values. However, many real-world datasets often lack such data redundancy, necessitating external data sources. In this paper, we introduce a retrieval-augmented imputation framework, LakeFill , which combines large language models (LLMs) and data lakes to address this challenge. Unlike existing ‚Äútable-level‚Äù retrieval methods designed for question answering, which retrieve data in the granularity of tables,  LakeFill  performs fine-grained ‚Äútuple-level‚Äù retrieval, optimized specifically for data imputation at the tuple level. It encodes (possibly incomplete) tuples to capture nuanced similarities and differences, enabling effective identification of candidate tuples. A novel reranking method that integrates checklist-based training data annotation with stratified training group construction further refines the retrieved tuples. Finally, a reasoner with a novel two-stage confidence-aware imputation ensures reliable imputation results. Extensive experiments show that LakeFill  significantly outperforms state-of-the-art methods for data imputation when there is limited data redundancy.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Data%20Imputation%20with%20Limited%20Data%20Redundancy%20Using%20Data%20Lakes",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3354-tang.pdf",
    "session": "Research 36: Data Cleaning and Preparation II",
    "authors": [
      {
        "Name": "Chenyu Yang",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Yuyu Luo",
        "Affiliation": "HKUST"
      },
      {
        "Name": "Chuanxuan Cui",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Ju Fan",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Chengliang Chai",
        "Affiliation": "BIT"
      },
      {
        "Name": "Nan Tang",
        "Affiliation": "HKUST"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "5f36f74d-17a1-4a5e-a781-e125bbd16d93",
    "title": "R-Bot: An LLM-based Query Rewrite System",
    "abstract": "Query rewrite is essential for optimizing SQL queries to improve their execution efficiency without changing their results. Traditionally, this task has been tackled through heuristic and learning-based methods, each with its limitations in terms of inferior quality and low robustness. Recent advancements in  LLMs  offer a new paradigm by leveraging their superior natural language and code comprehension abilities. Despite their potential, directly applying  LLMs  like GPT-4 has faced challenges due to problems such as hallucinations, where the model might generate inaccurate or irrelevant results. To address this, we propose  R-Bot , an  LLM -based query rewrite system with a systematic approach. We first design a multi-source rewrite evidence preparation pipeline to generate query rewrite evidences for guiding  LLMs  to avoid hallucinations. We then propose a hybrid structure-semantics retrieval method that combines structural and semantic analysis to retrieve the most relevant rewrite evidences for effectively answering an online query. We next propose a stepby-step  LLM  rewrite method that iteratively leverages the retrieved evidences to select and arrange rewrite rules with self-reflection. We conduct comprehensive experiments on real-world datasets and widely used benchmarks, and demonstrate the superior performance of our system,  R-Bot , surpassing state-of-the-art query rewrite methods. The  R-Bot  system has been deployed at Huawei and with real customers, and the results show that the proposed R-Bot  system achieves lower query latency.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/R-Bot%3A%20An%20LLM-based%20Query%20Rewrite%20System",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5031-li.pdf",
    "session": "Industry 4: Machine Learning, AI, and Databases",
    "authors": [
      {
        "Name": "Zhaoyan Sun",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xuanhe Zhou",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Guoliang Li",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Xiang Yu",
        "Affiliation": "Huawei Company"
      },
      {
        "Name": "Jianhua Feng",
        "Affiliation": "Tsinghua University"
      },
      {
        "Name": "Yong Zhang",
        "Affiliation": "Tsinghua University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "4619ede4-ff7c-472e-a64f-e63fe942ed10",
    "title": "Seer: Accelerating Blockchain Transaction Execution by Fine-Grained Branch Prediction",
    "abstract": "Increasingly popular  decentralized applications  (dApps) with com-Increasingly popular  decentralized applications  (dApps) with complex application logic incur significant overhead for executing smart contract transactions, which greatly limits public blockchain perfor-contract transactions, which greatly limits public blockchain performance. Pre-executing transactions off the critical path can mitigate substantial I/O and computation costs during execution. However, pre-execution does not yield any state transitions, rendering the sys-pre-execution does not yield any state transitions, rendering the system state inconsistent with actual execution. This inconsistency can lead to deviations in pre-execution paths when processing smart contracts with multiple state-related branches, thus diminishing pre-execution effectiveness. In this paper, we develop Seer, a novel public blockchain execution engine that incorporates fine-grained branch prediction to fully exploit pre-execution effectiveness. Seer predicts state-related branches using a two-level prediction ap-predicts state-related branches using a two-level prediction approach, reducing inconsistent execution paths more efficiently than executing all possible branches. To enable effective reuse of pre-executing all possible branches. To enable effective reuse of preexecution results, Seer employs checkpoint-based fast-path execu-execution results, Seer employs checkpoint-based fast-path execution, enhancing transaction execution for both successful and unsuc-tion, enhancing transaction execution for both successful and unsuccessful predictions. Evaluations with realistic blockchain workloads demonstrate that Seer delivers an average of 27.7 √ó transaction-level speedup and an overall 20.6 √ó speedup in the execution phase over vanilla Ethereum, outperforming existing blockchain execution acceleration solutions.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Seer%3A%20Accelerating%20Blockchain%20Transaction%20Execution%20by%20Fine-Grained%20Branch%20Prediction",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p822-xiao.pdf",
    "session": "Research 50: Access Control and Privacy, Blockchain",
    "authors": [
      {
        "Name": "Shijie Zhang",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Ru Cheng",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "xinpeng liu",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Jiang Xiao",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Hai Jin",
        "Affiliation": "Huazhong University of Science and Technology"
      },
      {
        "Name": "Bo Li",
        "Affiliation": "Hong Kong University of Science and Technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "b5679ca0-3aa7-4461-9097-2be70209a600",
    "title": "HyperBlocker: Accelerating Rule-based Blocking in Entity Resolution using GPUs",
    "abstract": "This paper studies rule-based blocking in Entity Resolution (ER). We propose HyperBlocker, a GPU-accelerated system for blocking in ER. As opposed to previous blocking algorithms and parallel blocking solvers, HyperBlocker employs a pipelined architecture to overlap data transfer and GPU operations. It generates a data-to overlap data transfer and GPU operations. It generates a dataaware and rule-aware execution plan on CPUs, for specifying how rules are evaluated, and develops a number of hardware-aware optimizations to achieve massive parallelism on GPUs. \nUsing real-life datasets, we show that HyperBlocker is at least 6.8√ó and 9.1√ó faster than prior CPU-powered distributed systems and GPU-based ER solvers, respectively. Better still, by combining HyperBlocker with the state-of-the-art ER matcher, we can speed up the overall ER process by at least 30% with comparable accuracy.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/HyperBlocker%3A%20Accelerating%20Rule-based%20Blocking%20in%20Entity%20Resolution%20using%20GPUs",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p308-xie.pdf",
    "session": "Research 51: Information Integration and Data Quality III",
    "authors": [
      {
        "Name": "Xiaoke Zhu",
        "Affiliation": "Beihang University"
      },
      {
        "Name": "Min Xie",
        "Affiliation": "Shenzhen Institute of Computing Sciences"
      },
      {
        "Name": "Ting Deng",
        "Affiliation": "\" Beihang University, China\""
      },
      {
        "Name": "Qi Zhang",
        "Affiliation": "Meta Platforms"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "acad6228-97c1-4eae-ac74-cb8910973b5d",
    "title": "A Hybrid Approach to Integrating Deterministic and Non-deterministic Concurrency Control in Database Systems",
    "abstract": "Deterministic and non-deterministic concurrency control algorith- ms have shown respective advantages under diverse workloads. Thus, a natural idea is to blend them together. However, because deterministic algorithms work with stringent assumptions, e.g., bat- ched execution and non-interactive transactions, they hardly work together with non-deterministic algorithms. To address this issue, we propose HDCC, a hybrid approach that adaptively employs Calvin and OCC, which have distinct concurrency control and log- ging schemes, in the same database system. To ensure serializabil- ity and recovery correctness, we introduce lock-sharing, global val- idation, and two-log-interleaving mechanisms. Additionally, we in- troduce a rule-based assignment mechanism to dynamically select Calvin or OCC based on workload characteristics. Experimental re- sults using TPC-C and YCSB benchmarks demonstrate that HDCC surpasses existing hybrid approaches by up to 3.1√ó.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/A%20Hybrid%20Approach%20to%20Integrating%20Deterministic%20and%20Non-deterministic%20Concurrency%20Control%20in%20Database%20Systems",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1376-lu.pdf",
    "session": "Research 52: Transaction Management",
    "authors": [
      {
        "Name": "Yinhao Hong",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Hongyao Zhao",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "WEI LU",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Xiaoyong Du",
        "Affiliation": "Renmin University of China"
      },
      {
        "Name": "Yuxing Chen",
        "Affiliation": "Tencent"
      },
      {
        "Name": "Anqun Pan",
        "Affiliation": "Tencent Inc., China"
      },
      {
        "Name": "zheng lixiong",
        "Affiliation": "Tencent"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "eea57257-90fd-4506-9601-c2022eb6ae15",
    "title": "FSMDTW: A Fast Index-free Subsequence Matching Algorithm for Dynamic Time Warping",
    "abstract": "The subsequence matching problem utilizing dynamic time warping as the similarity measurement has been recognized as a key operation in time series analysis for more than two decades. Existing index-free algorithms depend on DTW lower bounds to discard the unpromising candidate. However, these approaches typically cost  ùëÇ ( ùëö )  time for each candidate, where  ùëö is the length of the query. Consequently, the overhead of computing the DTW lower bounds occupies a significant portion of the time in subsequence matching tasks. This paper proposes new algorithms capable of computing the DTW lower bounds in average  ùëÇ ( log ùëö )  time for each candidate, substantially alleviating this bottleneck of the subsequence matching problem. In addition, this paper designs novel DTW lower bounds according to the characteristics of the subsequence matching problem, which is more effective without introducing significant computational overhead. Based on the above improvements, an efficient subsequence matching algorithm called FSMDTW is designed. Experiments conducted on both real and synthetic datasets show that the proposed algorithm is about 2.6 times faster than SOTA on short and medium-length queries and up to one order of magnitude faster on longer queries.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/FSMDTW%3A%20A%20Fast%20Index-free%20Subsequence%20Matching%20Algorithm%20for%20Dynamic%20Time%20Warping",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p3628-wang.pdf",
    "session": "Research 12: Time Series Data I",
    "authors": [
      {
        "Name": "Zemin Chao",
        "Affiliation": "Harbin institute of technology"
      },
      {
        "Name": "Qiaoyi Zheng",
        "Affiliation": "Harbin Institute of Technology"
      },
      {
        "Name": "Zhixin Qi",
        "Affiliation": "Harbin institute of technology"
      },
      {
        "Name": "Hongzhi Wang",
        "Affiliation": "Harbin institute of technology"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "65ee7488-a6c0-4530-bca7-e01162e9a0b3",
    "title": "Can Surrogate Keys Negatively Impact Data Quality?",
    "abstract": "Surrogate keys are now extensively utilized by database designers to implement keys in SQL tables. They are straightforward, easy to understand, enable efficient access, and are often considered a sufficient guarantee of data integrity despite lacking any real-world semantic meaning. In spite of all their benefits, one might wonder whether surrogate keys can negatively impact data quality. IT developers who rely exclusively on surrogate keys when designing database schemas may be tempted to not encode natural keys, as they are perceived as complex to manage at the application level. In such settings, surrogate keys allow the presence of so-called  artificial unicity , a complex form of redundancy that can be propagated through foreign keys, and other underlying data-quality issues. In the presence of artificial unicity, most data cleaning techniques, especially unsupervised, are likely to fail, making data preparation and analytics very challenging. \nFor relational databases implemented with surrogate keys but no natural keys, we developed RED2Hunt (RElational Databases REDundancy Hunting), a human-in-the-loop framework for identifying hidden redundancy and, if problems occur, clean the database. The framework was implemented on top of PostgreSQL within an eponym web-based platform to guide the expert through its application. In this paper, we present a demonstration of the RED2Hunt tool through three interactive scenarios on a polluted instance of the publicly available  Perfect Pet  database. During the demonstration, the visitor can take on one of two roles in the Perfect Pet database: a domain expert or a data scientist. As a domain expert, she will interact with RED2Hunt, for example to elicit natural keys, from simple yet very intuitive visualizations of tables‚Äô attributes. As a data scientist, she will explore two simple scenarios‚Äîexecuting SQL queries or applying learning models‚Äîon both the initial and cleaned databases to grasp the benefits of the approach.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Can%20Surrogate%20Keys%20Negatively%20Impact%20Data%20Quality%EF%BC%9F",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p5279-marcy.pdf",
    "session": "Demo B1:",
    "authors": [
      {
        "Name": "Mathilde MARCY",
        "Affiliation": "LIRIS"
      },
      {
        "Name": "Jean-Marc PETIT",
        "Affiliation": "LIRIS"
      },
      {
        "Name": "Vasile-Marian SCUTURICI",
        "Affiliation": "LIRIS"
      },
      {
        "Name": "Jocelyn BONJOUR",
        "Affiliation": "CETHIL"
      },
      {
        "Name": "Camille FERTEL",
        "Affiliation": "CEMAFROID"
      },
      {
        "Name": "Gerald CAVALIER",
        "Affiliation": "CEMAFROID"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "014976ed-b18e-4ffa-8d07-88bead88c50d",
    "title": "UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors Search",
    "abstract": "ABSTRACTThis paper presents an efficient and scalable framework for Range Filtered Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors associated with attribute values. Given a query vector  ùëû and a range  [ ùëô,‚Ñé ] , RF-ANNS aims to find the approximate  ùëò nearest neighbors of  ùëû among data whose attribute values fall within  [ ùëô,‚Ñé ] . Existing methods including pre-, post-, and hybrid filtering strategies that perform attribute range filtering before, after, or during the ANNS process, all suffer from significant performance degradation when query ranges shift. Though building dedicated indexes for each strategy and selecting the best one based on the query range can address this problem, it leads to index consistency and maintenance issues. \nOur framework, called UNIFY, constructs a unified Proximity Graph-based (PG-based) index that seamlessly supports all three strategies. In UNIFY, we introduce SIG, a novel   S egmented   I nclusive G raph, which segments the dataset by attribute values. It ensures the PG of objects from any segment combinations is a sub-graph of SIG, thereby enabling efficient hybrid filtering by reconstructing and searching a PG from relevant segments. Moreover, we present H ierarchical   S egmented   I nclusive   G raph (HSIG), a variant of SIG which incorporates a hierarchical structure inspired by HNSW to achieve logarithmic hybrid filtering complexity. We also implement pre- and post-filtering for HSIG by fusing skip list connections and compressed HNSW edges into the hierarchical graph. Experimental results show that UNIFY delivers state-of-the-art RF-ANNS performance across small, mid, and large query ranges.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/UNIFY%3A%20Unified%20Index%20for%20Range%20Filtered%20Approximate%20Nearest%20Neighbors%20Search",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p1118-yao.pdf",
    "session": "Research 3: Vector Data Management I",
    "authors": [
      {
        "Name": "Anqi Liang",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Pengcheng Zhang",
        "Affiliation": "Tencent Inc."
      },
      {
        "Name": "Bin Yao",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Zhongpu Chen",
        "Affiliation": "Southwestern University of Finance and Economics"
      },
      {
        "Name": "Yitong Song",
        "Affiliation": "Shanghai Jiao Tong University"
      },
      {
        "Name": "Guangxu Cheng",
        "Affiliation": "Tencent Inc."
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  },
  {
    "id": "f608e27a-7f01-41e4-ad55-fef82faa7d0d",
    "title": "Towards Sufficient GPU-accelerated Dynamic Graph Management: Survey and Experiment",
    "abstract": "Dynamic graph management (DGM) systems are designed to effec-Dynamic graph management (DGM) systems are designed to effectively handle changing graph data, which is a fundamental prob-tively handle changing graph data, which is a fundamental problem for many graph-based applications. Recently, researchers have designed GPU-based solutions for DGM and its downstream ap-designed GPU-based solutions for DGM and its downstream applications, thanks to GPUs‚Äô massive parallelism power. However, there is a lack of universal models that summarize the features and design principles of GPU-accelerated DGM systems. Addition-and design principles of GPU-accelerated DGM systems. Additionally, existing studies test GPU-based DGM systems without unified metrics and workloads. Under this circumstance, we propose a con-metrics and workloads. Under this circumstance, we propose a conceptual model for GPU-accelerated DGM to demonstrate a DGM system‚Äôs components, key primitives, and optimization choices. Next, we evaluate six representative systems, testing their update and query performance with unified metrics and workloads of dif-and query performance with unified metrics and workloads of different algorithmic behaviors. We also extend existing systems to seek insight to fill the current research gap in multi-GPU support, concurrency control, resource utilization, and so on. Our evaluation yielded new insights on the pros and cons of different systems: (1) Hashing-based systems perform best for graph updates but may not be suitable for all applications. (2) Finding a system that fits all workloads is challenging, and hybrid data storage may be a solution. (3) To select the most suitable DGM system for a specific workload, it is essential to consider hardware-related metrics. Finally, we pro-it is essential to consider hardware-related metrics. Finally, we provide recommendations and suggestions for future studies based on our experimental results and observations.",
    "conference_link": "https://vldb.org/pvldb/volumes/18/paper/Towards%20Sufficient%20GPU-accelerated%20Dynamic%20Graph%20Management%3A%20Survey%20and%20Experiment",
    "pdf_link": "https://www.vldb.org/pvldb/vol18/p599-lin.pdf",
    "session": "Research 6: Graph Data Management I",
    "authors": [
      {
        "Name": "Yinnian Lin",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Lei Zou",
        "Affiliation": "Peking University"
      },
      {
        "Name": "Xunbin Su",
        "Affiliation": "Peking University"
      }
    ],
    "date": "2025-09-01",
    "conference": "VLDB",
    "flags": []
  }
]